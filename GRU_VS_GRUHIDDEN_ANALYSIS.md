# GRU vs GRUHidden æ€§èƒ½å·®å¼‚åˆ†æ

## æ‰§è¡Œç»“æœå¯¹æ¯”

| æŒ‡æ ‡ | 39_GRU (æˆåŠŸ) | 40_GRUHidden (å¤±è´¥) | å·®å¼‚ |
|------|---------------|---------------------|------|
| **æœ€ç»ˆScore** | **1.37x** | **0.20x** | **6.9xå·®è·** |
| æ‰§è¡Œç­–ç•¥ | å•seed + ç®—æ³•åˆ†æ | 2 seeds + ç®—æ³•åˆ†æ |
| Tokenæ¶ˆè€— | 79,575 | 61,418 | -23% |
| æŒä¹…åŒ–kernelæ£€æµ‹ | âŒ æœªè§¦å‘ (æ‰§è¡Œäº†3-stage) | âœ… è§¦å‘ (è·³è¿‡3-stage) |
| æœ€ä½³kernelæ¥æº | ç®—æ³•åˆ†æä¼˜åŒ– (1.37x) | ç®—æ³•åˆ†æä¼˜åŒ– (0.20x) |

---

## æ ¹æœ¬å·®å¼‚ï¼šä»»åŠ¡å®šä¹‰ä¸åŒ

### 39_GRU.py (line 27)
```python
def forward(self, x, h0):
    output, h_n = self.gru(x, h0)
    return output  # âœ… è¿”å›å®Œæ•´çš„output [T, B, H]
```

### 40_GRUHidden.py (line 27)
```python
def forward(self, x, h0):
    output, h_n = self.gru(x, h0)
    return h_n  # âŒ åªè¿”å›æœ€åçš„hidden state [num_layers, B, H]
```

**å…³é”®å½±å“**ï¼š
- **39_GRU**ï¼šéœ€è¦ä¿å­˜æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡º â†’ persistent kernelåˆç†ä¸”é«˜æ•ˆ
- **40_GRUHidden**ï¼šåªéœ€è¦æœ€åçš„hidden state â†’ persistent kernelå¯èƒ½è¿‡åº¦è®¡ç®—

---

## ç®—æ³•åˆ†æå¯¹æ¯”

### 39_GRU çš„åˆ†æ (æ—§ç‰ˆpromptï¼ŒæˆåŠŸ)

**Bottleneck**:
> Excessive per-timestep kernel launches for tiny recurrent GEMMs and separate elementwise GRU ops cause launch overhead and poor reuse of W_hh/h state across time

**Optimization**:
> Algorithm replacement with a fused persistent GRU kernel: implement a single Triton kernel per layer that loops over time, computes h_t @ W_hh^T, adds gates_x, applies sigmoid/tanh, and updates h_t entirely inside the kernel

**Expected speedup**: 2-4x

**å®é™…ç»“æœ**: 0.12x â†’ **1.37x** (11.4xæå‡ï¼Œè¶…å‡ºé¢„æœŸ)

---

### 40_GRUHidden çš„åˆ†æ (æ–°ç‰ˆprompt)

**Seed 2 åˆ†æ** (æœ€ä½³å€™é€‰):

**Worth optimizing**: yes

**Reason**:
> The Triton implementation launches thousands of small kernels per forward pass, making it heavily launch-bound and far slower than the cuDNN-backed PyTorch GRU.

**Bottleneck**:
> For each layer and each of the 512 time steps, the code launches two Triton kernels (one matmul for h_gates and one GRU cell), leading to ~6000 kernel launches per forward. This per-timestep, per-layer launch pattern dominates runtime

**Optimization**:
> Kernel launch reduction via a fused, persistent GRU layer kernel: move the time-loop inside a single Triton kernel per layer

**Expected speedup**: 5-10x

**å®é™…ç»“æœ**: 0.11x â†’ **0.20x** (1.8xæå‡ï¼Œè¿œä½äºé¢„æœŸï¼Œä»æ…¢äºPyTorch)

---

## ä»£ç è´¨é‡å¯¹æ¯”

### 39_GRU ç”Ÿæˆçš„kernel (æˆåŠŸ)

**ç‰¹ç‚¹**:
1. **å®Œæ•´çš„persistent kernelå®ç°** (line 65-150+)
2. **åœ¨kernelå†…å¾ªç¯æ‰€æœ‰æ—¶é—´æ­¥** (`for t in range(0, T)`)
3. **å†…å­˜é«˜æ•ˆ**ï¼šæ¯ä¸ªæ—¶é—´æ­¥çš„h_stateåœ¨kernelå†…æ›´æ–°
4. **è¾“å‡ºå®Œæ•´**ï¼šå†™å…¥æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡ºåˆ° `h_out_ptr[T, B, H]`

```python
@triton.jit
def gru_persistent_layer_kernel(
    gates_x_ptr,        # [T, B, 3H]
    w_hh_t_ptr,         # [H, 3H]
    bias_hh_ptr,        # [3H]
    h_state_ptr,        # [B, H]  (updated in-place)
    h_out_ptr,          # [T, B, H]  âœ… è¾“å‡ºæ‰€æœ‰æ—¶é—´æ­¥
    ...
):
    for t in range(0, T):  # âœ… æ—¶é—´å¾ªç¯åœ¨kernelå†…
        # è®¡ç®—gates
        # æ›´æ–°h_state
        # å†™å…¥h_out[t]
```

---

### 40_GRUHidden ç”Ÿæˆçš„kernel (å¤±è´¥)

**ç‰¹ç‚¹**:
1. **å°è¯•å®ç°persistent kernel** (line 83-100+)
2. **ä½†å®ç°æœ‰é—®é¢˜**ï¼šä½¿ç”¨åŒç¼“å†² (`h_state0_ptr`, `h_state1_ptr`)
3. **è¾“å‡ºç­–ç•¥ä¸æ¸…æ™°**ï¼šè™½ç„¶åªéœ€è¦h_nï¼Œä½†ä»ç„¶è¾“å‡ºæ‰€æœ‰æ—¶é—´æ­¥
4. **å¯èƒ½çš„æ€§èƒ½é—®é¢˜**ï¼š

```python
@triton.jit
def gru_layer_forward_kernel(
    x_gates_ptr,      # [T, B, 3H]
    h_state0_ptr,     # [B, H]  buffer 0  â“ ä¸ºä»€ä¹ˆéœ€è¦åŒç¼“å†²ï¼Ÿ
    h_state1_ptr,     # [B, H]  buffer 1
    w_hh_ptr,         # [H, 3H]
    bias_hh_ptr,      # [3H]
    h_out_ptr,        # [T, B, H]  â“ ä¸ºä»€ä¹ˆè¾“å‡ºæ‰€æœ‰æ—¶é—´æ­¥ï¼ˆåªéœ€è¦æœ€åçš„h_nï¼‰
    ...
):
    # å®ç°ç»†èŠ‚å¤æ‚ï¼Œå¯èƒ½æœ‰æ€§èƒ½é—®é¢˜
```

---

## æŒä¹…åŒ–kernelæ£€æµ‹

### 39_GRU
```
[Optimization] Starting 3-stage optimization...
```
- âŒ **æœªæ£€æµ‹åˆ°persistent kernel**
- æ‰§è¡Œäº†3-stageä¼˜åŒ–
- ä½†3-stageéƒ½å¤±è´¥äº†ï¼ˆ0.09x, 0.08x, 0.16xï¼‰
- æœ€ç»ˆä¿ç•™ç®—æ³•åˆ†æçš„1.37x

### 40_GRUHidden
```
[3-Stage] Persistent kernel detected!
[3-Stage] Skipping 3-stage optimization to preserve performance.
```
- âœ… **æ£€æµ‹åˆ°persistent kernel**
- è·³è¿‡äº†3-stageä¼˜åŒ–
- ä¿ç•™ç®—æ³•åˆ†æçš„0.20xï¼ˆæ€§èƒ½å·®ï¼‰

---

## é—®é¢˜è¯Šæ–­

### ä¸ºä»€ä¹ˆ40_GRUHiddenæ€§èƒ½å·®ï¼Ÿ

#### 1. **ä»»åŠ¡ç‰¹æ€§ä¸åŒ¹é…**
- 40_GRUHiddenåªéœ€è¦æœ€åçš„h_n
- ä½†ç”Ÿæˆçš„kernelä»ç„¶è®¡ç®—å¹¶è¾“å‡ºæ‰€æœ‰æ—¶é—´æ­¥çš„output
- **æµªè´¹è®¡ç®—å’Œå†…å­˜å¸¦å®½**

#### 2. **å®ç°å¤æ‚åº¦è¿‡é«˜**
- ä½¿ç”¨åŒç¼“å†²æœºåˆ¶ (`h_state0`, `h_state1`)ï¼Œå¢åŠ å†…å­˜è®¿é—®
- 39_GRUçš„å®ç°æ›´ç®€æ´ï¼Œå•ä¸€`h_state`åŸåœ°æ›´æ–°

#### 3. **ç®—æ³•åˆ†æå¯èƒ½è¯¯åˆ¤**
- LLMå¯èƒ½æ²¡æœ‰è¯†åˆ«å‡º"åªéœ€è¦h_n"è¿™ä¸ªå…³é”®ä¿¡æ¯
- å¥—ç”¨äº†é€šç”¨çš„persistent GRUæ¨¡å¼
- æœªé’ˆå¯¹"åªè¿”å›hidden state"åšä¼˜åŒ–

#### 4. **æ–°promptå¯èƒ½è¿‡äºç²¾ç®€**
- æ–°çš„`optimization_from_analysis` promptå»æ‰äº†PyTorch reference code
- LLMå¯èƒ½æ²¡çœ‹åˆ°`return h_n`è¿™ä¸€è¡Œ
- æ— æ³•ç†è§£ä»»åŠ¡çš„çœŸå®éœ€æ±‚

---

## å¯¹æ¯”ï¼šæ—§ç®—æ³•åˆ†æprompt vs æ–°ä¼˜åŒ–prompt

### æ—§æ–¹å¼ (39_GRUï¼ŒæˆåŠŸ)
```python
# ä½¿ç”¨å®Œæ•´çš„algorithm_analysis prompt
analysis_prompt = build_algorithm_analysis_prompt(
    arch_path=task_path,  # âœ… åŒ…å«PyTorchä»£ç 
    gpu_name=args.gpu,
    cuda_code=seed_candidate.kernel.code,
    ncu_metrics_block=ncu_block,
    current_latency_ms=seed_latency_ms,
    baseline_latency_ms=pytorch_baseline_ms,
)
```

**Algorithm analysis promptåŒ…å«**:
```python
# PyTorch Reference
```python
{python_code}  # âœ… å®Œæ•´çš„PyTorchä»£ç ï¼ŒåŒ…æ‹¬returnè¯­å¥
```

# Current Triton Kernel
...
```

### æ–°æ–¹å¼ (40_GRUHiddenï¼Œå¤±è´¥)
```python
# ä½¿ç”¨ç²¾ç®€çš„optimization_from_analysis prompt
optimization_instruction = build_optimization_from_analysis_prompt(
    bottleneck=analysis_json.get('bottleneck', 'N/A'),
    optimization_method=analysis_json.get('optimisation method', 'N/A'),
    modification_plan=analysis_json.get('modification plan', 'N/A'),
    expected_speedup=analysis_json.get('expected_speedup', 'N/A'),
    current_kernel=seed_candidate.kernel.code,  # âŒ æ²¡æœ‰PyTorchä»£ç 
)
```

**Optimization promptåŒ…å«**:
```
# Analysis Results
Bottleneck: ...
Optimization Strategy: ...
Implementation Plan: ...

# Current Kernel (needs optimization)
...  # âŒ æ²¡æœ‰PyTorch referenceï¼ŒLLMä¸çŸ¥é“åªéœ€è¦h_n
```

---

## æ ¹æœ¬åŸå› 

### ğŸ”´ **æ ¸å¿ƒé—®é¢˜ï¼šæ–°promptç¼ºå°‘PyTorch reference**

1. **Algorithm analysisé˜¶æ®µ**ï¼š
   - æœ‰PyTorchä»£ç  â†’ LLMèƒ½çœ‹åˆ°`return h_n`
   - ä½†åˆ†æç»“æœä¸­æ²¡æœ‰å¼ºè°ƒè¿™ä¸ªå…³é”®å·®å¼‚

2. **Optimization generationé˜¶æ®µ**ï¼š
   - **æ²¡æœ‰PyTorchä»£ç ** â†’ LLMä¸çŸ¥é“ä»»åŠ¡åªéœ€è¦h_n
   - åªçœ‹åˆ°åˆ†æç»“æœè¯´"fuse into persistent kernel"
   - å¥—ç”¨é€šç”¨GRU persistentæ¨¡æ¿
   - ç”Ÿæˆäº†è¾“å‡ºæ‰€æœ‰æ—¶é—´æ­¥çš„kernelï¼ˆæµªè´¹è®¡ç®—ï¼‰

### å¯¹æ¯”39_GRUçš„æˆåŠŸåŸå› 

39_GRUä½¿ç”¨çš„æ˜¯**æ—§ç‰ˆprompt**ï¼Œç›´æ¥å°†analysisç»“æœ + PyTorchä»£ç  + seed_promptæ‹¼æ¥ï¼š
```python
optimization_instruction = f"""
Based on algorithmic analysis:
...

{seed_prompt}  # âœ… seed_promptåŒ…å«å®Œæ•´PyTorchä»£ç 
"""
```

è™½ç„¶å†—é•¿ï¼Œä½†**ä¿ç•™äº†PyTorch reference**ï¼ŒLLMèƒ½çœ‹åˆ°`return output`ï¼ŒçŸ¥é“éœ€è¦è¾“å‡ºæ‰€æœ‰æ—¶é—´æ­¥ã€‚

---

## è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: åœ¨optimization_from_analysisä¸­æ·»åŠ PyTorchä»£ç 

ä¿®æ”¹ `prompts/optimization_from_analysis.py`:

```python
def build_optimization_from_analysis_prompt(
    *,
    bottleneck: str,
    optimization_method: str,
    modification_plan: str,
    expected_speedup: str,
    current_kernel: str,
    pytorch_reference: str = "",  # âœ… æ–°å¢å‚æ•°
) -> str:
    prompt = optimization_from_analysis_tmpl.substitute(
        bottleneck=bottleneck,
        ...
        current_kernel=current_kernel.strip(),
        pytorch_reference=pytorch_reference.strip(),  # âœ… æ·»åŠ åˆ°æ¨¡æ¿
    )
```

**Templateä¿®æ”¹**:
```python
# PyTorch Reference (what we're trying to optimize)
```python
$pytorch_reference
```

# Current Kernel (needs optimization)
```python
$current_kernel
```
```

---

### æ–¹æ¡ˆ2: åœ¨algorithm_analysisä¸­å¼ºè°ƒoutput requirements

ä¿®æ”¹ `prompts/algorithm_analysis.py`ï¼Œè®©åˆ†æç»“æœæ˜ç¡®æŒ‡å‡ºï¼š

```json
{
  "worth_optimizing": "yes/no",
  "output_requirement": "full_sequence / final_hidden_only",  // âœ… æ–°å¢å­—æ®µ
  "bottleneck": "...",
  ...
}
```

ç„¶ååœ¨optimization promptä¸­ä½¿ç”¨è¿™ä¸ªä¿¡æ¯ã€‚

---

### æ–¹æ¡ˆ3: é’ˆå¯¹ä¸åŒoutputéœ€æ±‚ä½¿ç”¨ä¸åŒä¼˜åŒ–ç­–ç•¥

åœ¨main.pyä¸­æ ¹æ®ä»»åŠ¡ç‰¹æ€§é€‰æ‹©ä¼˜åŒ–ç­–ç•¥ï¼š

```python
# æ£€æµ‹ä»»åŠ¡æ˜¯å¦åªéœ€è¦hidden state
if "return h_n" in task_path.read_text() or "return h" in task_path.read_text():
    # åªéœ€è¦h_nï¼Œå¯èƒ½ä¸é€‚åˆpersistent kernel
    # æˆ–è€…ä½¿ç”¨specialized persistent kernel (ä¸è¾“å‡ºä¸­é—´ç»“æœ)
    optimization_hint = "Task only needs final hidden state, optimize for that"
else:
    # éœ€è¦å®Œæ•´output
    optimization_hint = "Task needs full output sequence"
```

---

## æ¨èæ–¹æ¡ˆ

**ä¼˜å…ˆé€‰æ‹©æ–¹æ¡ˆ1**ï¼šåœ¨`optimization_from_analysis` promptä¸­æ·»åŠ PyTorch reference

**åŸå› **:
1. **æœ€å°æ”¹åŠ¨**ï¼šåªéœ€è¦ä¿®æ”¹prompt builderå’Œè°ƒç”¨å¤„
2. **ä¿ç•™ä¸Šä¸‹æ–‡**ï¼šLLMèƒ½çœ‹åˆ°å®Œæ•´çš„ä»»åŠ¡å®šä¹‰
3. **Tokenå¯æ§**ï¼šPyTorchä»£ç é€šå¸¸åªæœ‰20-50è¡Œï¼Œä¸ä¼šåƒseed_prompté‚£æ ·å†—é•¿
4. **é€šç”¨æ€§å¼º**ï¼šé€‚ç”¨äºæ‰€æœ‰ä¼˜åŒ–åœºæ™¯ï¼Œä¸ä»…é™äºGRU

**é¢„æœŸæ•ˆæœ**:
- 40_GRUHiddenï¼šLLMçœ‹åˆ°`return h_n`ï¼Œç”Ÿæˆåªè®¡ç®—æœ€åh_nçš„ä¼˜åŒ–kernel
- Tokenå¢åŠ ï¼š~500-1000 (PyTorchä»£ç )ï¼Œè¿œå°‘äºseed_promptçš„~8000
- æ€§èƒ½æå‡ï¼šé¢„æœŸä»0.20xæå‡åˆ°1.0x+

---

## æ€»ç»“

| å› ç´  | 39_GRU (æˆåŠŸ) | 40_GRUHidden (å¤±è´¥) |
|------|---------------|---------------------|
| ä»»åŠ¡ç‰¹æ€§ | è¿”å›å®Œæ•´output | åªè¿”å›h_n |
| Promptæ–¹å¼ | æ—§æ–¹å¼ï¼ˆåŒ…å«PyTorchä»£ç ï¼‰ | æ–°æ–¹å¼ï¼ˆç¼ºå°‘PyTorchä»£ç ï¼‰ |
| LLMç†è§£ | âœ… çŸ¥é“éœ€è¦è¾“å‡ºæ‰€æœ‰æ—¶é—´æ­¥ | âŒ ä¸çŸ¥é“åªéœ€è¦æœ€åh_n |
| ç”Ÿæˆçš„kernel | è¾“å‡ºæ‰€æœ‰æ—¶é—´æ­¥ï¼ˆæ­£ç¡®ï¼‰ | è¾“å‡ºæ‰€æœ‰æ—¶é—´æ­¥ï¼ˆæµªè´¹ï¼‰ |
| æœ€ç»ˆæ€§èƒ½ | 1.37xï¼ˆæˆåŠŸï¼‰ | 0.20xï¼ˆå¤±è´¥ï¼‰ |

**æ ¸å¿ƒæ•™è®­**ï¼š
- âœ… TokenèŠ‚çœå¾ˆé‡è¦ï¼Œä½†**ä¸èƒ½ä¸¢å¤±å…³é”®ä¸Šä¸‹æ–‡**
- âœ… PyTorch reference codeè™½ç„¶å tokenï¼Œä½†å¯¹ç†è§£ä»»åŠ¡è‡³å…³é‡è¦
- âœ… éœ€è¦åœ¨tokenæ•ˆç‡å’ŒLLMç†è§£èƒ½åŠ›ä¹‹é—´æ‰¾å¹³è¡¡
- âœ… **å»ºè®®**ï¼šæ¢å¤PyTorch referenceåˆ°optimization promptä¸­
