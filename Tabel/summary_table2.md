# Kernel Benchmark Summary
| 算子 | best_score | 状态 |
| 1 | 1.72 | ✅ |
| 2_Standard_matrix_multiplication | 1.73 | ✅ |
| 3_Batched_matrix_multiplication | 1.44 | ✅ |
| 4_Matrix_vector_multiplication | 1.22 | ✅ |
| 5_Matrix_scalar_multiplication | 1.2 | ✅ |
| 6_Matmul_with_large_K_dimension | 1.04 | ✅ |
| 7_Matmul_with_small_K_dimension | 1.71 | ✅ |
| 8_Matmul_with_irregular_shapes | 1.2265 | ✅ |
| 9_Tall_skinny_matrix_multiplication | 1.57 | ✅ |
| 10_3D_tensor_matrix_multiplication | 1.2 | ✅ |
| 11 | 1.42 | ✅ |
| 12_Matmul_with_diagonal_matrices | 1.0363 | ✅ |
| 13_Matmul_for_symmetric_matrices | 1.05 | ✅ |
| 14_Matmul_for_upper_triangular_matrices | 8.8 | ✅ |
| 15_Matmul_for_lower_triangular_matrices | 1.08 | ✅ |
| 16_Matmul_with_transposed_A | 1.6 | ✅ |
| 17_Matmul_with_transposed_B | 1.023 | ✅ |
| 18_Matmul_with_transposed_both | 7.834 | ✅ |
| 19_ReLU | 1.25 | ✅ |
| 20_LeakyReLU | 2.6 | ✅ |
| 21_Sigmoid | 2 | ✅ |
| 22_Tanh | 1.0302 | ✅ |
| 23_Softmax | 1.09 | ✅ |
| 24_LogSoftmax | 1.39 | ✅ |
| 25_Swish | 1.0355 | ✅ |
| 26_GELU | 1.49 | ✅ |
| 27_SELU | 2.07 | ✅ |
| 28_HardSigmoid | 1.02 | ✅ |
| 29_Softplus | 1.84 | ✅ |
| 30_Softsign | 1.76 | ✅ |
| 31 | 1.65 | ✅ |
| 32 | 1.24 | ✅ |
| 33 | 1.33 | ✅ |
| 34 | 1.52 | ✅ |
| 35 | 2.1327 | ✅ |
| 36 | 1.09 | ✅ |
| 37 | 1.84 | ✅ |
| 38 | 1.33 | ✅ |
| 39 | 1.56 | ✅ |
| 40_LayerNorm | 1.74 | ✅ |
| 41_Max_Pooling_1D | 1.51 | ✅ |
| 42_Max_Pooling_2D | 18.91 | ✅ |
| 43_Max_Pooling_3D | 1.07 | ✅ |
| 44 | 26.1 | ✅ |
| 45_Average_Pooling_2D | 1.66 | ✅ |
| 46_Average_Pooling_3D | 2.04 | ✅ |
| 47_Sum_reduction_over_a_dimension | 1.33 | ✅ |
| 48_Mean_reduction_over_a_dimension | 1.54 | ✅ |
| 49_Max_reduction_over_a_dimension | 1.54 | ✅ |
| 50_conv_standard_2D_square_input_square_kernel | 6.19 | ✅ |
| 51_Argmax_over_a_dimension | 1.43 | ✅ |
| 52_Argmin_over_a_dimension | 1.83 | ✅ |
| 53_Min_reduction_over_a_dimension | 1.99 | ✅ |
| 54_conv_standard_3D_square_input_square_kernel | 2.05 | ✅ |
| 55_conv_standard_2D_asymmetric_input_square_kernel | 1.08 | ✅ |
| 56_conv_standard_2D_asymmetric_input_asymmetric_kernel | 1.15 | ✅ |
| 57_conv_transposed_2D_square_input_square_kernel | 5.2 | ✅ |
| 58_conv_transposed_3D_asymmetric_input_asymmetric_kernel | 1.66 | ✅ |
| 59_conv_standard_3D_asymmetric_input_square_kernel | 1.1189 | ✅ |
| 60_conv_standard_3D_square_input_asymmetric_kernel | 1.59 | ✅ |
| 61_conv_transposed_3D_square_input_square_kernel | 1.05 | ✅ |
| 62_conv_standard_2D_square_input_asymmetric_kernel | 1.6 | ✅ |
| 63_conv_standard_2D_square_input_square_kernel | 1.55 | ✅ |
| 64_conv_transposed_1D | 1.02 | ✅ |
| 65_conv_transposed_2D_square_input_asymmetric_kernel | 6.6830 | ✅ |
| 66_conv_standard_3D_asymmetric_input_asymmetric_kernel | 1.16 | ✅ |
| 67_conv_standard_1D | 1.44 | ✅ |
| 68_conv_transposed_3D_square_input_asymmetric_kernel | 1.31 | ✅ |
| 69_conv_transposed_2D_asymmetric_input_asymmetric_kernel | 1.82 | ✅ |
| 70_conv_transposed_3D_asymmetric_input_square_kernel | 3.0680 | ✅ |
| 71_conv_transposed_2D_asymmetric_input_square_kernel | 2.8572 | ✅ |
| 72_conv_transposed_3D_asymmetric_strided_padded_grouped | 1.36 | ✅ |
| 73_conv_transposed_3D_asymmetric_square_strided_padded_grouped | 2.88 | ✅ |
| 74_conv_transposed_1D_dilated | 2.4 | ✅ |
| 75_conv_transposed_2D_asymmetric_strided_grouped_padded_dilated | 1.69 | ✅ |
| 76_conv_standard_1D_dilated_strided | 1.56 |  ✅ |
| 77_conv_transposed_3D_square_padded_dilated_strided | 1.05 | ✅ |
| 78_conv_transposed_2D_asymmetric_padded | 7.898 | ✅ |
| 79_conv_transposed_1D_asymmetric_padded_strided_dilated | 1.48 | ✅ |
| 80_conv_standard_2D_square_asymmetric_dilated_padded | 1.4899 | ✅ |
| 81_conv_transposed_2D_asymmetric_square_dilated_padded_strided | 1.64 | ✅ |
| 82_conv_depthwise_2D_square_square | 7.291 | ✅ |
| 83_conv_depthwise_2D_square_asymmetric | 1.7376 | ✅ |
| 84_conv_depthwise_2D_asymmetric_square | 1.6462 | ✅ |
| 85_conv_depthwise_2D_asymmetric_asymmetric | 2.1756 | ✅ |
| 86_conv_depthwise_separable_2D | 1.76 | ✅ |
| 87_conv_pointwise_2D | 4.27 | ✅ |
| 88_MinGPTNewGelu | 1.41 | ✅ |
| 89_cumsum | 3.4355 | ✅ |
| 90_cumprod | 1.72 | ✅ |
| 91_cumsum_reverse | 1.56 | ✅ |
| 92_cumsum_exclusive | 2.3032 | ✅ |
| 93_masked_cumsum | 1.59 | ✅ |
| 94_MSELoss | 1.6176 | ✅ |
| 95_CrossEntropyLoss | 5.4 | ✅ |
| 96_HuberLoss_score | 1.25 | ✅ |
| 97_ScaledDotProductAttention | 1.02 | ✅ |
| 98_KLDivLoss | 5.36 | ✅ |
| 99_TripletMarginLoss | 1.41 | ✅ |
| 100_HingeLoss | 1.15 | ✅ |