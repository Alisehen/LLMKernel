# Kernel Benchmark Summary
| 算子 | best_score | 状态 |
| 1 | 1.91 | ✅ |
| 2_Standard_matrix_multiplication | 0 | ✅ |
| 3_Batched_matrix_multiplication | 0 | ✅ |
| 4_Matrix_vector_multiplication | 0 | ✅ |
| 5_Matrix_scalar_multiplication | 0.677 | ✅ |
| 6_Matmul_with_large_K_dimension | 0.03 | ✅ |
| 7_Matmul_with_small_K_dimension | 0 | ✅ |
| 8_Matmul_with_irregular_shapes | 0 | ✅ |
| 9_Tall_skinny_matrix_multiplication | 0 | ✅ |
| 10_3D_tensor_matrix_multiplication | 0 | ✅ |
| 11 | 0 | ✅ |
| 12_Matmul_with_diagonal_matrices | 0 | ✅ |
| 13_Matmul_for_symmetric_matrices | 0 | ✅ |
| 14_Matmul_for_upper_triangular_matrices | 0 | ✅ |
| 15_Matmul_for_lower_triangular_matrices | 0 | ✅ |
| 16_Matmul_with_transposed_A | 0 | ✅ |
| 17_Matmul_with_transposed_B | 0 | ✅ |
| 18_Matmul_with_transposed_both | 0 | ✅ |
| 19_ReLU | 0 | ✅ |
| 20_LeakyReLU | 0.6763 | ✅ |
| 21_Sigmoid | 0.6770 | ✅ |
| 22_Tanh | 0.6768 | ✅ |
| 23_Softmax | 0 | ✅ |
| 24_LogSoftmax | 0 | ✅ |
| 25_Swish | 1.68 | ✅ |
| 26_GELU | 0 | ✅ |
| 27_SELU | 0.676 | ✅ |
| 28_HardSigmoid | 0.676 | ✅ |
| 29_Softplus | 0.6755 | ✅ |
| 30_Softsign | 2.6923 | ✅ |
| 31 | 0 | ✅ |
| 32 | 0 | ✅ |
| 33 | 0.97 | ✅ |
| 34 | 0.97 | ✅ |
| 35 | 0.97 | ✅ |
| 36 | 0 | ✅ |
| 37 | 0 | ✅ |
| 38 | 0 | ✅ |
| 39 | 0 | ✅ |
| 40_LayerNorm | 0 | ✅ |
| 41_Max_Pooling_1D | 0 | ✅ |
| 42_Max_Pooling_2D | 0 | ✅ |
| 43_Max_Pooling_3D | 1.37 | ✅ |
| 44 | 0.84 | ✅ |
| 45_Average_Pooling_2D | 1.075 | ✅ |
| 46_Average_Pooling_3D | 0 | ✅ |
| 47_Sum_reduction_over_a_dimension | 0 | ✅ |
| 48_Mean_reduction_over_a_dimension | 0 | ✅ |
| 49_Max_reduction_over_a_dimension | 0.18 | ✅ |
| 50_conv_standard_2D_square_input_square_kernel | 0.2 | ✅ |
| 51_Argmax_over_a_dimension | 0 | ✅ |
| 52_Argmin_over_a_dimension | 0.9 | ✅ |
| 53_Min_reduction_over_a_dimension | 0.4 | ✅ |
| 54_conv_standard_3D_square_input_square_kernel | 0 | ✅ |
| 55_conv_standard_2D_asymmetric_input_square_kernel | 0 | ✅ |
| 56_conv_standard_2D_asymmetric_input_asymmetric_kernel | 0 | ✅ |
| 57_conv_transposed_2D_square_input_square_kernel | 0 | ✅ |
| 58_conv_transposed_3D_asymmetric_input_asymmetric_kernel | 0.2 | ✅ |
| 59_conv_standard_3D_asymmetric_input_square_kernel | 0 | ✅ |
| 60_conv_standard_3D_square_input_asymmetric_kernel | 0 | ✅ |
| 61_conv_transposed_3D_square_input_square_kernel | 0.5 | ✅ |
| 62_conv_standard_2D_square_input_asymmetric_kernel | 0 | ✅ |
| 63_conv_standard_2D_square_input_square_kernel | 0 | ✅ |
| 64_conv_transposed_1D | 0 | ✅ |
| 65_conv_transposed_2D_square_input_asymmetric_kernel | 0 | ✅ |
| 66_conv_standard_3D_asymmetric_input_asymmetric_kernel | 0 | ✅ |
| 67_conv_standard_1D | 0.6 | ✅ |
| 68_conv_transposed_3D_square_input_asymmetric_kernel | 0.08 | ✅ |
| 69_conv_transposed_2D_asymmetric_input_asymmetric_kernel | 0 | ✅ |
| 70_conv_transposed_3D_asymmetric_input_square_kernel | 0 | ✅ |
| 71_conv_transposed_2D_asymmetric_input_square_kernel | 0.35 | ✅ |
| 72_conv_transposed_3D_asymmetric_strided_padded_grouped | 0 | ✅ |
| 73_conv_transposed_3D_asymmetric_square_strided_padded_grouped | 0.08 | ✅ |
| 74_conv_transposed_1D_dilated | 0 | ✅ |
| 75_conv_transposed_2D_asymmetric_strided_grouped_padded_dilated | 0 | ✅ |
| 76_conv_standard_1D_dilated_strided | 1.7 |  ✅ |
| 77_conv_transposed_3D_square_padded_dilated_strided | 0 | ✅ |
| 78_conv_transposed_2D_asymmetric_padded | 0.2 | ✅ |
| 79_conv_transposed_1D_asymmetric_padded_strided_dilated | 0.6 | ✅ |
| 80_conv_standard_2D_square_asymmetric_dilated_padded | 0 | ✅ |
| 81_conv_transposed_2D_asymmetric_square_dilated_padded_strided | 0.45 | ✅ |
| 82_conv_depthwise_2D_square_square | 1.17 | ✅ |
| 83_conv_depthwise_2D_square_asymmetric | 0 | ✅ |
| 84_conv_depthwise_2D_asymmetric_square | 0.99 | ✅ |
| 85_conv_depthwise_2D_asymmetric_asymmetric | 0 | ✅ |
| 86_conv_depthwise_separable_2D | 0 | ✅ |
| 87_conv_pointwise_2D | 0 | ✅ |
| 88_MinGPTNewGelu | 6.0525 | ✅ |
| 89_cumsum | 0 | ✅ |
| 90_cumprod | 0.31 | ✅ |
| 91_cumsum_reverse | 0.76 | ✅ |
| 92_cumsum_exclusive | 0 | ✅ |
| 93_masked_cumsum | 0.2359 | ✅ |
| 94_MSELoss | 0 | ✅ |
| 95_CrossEntropyLoss | 0 | ✅ |
| 96_HuberLoss_score | 0 |
| 97_ScaledDotProductAttention | 1.47 | ✅ |
| 98_KLDivLoss | 0 | ✅ |
| 99_TripletMarginLoss | 0 | ✅ |
| 100_HingeLoss | 0 | ✅ |