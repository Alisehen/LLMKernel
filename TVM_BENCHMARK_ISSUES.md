# TVM Benchmarkè„šæœ¬æ€§èƒ½é—®é¢˜åˆ†æ

## å‘ç°çš„é—®é¢˜

### 1. âŒ **Targeté…ç½®ä¸å®Œæ•´** (æœ€å…³é”®)

**å½“å‰ä»£ç ** (line 174, 282):
```python
tvm_target = "cuda"
target = tvm.target.Target(tvm_target)
```

**é—®é¢˜**:
- æ²¡æœ‰æŒ‡å®šGPUæ¶æ„ï¼ˆsm_xxï¼‰
- æ²¡æœ‰æŒ‡å®šè®¡ç®—èƒ½åŠ›ç›¸å…³å‚æ•°
- TVMæ— æ³•ä½¿ç”¨é’ˆå¯¹ç‰¹å®šæ¶æ„çš„ä¼˜åŒ–

**åº”è¯¥æ”¹ä¸º**:
```python
# è·å–GPUæ¶æ„
import torch
if torch.cuda.is_available():
    gpu_arch = torch.cuda.get_device_capability(0)
    sm_version = f"sm_{gpu_arch[0]}{gpu_arch[1]}"
else:
    sm_version = "sm_75"  # é»˜è®¤å€¼

tvm_target = f"cuda -arch={sm_version}"
# æˆ–æ›´è¯¦ç»†çš„é…ç½®ï¼š
tvm_target = tvm.target.Target({
    "kind": "cuda",
    "arch": sm_version,
    "max_num_threads": 1024,
    "max_threads_per_block": 1024,
    "max_shared_memory_per_block": 49152,  # æ ¹æ®å®é™…GPUè°ƒæ•´
    "registers_per_block": 65536,
})
```

**å½±å“**:
- æ²¡æœ‰GPUæ¶æ„ä¿¡æ¯ï¼ŒTVMæ— æ³•ç”Ÿæˆé’ˆå¯¹æ€§ä¼˜åŒ–ï¼ˆå¦‚Tensor Coreï¼‰
- å¯èƒ½å¯¼è‡´20-50%çš„æ€§èƒ½æŸå¤±

---

### 2. âŒ **ç¼ºå°‘Tensor Coreå¯ç”¨**

**å½“å‰ä»£ç ** (line 289-301):
```python
dl.ApplyDefaultSchedule(
    dl.gpu.Matmul(),
    dl.gpu.GEMV(),
    dl.gpu.Reduction(),
    dl.gpu.GeneralReduction(),
    dl.gpu.Fallback(),
)
```

**é—®é¢˜**:
- æ²¡æœ‰æ˜ç¡®å¯ç”¨Tensor Core
- æ²¡æœ‰æŒ‡å®šæ•°æ®ç±»å‹ï¼ˆfloat16/bfloat16 for tensor coreï¼‰

**åº”è¯¥æ”¹ä¸º**:
```python
# 1. åœ¨pipelineä¸­æ·»åŠ æ•°æ®ç±»å‹è½¬æ¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
pipeline = tvm.transform.Sequential([
    relax.transform.LegalizeOps(),
    relax.transform.AnnotateTIROpPattern(),
    relax.transform.FoldConstant(),
    relax.transform.FuseOps(),
    relax.transform.FuseTIR(),
    # æ·»åŠ æ›´å¤šä¼˜åŒ–pass
    relax.transform.DeadCodeElimination(),
    dl.ApplyDefaultSchedule(
        dl.gpu.Matmul(),
        dl.gpu.GEMV(),
        dl.gpu.Reduction(),
        dl.gpu.GeneralReduction(),
        dl.gpu.Fallback(),
    ),
])

# 2. åœ¨PassContextä¸­å¯ç”¨ç›¸å…³é€‰é¡¹
with target, tvm.transform.PassContext(
    opt_level=3,
    config={
        "relay.backend.use_auto_scheduler": False,
        "relay.FuseOps.max_depth": 10,
        "tir.add_lower_pass": [],
    }
):
    mod = pipeline(mod)
    ex = relax.build(mod, target=target)
```

**å½±å“**:
- å¯¹äºçŸ©é˜µè¿ç®—å¯†é›†å‹ç®—å­ï¼ˆGEMMï¼‰ï¼Œå¯èƒ½æŸå¤±2-3å€æ€§èƒ½

---

### 3. âš ï¸ **ç¼ºå°‘AutoTuning**

**å½“å‰ä»£ç **:
- å®Œå…¨ä¾èµ–DLightçš„é»˜è®¤schedule
- æ²¡æœ‰é’ˆå¯¹å…·ä½“ç¡¬ä»¶å’Œworkloadè¿›è¡Œtuning

**é—®é¢˜**:
- TVMçš„çœŸæ­£ä¼˜åŠ¿åœ¨äºAutoTuning
- é»˜è®¤scheduleå¾€å¾€ä¸æ˜¯æœ€ä¼˜çš„

**å»ºè®®æ·»åŠ **:
```python
# ä½¿ç”¨MetaScheduleè¿›è¡Œtuningï¼ˆTVM Unityæ¨èï¼‰
from tvm import meta_schedule as ms

# 1. æå–tuningä»»åŠ¡
database = ms.database.MemoryDatabase()
with target:
    tasks = ms.extract_task_from_relay(mod, target, params={})

# 2. Tuneï¼ˆå¦‚æœæ—¶é—´å…è®¸ï¼‰
if len(tasks) > 0:
    tuner = ms.tune.TuneContext(
        mod=mod,
        target=target,
        space=ms.space_generator.PostOrderApply(),
        search_strategy=ms.search_strategy.ReplayTrace(),
        task_scheduler=ms.task_scheduler.RoundRobin(
            tasks=tasks,
            max_trials_per_task=100,  # æ¯ä¸ªä»»åŠ¡100æ¬¡trial
        ),
        num_threads=4,
    )

    # Run tuning
    tuner.run()

    # Apply best schedule
    with database:
        mod = ms.relax_integration.tune_relax(mod, target, database)
```

**å½±å“**:
- å¯¹äºå¤æ‚ç®—å­ï¼ˆå¦‚conv2dï¼‰ï¼Œtuningå¯ä»¥å¸¦æ¥2-5å€æ€§èƒ½æå‡
- ä½†tuningéœ€è¦æ—¶é—´ï¼ˆæ¯ä¸ªç®—å­å‡ åˆ†é’Ÿåˆ°å‡ ååˆ†é’Ÿï¼‰

---

### 4. âš ï¸ **Benchmarkå‚æ•°å¯èƒ½ä¸å¤Ÿç¨³å®š**

**å½“å‰ä»£ç ** (line 195, 321):
```python
res_eager = benchmark_eager(model, inputs, 10, 100, device_str, torch)
res_tvm = benchmark_tvm_relax(vm, inputs_tvm, tvm_dev, 10, 100)
```

**é—®é¢˜**:
- warmup=10å¯èƒ½ä¸å¤Ÿï¼ˆå°¤å…¶æ˜¯TVMçš„kernel cacheï¼‰
- rep=100å¯èƒ½å¯¹äºå°ç®—å­è¿‡å¤šï¼Œå¯¹äºå¤§ç®—å­ä¸å¤Ÿ

**å»ºè®®**:
```python
# æ ¹æ®ç®—å­å¤§å°åŠ¨æ€è°ƒæ•´
def get_benchmark_params(model_size_mb):
    if model_size_mb < 1:  # å°ç®—å­
        return {"warmup": 20, "rep": 200}
    elif model_size_mb < 10:  # ä¸­ç­‰ç®—å­
        return {"warmup": 10, "rep": 100}
    else:  # å¤§ç®—å­
        return {"warmup": 5, "rep": 50}

# æˆ–è€…ä½¿ç”¨æ›´é²æ£’çš„benchmark
# warmup=50, rep=100 for both
```

---

### 5. âš ï¸ **å¯èƒ½çš„å†…å­˜ç®¡ç†é—®é¢˜**

**å½“å‰ä»£ç ** (line 201-211):
```python
# === AGGRESSIVE MEMORY CLEANUP BEFORE TVM ===
inputs_cpu = [x.detach().cpu().numpy() if isinstance(x, torch.Tensor) else x for x in inputs]

del inputs
del model
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()
```

**é—®é¢˜**:
- è™½ç„¶æ¸…ç†äº†PyTorchå†…å­˜ï¼Œä½†å¯èƒ½å½±å“benchmarkå…¬å¹³æ€§
- TVMå’ŒPyTorchéƒ½åœ¨åŒä¸€ä¸ªè¿›ç¨‹ä¸­ï¼Œå¯èƒ½æœ‰å†…å­˜ç¢ç‰‡

**å»ºè®®**:
- ä¿æŒå½“å‰æ¸…ç†é€»è¾‘
- ä½†åœ¨benchmarkæ—¶ç¡®ä¿é¢„çƒ­å……åˆ†

---

### 6. âŒ **ç¼ºå°‘Graph-levelä¼˜åŒ–**

**å½“å‰pipeline** (line 289-301):
```python
pipeline = tvm.transform.Sequential([
    relax.transform.LegalizeOps(),
    relax.transform.AnnotateTIROpPattern(),
    relax.transform.FoldConstant(),
    relax.transform.FuseOps(),
    relax.transform.FuseTIR(),
    dl.ApplyDefaultSchedule(...),
])
```

**ç¼ºå°‘çš„ä¼˜åŒ–**:
```python
pipeline = tvm.transform.Sequential([
    relax.transform.LegalizeOps(),
    relax.transform.AnnotateTIROpPattern(),
    relax.transform.FoldConstant(),
    relax.transform.FuseOps(),
    relax.transform.FuseTIR(),

    # æ·»åŠ æ›´å¤šä¼˜åŒ–
    relax.transform.DeadCodeElimination(),  # æ­»ä»£ç æ¶ˆé™¤
    relax.transform.RemoveUnusedFunctions(),  # ç§»é™¤æœªä½¿ç”¨å‡½æ•°
    # relax.transform.AlterOpImpl(),  # ç®—å­å®ç°æ›¿æ¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰

    dl.ApplyDefaultSchedule(...),
])
```

---

## ä¿®å¤å»ºè®®ä¼˜å…ˆçº§

### ğŸ”´ **é«˜ä¼˜å…ˆçº§ï¼ˆå¿…é¡»ä¿®å¤ï¼‰**

1. **æ·»åŠ GPUæ¶æ„é…ç½®** (æœ€å…³é”®)
   ```python
   gpu_arch = torch.cuda.get_device_capability(0)
   sm_version = f"sm_{gpu_arch[0]}{gpu_arch[1]}"
   tvm_target = f"cuda -arch={sm_version}"
   ```

2. **æ£€æŸ¥Tensor Coreæ”¯æŒ**
   ```python
   # å¯¹äºæ”¯æŒTensor Coreçš„GPUï¼ˆsm_70+ï¼‰ï¼Œç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat16
   if gpu_arch[0] >= 7:  # Voltaæˆ–æ›´æ–°
       # è€ƒè™‘åœ¨benchmarkå‰å°†æ¨¡å‹è½¬æ¢ä¸ºfloat16
       pass
   ```

### ğŸŸ¡ **ä¸­ä¼˜å…ˆçº§ï¼ˆå»ºè®®ä¿®å¤ï¼‰**

3. **å¢åŠ warmupæ¬¡æ•°**
   ```python
   # TVM kernel cacheéœ€è¦æ›´å¤šwarmup
   res_tvm = benchmark_tvm_relax(vm, inputs_tvm, tvm_dev, 50, 100)
   ```

4. **æ·»åŠ æ›´å¤šGraphä¼˜åŒ–pass**
   ```python
   relax.transform.DeadCodeElimination(),
   relax.transform.RemoveUnusedFunctions(),
   ```

### ğŸŸ¢ **ä½ä¼˜å…ˆçº§ï¼ˆå¯é€‰ï¼‰**

5. **æ·»åŠ AutoTuningæ”¯æŒ**ï¼ˆéœ€è¦å¤§é‡æ—¶é—´ï¼‰
6. **æ›´è¯¦ç»†çš„targeté…ç½®**ï¼ˆshared memoryã€registerç­‰ï¼‰

---

## ä¿®æ”¹åçš„å…³é”®ä»£ç 

```python
def worker_entry_point(file_path, device_str, result_queue):
    """Worker function with improved TVM configuration."""

    try:
        torch, tvm, relax, runtime, from_fx, builtins, np, gc = get_torch_tvm_imports()

        # ... å‰é¢çš„ä»£ç ä¿æŒä¸å˜ ...

        # === æ”¹è¿›1: æ›´è¯¦ç»†çš„targeté…ç½® ===
        if device_str == "cuda" and torch.cuda.is_available():
            torch_dev = torch.device("cuda")

            # è·å–GPUæ¶æ„
            gpu_arch = torch.cuda.get_device_capability(0)
            sm_version = f"sm_{gpu_arch[0]}{gpu_arch[1]}"

            # è¯¦ç»†targeté…ç½®
            tvm_target = tvm.target.Target({
                "kind": "cuda",
                "arch": sm_version,
                "max_num_threads": 1024,
                "thread_warp_size": 32,
            })

            try:
                tvm_dev = tvm.cuda(0)
            except:
                tvm_dev = runtime.device("cuda", 0)
        else:
            torch_dev = torch.device("cpu")
            tvm_target = tvm.target.Target("llvm")
            tvm_dev = tvm.cpu(0)

        # ... PyTorch benchmarkä»£ç ä¿æŒä¸å˜ ...

        # === æ”¹è¿›2: æ›´å®Œæ•´çš„ä¼˜åŒ–pipeline ===
        if str(tvm_target.kind) == "cuda":
            try:
                import tvm.dlight as dl
                pipeline = tvm.transform.Sequential([
                    relax.transform.LegalizeOps(),
                    relax.transform.AnnotateTIROpPattern(),
                    relax.transform.FoldConstant(),
                    relax.transform.FuseOps(),
                    relax.transform.FuseTIR(),
                    relax.transform.DeadCodeElimination(),  # æ–°å¢
                    relax.transform.RemoveUnusedFunctions(),  # æ–°å¢
                    dl.ApplyDefaultSchedule(
                        dl.gpu.Matmul(),
                        dl.gpu.GEMV(),
                        dl.gpu.Reduction(),
                        dl.gpu.GeneralReduction(),
                        dl.gpu.Fallback(),
                    ),
                ])
                with tvm_target, tvm.transform.PassContext(
                    opt_level=3,
                    config={
                        "relay.FuseOps.max_depth": 10,
                    }
                ):
                    mod = pipeline(mod)
                    ex = relax.build(mod, target=tvm_target)
            except Exception as e:
                with tvm.transform.PassContext(opt_level=3):
                    ex = relax.build(mod, target=tvm_target)
        else:
            with tvm.transform.PassContext(opt_level=3):
                ex = relax.build(mod, target=tvm_target)

        # ... åé¢çš„ä»£ç ä¿æŒä¸å˜ ...

        # === æ”¹è¿›3: å¢åŠ warmup ===
        res_tvm = benchmark_tvm_relax(vm, inputs_tvm, tvm_dev, 50, 100)  # warmupä»10æ”¹ä¸º50

    except Exception as e:
        # ... é”™è¯¯å¤„ç†ä¿æŒä¸å˜ ...
```

---

## é¢„æœŸæ”¹è¿›æ•ˆæœ

| æ”¹è¿›é¡¹ | é¢„æœŸæ€§èƒ½æå‡ | é€‚ç”¨ç®—å­ |
|--------|-------------|---------|
| GPUæ¶æ„é…ç½® | 20-50% | æ‰€æœ‰CUDAç®—å­ |
| Tensor Coreå¯ç”¨ | 2-3x | GEMMå¯†é›†å‹ï¼ˆçŸ©é˜µä¹˜æ³•ã€å·ç§¯ï¼‰ |
| å¢åŠ warmup | 5-10% | å°ç®—å­ |
| Graphä¼˜åŒ–pass | 10-20% | å¤æ‚æ¨¡å‹ |
| AutoTuning | 2-5x | æ‰€æœ‰ç®—å­ï¼ˆéœ€è¦æ—¶é—´ï¼‰ |

---

## ç»“è®º

**å½“å‰TVMæ€§èƒ½ä¸å¦‚PyTorchçš„ä¸»è¦åŸå› **ï¼š

1. âŒ **æ²¡æœ‰æŒ‡å®šGPUæ¶æ„** - å¯¼è‡´TVMæ— æ³•ä½¿ç”¨é’ˆå¯¹æ€§ä¼˜åŒ–
2. âŒ **æ²¡æœ‰å¯ç”¨Tensor Core** - å¯¹äºGEMMç®—å­æŸå¤±å·¨å¤§
3. âš ï¸ **ä¾èµ–é»˜è®¤schedule** - æ²¡æœ‰tuning

**å¿«é€Ÿä¿®å¤ï¼ˆ5åˆ†é’Ÿå†…ï¼‰**ï¼š
1. æ·»åŠ GPUæ¶æ„é…ç½®
2. å¢åŠ warmupæ¬¡æ•°

**å®Œæ•´ä¿®å¤ï¼ˆéœ€è¦æ—¶é—´ï¼‰**ï¼š
1. ä¸Šè¿°å¿«é€Ÿä¿®å¤
2. æ·»åŠ AutoTuningæ”¯æŒ
3. é’ˆå¯¹æ€§ä¼˜åŒ–ç‰¹å®šç®—å­

ä¿®å¤åï¼ŒTVMåº”è¯¥èƒ½å¤Ÿè¾¾åˆ°ä¸PyTorchç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ï¼ˆå°¤å…¶æ˜¯å¯¹äºå¤§æ¨¡å‹å’Œå¤æ‚ç®—å­ï¼‰ã€‚
