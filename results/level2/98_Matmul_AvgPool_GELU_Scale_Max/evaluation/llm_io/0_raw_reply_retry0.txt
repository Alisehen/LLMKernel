```python
import math
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def matmul_gelu_scale_max_kernel(
    a_ptr,  # [M, K]
    w_ptr,  # [N, K]  (pooled weights)
    b_ptr,  # [N]     (pooled bias)
    out_ptr,  # [M]
    M, N, K,
    stride_am, stride_ak,
    stride_wm, stride_wk,
    stride_outm,
    scale,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    # Program id over rows (batch dimension)
    pid_m = tl.program_id(0)
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    mask_m = offs_m < M

    # Initialize per-row maxima
    neg_inf = -float("inf")
    max_vals = tl.full((BLOCK_M,), neg_inf, dtype=tl.float32)

    # Loop over N (pooled output features) in blocks of BLOCK_N
    for n_start in range(0, N, BLOCK_N):
        offs_n = n_start + tl.arange(0, BLOCK_N)
        mask_n = offs_n < N

        # Accumulator for this [M_tile, N_tile] block
        acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

        # Loop over K in blocks of BLOCK_K
        for k_start in range(0, K, BLOCK_K):
            offs_k = k_start + tl.arange(0, BLOCK_K)
            mask_k = offs_k < K

            # A tile: [BLOCK_M, BLOCK_K]
            a_ptrs = a_ptr + (
                offs_m[:, None] * stride_am
                + offs_k[None, :] * stride_ak
            )
            a = tl.load(
                a_ptrs,
                mask=mask_m[:, None] & mask_k[None, :],
                other=0.0,
            )

            # W tile arranged as [BLOCK_K, BLOCK_N]
            w_ptrs = w_ptr + (
                offs_k[:, None] * stride_wk
                + offs_n[None, :] * stride_wm
            )
            w = tl.load(
                w_ptrs,
                mask=mask_k[:, None] & mask_n[None, :],
                other=0.0,
            )

            # GEMM partial
            acc += tl.dot(a, w)

        # Add bias
        bias = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0)
        acc = acc + bias[None, :]

        # GELU approximation (tanh-based, implemented via exp)
        # gelu(x) ≈ 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 x^3)))
        x = acc
        x3 = x * x * x
        t = 0.7978845608028654 * (x + 0.044715 * x3)  # √(2/π) ≈ 0.79788456
        u = tl.exp(2.0 * t)
        tanh_t = (u - 1.0) / (u + 1.0)
        gelu = 0.5 * x * (1.0 + tanh_t)

        # Scale
        gelu_scaled = gelu * scale

        # Row-wise max over this N-tile, with N masking
        masked_vals = tl.where(mask_n[None, :], gelu_scaled, neg_inf)
        block_max = tl.max(masked_vals, axis=1)

        # Update running maxima
        max_vals = tl.maximum(max_vals, block_max)

    # Store results
    out_ptrs = out_ptr + offs_m * stride_outm
    tl.store(out_ptrs, max_vals, mask=mask_m)


def fused_matmul_avgpool_gelu_scale_max(x, weight, bias, pool_kernel_size, scale_factor):
    """
    Fused implementation of:
        y = x @ W^T + b
        y = AvgPool1d(kernel_size=pool_kernel_size, stride=pool_kernel_size) along features
        y = GELU(y)
        y = y * scale_factor
        out = max(y, dim=1)

    Using the identity:
        avg_pool(x @ W^T + b)  ==  x @ W_pooled^T + b_pooled

    where W_pooled and b_pooled are averages of weight/bias over pooling groups.
    """
    assert x.is_cuda, "Input must be on CUDA device"
    device = x.device

    # Ensure float32 compute for numerical stability
    orig_dtype = x.dtype
    if orig_dtype != torch.float32:
        x = x.float()
    if weight.dtype != torch.float32:
        w = weight.float()
    else:
        w = weight
    if bias.dtype != torch.float32:
        b = bias.float()
    else:
        b = bias

    M, K = x.shape
    N = w.shape[0]
    k = pool_kernel_size

    # Output length after AvgPool1d(kernel=k, stride=k, padding=0)
    # L_out = floor((N - k) / k) + 1
    N_pool = (N - k) // k + 1
    assert N_pool > 0, "Invalid configuration: out_features < pool_kernel_size"

    N_eff = N_pool * k  # Only the first N_eff features participate in pooling

    w_eff = w[:N_eff]            # [N_eff, K]
    b_eff = b[:N_eff]            # [N_eff]

    # Pool weights and biases along the output-feature dimension
    # Shape transformations:
    #   w_eff: [N_pool, k, K] -> mean over k -> [N_pool, K]
    #   b_eff: [N_pool, k]    -> mean over k -> [N_pool]
    w_pooled = w_eff.view(N_pool, k, K).mean(dim=1).contiguous()
    b_pooled = b_eff.view(N_pool, k).mean(dim=1).contiguous()

    # Allocate output [M]
    out = torch.empty((M,), device=device, dtype=torch.float32)

    # Launch Triton kernel
    BLOCK_M = 64
    BLOCK_N = 64
    BLOCK_K = 32

    grid = lambda META: (triton.cdiv(M, META["BLOCK_M"]),)

    matmul_gelu_scale_max_kernel[grid](
        x, w_pooled, b_pooled, out,
        M, N_pool, K,
        x.stride(0), x.stride(1),
        w_pooled.stride(0), w_pooled.stride(1),
        out.stride(0),
        float(scale_factor),
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
        BLOCK_K=BLOCK_K,
    )

    # Cast back to original dtype
    if orig_dtype != torch.float32:
        out = out.to(orig_dtype)
    return out


class ModelNew(nn.Module):
    """
    Triton-optimized version of the reference model implementing:
        Matmul -> AvgPool1d -> GELU -> Scale -> Max
    """

    def __init__(self, in_features, out_features, pool_kernel_size, scale_factor):
        super(ModelNew, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.pool_kernel_size = pool_kernel_size
        self.scale_factor = float(scale_factor)

        # Match nn.Linear(in_features, out_features) semantics
        self.weight = nn.Parameter(torch.empty(out_features, in_features))
        self.bias = nn.Parameter(torch.empty(out_features))

        # Simple initialization similar in spirit to nn.Linear
        bound = 1 / math.sqrt(in_features)
        nn.init.uniform_(self.weight, -bound, bound)
        nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return fused_matmul_avgpool_gelu_scale_max(
            x, self.weight, self.bias, self.pool_kernel_size, self.scale_factor
        )
```