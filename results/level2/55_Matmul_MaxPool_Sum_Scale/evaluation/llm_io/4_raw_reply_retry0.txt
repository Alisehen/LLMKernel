```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config(
            {
                "BLOCK_M": 64,
                "BLOCK_N": 128,
                "BLOCK_K": 32,
                "GROUP_M": 8,
            },
            num_warps=4,
            num_stages=3,
        ),
        triton.Config(
            {
                "BLOCK_M": 128,
                "BLOCK_N": 64,
                "BLOCK_K": 32,
                "GROUP_M": 8,
            },
            num_warps=4,
            num_stages=3,
        ),
        triton.Config(
            {
                "BLOCK_M": 128,
                "BLOCK_N": 128,
                "BLOCK_K": 32,
                "GROUP_M": 4,
            },
            num_warps=8,
            num_stages=3,
        ),
        triton.Config(
            {
                "BLOCK_M": 64,
                "BLOCK_N": 64,
                "BLOCK_K": 32,
                "GROUP_M": 8,
            },
            num_warps=4,
            num_stages=2,
        ),
    ],
    key=["M", "N", "K"],
)
@triton.jit
def linear_maxpool_sum_scale_kernel(
    a_ptr,        # [M, K] input
    w_ptr,        # [N, K] weight (same layout as nn.Linear.weight)
    bias_ptr,     # [N]    bias
    out_ptr,      # [M]    output (row-wise pooled + summed + scaled)
    M, N, K,
    stride_am, stride_ak,
    stride_wn, stride_wk,
    stride_outm,
    scale,        # float32
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,  # must be even
    BLOCK_K: tl.constexpr,
    GROUP_M: tl.constexpr,
):
    pid = tl.program_id(axis=0)

    # 2D tiling over M and N, grouped by M to improve L2 locality
    num_pid_m = tl.cdiv(M, BLOCK_M)
    num_pid_n = tl.cdiv(N, BLOCK_N)

    num_pid_in_group = GROUP_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_M

    group_size_m = tl.minimum(num_pid_m - first_pid_m, GROUP_M)
    pid_m = first_pid_m + (pid % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m

    # Row and column indices for this tile
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    mask_m = offs_m < M
    mask_n = offs_n < N

    # Accumulator for GEMM tile in fp32
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Pointers for current K-slice
    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    # w_ptr is [N, K]: index as w[n, k], tile is [BLOCK_K, BLOCK_N]
    w_ptrs = w_ptr + offs_n[None, :] * stride_wn + offs_k[:, None] * stride_wk

    # GEMM K-loop
    for k in range(0, K, BLOCK_K):
        k_rem = K - k
        k_mask = offs_k < k_rem

        a = tl.load(
            a_ptrs,
            mask=mask_m[:, None] & k_mask[None, :],
            other=0.0,
        )
        b = tl.load(
            w_ptrs,
            mask=k_mask[:, None] & mask_n[None, :],
            other=0.0,
        )

        acc += tl.dot(a, b, allow_tf32=True)

        a_ptrs += BLOCK_K * stride_ak
        w_ptrs += BLOCK_K * stride_wk

    # Fused bias add
    bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)
    acc += bias[None, :]

    # Row-wise accumulation of MaxPool1d(kernel=2,stride=2) over N, then sum.
    # Vectorized over all local pairs instead of a Python loop to stay in Triton IR.
    # Local pair indices: (0,1), (2,3), ..., (BLOCK_N-2, BLOCK_N-1)
    num_pairs = BLOCK_N // 2
    pair_idx = tl.arange(0, num_pairs)
    col0_idx = 2 * pair_idx
    col1_idx = col0_idx + 1

    # Valid pair only if both global columns are in range
    pair_valid = mask_n[col0_idx] & mask_n[col1_idx]  # [num_pairs]

    # Gather the two columns for all pairs
    col0 = acc[:, col0_idx]  # [BLOCK_M, num_pairs]
    col1 = acc[:, col1_idx]  # [BLOCK_M, num_pairs]

    vmax = tl.maximum(col0, col1)

    # Apply row and pair masks
    vmax = tl.where(
        mask_m[:, None] & pair_valid[None, :],
        vmax,
        0.0,
    )

    # Sum over pooled positions to get one value per row
    row_partial = tl.sum(vmax, axis=1)  # [BLOCK_M]

    # Scale and atomically accumulate into global output buffer.
    # Multiple CTAs contribute different N-tiles for the same rows.
    row_partial *= scale
    out_ptrs = out_ptr + offs_m * stride_outm
    tl.atomic_add(out_ptrs, row_partial, mask=mask_m)


def fused_linear_maxpool_sum_scale(x: torch.Tensor,
                                   weight: torch.Tensor,
                                   bias: torch.Tensor,
                                   scale: float) -> torch.Tensor:
    """
    Fused operation:
      y = x @ weight.T + bias                  # [M, N]
      z = MaxPool1d(kernel_size=2, stride=2)   # along last dim of y
      out = scale * z.sum(dim=-1)              # [M]

    x:      [M, K]
    weight: [N, K] (same as nn.Linear.weight)
    bias:   [N]
    scale:  float
    returns: [M]
    """
    assert x.is_cuda and weight.is_cuda and bias.is_cuda
    M, K = x.shape
    N, Kw = weight.shape
    assert Kw == K, "in_features mismatch"
    assert bias.shape[0] == N, "bias/out_features mismatch"

    # Output is accumulated in fp32 for stability and atomic support
    out_fp32 = torch.zeros((M,), device=x.device, dtype=torch.float32)

    def grid(meta):
        return (triton.cdiv(M, meta["BLOCK_M"]) * triton.cdiv(N, meta["BLOCK_N"]),)

    linear_maxpool_sum_scale_kernel[grid](
        x,
        weight,
        bias,
        out_fp32,
        M,
        N,
        K,
        x.stride(0),
        x.stride(1),
        weight.stride(0),
        weight.stride(1),
        out_fp32.stride(0),
        float(scale),
    )

    # Match input dtype on return
    return out_fp32.to(x.dtype)


class ModelNew(nn.Module):
    """
    Fused model:
      - Linear: x @ W^T + b
      - MaxPool1d(kernel_size=2, stride=2) over features
      - Sum over pooled positions
      - Scale by a factor

    Implemented as a single high-performance Triton kernel to
    eliminate intermediate global-memory stores.
    """
    def __init__(self, in_features, out_features, kernel_size, scale_factor):
        super(ModelNew, self).__init__()
        assert kernel_size == 2, "This implementation assumes kernel_size=2"
        self.in_features = in_features
        self.out_features = out_features
        self.scale_factor = float(scale_factor)

        # Same layout as nn.Linear
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.bias = nn.Parameter(torch.randn(out_features))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        assert x.is_cuda, "Input must be a CUDA tensor for Triton kernels"
        # x: [B, in_features] => output: [B]
        return fused_linear_maxpool_sum_scale(
            x, self.weight, self.bias, self.scale_factor
        )
```