{
  "critical_issue": "Using TF32 precision in tl.dot causes numerical differences vs PyTorch FP32 GEMM.",
  "why_it_matters": "TF32 has lower precision than FP32, leading to error exceeding tolerance (1.06e-02 > 1e-02).",
  "minimal_fix_hint": "Change tl.dot's allow_tf32=True to allow_tf32=False for FP32 precision."
}