```python
# <complete ModelNew code with optimized Triton kernels>
import torch
import torch.nn as nn
import triton
import triton.language as tl


# ---------------------------------------------------------------------------
# 3D Conv (NCDHW) – implicit GEMM, Tensor-Core friendly, aggressively tuned
# ---------------------------------------------------------------------------

@triton.autotune(
    configs=[
        # High-throughput configs for Ada (4090) – favor larger tiles first
        triton.Config(
            {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32},
            num_warps=8,
            num_stages=2,
        ),
        triton.Config(
            {'BLOCK_M': 32, 'BLOCK_N': 128, 'BLOCK_K': 32},
            num_warps=8,
            num_stages=2,
        ),
        triton.Config(
            {'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32},
            num_warps=8,
            num_stages=2,
        ),

        # Balanced configs
        triton.Config(
            {'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 32},
            num_warps=4,
            num_stages=2,
        ),
        triton.Config(
            {'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32},
            num_warps=4,
            num_stages=2,
        ),

        # Lower-register / high-occupancy fallbacks
        triton.Config(
            {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 32},
            num_warps=4,
            num_stages=2,
        ),
        triton.Config(
            {'BLOCK_M': 16, 'BLOCK_N': 64, 'BLOCK_K': 32},
            num_warps=4,
            num_stages=2,
        ),
        triton.Config(
            {'BLOCK_M': 16, 'BLOCK_N': 32, 'BLOCK_K': 16},
            num_warps=2,
            num_stages=2,
        ),
    ],
    key=["N", "C_in", "D_out", "H_out", "W_out", "C_out"],
)
@triton.jit
def conv3d_ncdhw_kernel(
    x_ptr,  # [N, C_in, D_in, H_in, W_in]
    w_ptr,  # [C_out, C_in, K_D, K_H, K_W]
    b_ptr,  # [C_out]
    y_ptr,  # [N, C_out, D_out, H_out, W_out]
    N,
    C_in,
    D_in,
    H_in,
    W_in,
    C_out,
    D_out,
    H_out,
    W_out,
    stride_xn,
    stride_xc,
    stride_xd,
    stride_xh,
    stride_xw,
    stride_wn,
    stride_wc,
    stride_wd,
    stride_wh,
    stride_ww,
    stride_yn,
    stride_yc,
    stride_yd,
    stride_yh,
    stride_yw,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
    K_D: tl.constexpr,
    K_H: tl.constexpr,
    K_W: tl.constexpr,
):
    # Program IDs: 2D grid over (M = N*D_out*H_out*W_out, N = C_out)
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    P = N * D_out * H_out * W_out
    mask_m = offs_m < P
    mask_n = offs_n < C_out

    # Decode offs_m -> (n, od, oh, ow)
    DHW = D_out * H_out * W_out
    HW = H_out * W_out

    n = offs_m // DHW
    rem = offs_m % DHW
    od = rem // HW
    rem2 = rem % HW
    oh = rem2 // W_out
    ow = rem2 % W_out

    # Flattened reduction dimension size
    K_total = C_in * K_D * K_H * K_W

    # Precompute kernel products (constexpr-friendly)
    KHW = K_H * K_W
    KDKHW = K_D * KHW

    # Accumulator in FP32 (Tensor Cores will accumulate here for fp16/bf16)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Base pointer for all output pixels in this tile (independent of K)
    x_base_ptrs = (
        x_ptr
        + n * stride_xn
        + od * stride_xd
        + oh * stride_xh
        + ow * stride_xw
    )

    # Base pointer for each output channel (independent of K)
    w_base = w_ptr + offs_n * stride_wn

    # Give compiler alignment hints for better vectorization
    tl.multiple_of(offs_m, BLOCK_M)
    tl.multiple_of(offs_n, BLOCK_N)

    # Main implicit-GEMM loop over flattened (C_in * K_D * K_H * K_W) dimension
    k_range = tl.arange(0, BLOCK_K)
    for k0 in range(0, K_total, BLOCK_K):
        offs_k = k0 + k_range
        k_mask = offs_k < K_total

        tl.multiple_of(offs_k, BLOCK_K)

        # Decode flattened K index -> (ic, kd, kh, kw)
        tmp = offs_k
        ic = tmp // KDKHW
        tmp = tmp % KDKHW
        kd = tmp // KHW
        tmp = tmp % KHW
        kh = tmp // K_W
        kw = tmp % K_W

        # Offsets in input and weight tensors for the K tile
        x_k_offsets = (
            ic * stride_xc
            + kd * stride_xd
            + kh * stride_xh
            + kw * stride_xw
        )
        w_k_offsets = (
            ic * stride_wc
            + kd * stride_wd
            + kh * stride_wh
            + kw * stride_ww
        )

        # Input pointers for this (M, K) tile
        x_ptrs = x_base_ptrs[:, None] + x_k_offsets[None, :]
        # Weight pointers for this (K, N) tile
        w_ptrs = w_base[None, :] + w_k_offsets[:, None]

        # Masks for loads
        mask_x = mask_m[:, None] & k_mask[None, :]
        mask_w = k_mask[:, None] & mask_n[None, :]

        # Load tiles; keep in original dtype for Tensor Cores
        x_vals = tl.load(x_ptrs, mask=mask_x, other=0.0)
        w_vals = tl.load(w_ptrs, mask=mask_w, other=0.0)

        # TensorCore-friendly dot: (BLOCK_M, BLOCK_K) x (BLOCK_K, BLOCK_N)
        acc += tl.dot(x_vals, w_vals)

    # Bias add (broadcast along BLOCK_M)
    bias_vals = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0)
    acc += bias_vals[None, :]

    # Store output [N, C_out, D_out, H_out, W_out]
    y_ptrs = (
        y_ptr
        + n[:, None] * stride_yn
        + offs_n[None, :] * stride_yc
        + od[:, None] * stride_yd
        + oh[:, None] * stride_yh
        + ow[:, None] * stride_yw
    )
    mask_store = mask_m[:, None] & mask_n[None, :]
    tl.store(y_ptrs, acc, mask=mask_store)


# ---------------------------------------------------------------------------
# GroupNorm + global mean over (C,D,H,W), fused into a single kernel.
# No intermediate normalized tensor is ever stored to memory.
# ---------------------------------------------------------------------------

@triton.autotune(
    configs=[
        triton.Config({'BLOCK': 512}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK': 256}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK': 128}, num_warps=2, num_stages=2),
    ],
    key=["C", "D", "H", "W"],
)
@triton.jit
def groupnorm_mean_kernel(
    x_ptr,         # [N, C, D, H, W]
    gamma_ptr,     # [C]
    beta_ptr,      # [C]
    out_ptr,       # [N] (FP32 accumulation)
    N,
    C,
    D,
    H,
    W,
    num_groups,
    stride_xn,
    stride_xc,
    stride_xd,
    stride_xh,
    stride_xw,
    eps,
    inv_total_elems,  # 1.0 / (C*D*H*W)
    BLOCK: tl.constexpr,
):
    # grid: (N, num_groups)
    pid_n = tl.program_id(0)
    pid_g = tl.program_id(1)

    if pid_n >= N:
        return
    if pid_g >= num_groups:
        return

    group_size = C // num_groups
    c_start = pid_g * group_size
    c_end = c_start + group_size

    spatial = D * H * W
    group_elems = group_size * spatial

    offs = tl.arange(0, BLOCK)

    # ------------------------ Pass 1: mean and variance ---------------------
    sum_val = tl.zeros((), dtype=tl.float32)
    sum_sq = tl.zeros((), dtype=tl.float32)

    for base in range(0, group_elems, BLOCK):
        idx = base + offs
        mask = idx < group_elems

        c_rel = idx // spatial
        rem = idx % spatial
        d = rem // (H * W)
        rem2 = rem % (H * W)
        h = rem2 // W
        w = rem2 % W

        c = c_start + c_rel

        x_ptrs = (
            x_ptr
            + pid_n * stride_xn
            + c * stride_xc
            + d * stride_xd
            + h * stride_xh
            + w * stride_xw
        )
        x = tl.load(x_ptrs, mask=mask, other=0.0)
        x_f = x.to(tl.float32)

        sum_val += tl.sum(x_f, axis=0)
        sum_sq += tl.sum(x_f * x_f, axis=0)

    group_elems_f = tl.full((), group_elems, dtype=tl.float32)
    mean = sum_val / group_elems_f
    var = sum_sq / group_elems_f - mean * mean
    inv_std = 1.0 / tl.sqrt(var + eps)

    # ------------------------ Pass 2: normalize + final mean ----------------
    partial_sum = tl.zeros((), dtype=tl.float32)

    for base in range(0, group_elems, BLOCK):
        idx = base + offs
        mask = idx < group_elems

        c_rel = idx // spatial
        rem = idx % spatial
        d = rem // (H * W)
        rem2 = rem % (H * W)
        h = rem2 // W
        w = rem2 % W

        c = c_start + c_rel

        x_ptrs = (
            x_ptr
            + pid_n * stride_xn
            + c * stride_xc
            + d * stride_xd
            + h * stride_xh
            + w * stride_xw
        )
        x = tl.load(x_ptrs, mask=mask, other=0.0)
        x_f = x.to(tl.float32)

        gamma = tl.load(gamma_ptr + c, mask=mask, other=1.0).to(tl.float32)
        beta = tl.load(beta_ptr + c, mask=mask, other=0.0).to(tl.float32)

        norm = (x_f - mean) * inv_std
        y = norm * gamma + beta

        partial_sum += tl.sum(y, axis=0)

    # Each (n, g) block contributes its share to the final mean over (C,D,H,W)
    contrib = partial_sum * inv_total_elems
    tl.atomic_add(out_ptr + pid_n, contrib)


# ---------------------------------------------------------------------------
# Wrapper functions
# ---------------------------------------------------------------------------

def conv3d_triton(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:
    """
    NCDHW Conv3d (stride=1, padding=0, dilation=1) using an implicit-GEMM Triton kernel.

    Args:
        x:      [N, C_in, D_in, H_in, W_in]
        weight: [C_out, C_in, K_D, K_H, K_W]
        bias:   [C_out]

    Returns:
        y: [N, C_out, D_out, H_out, W_out]
           where D_out = D_in - K_D + 1, etc.
    """
    assert x.is_cuda and weight.is_cuda and bias.is_cuda
    assert x.dtype == weight.dtype == bias.dtype

    N, C_in, D_in, H_in, W_in = x.shape
    C_out, C_in_w, K_D, K_H, K_W = weight.shape
    assert C_in == C_in_w

    D_out = D_in - K_D + 1
    H_out = H_in - K_H + 1
    W_out = W_in - K_W + 1
    assert D_out > 0 and H_out > 0 and W_out > 0

    y = torch.empty((N, C_out, D_out, H_out, W_out), device=x.device, dtype=x.dtype)

    def grid(meta):
        return (
            triton.cdiv(N * D_out * H_out * W_out, meta["BLOCK_M"]),
            triton.cdiv(C_out, meta["BLOCK_N"]),
        )

    conv3d_ncdhw_kernel[grid](
        x,
        weight,
        bias,
        y,
        N,
        C_in,
        D_in,
        H_in,
        W_in,
        C_out,
        D_out,
        H_out,
        W_out,
        x.stride(0),
        x.stride(1),
        x.stride(2),
        x.stride(3),
        x.stride(4),
        weight.stride(0),
        weight.stride(1),
        weight.stride(2),
        weight.stride(3),
        weight.stride(4),
        y.stride(0),
        y.stride(1),
        y.stride(2),
        y.stride(3),
        y.stride(4),
        K_D=K_D,
        K_H=K_H,
        K_W=K_W,
    )

    return y


def groupnorm_mean_triton(
    x: torch.Tensor,
    gamma: torch.Tensor,
    beta: torch.Tensor,
    num_groups: int,
    eps: float,
) -> torch.Tensor:
    """
    Fused GroupNorm + mean over (C, D, H, W).

    Args:
        x:      [N, C, D, H, W]
        gamma:  [C] (GroupNorm weight)
        beta:   [C] (GroupNorm bias)
        num_groups: number of groups in GroupNorm
        eps:    GroupNorm epsilon

    Returns:
        out: [N] (same dtype as x), where out[n] = mean(GroupNorm(x[n]) over (C,D,H,W))
    """
    assert x.is_cuda and gamma.is_cuda and beta.is_cuda
    assert x.dtype == gamma.dtype == beta.dtype

    N, C, D, H, W = x.shape
    assert C % num_groups == 0

    # Use FP32 accumulation for better numerical stability
    out = torch.zeros((N,), device=x.device, dtype=torch.float32)

    total_elems = C * D * H * W
    inv_total = 1.0 / float(total_elems)

    def grid(meta):
        return (N, num_groups)

    groupnorm_mean_kernel[grid](
        x,
        gamma,
        beta,
        out,
        N,
        C,
        D,
        H,
        W,
        num_groups,
        x.stride(0),
        x.stride(1),
        x.stride(2),
        x.stride(3),
        x.stride(4),
        eps,
        inv_total,
    )

    return out.to(x.dtype)


# ---------------------------------------------------------------------------
# Model
# ---------------------------------------------------------------------------

class ModelNew(nn.Module):
    """
    3D convolution via Triton, followed by GroupNorm and mean over (C, D, H, W).

    Conv3d is implemented as a high-performance implicit-GEMM Triton kernel.
    GroupNorm and the final spatial+channel mean are fused into a single Triton
    kernel that never stores the intermediate normalized tensor to memory.
    """

    def __init__(self, in_channels, out_channels, kernel_size, num_groups):
        super(ModelNew, self).__init__()
        if isinstance(kernel_size, int):
            k_d = k_h = k_w = kernel_size
        else:
            k_d, k_h, k_w = kernel_size
        assert k_d == k_h == k_w, "This implementation assumes cubic kernels."

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = k_d

        # Conv3d parameters (NCDHW)
        self.weight = nn.Parameter(
            torch.empty(out_channels, in_channels, k_d, k_h, k_w)
        )
        self.bias = nn.Parameter(torch.empty(out_channels))

        # Initialize similar to nn.Conv3d default
        import math

        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        fan_in = in_channels * k_d * k_h * k_w
        bound = 1.0 / math.sqrt(fan_in)
        nn.init.uniform_(self.bias, -bound, bound)

        # GroupNorm parameters (we use the module for its parameters/eps)
        self.group_norm = nn.GroupNorm(num_groups, out_channels)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # High-performance Triton Conv3d
        x = conv3d_triton(x, self.weight, self.bias)

        # Fused GroupNorm + mean over (C, D, H, W) in Triton
        gn_mean = groupnorm_mean_triton(
            x,
            self.group_norm.weight,
            self.group_norm.bias,
            self.group_norm.num_groups,
            self.group_norm.eps,
        )

        return gn_mean.unsqueeze(1)
```