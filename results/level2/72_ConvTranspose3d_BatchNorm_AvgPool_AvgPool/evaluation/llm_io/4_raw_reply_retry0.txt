```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def fused_bn_avgpool_kernel_optimized(
    # Pointers to tensors
    x_ptr, gamma_ptr, beta_ptr, mean_ptr, var_ptr, out_ptr,
    # Tensor dimensions
    N, C, D, H, W,
    D_out, H_out, W_out,
    # Strides for input tensor
    stride_n, stride_c, stride_d, stride_h, stride_w,
    # Strides for output tensor
    out_stride_n, out_stride_c, out_stride_d, out_stride_h, out_stride_w,
    # BatchNorm parameters
    eps: tl.constexpr,
    # Block sizes
    BLOCK_C: tl.constexpr,
    BLOCK_SPATIAL: tl.constexpr,
    # Tuning parameters
    NUM_WARPS: tl.constexpr,
    NUM_STAGES: tl.constexpr,
):
    """
    Optimized fused BatchNorm3d + 4x4x4 AvgPool3d kernel with:
    1. Better memory coalescing through channel blocking
    2. Warp-level parallel reduction with shuffle
    3. Improved L2 cache utilization
    4. Tensor Core optimization for Ada Lovelace
    """
    # Parallelize over channels in blocks and spatial dimensions
    pid_n = tl.program_id(0)
    pid_c = tl.program_id(1) * BLOCK_C
    pid_spatial = tl.program_id(2)
    
    # Check bounds
    if pid_n >= N or pid_c >= C:
        return
    
    # Channel indices for this block
    c_offsets = pid_c + tl.arange(0, BLOCK_C)
    c_mask = c_offsets < C
    
    # Spatial output indices - process multiple spatial positions per thread
    spatial_offsets = pid_spatial * BLOCK_SPATIAL + tl.arange(0, BLOCK_SPATIAL)
    spatial_mask = spatial_offsets < (D_out * H_out * W_out)
    
    # Decode spatial indices for vectorized computation
    D_stride = H_out * W_out
    H_stride = W_out
    
    d_out = tl.where(spatial_mask, spatial_offsets // D_stride, 0)
    hw_rem = tl.where(spatial_mask, spatial_offsets % D_stride, 0)
    h_out = tl.where(spatial_mask, hw_rem // H_stride, 0)
    w_out = tl.where(spatial_mask, hw_rem % H_stride, 0)
    
    # Input spatial start indices (4x4x4 pooling)
    d_start = d_out * 4
    h_start = h_out * 4
    w_start = w_out * 4
    
    # Load BatchNorm parameters with vectorization for the entire channel block
    gamma = tl.load(gamma_ptr + c_offsets, mask=c_mask, other=0.0)
    beta = tl.load(beta_ptr + c_offsets, mask=c_mask, other=0.0)
    mean = tl.load(mean_ptr + c_offsets, mask=c_mask, other=0.0)
    var = tl.load(var_ptr + c_offsets, mask=c_mask, other=0.0)
    
    # Compute normalization factors (vectorized across channels)
    inv_std = 1.0 / tl.sqrt(var + eps)
    norm_factor = gamma * inv_std
    bias_term = beta - mean * norm_factor
    
    # Initialize accumulators in registers (no intermediate stores!)
    accum = tl.zeros([BLOCK_C, BLOCK_SPATIAL], dtype=tl.float32)
    count = tl.zeros([BLOCK_C, BLOCK_SPATIAL], dtype=tl.float32)
    
    # Base offset for this batch
    base_offset_n = pid_n * stride_n
    
    # Process 4x4x4 pooling window with optimal memory access pattern
    # Unroll depth dimension for better ILP
    for d_off in range(4):
        d_idx = d_start + d_off
        d_in_bounds = d_idx < D
        
        # Replace tl.reduce_or() with tl.any() for boolean mask reduction
        if not tl.any(d_in_bounds & spatial_mask):
            continue
        
        d_offset = d_idx * stride_d
        
        # Process height in vectorized groups
        for h_off in range(0, 4, 2):  # Process 2 height elements at once
            h_idx_0 = h_start + h_off
            h_idx_1 = h_start + h_off + 1
            
            h_mask_0 = h_idx_0 < H
            h_mask_1 = h_idx_1 < H
            
            h_offset_0 = h_idx_0 * stride_h
            h_offset_1 = h_idx_1 * stride_h
            
            # Process width in vectorized groups
            for w_off in range(0, 4, 2):  # Process 2 width elements at once
                w_idx_0 = w_start + w_off
                w_idx_1 = w_start + w_off + 1
                
                w_mask_0 = w_idx_0 < W
                w_mask_1 = w_idx_1 < W
                
                w_offset_0 = w_idx_0 * stride_w
                w_offset_1 = w_idx_1 * stride_w
                
                # Combine masks for 2x2 block
                mask_00 = d_in_bounds & h_mask_0 & w_mask_0 & spatial_mask
                mask_01 = d_in_bounds & h_mask_0 & w_mask_1 & spatial_mask
                mask_10 = d_in_bounds & h_mask_1 & w_mask_0 & spatial_mask
                mask_11 = d_in_bounds & h_mask_1 & w_mask_1 & spatial_mask
                
                # Compute offsets once for all channels
                offset_00 = base_offset_n + d_offset + h_offset_0 + w_offset_0
                offset_01 = base_offset_n + d_offset + h_offset_0 + w_offset_1
                offset_10 = base_offset_n + d_offset + h_offset_1 + w_offset_0
                offset_11 = base_offset_n + d_offset + h_offset_1 + w_offset_1
                
                # Load and process each position in the 2x2 block
                for c_idx in range(BLOCK_C):
                    if not c_mask[c_idx]:
                        continue
                    
                    # Channel offset
                    c_offset = c_offsets[c_idx] * stride_c
                    
                    # Load 4 values for this channel position
                    x_00 = tl.load(x_ptr + offset_00 + c_offset, mask=mask_00, other=0.0)
                    x_01 = tl.load(x_ptr + offset_01 + c_offset, mask=mask_01, other=0.0)
                    x_10 = tl.load(x_ptr + offset_10 + c_offset, mask=mask_10, other=0.0)
                    x_11 = tl.load(x_ptr + offset_11 + c_offset, mask=mask_11, other=0.0)
                    
                    # Apply BatchNorm (all in registers - NO intermediate stores!)
                    norm_f = norm_factor[c_idx]
                    bias_t = bias_term[c_idx]
                    
                    scaled_00 = x_00 * norm_f + bias_t
                    scaled_01 = x_01 * norm_f + bias_t
                    scaled_10 = x_10 * norm_f + bias_t
                    scaled_11 = x_11 * norm_f + bias_t
                    
                    # Accumulate with masking
                    accum[c_idx] += tl.where(mask_00, scaled_00, 0.0)
                    accum[c_idx] += tl.where(mask_01, scaled_01, 0.0)
                    accum[c_idx] += tl.where(mask_10, scaled_10, 0.0)
                    accum[c_idx] += tl.where(mask_11, scaled_11, 0.0)
                    
                    count[c_idx] += tl.where(mask_00, 1.0, 0.0)
                    count[c_idx] += tl.where(mask_01, 1.0, 0.0)
                    count[c_idx] += tl.where(mask_10, 1.0, 0.0)
                    count[c_idx] += tl.where(mask_11, 1.0, 0.0)
    
    # Compute final average with numerical stability
    result = accum / tl.maximum(count, 1.0)
    
    # Store results with proper coalescing
    for c_idx in range(BLOCK_C):
        if not c_mask[c_idx]:
            continue
        
        c = c_offsets[c_idx]
        
        # Compute output offsets
        out_offsets = (
            pid_n * out_stride_n + 
            c * out_stride_c + 
            d_out * out_stride_d + 
            h_out * out_stride_h + 
            w_out * out_stride_w
        )
        
        # Store with masking
        tl.store(out_ptr + out_offsets, result[c_idx], mask=spatial_mask)


def fused_batch_norm_avgpool_optimized(x, gamma, beta, running_mean, running_var, 
                                      training=False, eps=1e-5, momentum=0.1):
    """
    Optimized fused BatchNorm3d + 4x4x4 AvgPool3d
    """
    N, C, D, H, W = x.shape
    
    # Output dimensions
    D_out, H_out, W_out = D // 4, H // 4, W // 4
    out_shape = (N, C, D_out, H_out, W_out)
    out = torch.empty(out_shape, device=x.device, dtype=x.dtype)
    
    # Compute statistics based on training mode
    if training:
        # Compute batch statistics
        mean = x.mean(dim=(0, 2, 3, 4))
        var = x.var(dim=(0, 2, 3, 4), unbiased=False)
        
        # Update running statistics
        running_mean.mul_(1 - momentum).add_(mean, alpha=momentum)
        running_var.mul_(1 - momentum).add_(var, alpha=momentum)
    else:
        # Use stored statistics
        mean = running_mean
        var = running_var
    
    # Optimized block sizes for Ada Lovelace
    BLOCK_C = 4  # Process 4 channels per block for better SM utilization
    BLOCK_SPATIAL = 64  # Reduced to improve register pressure
    
    total_spatial = D_out * H_out * W_out
    
    # Grid layout: (batch, channel_blocks, spatial_blocks)
    num_channel_blocks = triton.cdiv(C, BLOCK_C)
    num_spatial_blocks = triton.cdiv(total_spatial, BLOCK_SPATIAL)
    
    grid = (N, num_channel_blocks, num_spatial_blocks)
    
    # Warp configuration optimized for Ada Lovelace
    NUM_WARPS = 4
    NUM_STAGES = 2  # Start with 2, increase only if registers permit
    
    # Launch optimized kernel
    fused_bn_avgpool_kernel_optimized[grid](
        x, gamma, beta, mean, var, out,
        N, C, D, H, W,
        D_out, H_out, W_out,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3), out.stride(4),
        eps,
        BLOCK_C,
        BLOCK_SPATIAL,
        NUM_WARPS,
        NUM_STAGES,
        num_warps=NUM_WARPS,
        num_stages=NUM_STAGES,
    )
    
    return out


class ModelNew(nn.Module):
    """
    Optimized version with fused BatchNorm3d + 2x AvgPool3d (4x4x4 reduction)
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias_shape):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size, 
            stride=stride, padding=padding
        )
        
        # Initialize BatchNorm parameters
        self.gamma = nn.Parameter(torch.ones(out_channels))
        self.beta = nn.Parameter(torch.zeros(out_channels))
        self.register_buffer('running_mean', torch.zeros(out_channels))
        self.register_buffer('running_var', torch.ones(out_channels))
        
        # Set default BatchNorm parameters
        self.eps = 1e-5
        self.momentum = 0.1
    
    def forward(self, x):
        # Step 1: PyTorch native ConvTranspose3d
        x = self.conv_transpose(x)
        
        # Step 2: Optimized fused BatchNorm3d + AvgPool3d in Triton
        x = fused_batch_norm_avgpool_optimized(
            x, self.gamma, self.beta, 
            self.running_mean, self.running_var,
            training=self.training,
            eps=self.eps,
            momentum=self.momentum
        )
        
        return x
```