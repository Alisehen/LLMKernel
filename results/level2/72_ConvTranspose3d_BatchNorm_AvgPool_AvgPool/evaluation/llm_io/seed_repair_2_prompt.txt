Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 538, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_114735_batch_range56to80_deepseek_deepseek/72_ConvTranspose3d_BatchNorm_AvgPool_AvgPool/code/kernel_20251214_211413.py", line 209, in forward
    x = fused_batch_norm_avgpool(
        ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_114735_batch_range56to80_deepseek_deepseek/72_ConvTranspose3d_BatchNorm_AvgPool_AvgPool/code/kernel_20251214_211413.py", line 166, in fused_batch_norm_avgpool
    fused_bn_avgpool_kernel[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 278, in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 81, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 71:12:
    count = 0

    # We'll process the 4x4x4 block in smaller chunks to manage register pressure
    for d_off in range(0, 4, BLOCK_D):
        d_idx = d_start + d_off
        d_mask = d_idx < D

        for h_off in range(0, 4, BLOCK_H):
            h_idx = h_start + h_off
            h_mask = h_idx < H

            for w_off in range(0, 4, BLOCK_W):
            ^
AssertionError('Loop-carried variable count has initial type int32 but is re-assigned to fp32 in loop! Please make sure that the type stays consistent.')

Main Critical Problem Analysis:
Problem Analysis (from expert diagnosis):
critical_issue: Loop-carried variable 'count' initialized as int32 then assigned fp32 values in loop, violating Triton's type consistency rule.
why_it_matters: Triton prohibits type changes in loop-carried variables, causing compilation failure at the inner loop assignment.
minimal_fix_hint: Change 'count = 0' to 'count = 0.0' to maintain consistent fp32 type throughout the loop.

Focus your fix on addressing the identified critical issue.


Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def fused_bn_avgpool_kernel(
    # Pointers to tensors
    x_ptr, gamma_ptr, beta_ptr, running_mean_ptr, running_var_ptr, out_ptr,
    # Tensor dimensions
    N, C, D, H, W,
    D_out, H_out, W_out,
    # Strides for input tensor
    stride_n, stride_c, stride_d, stride_h, stride_w,
    # Strides for output tensor
    out_stride_n, out_stride_c, out_stride_d, out_stride_h, out_stride_w,
    # BatchNorm parameters
    eps: tl.constexpr,
    momentum: tl.constexpr,
    training: tl.constexpr,
    # Kernel configuration
    BLOCK_D: tl.constexpr, BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr,
):
    """
    Fused BatchNorm3d + AvgPool3d(kernel_size=2) + AvgPool3d(kernel_size=2) kernel.
    Input: [N, C, D, H, W]
    Output: [N, C, D_out, H_out, W_out] where D_out = D//4, H_out = H//4, W_out = W//4
    """
    # Parallelize over channels and spatial dimensions
    pid_n = tl.program_id(0)  # batch index
    pid_c = tl.program_id(1)  # channel index
    pid_s = tl.program_id(2)  # spatial index (combined D_out * H_out * W_out)
    
    if pid_n >= N or pid_c >= C:
        return
    
    # Decode spatial index
    num_h_out = H_out
    num_w_out = W_out
    d_out = pid_s // (num_h_out * num_w_out)
    hw_rem = pid_s % (num_h_out * num_w_out)
    h_out = hw_rem // num_w_out
    w_out = hw_rem % num_w_out
    
    # Load BatchNorm parameters for this channel
    gamma = tl.load(gamma_ptr + pid_c)
    beta = tl.load(beta_ptr + pid_c)
    
    if training:
        mean = tl.load(running_mean_ptr + pid_c)
        var = tl.load(running_var_ptr + pid_c)
    else:
        mean = tl.load(running_mean_ptr + pid_c)
        var = tl.load(running_var_ptr + pid_c)
    
    # Precompute normalization factor
    inv_std = 1.0 / tl.sqrt(var + eps)
    
    # Input spatial start indices
    d_start = d_out * 4
    h_start = h_out * 4
    w_start = w_out * 4
    
    # Accumulate over 4x4x4 block
    accum = 0.0
    count = 0
    
    # We'll process the 4x4x4 block in smaller chunks to manage register pressure
    for d_off in range(0, 4, BLOCK_D):
        d_idx = d_start + d_off
        d_mask = d_idx < D
        
        for h_off in range(0, 4, BLOCK_H):
            h_idx = h_start + h_off
            h_mask = h_idx < H
            
            for w_off in range(0, 4, BLOCK_W):
                w_idx = w_start + w_off
                w_mask = w_idx < W
                
                # Create mask for all active threads
                mask = d_mask & h_mask & w_mask
                
                # Compute pointer offsets
                base_offset = (
                    pid_n * stride_n + 
                    pid_c * stride_c + 
                    d_idx * stride_d + 
                    h_idx * stride_h + 
                    w_idx * stride_w
                )
                
                # Load input values
                x_val = tl.load(x_ptr + base_offset, mask=mask, other=0.0)
                
                # Apply BatchNorm: y = (x - mean) / sqrt(var + eps) * gamma + beta
                normalized = (x_val - mean) * inv_std
                scaled = normalized * gamma + beta
                
                # Accumulate for average pooling
                accum += tl.where(mask, scaled, 0.0)
                count += tl.where(mask, 1.0, 0.0)
    
    # Compute average (two AvgPool3d with kernel_size=2 is equivalent to one with kernel_size=4)
    # But we need to handle boundary conditions
    result = accum / tl.maximum(count, 1.0)
    
    # Compute output index
    out_offset = (
        pid_n * out_stride_n + 
        pid_c * out_stride_c + 
        d_out * out_stride_d + 
        h_out * out_stride_h + 
        w_out * out_stride_w
    )
    
    # Store result
    tl.store(out_ptr + out_offset, result)


def fused_batch_norm_avgpool(x, gamma, beta, running_mean, running_var, 
                            training=False, eps=1e-5, momentum=0.1):
    """
    Fused BatchNorm3d + AvgPool3d(kernel_size=2) + AvgPool3d(kernel_size=2)
    
    Args:
        x: Input tensor of shape [N, C, D, H, W]
        gamma: Weight parameter of shape [C]
        beta: Bias parameter of shape [C]
        running_mean: Running mean of shape [C]
        running_var: Running variance of shape [C]
        training: Whether in training mode
        eps: Added to denominator for numerical stability
        momentum: Momentum for updating running statistics
    
    Returns:
        Output tensor of shape [N, C, D//4, H//4, W//4]
    """
    N, C, D, H, W = x.shape
    
    # Use floor division for non-divisible dimensions
    D_out, H_out, W_out = D // 4, H // 4, W // 4
    out_shape = (N, C, D_out, H_out, W_out)
    out = torch.empty(out_shape, device=x.device, dtype=x.dtype)
    
    # For now, we'll update running statistics in PyTorch for simplicity
    # In a full implementation, you'd compute statistics in Triton
    if training:
        # Compute mean and variance per channel
        mean = x.mean(dim=(0, 2, 3, 4))
        var = x.var(dim=(0, 2, 3, 4), unbiased=False)
        
        # Update running statistics
        running_mean.mul_(1 - momentum).add_(mean, alpha=momentum)
        running_var.mul_(1 - momentum).add_(var, alpha=momentum)
    else:
        # Use stored statistics
        mean = running_mean
        var = running_var
    
    # Grid configuration
    grid = (N, C, D_out * H_out * W_out)
    
    # Launch kernel
    fused_bn_avgpool_kernel[grid](
        x, gamma, beta, running_mean, running_var, out,
        N, C, D, H, W,
        D_out, H_out, W_out,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3), out.stride(4),
        eps, 0.1, training,
        BLOCK_D=2, BLOCK_H=2, BLOCK_W=2,
    )
    
    return out


class ModelNew(nn.Module):
    """
    ConvTranspose3d (PyTorch native) + Fused BatchNorm3d + 2x AvgPool3d (Triton fused)
    
    Note: ConvTranspose3d has complex index mapping - keep it in PyTorch.
    Only fuse the subsequent operations (BatchNorm and two average pools) in Triton.
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias_shape):
        super(ModelNew, self).__init__()
        # Keep ConvTranspose3d as PyTorch native - DO NOT reimplement in Triton
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size, 
            stride=stride, padding=padding
        )
        
        # Initialize BatchNorm parameters
        self.gamma = nn.Parameter(torch.ones(out_channels))
        self.beta = nn.Parameter(torch.zeros(out_channels))
        self.register_buffer('running_mean', torch.zeros(out_channels))
        self.register_buffer('running_var', torch.ones(out_channels))
        
        # Set default BatchNorm parameters
        self.eps = 1e-5
        self.momentum = 0.1
    
    def forward(self, x):
        # Step 1: PyTorch native ConvTranspose3d
        x = self.conv_transpose(x)
        
        # Step 2: Fused BatchNorm3d + AvgPool3d + AvgPool3d in Triton
        x = fused_batch_norm_avgpool(
            x, self.gamma, self.beta, 
            self.running_mean, self.running_var,
            training=self.training,
            eps=self.eps,
            momentum=self.momentum
        )
        
        return x
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl, AND any other modules used (e.g., import math if using math.sqrt)
   2. @triton.jit decorated kernel function(s) — NO continue/break/return inside loops (use masking)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels — THIS CLASS IS REQUIRED
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes
4. Ensure ALL imports are included at the top (common mistake: forgetting `import math`)

```python
# <corrected code>
```
