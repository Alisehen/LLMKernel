You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU: 4090

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Memory optimization and final parameter tuning.

Metrics:
- dram__throughput.avg.pct_of_peak_sustained_elapsed
- lts__t_sector_hit_rate.pct
- smsp__warp_issue_stalled_memory_dependency_per_warp_active.pct (<20%)
- sm__warps_active.avg.pct_of_peak_sustained_active

Parameters to tune:
- num_stages ∈ {2, 3, 4}
- num_warps ∈ {4, 8} (based on occupancy)

Rules:
- Increase num_stages only if memory stalls > 20%
- Change num_warps only if occupancy suggests it
- Larger BLOCK_K improves reuse but increases register pressure
- Do NOT modify grid or BLOCK sizes (fixed in earlier stages)
- Do not rewrite access patterns without metric evidence

Autotune:
- Max 3-4 configs combining num_stages and num_warps
- Always include original config as baseline
- Revert if gain < 2% or unstable



[CURRENT CODE]
```python
import torch, torch.nn as nn, triton, triton.language as tl


@triton.jit
def bias_add_sum_kernel(
    x_ptr,           # *f32, shape [B, C]
    bias_ptr,        # *f32, shape [C]
    out_ptr,         # *f32, shape [B]
    B,               # int32
    C,               # int32
    stride_x_batch,  # int32
    stride_x_channel,# int32
    stride_bias,     # int32
    stride_out_batch,# int32
    BLOCK_C: tl.constexpr,
):
    pid_b = tl.program_id(axis=0)  # batch index
    pid_blk = tl.program_id(axis=1)  # channel block index

    block_start = pid_blk * BLOCK_C
    offs_c = block_start + tl.arange(0, BLOCK_C)

    # masks
    mask_valid_b = pid_b < B
    mask_c = offs_c < C
    mask_x = mask_c & mask_valid_b

    # base pointers
    x_batch_ptr = x_ptr + pid_b * stride_x_batch

    # load x and bias
    x_vals = tl.load(
        x_batch_ptr + offs_c * stride_x_channel,
        mask=mask_x,
        other=0.0,
    )
    bias_vals = tl.load(
        bias_ptr + offs_c * stride_bias,
        mask=mask_c,
        other=0.0,
    )

    vals = x_vals + bias_vals
    partial = tl.sum(vals, axis=0)

    # accumulate into output with atomic add
    tl.atomic_add(
        out_ptr + pid_b * stride_out_batch,
        partial,
        mask=mask_valid_b,
    )


def triton_bias_add_sum(x: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:
    """
    x:    [B, C, 1, 1, 1]  (CUDA)
    bias: [C, 1, 1, 1]     (CUDA)

    Returns:
        out: [B, 1, 1, 1] where
             out[b] = sum_c (x[b, c, 0, 0, 0] + bias[c, 0, 0, 0])
    """
    assert x.is_cuda and bias.is_cuda, "triton_bias_add_sum requires CUDA tensors"
    assert x.ndim == 5 and bias.ndim == 4, "Unexpected tensor ranks"
    B, C, D, H, W = x.shape
    assert D == 1 and H == 1 and W == 1, "triton_bias_add_sum expects D=H=W=1"
    assert bias.shape[0] == C, "Bias channels must match x channels"

    x2 = x.contiguous().view(B, C)
    bias2 = bias.contiguous().view(C)

    out = torch.zeros(B, device=x.device, dtype=x.dtype)

    stride_x_batch, stride_x_channel = x2.stride()
    stride_bias = bias2.stride()[0]
    stride_out_batch = out.stride()[0]

    BLOCK_C = 128  # power-of-2 block size

    grid = lambda meta: (
        max(1, B),
        triton.cdiv(C, meta["BLOCK_C"]),
    )

    bias_add_sum_kernel[grid](
        x2,
        bias2,
        out,
        B,
        C,
        stride_x_batch,
        stride_x_channel,
        stride_bias,
        stride_out_batch,
        BLOCK_C=BLOCK_C,
    )

    return out.view(B, 1, 1, 1)


class ModelNew(nn.Module):
    """
    Model that performs a 3D convolution, divides by a constant, applies max pooling,
    global average pooling, adds a bias term, and sums along a specific dimension.

    The bias-add + sum(dim=1) stage is implemented with a Triton kernel when possible.
    """
    def __init__(self, in_channels, out_channels, kernel_size, divisor, pool_size, bias_shape, sum_dim):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.divisor = divisor
        self.max_pool = nn.MaxPool3d(pool_size)
        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.sum_dim = sum_dim

    def forward(self, x):
        x = self.conv(x)
        x = x / self.divisor
        x = self.max_pool(x)
        x = self.global_avg_pool(x)

        # Use Triton kernel only in the specific case it is written for:
        #   - CUDA tensors
        #   - 5D tensor with D=H=W=1 after global average pooling
        #   - sum along channel dimension (dim=1)
        if (
            x.is_cuda
            and x.ndim == 5
            and x.shape[2:] == (1, 1, 1)
            and self.bias.is_cuda
            and self.sum_dim == 1
        ):
            x = triton_bias_add_sum(x, self.bias)
        else:
            x = x + self.bias
            x = torch.sum(x, dim=self.sum_dim)

        return x
```

[NCU PROFILING METRICS]
No NCU metrics available

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

## CRITICAL — Code MUST compile and run:
1. EVERY kernel function MUST have `@triton.jit` decorator
2. Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
3. BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
4. `tl.program_id(axis)` only supports axis = 0, 1, 2
5. No `continue`, `break`, `return` inside loops — use masking
6. No tensor indexing with loop vars: `x[:, i]` is INVALID
7. mask shape MUST match data shape in tl.load/tl.store

## Missing Triton Functions (implement manually):
- tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

## OUTPUT FORMAT (STRICT):
1. Imports: torch, torch.nn, triton, triton.language as tl
2. @triton.jit decorated kernel function(s)
3. Wrapper function(s) for grid calculation and kernel launch
4. class ModelNew(nn.Module) that calls your kernels

Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
