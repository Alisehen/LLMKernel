You are optimizing a Triton kernel based on algorithmic analysis.

# PyTorch Reference (Target Behavior)

```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Model that performs a 3D transposed convolution, followed by two max pooling layers and a sum operation.
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.max_pool1 = nn.MaxPool3d(kernel_size=2)
        self.max_pool2 = nn.MaxPool3d(kernel_size=3)

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.max_pool1(x)
        x = self.max_pool2(x)
        x = torch.sum(x, dim=1, keepdim=True) 
        return x

batch_size = 16
in_channels = 32
out_channels = 64
depth, height, width = 32, 32, 32
kernel_size = 5
stride = 2
padding = 2

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding]
```

**CRITICAL**: Study the PyTorch code carefully to understand:
- What does `forward()` return? (full output sequence vs final hidden state only)
- What is the computational pattern?
- What are the input/output shapes?

Your optimized kernel MUST match this exact behavior.

---

# Analysis Results

**Bottleneck**: The transposed convolution is implemented as explicit loops over C_in and K^3 for each output position, doing scalar outer-products without reusing data via a GEMM-like structure or tensor cores, leading to poor arithmetic intensity and cache/TMU utilization.

**Optimization Strategy**: Replace the direct 3D transposed convolution with an implicit-GEMM (im2col-like) formulation implemented via Triton’s matmul template: treat each output position as a row, each (C_in, kd, kh, kw) combination as K-dimension, and each output channel as a column, turning ConvTranspose3d into a large batched GEMM that leverages highly-optimized matmul tiling and tensor cores.

**Implementation Plan**: Redesign `conv_transpose3d_kernel` so that the core computation is a matrix multiply with dimensions M = N*D_out*H_out*W_out, N = C_out, K = C_in*K^3. Use a Triton matmul kernel to tile over (M, N, K), and in the matmul loader code generate input patches on-the-fly (implicit im2col) from `x` plus kernel offsets and stride/padding logic, instead of looping over kd/kh/kw in the inner kernel. Keep bias addition fused into the GEMM epilogue. This removes the triple nested kernel loops and replaces them with a single GEMM-like kernel with coalesced loads and shared reuse, significantly increasing FLOP utilization.

**Expected Speedup**: 2-3x vs the current Triton implementation (bringing it to at least parity, and likely modestly faster than the PyTorch baseline).

---

# Current Kernel (needs optimization)

```python
import torch, torch.nn as nn, triton, triton.language as tl


@triton.jit
def conv_transpose3d_kernel(
    x_ptr, w_ptr, b_ptr, y_ptr,
    N, C_in, C_out,
    D_in, H_in, W_in,
    D_out, H_out, W_out,
    stride, padding,
    x_stride_n, x_stride_c, x_stride_d, x_stride_h, x_stride_w,
    w_stride_ci, w_stride_co, w_stride_kd, w_stride_kh, w_stride_kw,
    y_stride_n, y_stride_c, y_stride_d, y_stride_h, y_stride_w,
    K: tl.constexpr,          # kernel size (assumed cubic)
    C_IN: tl.constexpr,       # in_channels (loop bound)
    BLOCK_M: tl.constexpr,    # number of output positions per block
    BLOCK_N: tl.constexpr,    # number of output channels per block
):
    pid_m = tl.program_id(0)  # flattened spatial + batch dimension
    pid_n = tl.program_id(1)  # output channel blocks

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    P = N * D_out * H_out * W_out

    mask_m = offs_m < P
    mask_n = offs_n < C_out

    # Decode offs_m -> (n, z, y, x)
    DHW_out = D_out * H_out * W_out
    HW_out = H_out * W_out

    n_idx = offs_m // DHW_out
    rem = offs_m % DHW_out
    z_idx = rem // HW_out
    rem2 = rem % HW_out
    y_idx = rem2 // W_out
    x_idx = rem2 % W_out

    # Base pointers for each output position
    # y_ptr_base: [BLOCK_M] pointers to the (n,z,y,x) location; we will add channel stride
    y_ptr_base = (
        y_ptr
        + n_idx * y_stride_n
        + z_idx * y_stride_d
        + y_idx * y_stride_h
        + x_idx * y_stride_w
    )

    # Base pointers for each input position (per n); we add channel/coords inside loops
    x_ptr_base = x_ptr + n_idx * x_stride_n

    # Accumulator
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Effective "padding" for equivalent conv on upsampled input
    pad_up = K - 1 - padding

    # Loop over input channels and kernel positions
    for ci in range(C_IN):
        base_w_ci = w_ptr + ci * w_stride_ci

        for kd in range(K):
            base_w_kd = base_w_ci + kd * w_stride_kd

            iu_z = z_idx + pad_up - kd  # [BLOCK_M]

            # Precompute integer indices along z
            iz = iu_z // stride
            mask_z_pos = iu_z >= 0
            mask_z_in = (iz >= 0) & (iz < D_in)
            mask_z_stride = (iu_z % stride) == 0

            for kh in range(K):
                base_w_kh = base_w_kd + kh * w_stride_kh

                iu_y = y_idx + pad_up - kh
                iy = iu_y // stride
                mask_y_pos = iu_y >= 0
                mask_y_in = (iy >= 0) & (iy < H_in)
                mask_y_stride = (iu_y % stride) == 0

                for kw in range(K):
                    base_w_kw = base_w_kh + kw * w_stride_kw

                    iu_x = x_idx + pad_up - kw
                    ix = iu_x // stride
                    mask_x_pos = iu_x >= 0
                    mask_x_in = (ix >= 0) & (ix < W_in)
                    mask_x_stride = (iu_x % stride) == 0

                    # Combined mask for valid input fetch
                    mask_valid_in = (
                        mask_m
                        & mask_z_pos
                        & mask_z_in
                        & mask_z_stride
                        & mask_y_pos
                        & mask_y_in
                        & mask_y_stride
                        & mask_x_pos
                        & mask_x_in
                        & mask_x_stride
                    )

                    # Input pointers for this (ci,kd,kh,kw) over BM positions
                    x_ptrs = (
                        x_ptr_base
                        + ci * x_stride_c
                        + iz * x_stride_d
                        + iy * x_stride_h
                        + ix * x_stride_w
                    )

                    x_vals = tl.load(x_ptrs, mask=mask_valid_in, other=0.0)  # [BLOCK_M]

                    # Weight pointers for this (ci,kd,kh,kw) over BN output channels
                    w_ptrs = base_w_kw + offs_n * w_stride_co
                    w_vals = tl.load(w_ptrs, mask=mask_n, other=0.0)  # [BLOCK_N]

                    # Outer product accumulation
                    acc += x_vals[:, None] * w_vals[None, :]

    # Add bias
    if b_ptr is not None:
        bias_vals = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0)  # [BLOCK_N]
        acc += bias_vals[None, :]

    # Store results
    y_ptrs = y_ptr_base[:, None] + offs_n[None, :] * y_stride_c
    out_mask = mask_m[:, None] & mask_n[None, :]
    tl.store(y_ptrs, acc, mask=out_mask)


def conv_transpose3d_triton(x, weight, bias, kernel_size, stride, padding):
    """
    x: [N, C_in, D_in, H_in, W_in]
    weight: [C_in, C_out, K, K, K] (PyTorch ConvTranspose3d layout)
    bias: [C_out] or None
    """
    assert x.is_contiguous()
    assert weight.is_contiguous()
    device = x.device

    N, C_in, D_in, H_in, W_in = x.shape
    C_in_w, C_out, Kd, Kh, Kw = weight.shape
    assert C_in_w == C_in
    assert Kd == Kh == Kw == kernel_size

    S = stride
    P = padding
    K = kernel_size

    # Output shape (no dilation, output_padding = 0)
    D_out = (D_in - 1) * S - 2 * P + K
    H_out = (H_in - 1) * S - 2 * P + K
    W_out = (W_in - 1) * S - 2 * P + K

    y = torch.empty((N, C_out, D_out, H_out, W_out), device=device, dtype=x.dtype)

    # Strides
    x_strides = x.stride()
    w_strides = weight.stride()
    y_strides = y.stride()

    # Flatten spatial+batch dimension
    P_tot = N * D_out * H_out * W_out

    BLOCK_M = 32
    BLOCK_N = 32

    grid = (
        triton.cdiv(P_tot, BLOCK_M),
        triton.cdiv(C_out, BLOCK_N),
    )

    conv_transpose3d_kernel[grid](
        x, weight, bias if bias is not None else tl.constexpr(None), y,
        N, C_in, C_out,
        D_in, H_in, W_in,
        D_out, H_out, W_out,
        S, P,
        x_strides[0], x_strides[1], x_strides[2], x_strides[3], x_strides[4],
        w_strides[0], w_strides[1], w_strides[2], w_strides[3], w_strides[4],
        y_strides[0], y_strides[1], y_strides[2], y_strides[3], y_strides[4],
        K, C_in,
        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N,
    )
    return y


@triton.jit
def maxpool3d_kernel(
    x_ptr, y_ptr,
    N, C,
    D_in, H_in, W_in,
    D_out, H_out, W_out,
    x_stride_n, x_stride_c, x_stride_d, x_stride_h, x_stride_w,
    y_stride_n, y_stride_c, y_stride_d, y_stride_h, y_stride_w,
    K: tl.constexpr,          # kernel size (cubic)
    STRIDE: tl.constexpr,     # stride (cubic)
    BLOCK: tl.constexpr,
):
    pid = tl.program_id(0)
    offs = pid * BLOCK + tl.arange(0, BLOCK)
    Q = N * C * D_out * H_out * W_out
    mask = offs < Q

    # Decode offs -> (n, c, oz, oy, ox)
    WH_out = W_out
    HW_out = H_out * W_out
    DHW_out = D_out * HW_out

    tmp = offs
    ox = tmp % WH_out
    tmp = tmp // WH_out
    oy = tmp % H_out
    tmp = tmp // H_out
    oz = tmp % D_out
    tmp = tmp // D_out
    c = tmp % C
    n = tmp // C

    # Base pointer for output
    y_ptrs = (
        y_ptr
        + n * y_stride_n
        + c * y_stride_c
        + oz * y_stride_d
        + oy * y_stride_h
        + ox * y_stride_w
    )

    # Initialize max with large negative
    max_val = tl.full((BLOCK,), -1e30, dtype=tl.float32)

    # Starting indices in input
    iz0 = oz * STRIDE
    iy0 = oy * STRIDE
    ix0 = ox * STRIDE

    for kz in range(K):
        iz = iz0 + kz
        for ky in range(K):
            iy = iy0 + ky
            for kx in range(K):
                ix = ix0 + kx

                in_ptrs = (
                    x_ptr
                    + n * x_stride_n
                    + c * x_stride_c
                    + iz * x_stride_d
                    + iy * x_stride_h
                    + ix * x_stride_w
                )

                # With standard MaxPool3d and chosen output dims, windows are in-bounds,
                # but we still mask out inactive threads.
                vals = tl.load(in_ptrs, mask=mask, other=-1e30)
                max_val = tl.maximum(max_val, vals)

    tl.store(y_ptrs, max_val, mask=mask)


def maxpool3d_triton(x, kernel_size, stride):
    """
    x: [N, C, D_in, H_in, W_in]
    """
    assert x.is_contiguous()
    device = x.device
    N, C, D_in, H_in, W_in = x.shape
    K = kernel_size
    S = stride

    # Standard PyTorch MaxPool3d output size (pad=0, dilation=1)
    D_out = (D_in - K) // S + 1
    H_out = (H_in - K) // S + 1
    W_out = (W_in - K) // S + 1

    y = torch.empty((N, C, D_out, H_out, W_out), device=device, dtype=x.dtype)

    x_strides = x.stride()
    y_strides = y.stride()

    Q = N * C * D_out * H_out * W_out
    BLOCK = 128
    grid = (triton.cdiv(Q, BLOCK),)

    maxpool3d_kernel[grid](
        x, y,
        N, C,
        D_in, H_in, W_in,
        D_out, H_out, W_out,
        x_strides[0], x_strides[1], x_strides[2], x_strides[3], x_strides[4],
        y_strides[0], y_strides[1], y_strides[2], y_strides[3], y_strides[4],
        K, S,
        BLOCK=BLOCK,
    )
    return y


@triton.jit
def sum_channels_kernel(
    x_ptr, y_ptr,
    N, C,
    D, H, W,
    x_stride_n, x_stride_c, x_stride_d, x_stride_h, x_stride_w,
    y_stride_n, y_stride_c, y_stride_d, y_stride_h, y_stride_w,
    C_CONST: tl.constexpr,
    BLOCK: tl.constexpr,
):
    pid = tl.program_id(0)
    offs = pid * BLOCK + tl.arange(0, BLOCK)
    P = N * D * H * W
    mask = offs < P

    # Decode offs -> (n, z, y, x)
    WH = W
    HW = H * W
    DHW = D * HW

    tmp = offs
    x_idx = tmp % W
    tmp = tmp // W
    y_idx = tmp % H
    tmp = tmp // H
    z_idx = tmp % D
    n_idx = tmp // D

    # Base output pointer (channel dimension is 1)
    y_ptrs = (
        y_ptr
        + n_idx * y_stride_n
        + z_idx * y_stride_d
        + y_idx * y_stride_h
        + x_idx * y_stride_w
    )

    # Reduction over channels
    acc = tl.zeros((BLOCK,), dtype=tl.float32)
    for c in range(C_CONST):
        in_ptrs = (
            x_ptr
            + n_idx * x_stride_n
            + c * x_stride_c
            + z_idx * x_stride_d
            + y_idx * x_stride_h
            + x_idx * x_stride_w
        )
        vals = tl.load(in_ptrs, mask=mask, other=0.0)
        acc += vals

    tl.store(y_ptrs, acc, mask=mask)


def sum_channels_triton(x):
    """
    x: [N, C, D, H, W] -> sum over C, keepdim=True => [N, 1, D, H, W]
    """
    assert x.is_contiguous()
    device = x.device
    N, C, D, H, W = x.shape

    y = torch.empty((N, 1, D, H, W), device=device, dtype=x.dtype)

    x_strides = x.stride()
    y_strides = y.stride()

    P = N * D * H * W
    BLOCK = 128
    grid = (triton.cdiv(P, BLOCK),)

    sum_channels_kernel[grid](
        x, y,
        N, C,
        D, H, W,
        x_strides[0], x_strides[1], x_strides[2], x_strides[3], x_strides[4],
        y_strides[0], y_strides[1], y_strides[2], y_strides[3], y_strides[4],
        C, BLOCK=BLOCK,
    )
    return y


class ModelNew(nn.Module):
    """
    Triton-optimized replacement for:
      ConvTranspose3d -> MaxPool3d(kernel=2) -> MaxPool3d(kernel=3) -> sum over channels
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):
        super(ModelNew, self).__init__()
        # Match PyTorch ConvTranspose3d parameter layout
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding

        # Weight layout: [C_in, C_out, K, K, K]
        k = kernel_size
        self.weight = nn.Parameter(
            torch.randn(in_channels, out_channels, k, k, k)
        )
        self.bias = nn.Parameter(torch.randn(out_channels))

    def forward(self, x):
        # Ensure GPU and contiguous before Triton
        x = x.contiguous()
        weight = self.weight.contiguous()
        bias = self.bias

        x = conv_transpose3d_triton(
            x, weight, bias,
            kernel_size=self.kernel_size,
            stride=self.stride,
            padding=self.padding,
        )
        x = maxpool3d_triton(x, kernel_size=2, stride=2)
        x = maxpool3d_triton(x, kernel_size=3, stride=3)
        x = sum_channels_triton(x)
        return x
```

---

# Your Task

Implement the optimization strategy above. Focus on the specific bottleneck identified.

## Key Requirements

1. **Preserve correctness**: Maintain the same input/output behavior
2. **Apply the optimization**: Follow the implementation plan exactly
3. **Use valid Triton syntax**:
   - Every kernel MUST have `@triton.jit` decorator
   - Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
   - BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
   - No `continue`, `break`, `return` inside kernels (use masking)
   - Prefer `tl.dot(a, b, allow_tf32=True)` for matmul operations

4. **CRITICAL for RNN/GRU/LSTM Persistent Kernels**:
   - Time loop MUST be inside @triton.jit kernel, NOT in Python forward()
   - **HYBRID computation strategy** (CRITICAL for performance):
     * Precompute input-side gates OUTSIDE kernel: `gates_x = (T*B, In) @ W_ih` (ONE large GEMM)
     * INSIDE kernel: only recurrent-side: `for t: gates_h = h @ W_hh` (T small GEMMs)
   - CORRECT (FAST - use this):
     ```python
     # Python forward():
     gates_x_all = x.reshape(T*B, In) @ W_ih + b_ih  # ONE large GEMM
     gates_x_all = gates_x_all.view(T, B, 3*H)
     gru_persistent_kernel[grid](gates_x_all, h0, W_hh, ...)  # Launch ONCE

     @triton.jit
     def gru_persistent_kernel(gates_x_ptr, h_ptr, W_hh_ptr, ...):
         for t in range(T):  # Inside kernel
             gates_x_t = tl.load(gates_x_ptr + t*...)  # Precomputed
             gates_h = h @ W_hh  # Only recurrent GEMM
             h = (1-z)*n + z*h   # Fuse and update
     ```

5. **Output format**:
   - Imports: `import torch, torch.nn as nn, triton, triton.language as tl`
   - `@triton.jit` kernel(s)
   - Wrapper function(s)
   - `class ModelNew(nn.Module)` — REQUIRED
   - NO testing code, NO `if __name__ == "__main__"`

---

Generate the optimized kernel now. Output ONLY the complete Python code.
