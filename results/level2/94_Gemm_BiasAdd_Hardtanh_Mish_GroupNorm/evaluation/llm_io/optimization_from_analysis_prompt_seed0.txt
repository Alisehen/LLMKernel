You are optimizing a Triton kernel based on algorithmic analysis.

# PyTorch Reference (Target Behavior)

```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    A model that performs a GEMM, BiasAdd, Hardtanh, Mish, and GroupNorm operations in sequence.
    """
    def __init__(self, in_features, out_features, bias_shape, num_groups):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.hardtanh = nn.Hardtanh()
        self.mish = nn.Mish()
        self.groupnorm = nn.GroupNorm(num_groups=num_groups, num_channels=out_features)

    def forward(self, x):
        """
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_features).
        """
        x = self.gemm(x)
        x = x + self.bias
        x = self.hardtanh(x)
        x = self.mish(x)
        x = self.groupnorm(x)
        return x


batch_size = 1024
in_features = 8192
out_features = 8192
bias_shape = (out_features,)
num_groups = 256

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, bias_shape, num_groups]
```

**CRITICAL**: Study the PyTorch code carefully to understand:
- What does `forward()` return? (full output sequence vs final hidden state only)
- What is the computational pattern?
- What are the input/output shapes?

Your optimized kernel MUST match this exact behavior.

---

# Analysis Results

**Bottleneck**: The main inefficiency is that GroupNorm is implemented as a separate kernel after fused GEMM+Bias+Hardtanh+Mish, forcing an additional full pass over the activation tensor and an extra kernel launch, while the channel grouping matches the GEMM output layout and could be handled in the same kernel.

**Optimization Strategy**: Fuse GroupNorm into the existing fused_linear_bias_hardtanh_mish kernel so that, for each (batch, group) tile of the GEMM output, the kernel directly computes group-wise mean/variance and applies the affine transform before writing to global memory, eliminating the intermediate write/read and the second kernel launch.

**Implementation Plan**: Retile the GEMM kernel so BLOCK_N is an integer multiple of group_size (here 32) and aligns tile boundaries with groups, then, after finishing the accumulation for a [BLOCK_M, BLOCK_N] tile, perform an in-kernel two-pass reduction over each (row, group) segment: first compute mean and variance for each group within the tile, then normalize and apply gn_weight/gn_bias to the tile’s values. Replace the separate triton_groupnorm call in ModelNew.forward with this extended fused kernel, passing in the GroupNorm parameters and removing the extra allocation for the intermediate x tensor.

**Expected Speedup**: 20-30% over the current Triton implementation (bringing it closer to or slightly above the PyTorch baseline), mainly from removing one full read/write of the [B, O] tensor and a kernel launch, while retaining fusion of all pointwise ops.

---

# Current Kernel (needs optimization)

```python
# complete ModelNew code with optimized Triton kernels
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def fused_linear_bias_hardtanh_mish_kernel(
    a_ptr, b_ptr, bias_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    # 2D tiling over output matrix C[M, N]
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # GEMM
    for k in range(0, K, BLOCK_K):
        k_remaining = K - k
        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < k_remaining)
        b_mask = (offs_k[:, None] < k_remaining) & (offs_n[None, :] < N)

        a = tl.load(a_ptrs, mask=a_mask, other=0.0)
        b = tl.load(b_ptrs, mask=b_mask, other=0.0)
        acc += tl.dot(a, b, allow_tf32=True)

        a_ptrs += BLOCK_K * stride_ak
        b_ptrs += BLOCK_K * stride_bk

    # Add bias: [N]
    bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)
    acc += bias[None, :]

    # Hardtanh: clamp between -1 and 1
    acc = tl.minimum(tl.maximum(acc, -1.0), 1.0)

    # Mish: x * tanh(softplus(x)), softplus(x) = log(1 + exp(x))
    # Numerically-stable-ish but simple implementation
    softplus = tl.log(1.0 + tl.exp(acc))
    two_sp = 2.0 * softplus
    t = tl.exp(two_sp)
    tanh_sp = (t - 1.0) / (t + 1.0)
    acc = acc * tanh_sp

    # Store result
    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(c_ptrs, acc, mask=out_mask)


def fused_linear_bias_hardtanh_mish(x, weight, bias):
    """
    x:      [B, I]
    weight: [O, I]
    bias:   [O]
    returns: [B, O]
    """
    assert x.is_cuda and weight.is_cuda and bias.is_cuda
    B, I = x.shape
    O = weight.shape[0]

    # Use W^T as [I, O] for GEMM
    w_t = weight.t().contiguous()
    out = torch.empty((B, O), device=x.device, dtype=x.dtype)

    def grid(META):
        return (
            max(1, triton.cdiv(B, META['BLOCK_M'])),
            max(1, triton.cdiv(O, META['BLOCK_N'])),
        )

    fused_linear_bias_hardtanh_mish_kernel[grid](
        x, w_t, bias, out,
        B, O, I,
        x.stride(0), x.stride(1),
        w_t.stride(0), w_t.stride(1),
        out.stride(0), out.stride(1),
        BLOCK_M=128, BLOCK_N=128, BLOCK_K=32,
    )
    return out


@triton.jit
def groupnorm_kernel(
    x_ptr, weight_ptr, bias_ptr, y_ptr,
    B, C, G, group_size, eps,
    stride_xn, stride_xc, stride_yn, stride_yc,
    BLOCK_C: tl.constexpr,
):
    # One program per (batch, group) pair
    pid = tl.program_id(0)
    n = pid // G
    g = pid % G

    # Optional safety guard if grid is over-provisioned (no loops here, allowed)
    if n >= B:
        return

    offs_c = tl.arange(0, BLOCK_C)

    # First pass: compute mean and variance for this (n, g)
    sum_val = 0.0
    sum_sq = 0.0

    for c0 in range(0, group_size, BLOCK_C):
        rel_c = c0 + offs_c          # [BLOCK_C]
        mask = rel_c < group_size    # [BLOCK_C]
        c_idx = g * group_size + rel_c  # [BLOCK_C] -> channel indices in this group

        x_ptrs = x_ptr + n * stride_xn + c_idx * stride_xc
        x = tl.load(x_ptrs, mask=mask, other=0.0)

        # Aggregate into scalars
        sum_val += tl.sum(x, axis=0)
        sum_sq += tl.sum(x * x, axis=0)

    # Use group_size directly; Triton handles int->float promotion
    mean = sum_val / group_size
    var = sum_sq / group_size - mean * mean
    inv_std = 1.0 / tl.sqrt(var + eps)

    # Second pass: normalize and apply affine
    for c0 in range(0, group_size, BLOCK_C):
        rel_c = c0 + offs_c
        mask = rel_c < group_size
        c_idx = g * group_size + rel_c

        x_ptrs = x_ptr + n * stride_xn + c_idx * stride_xc
        x = tl.load(x_ptrs, mask=mask, other=0.0)

        gamma = tl.load(weight_ptr + c_idx, mask=mask, other=1.0)
        beta = tl.load(bias_ptr + c_idx, mask=mask, other=0.0)

        x_norm = (x - mean) * inv_std
        y = x_norm * gamma + beta

        y_ptrs = y_ptr + n * stride_yn + c_idx * stride_yc
        tl.store(y_ptrs, y, mask=mask)


def triton_groupnorm(x, weight, bias, num_groups, eps=1e-5):
    """
    x: [B, C]
    weight, bias: [C]
    """
    assert x.is_cuda and weight.is_cuda and bias.is_cuda
    B, C = x.shape
    assert C % num_groups == 0
    group_size = C // num_groups

    y = torch.empty_like(x)

    def grid(META):
        # One program per (batch, group) pair
        return (max(1, B * num_groups),)

    groupnorm_kernel[grid](
        x, weight, bias, y,
        B, C, num_groups, group_size, eps,
        x.stride(0), x.stride(1),
        y.stride(0), y.stride(1),
        BLOCK_C=128,
    )
    return y


class ModelNew(nn.Module):
    """
    Triton-optimized version of:
        Linear -> BiasAdd -> Hardtanh -> Mish -> GroupNorm
    """
    def __init__(self, in_features, out_features, bias_shape, num_groups):
        super(ModelNew, self).__init__()
        # Match original Linear parameter shapes
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.bias = nn.Parameter(torch.randn(bias_shape))

        # GroupNorm affine parameters over channels (out_features)
        self.gn_weight = nn.Parameter(torch.ones(out_features))
        self.gn_bias = nn.Parameter(torch.zeros(out_features))
        self.num_groups = num_groups
        self.eps = 1e-5

    def forward(self, x):
        # Move to CUDA if not already (to use Triton kernels)
        if not x.is_cuda:
            x = x.cuda()
        if not self.weight.is_cuda:
            self.weight.data = self.weight.data.cuda()
            self.bias.data = self.bias.data.cuda()
            self.gn_weight.data = self.gn_weight.data.cuda()
            self.gn_bias.data = self.gn_bias.data.cuda()

        x = fused_linear_bias_hardtanh_mish(x, self.weight, self.bias)
        x = triton_groupnorm(x, self.gn_weight, self.gn_bias, self.num_groups, self.eps)
        return x
```

---

# Your Task

Implement the optimization strategy above. Focus on the specific bottleneck identified.

## Key Requirements

1. **Preserve correctness**: Maintain the same input/output behavior
2. **Apply the optimization**: Follow the implementation plan exactly
3. **Use valid Triton syntax**:
   - Every kernel MUST have `@triton.jit` decorator
   - Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
   - BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
   - No `continue`, `break`, `return` inside kernels (use masking)
   - Prefer `tl.dot(a, b, allow_tf32=True)` for matmul operations

4. **CRITICAL for RNN/GRU/LSTM Persistent Kernels**:
   - Time loop MUST be inside @triton.jit kernel, NOT in Python forward()
   - **HYBRID computation strategy** (CRITICAL for performance):
     * Precompute input-side gates OUTSIDE kernel: `gates_x = (T*B, In) @ W_ih` (ONE large GEMM)
     * INSIDE kernel: only recurrent-side: `for t: gates_h = h @ W_hh` (T small GEMMs)
   - CORRECT (FAST - use this):
     ```python
     # Python forward():
     gates_x_all = x.reshape(T*B, In) @ W_ih + b_ih  # ONE large GEMM
     gates_x_all = gates_x_all.view(T, B, 3*H)
     gru_persistent_kernel[grid](gates_x_all, h0, W_hh, ...)  # Launch ONCE

     @triton.jit
     def gru_persistent_kernel(gates_x_ptr, h_ptr, W_hh_ptr, ...):
         for t in range(T):  # Inside kernel
             gates_x_t = tl.load(gates_x_ptr + t*...)  # Precomputed
             gates_h = h @ W_hh  # Only recurrent GEMM
             h = (1-z)*n + z*h   # Fuse and update
     ```

5. **Output format**:
   - Imports: `import torch, torch.nn as nn, triton, triton.language as tl`
   - `@triton.jit` kernel(s)
   - Wrapper function(s)
   - `class ModelNew(nn.Module)` — REQUIRED
   - NO testing code, NO `if __name__ == "__main__"`

---

Generate the optimized kernel now. Output ONLY the complete Python code.
