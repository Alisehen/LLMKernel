You are a GPU kernel optimization architect. Analyze the kernel and identify **ONE high-level algorithmic optimization**.

# PyTorch Reference
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    """
    Model that performs:
    1. Conv3D
    2. HardSwish activation
    3. GroupNorm  
    4. Mean pooling across spatial dimensions
    """
    def __init__(self, in_channels, out_channels, kernel_size, num_groups=4, bias=True):
        super(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias)
        self.group_norm = nn.GroupNorm(num_groups, out_channels)

    def forward(self, x):
        x = self.conv(x)                             # (B, C, D, H, W)
        x = F.hardswish(x)                           # Nonlinear activation
        x = self.group_norm(x)                       # Normalization over channels
        x = torch.mean(x, dim=[2, 3, 4])             # Mean over spatial dims â†’ (B, C)
        return x

# === Test config ===
batch_size = 1024
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 4

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]
```

# Current Triton Kernel
```python
import math
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def conv3d_hswish_kernel(
    x_ptr, w_ptr, b_ptr, y_ptr,
    B, C_in, D, H, W,
    C_out, D_out, H_out, W_out,
    K_total,
    stride_xn, stride_xc, stride_xd, stride_xh, stride_xw,
    stride_wm, stride_wn,
    stride_yn, stride_yc, stride_yd, stride_yh, stride_yw,
    Kd, Kh, Kw,
    BLOCK_CO: tl.constexpr, BLOCK_SP: tl.constexpr, BLOCK_K: tl.constexpr,
):
    pid_b = tl.program_id(0)
    pid_co = tl.program_id(1)
    pid_sp = tl.program_id(2)

    offs_co = pid_co * BLOCK_CO + tl.arange(0, BLOCK_CO)
    offs_sp = pid_sp * BLOCK_SP + tl.arange(0, BLOCK_SP)

    S = D_out * H_out * W_out

    # Compute output spatial indices from flattened index
    w_out_idx = offs_sp % W_out
    tmp = offs_sp // W_out
    h_out_idx = tmp % H_out
    d_out_idx = tmp // H_out

    # Initialize accumulator
    acc = tl.zeros((BLOCK_CO, BLOCK_SP), dtype=tl.float32)

    Kdhw = Kd * Kh * Kw
    KhW = Kh * Kw

    offs_k = tl.arange(0, BLOCK_K)

    for k_start in range(0, K_total, BLOCK_K):
        k_idx = k_start + offs_k  # (BLOCK_K,)
        mask_k = k_idx < K_total

        # Map flattened kernel index to (ci, kd, kh, kw)
        ci = k_idx // Kdhw
        rem1 = k_idx % Kdhw
        kd = rem1 // KhW
        rem2 = rem1 % KhW
        kh = rem2 // Kw
        kw = rem2 % Kw

        # Build pointers for input x: shape (BLOCK_K, BLOCK_SP)
        ptr_x = (
            x_ptr
            + pid_b * stride_xn
            + ci[:, None] * stride_xc
            + (d_out_idx[None, :] + kd[:, None]) * stride_xd
            + (h_out_idx[None, :] + kh[:, None]) * stride_xh
            + (w_out_idx[None, :] + kw[:, None]) * stride_xw
        )

        mask_sp = offs_sp < S
        mask_x = mask_k[:, None] & mask_sp[None, :]

        x = tl.load(ptr_x, mask=mask_x, other=0.0).to(tl.float32)

        # Load weight tile: w_flat is [C_out, K_total]
        ptr_w = w_ptr + offs_co[:, None] * stride_wm + k_idx[None, :] * stride_wn
        mask_w = (offs_co[:, None] < C_out) & mask_k[None, :]
        w = tl.load(ptr_w, mask=mask_w, other=0.0).to(tl.float32)

        acc += tl.dot(w, x, allow_tf32=True)

    # Add bias
    bias = tl.load(b_ptr + offs_co, mask=offs_co < C_out, other=0.0).to(tl.float32)
    acc = acc + bias[:, None]

    # HardSwish: x * relu6(x + 3) / 6
    tmp_hs = acc + 3.0
    tmp_hs = tl.minimum(tl.maximum(tmp_hs, 0.0), 6.0)
    acc = acc * tmp_hs * (1.0 / 6.0)

    # Store result
    ptr_y = (
        y_ptr
        + pid_b * stride_yn
        + offs_co[:, None] * stride_yc
        + d_out_idx[None, :] * stride_yd
        + h_out_idx[None, :] * stride_yh
        + w_out_idx[None, :] * stride_yw
    )
    mask_sp = offs_sp < S
    mask_y = (offs_co[:, None] < C_out) & mask_sp[None, :]
    tl.store(ptr_y, acc.to(tl.float32), mask=mask_y)


def conv3d_hardswish_triton(x, weight, bias):
    """
    Conv3D (valid, stride=1, no padding, no dilation) + HardSwish activation.
    x: [B, C_in, D, H, W]
    weight: [C_out, C_in, Kd, Kh, Kw]
    bias: [C_out] or None
    """
    assert x.is_cuda and weight.is_cuda
    x = x.contiguous()
    weight = weight.contiguous()

    B, C_in, D, H, W = x.shape
    C_out, C_in_w, Kd, Kh, Kw = weight.shape
    assert C_in_w == C_in

    if bias is None:
        bias = torch.zeros(C_out, device=weight.device, dtype=weight.dtype)
    else:
        bias = bias.contiguous()

    D_out = D - Kd + 1
    H_out = H - Kh + 1
    W_out = W - Kw + 1

    y = torch.empty((B, C_out, D_out, H_out, W_out), device=x.device, dtype=x.dtype)

    w_flat = weight.view(C_out, -1).contiguous()
    K_total = w_flat.shape[1]
    S = D_out * H_out * W_out

    def grid(meta):
        return (
            B,
            triton.cdiv(C_out, meta["BLOCK_CO"]),
            triton.cdiv(S, meta["BLOCK_SP"]),
        )

    conv3d_hswish_kernel[grid](
        x, w_flat, bias, y,
        B, C_in, D, H, W,
        C_out, D_out, H_out, W_out,
        K_total,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),
        w_flat.stride(0), w_flat.stride(1),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3), y.stride(4),
        Kd, Kh, Kw,
        BLOCK_CO=16, BLOCK_SP=64, BLOCK_K=32,
    )
    return y


@triton.jit
def groupnorm_mean_kernel(
    x_ptr, gamma_ptr, beta_ptr, out_ptr,
    B, C, D, H, W,
    G, eps,
    stride_xn, stride_xc, stride_xd, stride_xh, stride_xw,
    stride_outn, stride_outc,
    BLOCK_S: tl.constexpr,
):
    """
    Fused GroupNorm + mean pooling over spatial dims.
    Input:  x [B, C, D, H, W]
    Output: out [B, C]  (mean over D,H,W of normalized activations)
    """
    pid = tl.program_id(0)
    # Map program id to (b, g)
    g = pid % G
    b = pid // G

    C_per_group = C // G
    group_c_start = g * C_per_group

    S = D * H * W
    group_count = C_per_group * S

    # Phase 1: compute group mean and variance
    sum_x = 0.0
    sum_x2 = 0.0

    for c_offset in range(0, C_per_group):
        ch = group_c_start + c_offset  # scalar

        for s0 in range(0, S, BLOCK_S):
            offs_s = s0 + tl.arange(0, BLOCK_S)
            mask_s = offs_s < S

            w_idx = offs_s % W
            tmp = offs_s // W
            h_idx = tmp % H
            d_idx = tmp // H

            ptr_x = (
                x_ptr
                + b * stride_xn
                + ch * stride_xc
                + d_idx * stride_xd
                + h_idx * stride_xh
                + w_idx * stride_xw
            )

            x = tl.load(ptr_x, mask=mask_s, other=0.0).to(tl.float32)
            sum_x += tl.sum(x, axis=0)
            sum_x2 += tl.sum(x * x, axis=0)

    mean = sum_x / group_count
    var = sum_x2 / group_count - mean * mean
    inv_std = 1.0 / tl.sqrt(var + eps)

    # Phase 2: compute per-channel pooled normalized outputs
    for c_offset in range(0, C_per_group):
        ch = group_c_start + c_offset

        sum_c = 0.0
        for s0 in range(0, S, BLOCK_S):
            offs_s = s0 + tl.arange(0, BLOCK_S)
            mask_s = offs_s < S

            w_idx = offs_s % W
            tmp = offs_s // W
            h_idx = tmp % H
            d_idx = tmp // H

            ptr_x = (
                x_ptr
                + b * stride_xn
                + ch * stride_xc
                + d_idx * stride_xd
                + h_idx * stride_xh
                + w_idx * stride_xw
            )

            x = tl.load(ptr_x, mask=mask_s, other=0.0).to(tl.float32)
            sum_c += tl.sum(x, axis=0)

        mean_c = sum_c / S
        gamma_c = tl.load(gamma_ptr + ch)
        beta_c = tl.load(beta_ptr + ch)

        out_val = gamma_c * inv_std * (mean_c - mean) + beta_c

        ptr_out = out_ptr + b * stride_outn + ch * stride_outc
        tl.store(ptr_out, out_val)


def groupnorm_mean_triton(x, gamma, beta, num_groups, eps=1e-5):
    """
    x: [B, C, D, H, W]
    gamma, beta: [C]
    returns: [B, C]  (GroupNorm + mean over spatial dims)
    """
    assert x.is_cuda and gamma.is_cuda and beta.is_cuda
    x = x.contiguous()
    gamma = gamma.contiguous()
    beta = beta.contiguous()

    B, C, D, H, W = x.shape
    assert C % num_groups == 0

    out = torch.empty((B, C), device=x.device, dtype=x.dtype)

    def grid(meta):
        return (B * num_groups,)

    groupnorm_mean_kernel[grid](
        x, gamma, beta, out,
        B, C, D, H, W,
        num_groups, eps,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),
        out.stride(0), out.stride(1),
        BLOCK_S=128,
    )
    return out


class ModelNew(nn.Module):
    """
    Triton-optimized replacement for:

    Conv3D -> HardSwish -> GroupNorm -> mean over spatial dims
    """

    def __init__(self, in_channels, out_channels, kernel_size, num_groups=4, bias=True):
        super(ModelNew, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.num_groups = num_groups
        self.eps = 1e-5

        k = kernel_size
        self.weight = nn.Parameter(
            torch.empty(out_channels, in_channels, k, k, k)
        )
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter("bias", None)

        # GroupNorm parameters
        self.gn_weight = nn.Parameter(torch.ones(out_channels))
        self.gn_bias = nn.Parameter(torch.zeros(out_channels))

        # Simple initialization similar to Conv3d default
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in = in_channels * k * k * k
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x):
        # x: [B, C_in, D, H, W]
        y = conv3d_hardswish_triton(x, self.weight, self.bias)
        y = groupnorm_mean_triton(y, self.gn_weight, self.gn_bias, self.num_groups, self.eps)
        return y
```

# NCU Metrics
{
  "conv3d_hswish_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 56.37,
    "launch__grid_size": 175104,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 24.62,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 26.8,
    "lts__t_sector_hit_rate.pct": 89.4
  },
  "groupnorm_mean_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 73.44,
    "launch__grid_size": 4096,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 89.28,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 77.57,
    "lts__t_sector_hit_rate.pct": 0.33
  }
}


---

## Analysis Steps

1. **Code Analysis**: Count kernels, identify operations, check for inefficiencies
2. **Performance Diagnosis**: Use metrics/latency to identify bottleneck type
3. **Root Cause**: Combine code + performance to find the core issue

## Optimization Categories (pick ONE if worth optimizing):

### 1. Operator Fusion
Fuse consecutive ops into fewer kernels to reduce memory traffic and launch overhead.

### 2. Algorithm Replacement
Replace naive algorithm with optimized variant.
- For Attention: Flash Attention, online softmax
- For Convolution: Winograd, im2col
- **For RNN/GRU/LSTM**: Persistent kernel with HYBRID computation
  - **CRITICAL**: Use hybrid approach for best performance:
    * Precompute input-side gates ONCE (outside kernel): `gates_x = (T*B, In) @ W_ih`
    * Persistent kernel (inside): only recurrent-side: `for t: gates_h = h @ W_hh`
  - Time loop `for t in range(T)` must be inside kernel, NOT in Python
  - Launch kernel once per layer, not once per timestep
  - Expected speedup: 10-100x (vs per-timestep launches)

### 3. Kernel Launch Reduction
Combine multiple small kernels to reduce overhead.
- **For RNN/GRU/LSTM**: See "Algorithm Replacement" above for persistent kernel approach

### 4. Memory Layout Optimization
Use in-place operations, buffer reuse, or better layouts.

## Should We Optimize?

Before proposing optimization, determine if it's worthwhile:
- **Not worth optimizing** if:
  - Code is already near-optimal (expected speedup < 10%)
  - Bottleneck cannot be addressed (hardware limited, already optimal algorithm)
  - Optimization would add significant complexity with minimal gain

- **Worth optimizing** if:
  - Clear algorithmic inefficiency exists (multiple kernels, suboptimal algorithm)
  - Expected speedup >= 20%
  - Concrete optimization path available

## Output (JSON)

```json
{
  "worth_optimizing": "yes/no",
  "reason": "<Why worth or not worth optimizing, 1 sentence>",
  "bottleneck": "<Root cause in 1-2 sentences, empty if not worth optimizing>",
  "optimisation method": "<Specific optimization in 1-2 sentences, empty if not worth optimizing>",
  "modification plan": "<Implementation steps in 2-3 sentences, empty if not worth optimizing>",
  "expected_speedup": "<e.g., '30-40%', empty if not worth optimizing>"
}
```

Return JSON only.
