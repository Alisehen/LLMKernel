You are optimizing a Triton kernel based on algorithmic analysis.

# PyTorch Reference (Target Behavior)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    """
    Model that performs:
    1. Conv3D
    2. HardSwish activation
    3. GroupNorm  
    4. Mean pooling across spatial dimensions
    """
    def __init__(self, in_channels, out_channels, kernel_size, num_groups=4, bias=True):
        super(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias)
        self.group_norm = nn.GroupNorm(num_groups, out_channels)

    def forward(self, x):
        x = self.conv(x)                             # (B, C, D, H, W)
        x = F.hardswish(x)                           # Nonlinear activation
        x = self.group_norm(x)                       # Normalization over channels
        x = torch.mean(x, dim=[2, 3, 4])             # Mean over spatial dims → (B, C)
        return x

# === Test config ===
batch_size = 1024
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 4

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]
```

**CRITICAL**: Study the PyTorch code carefully to understand:
- What does `forward()` return? (full output sequence vs final hidden state only)
- What is the computational pattern?
- What are the input/output shapes?

Your optimized kernel MUST match this exact behavior.

---

# Analysis Results

**Bottleneck**: After HardSwish+GroupNorm, the kernel stores a normalized 5D tensor to global memory, and a second kernel (spatial_mean_kernel) immediately rereads all of it to reduce over spatial dimensions, causing redundant global memory traffic and an extra launch.

**Optimization Strategy**: Fuse the spatial mean pooling into the existing HardSwish+GroupNorm Triton kernel so that, after applying normalization and affine, each program accumulates per-channel spatial sums and directly writes (B, C) outputs, eliminating the intermediate 5D tensor and the separate spatial_mean kernel.

**Implementation Plan**: Extend hardswish_groupnorm_kernel to also track, for each (b, c), the sum of the final normalized+affine values over D×H×W (possibly via an additional reduction pass or by restructuring the mapping from programs to (b, c)). At the end of processing each (b, c), divide by the spatial size and write directly into the (B, C) output tensor instead of storing the full y: (B, C, D, H, W). Remove spatial_mean_kernel and adjust ModelNew.forward to call only this fused kernel after the Conv3D. This reduces one full read/write of the 5D tensor and one kernel launch.

**Expected Speedup**: 20-30%

---

# Current Kernel (needs optimization)

```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def hardswish_groupnorm_kernel(
    x_ptr,        # (B, C, D, H, W)
    weight_ptr,   # (C,)
    bias_ptr,     # (C,)
    y_ptr,        # (B, C, D, H, W)
    B, C, D, H, W,
    num_groups,
    eps,
    stride_n, stride_c, stride_d, stride_h, stride_w,
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(0)  # range: [0, B * num_groups)
    # Map to (b, g)
    b = pid // num_groups
    g = pid % num_groups

    group_size = C // num_groups
    total_spatial = D * H * W
    L = group_size * total_spatial  # number of elements in this (b, g) group

    # Base pointers for this sample/group
    c0 = g * group_size
    x_base = x_ptr + b * stride_n + c0 * stride_c
    y_base = y_ptr + b * stride_n + c0 * stride_c

    offs = tl.arange(0, BLOCK_SIZE)
    sum_hs = 0.0
    sum_hs_sq = 0.0

    # ---- First pass: compute mean and variance of HardSwish(x) over the group ----
    for offset in range(0, L, BLOCK_SIZE):
        idx = offset + offs
        mask = idx < L

        # Decompose idx into (channel_offset, d, h, w)
        ch_off = idx // total_spatial
        rem = idx % total_spatial
        d_idx = rem // (H * W)
        rem2 = rem % (H * W)
        h_idx = rem2 // W
        w_idx = rem2 % W

        # Global channel index
        c_idx = c0 + ch_off

        x_ptrs = x_base + ch_off * stride_c + d_idx * stride_d + h_idx * stride_h + w_idx * stride_w
        x = tl.load(x_ptrs, mask=mask, other=0.0)

        # HardSwish: x * clamp(x + 3, 0, 6) / 6
        t = x + 3.0
        t = tl.minimum(t, 6.0)
        t = tl.maximum(t, 0.0)
        hs = x * t * (1.0 / 6.0)

        sum_hs += tl.sum(hs, axis=0)
        sum_hs_sq += tl.sum(hs * hs, axis=0)

    # Cast L to float without calling tl.float32 (which is a dtype, not a function)
    L_f = 1.0 * L
    mean = sum_hs / L_f
    mean_sq = sum_hs_sq / L_f
    var = mean_sq - mean * mean
    var = tl.maximum(var, 0.0)  # numerical stability
    inv_std = 1.0 / tl.sqrt(var + eps)

    # ---- Second pass: normalize and apply affine (gamma, beta) ----
    for offset in range(0, L, BLOCK_SIZE):
        idx = offset + offs
        mask = idx < L

        ch_off = idx // total_spatial
        rem = idx % total_spatial
        d_idx = rem // (H * W)
        rem2 = rem % (H * W)
        h_idx = rem2 // W
        w_idx = rem2 % W

        c_idx = c0 + ch_off

        x_ptrs = x_base + ch_off * stride_c + d_idx * stride_d + h_idx * stride_h + w_idx * stride_w
        x = tl.load(x_ptrs, mask=mask, other=0.0)

        # HardSwish again
        t = x + 3.0
        t = tl.minimum(t, 6.0)
        t = tl.maximum(t, 0.0)
        hs = x * t * (1.0 / 6.0)

        # GroupNorm
        hs_norm = (hs - mean) * inv_std

        gamma = tl.load(weight_ptr + c_idx, mask=mask, other=1.0)
        beta = tl.load(bias_ptr + c_idx, mask=mask, other=0.0)

        y_val = hs_norm * gamma + beta

        y_ptrs = y_base + ch_off * stride_c + d_idx * stride_d + h_idx * stride_h + w_idx * stride_w
        tl.store(y_ptrs, y_val, mask=mask)


def hardswish_groupnorm(x: torch.Tensor,
                        weight: torch.Tensor,
                        bias: torch.Tensor,
                        num_groups: int,
                        eps: float) -> torch.Tensor:
    """
    x: (B, C, D, H, W)
    weight, bias: (C,)
    """
    assert x.is_cuda, "Input must be CUDA tensor"
    B, C, D, H, W = x.shape
    y = torch.empty_like(x)

    stride_n, stride_c, stride_d, stride_h, stride_w = x.stride()

    BLOCK_SIZE = 256

    # Ensure grid size > 0
    grid = lambda META: (max(1, B * num_groups),)

    hardswish_groupnorm_kernel[grid](
        x, weight, bias, y,
        B, C, D, H, W,
        num_groups, eps,
        stride_n, stride_c, stride_d, stride_h, stride_w,
        BLOCK_SIZE=BLOCK_SIZE,
    )
    return y


@triton.jit
def spatial_mean_kernel(
    x_ptr,      # (B, C, D, H, W)
    out_ptr,    # (B, C)
    B, C, D, H, W,
    stride_n, stride_c, stride_d, stride_h, stride_w,
    stride_on, stride_oc,
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(0)  # range: [0, B*C)
    b = pid // C
    c = pid % C

    total_spatial = D * H * W
    offs = tl.arange(0, BLOCK_SIZE)

    x_base = x_ptr + b * stride_n + c * stride_c
    sum_val = 0.0

    for offset in range(0, total_spatial, BLOCK_SIZE):
        idx = offset + offs
        mask = idx < total_spatial

        d_idx = idx // (H * W)
        rem = idx % (H * W)
        h_idx = rem // W
        w_idx = rem % W

        x_ptrs = x_base + d_idx * stride_d + h_idx * stride_h + w_idx * stride_w
        x = tl.load(x_ptrs, mask=mask, other=0.0)

        sum_val += tl.sum(x, axis=0)

    # Cast to float without calling tl.float32
    total_f = 1.0 * total_spatial
    mean = sum_val / total_f

    out_ptr_single = out_ptr + b * stride_on + c * stride_oc
    tl.store(out_ptr_single, mean)


def spatial_mean(x: torch.Tensor) -> torch.Tensor:
    """
    Mean over spatial dims [2, 3, 4], returning (B, C)
    """
    assert x.is_cuda, "Input must be CUDA tensor"
    B, C, D, H, W = x.shape
    out = torch.empty((B, C), device=x.device, dtype=x.dtype)

    stride_n, stride_c, stride_d, stride_h, stride_w = x.stride()
    stride_on, stride_oc = out.stride()

    BLOCK_SIZE = 256
    # Ensure grid size > 0
    grid = lambda META: (max(1, B * C),)

    spatial_mean_kernel[grid](
        x, out,
        B, C, D, H, W,
        stride_n, stride_c, stride_d, stride_h, stride_w,
        stride_on, stride_oc,
        BLOCK_SIZE=BLOCK_SIZE,
    )
    return out


class ModelNew(nn.Module):
    """
    Conv3D -> HardSwish -> GroupNorm -> spatial mean,
    where everything after Conv3D is fused into high-performance Triton kernels.
    """
    def __init__(self, in_channels, out_channels, kernel_size, num_groups=4, bias=True):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias)
        self.group_norm = nn.GroupNorm(num_groups, out_channels)

    def forward(self, x):
        # Conv3D via cuDNN / PyTorch
        x = self.conv(x)

        # Fused HardSwish + GroupNorm via Triton
        x = hardswish_groupnorm(
            x,
            self.group_norm.weight,
            self.group_norm.bias,
            self.group_norm.num_groups,
            self.group_norm.eps,
        )

        # Mean over spatial dims via Triton
        x = spatial_mean(x)
        return x
```

---

# Your Task

Implement the optimization strategy above. Focus on the specific bottleneck identified.

## Key Requirements

1. **Preserve correctness**: Maintain the same input/output behavior
2. **Apply the optimization**: Follow the implementation plan exactly
3. **Use valid Triton syntax**:
   - Every kernel MUST have `@triton.jit` decorator
   - Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
   - BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
   - No `continue`, `break`, `return` inside kernels (use masking)
   - Prefer `tl.dot(a, b, allow_tf32=True)` for matmul operations

4. **CRITICAL for RNN/GRU/LSTM Persistent Kernels**:
   - Time loop MUST be inside @triton.jit kernel, NOT in Python forward()
   - **HYBRID computation strategy** (CRITICAL for performance):
     * Precompute input-side gates OUTSIDE kernel: `gates_x = (T*B, In) @ W_ih` (ONE large GEMM)
     * INSIDE kernel: only recurrent-side: `for t: gates_h = h @ W_hh` (T small GEMMs)
   - CORRECT (FAST - use this):
     ```python
     # Python forward():
     gates_x_all = x.reshape(T*B, In) @ W_ih + b_ih  # ONE large GEMM
     gates_x_all = gates_x_all.view(T, B, 3*H)
     gru_persistent_kernel[grid](gates_x_all, h0, W_hh, ...)  # Launch ONCE

     @triton.jit
     def gru_persistent_kernel(gates_x_ptr, h_ptr, W_hh_ptr, ...):
         for t in range(T):  # Inside kernel
             gates_x_t = tl.load(gates_x_ptr + t*...)  # Precomputed
             gates_h = h @ W_hh  # Only recurrent GEMM
             h = (1-z)*n + z*h   # Fuse and update
     ```

5. **Output format**:
   - Imports: `import torch, torch.nn as nn, triton, triton.language as tl`
   - `@triton.jit` kernel(s)
   - Wrapper function(s)
   - `class ModelNew(nn.Module)` — REQUIRED
   - NO testing code, NO `if __name__ == "__main__"`

---

Generate the optimized kernel now. Output ONLY the complete Python code.
