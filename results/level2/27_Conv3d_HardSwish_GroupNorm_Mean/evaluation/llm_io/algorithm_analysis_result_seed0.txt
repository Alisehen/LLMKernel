{
  "worth_optimizing": "yes",
  "reason": "The current pipeline writes and rereads a large 5D activation tensor between two kernels, and the second kernel is clearly DRAMâ€‘bound.",
  "bottleneck": "GroupNorm+mean is limited by global memory bandwidth, repeatedly streaming the full [B, C, D, H, W] tensor while the conv kernel already produced it once. Writing conv+hswish output to DRAM and then reading it back for GroupNorm+pool dominates memory traffic and underutilizes compute.",
  "optimisation method": "Fuse Conv3D+HardSwish and GroupNorm+spatial-mean into a single kernel that computes conv outputs, accumulates per-(B, group) statistics (sum, sum of squares, and per-channel sums) on the fly, and only writes the final [B, C] normalized-pooled values to global memory.",
  "modification plan": "Redesign the conv3d_hswish kernel so each program instance covers a (batch, group, spatial tile) region: during the convolution loop, accumulate group-wise sum and sum of squares plus per-channel spatial sums in registers/shared memory instead of storing full activations. After finishing over spatial tiles and kernel K, compute mean/variance per group and apply the GroupNorm affine + spatial mean pooling to the per-channel sums, then write just out[b, c] to global memory; remove the separate groupnorm_mean kernel entirely.",
  "expected_speedup": "30-50%"
}