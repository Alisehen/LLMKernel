You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Grid layout & parallelism.

Metrics:
- sm__throughput.avg.pct_of_peak_sustained_elapsed (>60%)
- launch__grid_size

Rules:
- 1D: (cdiv(N, BLOCK))
- 2D: (cdiv(M, BLOCK_M), cdiv(N, BLOCK_N))
- 3D: (batch, cdiv(M, BLOCK_M), cdiv(N, BLOCK_N))
- >3D: flatten ONLY independent dims
- Prefer batch / head / expert parallelism before shrinking BLOCK
- Change grid only if SM utilization is clearly low

Safety:
- Max 3 grid dims, static rank
- grid=(G0,G1,G2) must match tl.program_id(0/1/2)
- If unsure about correctness, do NOT change grid

Autotune:
- Autotune either BLOCK_* OR (num_warps, num_stages)
- If autotuning BLOCK_*, use grid=lambda META: (...)
- Never redefine BLOCK_* in both kernel and launch



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def fused_forward_kernel(
    # Pointers to input and parameters
    input_ptr,
    weight_ptr,
    bias_ptr,
    subtract_ptr,
    output_ptr,
    # Matrix dimensions
    B,  # batch size
    N,  # input features
    K,  # output features (must equal N for residual add)
    # Strides
    stride_input_batch,
    stride_input_feat,
    stride_weight_in,
    stride_weight_out,
    # Meta-parameters
    BLOCK_SIZE_B: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    USE_BIAS: tl.constexpr,
):
    # Parallelize over batches and output features
    pid_b = tl.program_id(0)
    pid_k = tl.program_id(1)
    
    # Block indices
    offs_b = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)
    offs_k = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)
    
    # Create masks for boundary checks
    mask_b = offs_b < B
    mask_k = offs_k < K
    
    # Initialize accumulator for GEMM
    accumulator = tl.zeros((BLOCK_SIZE_B, BLOCK_SIZE_K), dtype=tl.float32)
    
    # Blocked matrix multiplication
    for n_block in range(0, tl.cdiv(N, BLOCK_SIZE_N)):
        offs_n = n_block * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
        mask_n = offs_n < N
        
        # Load input block
        input_ptrs = (
            input_ptr + 
            offs_b[:, None] * stride_input_batch + 
            offs_n[None, :] * stride_input_feat
        )
        input_block = tl.load(
            input_ptrs, 
            mask=mask_b[:, None] & mask_n[None, :],
            other=0.0
        )
        
        # Load weight block
        weight_ptrs = (
            weight_ptr + 
            offs_n[:, None] * stride_weight_in + 
            offs_k[None, :] * stride_weight_out
        )
        weight_block = tl.load(
            weight_ptrs,
            mask=mask_n[:, None] & mask_k[None, :],
            other=0.0
        )
        
        # Accumulate GEMM result
        accumulator += tl.dot(input_block, weight_block)
    
    # Apply bias if needed
    if USE_BIAS:
        bias_ptrs = bias_ptr + offs_k[None, :]
        bias = tl.load(bias_ptrs, mask=mask_k[None, :], other=0.0)
        accumulator += bias
    
    # Apply subtract parameter (broadcast over batch dimension)
    subtract_ptrs = subtract_ptr + offs_k[None, :]
    subtract = tl.load(subtract_ptrs, mask=mask_k[None, :], other=0.0)
    accumulator -= subtract
    
    # Store to temporary output for reductions
    gemm_output_ptrs = (
        output_ptr + 
        offs_b[:, None] * K + 
        offs_k[None, :]
    )
    tl.store(
        gemm_output_ptrs,
        accumulator,
        mask=mask_b[:, None] & mask_k[None, :]
    )


@triton.jit
def reduce_and_activate_kernel(
    # Pointers
    gemm_output_ptr,
    original_input_ptr,
    final_output_ptr,
    # Dimensions
    B,
    K,
    # Strides
    stride_gemm_batch,
    stride_gemm_feat,
    stride_input_batch,
    stride_input_feat,
    # Meta-parameters
    BLOCK_SIZE_B: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
):
    # Parallelize over batches only
    pid_b = tl.program_id(0)
    
    offs_b = pid_b * BLOCK_SIZE_B + tl.arange(0, BLOCK_SIZE_B)
    mask_b = offs_b < B
    
    # Allocate block for reduction
    block_vals = tl.zeros((BLOCK_SIZE_B,), dtype=tl.float32)
    
    # Load block of values for this batch
    for k_block in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        offs_k = k_block * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)
        mask_k = offs_k < K
        
        # Load gemm output values
        gemm_ptrs = (
            gemm_output_ptr + 
            offs_b[:, None] * stride_gemm_batch + 
            offs_k[None, :] * stride_gemm_feat
        )
        vals = tl.load(
            gemm_ptrs,
            mask=mask_b[:, None] & mask_k[None, :],
            other=0.0
        )
        
        # Accumulate for mean
        block_vals += tl.sum(vals, axis=1)
    
    # Compute mean (divide by K)
    mean_val = block_vals / K
    
    # GELU activation using erf approximation (more stable than tanh)
    # gelu(x) = 0.5 * x * (1 + erf(x / sqrt(2)))
    sqrt_2_over_pi = 0.7978845608028654
    gelu_coef = 0.044715
    
    x = mean_val
    # Use erf-based GELU approximation
    # erf(x/sqrt(2)) approximation using polynomial
    x_sq = x * x
    x_cubed = x * x_sq
    inner = sqrt_2_over_pi * (x + gelu_coef * x_cubed)
    # tanh(inner) = 1 - 2/(exp(2*inner)+1)
    # Use stable implementation
    exp_2inner = tl.exp(2.0 * inner)
    tanh_inner = 1.0 - 2.0 / (exp_2inner + 1.0)
    gelu_val = 0.5 * x * (1.0 + tanh_inner)
    
    # Residual add: add original input
    # Original input shape: (B, N) where N = K
    # We need to add gelu_val[batch] to each element of original input row
    
    # For each batch, add gelu_val[batch] to all features of original input
    for k_block in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        offs_k = k_block * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)
        mask_k = offs_k < K
        
        # Load original input
        input_ptrs = (
            original_input_ptr + 
            offs_b[:, None] * stride_input_batch + 
            offs_k[None, :] * stride_input_feat
        )
        orig_vals = tl.load(
            input_ptrs,
            mask=mask_b[:, None] & mask_k[None, :],
            other=0.0
        )
        
        # Add GELU value (broadcast over features)
        result = orig_vals + gelu_val[:, None]
        
        # Store final output
        output_ptrs = (
            final_output_ptr + 
            offs_b[:, None] * K + 
            offs_k[None, :]
        )
        tl.store(
            output_ptrs,
            result,
            mask=mask_b[:, None] & mask_k[None, :]
        )


def triton_fused_forward(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    subtract: torch.Tensor,
) -> torch.Tensor:
    """
    Fused forward pass implementing:
    1. GEMM (linear layer)
    2. Subtract parameter
    3. Global average pool (mean over features)
    4. LogSumExp (simplified to identity after mean)
    5. GELU activation
    6. Residual add with original input
    """
    B, N = x.shape
    K = weight.shape[0]  # output features
    
    # Ensure K == N for residual add
    if K != N:
        raise ValueError(f"Output features ({K}) must equal input features ({N}) for residual add")
    
    # Intermediate storage for GEMM output
    gemm_output = torch.empty((B, K), device=x.device, dtype=x.dtype)
    
    # Configure GEMM kernel
    BLOCK_SIZE_B = 64
    BLOCK_SIZE_K = 64
    BLOCK_SIZE_N = 64
    
    grid = (
        triton.cdiv(B, BLOCK_SIZE_B),
        triton.cdiv(K, BLOCK_SIZE_K),
    )
    
    fused_forward_kernel[grid](
        x,
        weight,
        bias,
        subtract,
        gemm_output,
        B,
        N,
        K,
        x.stride(0),
        x.stride(1),
        weight.stride(1),
        weight.stride(0),
        BLOCK_SIZE_B=BLOCK_SIZE_B,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        USE_BIAS=bias is not None,
    )
    
    # Allocate final output
    final_output = torch.empty_like(x)
    
    # Configure reduction and activation kernel
    BLOCK_SIZE_B_RED = 64
    BLOCK_SIZE_K_RED = 64
    
    grid_red = (triton.cdiv(B, BLOCK_SIZE_B_RED),)
    
    reduce_and_activate_kernel[grid_red](
        gemm_output,
        x,
        final_output,
        B,
        K,
        gemm_output.stride(0),
        gemm_output.stride(1),
        x.stride(0),
        x.stride(1),
        BLOCK_SIZE_B=BLOCK_SIZE_B_RED,
        BLOCK_SIZE_K=BLOCK_SIZE_K_RED,
    )
    
    return final_output


class ModelNew(nn.Module):
    """
    Model that performs a series of operations: Gemm, Subtract, GlobalAvgPool, LogSumExp, GELU, and ResidualAdd.
    Optimized with fused Triton kernels.
    """
    def __init__(self, in_features, out_features, bias=True):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features, bias=bias)
        self.subtract = nn.Parameter(torch.randn(out_features))
        
        # Store original weight and bias for Triton kernel
        self._weight = self.gemm.weight
        self._bias = self.gemm.bias if bias else None

    def forward(self, x):
        original_x = x
        
        # Fused forward pass with Triton kernels
        x = triton_fused_forward(
            x,
            self._weight,
            self._bias,
            self.subtract,
        )
        
        return x
```

[NCU PROFILING METRICS]
{
  "fused_forward_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 41.23,
    "launch__grid_size": 4096,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 8.33,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 20.73,
    "lts__t_sector_hit_rate.pct": 95.01
  },
  "reduce_and_activate_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 1.69,
    "launch__grid_size": 32,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 8.24,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 87.16,
    "lts__t_sector_hit_rate.pct": 33.44
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
