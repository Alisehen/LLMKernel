```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        # Baseline, conservative occupancy
        triton.Config({'BLOCK_HW': 128}, num_warps=4, num_stages=2),
        # More ILP + warp-level parallelism for large H*W
        triton.Config({'BLOCK_HW': 256}, num_warps=8, num_stages=3),
    ],
    key=['H', 'W'],
)
@triton.jit
def gap_bias_kernel(
    x_ptr,          # *f32 / *f16 / *bf16, [N, C, H, W]
    bias_ptr,       # *f32 / *f16 / *bf16, [C]
    y_ptr,          # *f32 / *f16 / *bf16, [N, C]
    N, C, H, W,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_yn, stride_yc,
    inv_hw,         # f32 = 1.0 / (H * W)
    BLOCK_HW: tl.constexpr,
):
    """
    Kernel 1: Global Average Pool over H*W + Bias

      y[n, c] = mean_{h,w} x[n, c, h, w] + bias[c]

    - Grid:  program_id(0) = n, program_id(1) = c
    - Each program reduces one (n, c) over H*W.
    - Accumulation is in fp32 for numerical accuracy.
    - Tuned for Ada (4090) with num_warps in {4, 8}, num_stages in {2, 3}.
    """
    pid_n = tl.program_id(0)
    pid_c = tl.program_id(1)

    if (pid_n >= N) | (pid_c >= C):
        return

    tl.static_assert(BLOCK_HW % 32 == 0)

    # Base pointer for this (n, c)
    x_base = x_ptr + pid_n * stride_xn + pid_c * stride_xc

    # 1D reduction over HW = H * W with coalesced loads
    hw = H * W
    offsets = tl.arange(0, BLOCK_HW)
    acc = tl.zeros((), dtype=tl.float32)

    idx = 0
    while idx < hw:
        cur = idx + offsets
        mask = cur < hw

        # Map linear index -> (h, w)
        h = cur // W
        w = cur - h * W

        x_ptrs = x_base + h * stride_xh + w * stride_xw
        x_vals = tl.load(x_ptrs, mask=mask, other=0.0)

        # Accumulate in fp32
        acc += tl.sum(x_vals.to(tl.float32), axis=0)

        idx += BLOCK_HW

    # Global average
    acc = acc * inv_hw

    # Add bias[c]
    bias_val = tl.load(bias_ptr + pid_c).to(tl.float32)
    acc = acc + bias_val

    # Store result y[n, c]
    y_ptrs = y_ptr + pid_n * stride_yn + pid_c * stride_yc
    tl.store(y_ptrs, acc.to(tl.float32))


@triton.autotune(
    configs=[
        # Baseline
        triton.Config({'BLOCK_C': 128}, num_warps=4, num_stages=2),
        # Higher warp count + staging to hide latency for larger C
        triton.Config({'BLOCK_C': 128}, num_warps=8, num_stages=3),
    ],
    key=['C'],
)
@triton.jit
def logsumexp_mul_kernel(
    y_ptr,      # *f32 / *f16 / *bf16, [N, C]
    out_ptr,    # *f32 / *f16 / *bf16, [N, 1] (actually [N])
    N, C,
    stride_yn, stride_yc,
    BLOCK_C: tl.constexpr,
):
    """
    Kernel 2: Row-wise LogSumExp over C, then multiply by 10

      out[n] = 10 * logsumexp_c ( y[n, c] )

    - Grid: program_id(0) = n
    - Two-pass stable logsumexp:
        1) compute max over C
        2) compute sum(exp(y - max)) over C
    - All math in fp32.
    """
    pid_n = tl.program_id(0)
    if pid_n >= N:
        return

    row_base = y_ptr + pid_n * stride_yn
    offs_c = tl.arange(0, BLOCK_C)

    # Pass 1: max over C
    max_val = -float('inf')
    c0 = 0
    while c0 < C:
        cur_offs = c0 + offs_c
        mask = cur_offs < C
        y_ptrs = row_base + cur_offs * stride_yc
        y_vals = tl.load(y_ptrs, mask=mask, other=-float('inf')).to(tl.float32)
        block_max = tl.max(y_vals, axis=0)
        max_val = tl.maximum(max_val, block_max)
        c0 += BLOCK_C

    # Pass 2: sum(exp(y - max_val))
    sum_exp = 0.0
    c0 = 0
    while c0 < C:
        cur_offs = c0 + offs_c
        mask = cur_offs < C
        y_ptrs = row_base + cur_offs * stride_yc
        y_vals = tl.load(y_ptrs, mask=mask, other=-float('inf')).to(tl.float32)
        sum_exp += tl.sum(tl.exp(y_vals - max_val), axis=0)
        c0 += BLOCK_C

    # Final logsumexp and scale by 10.0
    logsumexp_val = max_val + tl.log(sum_exp)
    result = logsumexp_val * 10.0

    tl.store(out_ptr + pid_n, result.to(tl.float32))


def fused_post_convtranspose(x: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:
    """
    Two-kernel fused implementation:

      1) gap_bias_kernel:
           y[n, c] = mean(x[n, c, :, :]) + bias[c]   -> y: [N, C]

      2) logsumexp_mul_kernel:
           out[n] = 10 * logsumexp(y[n, :])          -> out: [N, 1]

    Optimized for Ada (RTX 4090):
      - num_warps ∈ {4, 8}, num_stages ∈ {2, 3}
      - Aggressive ILP, but conservative baseline maintained for stability.
    """
    assert x.is_cuda, "Triton kernels require CUDA tensors"
    assert bias.is_cuda, "Bias must be on CUDA device"
    assert x.dim() == 4, "x must be [N, C, H, W]"

    N, C, H, W = x.shape
    assert bias.numel() == C, "Bias must have C elements"

    # Intermediate [N, C] buffer
    # Keep dtype consistent with x; computations are internally fp32.
    y = torch.empty((N, C), device=x.device, dtype=x.dtype)
    out = torch.empty((N,), device=x.device, dtype=x.dtype)

    inv_hw = 1.0 / float(H * W)

    # Kernel 1 grid: one program per (n, c)
    grid_gap = lambda META: (N, C)

    gap_bias_kernel[grid_gap](
        x, bias, y,
        N, C, H, W,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        y.stride(0), y.stride(1),
        inv_hw,
    )

    # Kernel 2 grid: one program per n
    grid_lse = (N,)

    logsumexp_mul_kernel[grid_lse](
        y, out,
        N, C,
        y.stride(0), y.stride(1),
    )

    # Return as [N, 1] to match original spec
    return out.view(N, 1)


class ModelNew(nn.Module):
    """
    Model:
      - ConvTranspose2d (PyTorch native)
      - Triton kernels:
          1) GlobalAvgPool over (H, W) + BiasAdd  -> [N, C]
          2) LogSumExp over C + Multiply(10.0)    -> [N, 1]
    """
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size
        )
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.conv_transpose(x)
        bias_1d = self.bias.view(-1)
        x = fused_post_convtranspose(x, bias_1d)
        return x
```