You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Fine-tuning fused kernel parameters.

Params:
- num_warps ∈ {4, 8}
- num_stages ∈ {2, 3}

Conditional Rules (NOT one-size-fits-all):

IF register pressure LOW (regs < 96, no spill):
  - Try num_warps=8 for compute-bound fusion
  - num_stages=3 may help hide latency

IF register pressure HIGH (regs > 128 or occupancy_limit_registers):
  - Use num_warps=4 (fewer warps = more registers per warp)
  - Keep num_stages=2 (higher stages need more registers)

IF multi-input fusion (3+ distinct loads):
  - num_stages=2 preferred (each stage buffers all inputs)
  - num_warps=4 often better than 8

Autotune:
- Max 2-3 configs to reduce compilation time
- Always include conservative baseline (num_warps=4, num_stages=2)
- Test before/after: revert if gain < 2%



[CURRENT CODE]
```python
# 1. Imports
import torch
import torch.nn as nn
import triton
import triton.language as tl


# 2. Triton kernel(s)

@triton.autotune(
    configs=[
        # Smaller block for low register pressure / small H*W
        triton.Config({'BLOCK_HW': 64}, num_warps=2, num_stages=2),
        # Balanced
        triton.Config({'BLOCK_HW': 128}, num_warps=4, num_stages=2),
        # Larger block for big H*W (more ILP)
        triton.Config({'BLOCK_HW': 256}, num_warps=4, num_stages=2),
    ],
    key=['H', 'W'],
)
@triton.jit
def gap_bias_kernel(
    x_ptr,          # *f32 / *f16, [N, C, H, W]
    bias_ptr,       # *f32 / *f16, [C]
    y_ptr,          # *f32 / *f16, [N, C] (global-avg pooled + bias)
    N, C, H, W,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_yn, stride_yc,
    inv_hw,         # f32 = 1.0 / (H * W)
    BLOCK_HW: tl.constexpr,
):
    """
    Optimized Kernel 1:
      y[n, c] = mean_{h,w} x[n, c, h, w] + bias[c]

    Each program instance computes a single (n, c) output, reducing over H*W.
    This greatly increases grid size (N * C) and SM occupancy compared to
    the original BLOCK_C-tiled version, while keeping register pressure low.
    """
    pid_n = tl.program_id(0)
    pid_c = tl.program_id(1)

    if pid_n >= N or pid_c >= C:
        return

    # Base pointer for this (n, c)
    x_base = x_ptr + pid_n * stride_xn + pid_c * stride_xc

    # 1D reduction over HW = H * W with coalesced loads
    hw = H * W
    offsets = tl.arange(0, BLOCK_HW)
    acc = tl.zeros((), dtype=tl.float32)

    idx = 0
    while idx < hw:
        cur = idx + offsets
        mask = cur < hw

        # Map linear index -> (h, w)
        h = cur // W
        w = cur - h * W

        x_ptrs = x_base + h * stride_xh + w * stride_xw
        x_vals = tl.load(x_ptrs, mask=mask, other=0.0)

        # Accumulate in fp32 for precision
        acc += tl.sum(x_vals.to(tl.float32), axis=0)

        idx += BLOCK_HW

    # Global average
    acc = acc * inv_hw

    # Add bias[c]
    bias_val = tl.load(bias_ptr + pid_c).to(tl.float32)
    acc = acc + bias_val

    # Store result y[n, c]
    y_ptrs = y_ptr + pid_n * stride_yn + pid_c * stride_yc
    tl.store(y_ptrs, acc.to(tl.float32))


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_C': 32}, num_warps=1, num_stages=2),
        triton.Config({'BLOCK_C': 64}, num_warps=2, num_stages=2),
        triton.Config({'BLOCK_C': 128}, num_warps=2, num_stages=2),
    ],
    key=['C'],
)
@triton.jit
def logsumexp_mul_kernel(
    y_ptr,      # *f32, [N, C]
    out_ptr,    # *f32, [N, 1]
    N, C,
    stride_yn, stride_yc,
    BLOCK_C: tl.constexpr,
):
    """
    Optimized Kernel 2:
      out[n] = 10 * logsumexp_c ( y[n, c] )

    Each program instance computes a single n (row-wise reduction over C).
    Two-pass logsumexp for numerical stability.
    """
    pid_n = tl.program_id(0)
    if pid_n >= N:
        return

    row_base = y_ptr + pid_n * stride_yn
    offs_c = tl.arange(0, BLOCK_C)

    # Pass 1: max over C
    max_val = -float('inf')
    c0 = 0
    while c0 < C:
        cur_offs = c0 + offs_c
        mask = cur_offs < C
        y_ptrs = row_base + cur_offs * stride_yc
        y_vals = tl.load(y_ptrs, mask=mask, other=-float('inf')).to(tl.float32)
        block_max = tl.max(y_vals, axis=0)
        max_val = tl.maximum(max_val, block_max)
        c0 += BLOCK_C

    # Pass 2: sum(exp(y - max_val))
    sum_exp = 0.0
    c0 = 0
    while c0 < C:
        cur_offs = c0 + offs_c
        mask = cur_offs < C
        y_ptrs = row_base + cur_offs * stride_yc
        y_vals = tl.load(y_ptrs, mask=mask, other=-float('inf')).to(tl.float32)
        sum_exp += tl.sum(tl.exp(y_vals - max_val), axis=0)
        c0 += BLOCK_C

    # Final logsumexp and scale by 10.0
    logsumexp_val = max_val + tl.log(sum_exp)
    result = logsumexp_val * 10.0

    tl.store(out_ptr + pid_n, result)


# 3. Wrapper function(s)

def fused_post_convtranspose(x: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:
    """
    Two-kernel fused implementation:

      1) gap_bias_kernel:
           y[n, c] = mean(x[n, c, :, :]) + bias[c]   -> y: [N, C]

      2) logsumexp_mul_kernel:
           out[n] = 10 * logsumexp(y[n, :])          -> out: [N, 1]

    Optimizations:
      - gap_bias_kernel: per-(n,c) reduction over H*W with large grid size (N*C),
        improving SM occupancy and DRAM utilization; minimal register pressure.
      - logsumexp_mul_kernel: autotuned BLOCK_C, low-register design.
    """
    assert x.is_cuda, "Triton kernels require CUDA tensors"
    assert bias.is_cuda, "Bias must be on CUDA device"
    assert x.dim() == 4, "x must be [N, C, H, W]"

    N, C, H, W = x.shape
    assert bias.numel() == C, "Bias must have C elements"

    # Intermediate [N, C] buffer
    y = torch.empty((N, C), device=x.device, dtype=x.dtype)
    out = torch.empty((N, 1), device=x.device, dtype=x.dtype)

    inv_hw = 1.0 / float(H * W)

    # Launch kernel 1: GlobalAvgPool + Bias
    grid_gap = lambda META: (N, C)

    gap_bias_kernel[grid_gap](
        x, bias, y,
        N, C, H, W,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        y.stride(0), y.stride(1),
        inv_hw,
    )

    # Launch kernel 2: LogSumExp over C + *10.0
    grid_lse = (N,)

    logsumexp_mul_kernel[grid_lse](
        y, out,
        N, C,
        y.stride(0), y.stride(1),
    )

    return out


# 4. ModelNew definition

class ModelNew(nn.Module):
    """
    Model that performs a transposed convolution (PyTorch native),
    followed by optimized Triton kernels for:
      1) GlobalAvgPool over (H, W) + BiasAdd  -> [N, C]
      2) LogSumExp over C + Multiply(10.0)    -> [N, 1]
    """
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size
        )
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.conv_transpose(x)
        bias_1d = self.bias.view(-1)
        x = fused_post_convtranspose(x, bias_1d)
        return x
```

[NCU PROFILING METRICS]
{
  "gap_bias_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 47.76,
    "launch__grid_size": 2048,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 65.29,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 80.47,
    "lts__t_sector_hit_rate.pct": 0.16
  },
  "logsumexp_mul_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 0.35,
    "launch__grid_size": 16,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 4.14,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 0.47,
    "lts__t_sector_hit_rate.pct": 82.64
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

TRITON API CONSTRAINTS (CRITICAL):
- Triton has NO: tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
