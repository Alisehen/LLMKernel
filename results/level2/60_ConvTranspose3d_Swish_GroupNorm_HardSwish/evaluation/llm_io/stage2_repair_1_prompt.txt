Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 34, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 1451, in arange
    return semantic.arange(start, end, _builder)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/semantic.py", line 614, in arange
    raise ValueError("arange's arguments must be of type tl.constexpr")
ValueError: arange's arguments must be of type tl.constexpr

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 538, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_114735_batch_range56to80_deepseek_deepseek/60_ConvTranspose3d_Swish_GroupNorm_HardSwish/code/kernel_20251214_150036.py", line 199, in forward
    x = fused_post_convtranspose_3d_optimized(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_114735_batch_range56to80_deepseek_deepseek/60_ConvTranspose3d_Swish_GroupNorm_HardSwish/code/kernel_20251214_150036.py", line 168, in fused_post_convtranspose_3d_optimized
    fused_swish_groupnorm_hardswish_kernel_optimized[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 192, in run
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 192, in <dictcomp>
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 170, in _bench
    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/testing.py", line 145, in do_bench
    fn()
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 156, in kernel_call
    self.fn.run(
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 278, in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 81, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 70:48:
            # Accumulate statistics
            sum_val += tl.sum(x_swish)
            sum_sq += tl.sum(x_swish * x_swish)

    # Compute group mean and variance
    count = DHW * group_size
    group_mean = sum_val / (count + eps)
    group_var = sum_sq / (count + eps) - group_mean * group_mean
    inv_std = 1.0 / tl.sqrt(group_var + eps)

    # Preload weight and bias for this group
    weight = tl.load(weight_ptr + group_start + tl.arange(0, group_size))
                                                ^

Main Critical Problem Analysis:
Problem Analysis (from expert diagnosis):
critical_issue: tl.arange() called with non-constexpr group_size parameter at line 70
why_it_matters: Triton requires arange dimensions to be compile-time constants, but group_size is a runtime variable
minimal_fix_hint: Replace tl.arange(0, group_size) with tl.arange(0, TL_CONSTANT) or restructure weight loading

Focus your fix on addressing the identified critical issue.


Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def swish_activation_fast(x):
    """Optimized Swish with minimal ops"""
    # Swish: x * sigmoid(x) = x / (1 + exp(-x))
    neg_x = -x
    exp_neg_x = tl.math.exp(neg_x)
    sigmoid = 1.0 / (1.0 + exp_neg_x)
    return x * sigmoid


@triton.jit
def hardswish_activation_fast(x):
    """Optimized HardSwish with branchless ops"""
    # HardSwish: x * relu6(x + 3) / 6
    x_plus_3 = x + 3.0
    # relu6(x+3) = min(max(x+3, 0), 6)
    # Use tl.minimum/maximum for better performance
    relu6_val = tl.minimum(tl.maximum(x_plus_3, 0.0), 6.0)
    return x * relu6_val * (1.0 / 6.0)


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),
        triton.Config({'BLOCK_SIZE': 512}, num_warps=4),
        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),
    ],
    key=['N', 'C', 'DHW', 'group_size'],
)
@triton.jit
def fused_swish_groupnorm_hardswish_kernel_optimized(
    x_ptr,
    weight_ptr,
    bias_ptr,
    out_ptr,
    N, C, DHW,  # DHW = D*H*W precomputed
    groups,
    group_size,
    eps,
    stride_xn, stride_xc, stride_xdhw,
    BLOCK_SIZE: tl.constexpr,
):
    """
    Optimized fused kernel: Swish + GroupNorm + HardSwish
    Key optimizations:
    1. Precomputed DHW to reduce integer division
    2. Single spatial loop with recomputation of cheap ops
    3. Warp-level parallelism optimization
    4. Register pressure management via BLOCK_SIZE tuning
    """
    pid_n = tl.program_id(0)  # batch index
    pid_g = tl.program_id(1)  # group index
    
    if pid_n >= N or pid_g >= groups:
        return
    
    # Calculate group start
    group_start = pid_g * group_size
    
    # Allocate accumulators per thread (not per channel block)
    sum_val = 0.0
    sum_sq = 0.0
    
    # Step 1: Compute group statistics
    # Process spatial dimensions in blocks
    for spatial_base in range(0, DHW, BLOCK_SIZE):
        spatial_offs = spatial_base + tl.arange(0, BLOCK_SIZE)
        spatial_mask = spatial_offs < DHW
        
        # Process all channels in the group sequentially (reduces register pressure)
        for c in range(group_size):
            channel_idx = group_start + c
            
            # Calculate pointer offset for this channel and spatial block
            base_offset = (pid_n * stride_xn + 
                          channel_idx * stride_xc)
            
            # Load input values
            x_ptrs = base_offset + spatial_offs * stride_xdhw
            x_vals = tl.load(x_ptr + x_ptrs, mask=spatial_mask, other=0.0)
            
            # Apply Swish activation (keep in registers since exp is expensive)
            # Swish: x * sigmoid(x) = x / (1 + exp(-x))
            neg_x = -x_vals
            exp_neg_x = tl.math.exp(neg_x)
            sigmoid = 1.0 / (1.0 + exp_neg_x)
            x_swish = x_vals * sigmoid
            
            # Accumulate statistics
            sum_val += tl.sum(x_swish)
            sum_sq += tl.sum(x_swish * x_swish)
    
    # Compute group mean and variance
    count = DHW * group_size
    group_mean = sum_val / (count + eps)
    group_var = sum_sq / (count + eps) - group_mean * group_mean
    inv_std = 1.0 / tl.sqrt(group_var + eps)
    
    # Preload weight and bias for this group
    weight = tl.load(weight_ptr + group_start + tl.arange(0, group_size))
    bias = tl.load(bias_ptr + group_start + tl.arange(0, group_size))
    
    # Step 2: Apply normalization and HardSwish
    for spatial_base in range(0, DHW, BLOCK_SIZE):
        spatial_offs = spatial_base + tl.arange(0, BLOCK_SIZE)
        spatial_mask = spatial_offs < DHW
        
        # Process all channels in the group
        for c in range(group_size):
            channel_idx = group_start + c
            
            # Calculate pointer offset
            base_offset = (pid_n * stride_xn + 
                          channel_idx * stride_xc)
            
            # Load input values
            x_ptrs = base_offset + spatial_offs * stride_xdhw
            x_vals = tl.load(x_ptr + x_ptrs, mask=spatial_mask, other=0.0)
            
            # Apply Swish (recomputed - cheap relative to memory access)
            # Reuse same Swish computation pattern
            neg_x = -x_vals
            exp_neg_x = tl.math.exp(neg_x)
            sigmoid = 1.0 / (1.0 + exp_neg_x)
            x_swish = x_vals * sigmoid
            
            # Apply GroupNorm
            x_norm = (x_swish - group_mean) * inv_std
            
            # Apply affine transformation
            x_scaled = x_norm * weight[c] + bias[c]
            
            # Apply HardSwish
            # HardSwish: x * relu6(x + 3) / 6
            x_plus_3 = x_scaled + 3.0
            relu6_val = tl.minimum(tl.maximum(x_plus_3, 0.0), 6.0)
            x_final = x_scaled * relu6_val * (1.0 / 6.0)
            
            # Store result
            out_ptrs = base_offset + spatial_offs * stride_xdhw
            tl.store(out_ptr + out_ptrs, x_final, mask=spatial_mask)


def fused_post_convtranspose_3d_optimized(x, weight, bias, groups, eps):
    """
    Optimized wrapper for fused post-ops
    """
    N, C, D, H, W = x.shape
    out = torch.empty_like(x)
    
    if C % groups != 0:
        raise ValueError(f"Number of channels {C} must be divisible by groups {groups}")
    
    # Precompute DHW
    DHW = D * H * W
    group_size = C // groups
    
    # Grid calculation
    grid = (N, groups)
    
    # Launch optimized kernel
    fused_swish_groupnorm_hardswish_kernel_optimized[grid](
        x, weight, bias, out,
        N, C, DHW,
        groups, group_size, eps,
        x.stride(0), x.stride(1), x.stride(2),
    )
    
    return out


class ModelNew(nn.Module):
    """
    ConvTranspose3d (PyTorch native) + Optimized fused post-ops (Triton)
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, eps, bias=True):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size, 
            stride=stride, padding=padding, bias=bias
        )
        
        # GroupNorm parameters
        self.group_norm = nn.GroupNorm(num_groups=groups, num_channels=out_channels, eps=eps)
        self.groups = groups
        self.eps = eps

    def forward(self, x):
        # Step 1: PyTorch native ConvTranspose3d
        x = self.conv_transpose(x)
        
        # Step 2: Optimized fused post-ops
        x = fused_post_convtranspose_3d_optimized(
            x, 
            self.group_norm.weight, 
            self.group_norm.bias,
            self.groups,
            self.eps
        )
        
        return x
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl, AND any other modules used (e.g., import math if using math.sqrt)
   2. @triton.jit decorated kernel function(s) — NO continue/break/return inside loops (use masking)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels — THIS CLASS IS REQUIRED
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes
4. Ensure ALL imports are included at the top (common mistake: forgetting `import math`)

```python
# <corrected code>
```
