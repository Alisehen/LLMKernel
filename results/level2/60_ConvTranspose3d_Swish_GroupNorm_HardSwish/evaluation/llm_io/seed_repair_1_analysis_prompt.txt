You are a Triton kernel debugging expert. Analyze the error and identify the root cause.

## ERROR LOG
```
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 538, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_114735_batch_range56to80_deepseek_deepseek/60_ConvTranspose3d_Swish_GroupNorm_HardSwish/code/kernel_20251214_140743.py", line 208, in forward
    x = fused_post_convtranspose_3d(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_114735_batch_range56to80_deepseek_deepseek/60_ConvTranspose3d_Swish_GroupNorm_HardSwish/code/kernel_20251214_140743.py", line 174, in fused_post_convtranspose_3d
    fused_swish_groupnorm_hardswish_kernel[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 278, in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 81, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.UnsupportedLanguageConstruct: at 59:16:
        spatial_mask = spatial_offs < DHW

        # Reconstruct spatial indices
        d_idx = spatial_offs // (H * W)
        h_idx = (spatial_offs % (H * W)) // W
        w_idx = spatial_offs % W

        # Accumulate across all channels in the group
        for c in range(group_size):
            c_idx = start_c + c
            if c_idx >= C:
                break
                ^
unsupported AST node type: Break
```

## Expected Behavior (PyTorch Reference)
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Model that performs a 3D transposed convolution, applies Swish activation, 
    group normalization, and then HardSwish activation.
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, eps, bias=True):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)
        self.group_norm = nn.GroupNorm(num_groups=groups, num_channels=out_channels, eps=eps)

    def forward(self, x):
        x = self.conv_transpose(x)
        x = torch.sigmoid(x) * x  # Swish activation
        x = self.group_norm(x)
        x = torch.nn.functional.hardswish(x)  # HardSwish activation
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
groups = 4
eps = 1e-5

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, groups, eps]
```

## Current Implementation (Broken Triton Kernel)
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl
import math


@triton.jit
def swish_activation(x):
    """Swish activation: x * sigmoid(x)"""
    sigmoid = 1.0 / (1.0 + tl.exp(-x))
    return x * sigmoid


@triton.jit
def hardswish_activation(x):
    """HardSwish activation: x * relu6(x + 3) / 6"""
    # Compute relu6(x + 3) - clip between 0 and 6
    x_plus_3 = x + 3.0
    relu6 = tl.where(x_plus_3 < 0.0, 0.0, 
                    tl.where(x_plus_3 > 6.0, 6.0, x_plus_3))
    return x * relu6 / 6.0


@triton.jit
def fused_swish_groupnorm_hardswish_kernel(
    x_ptr,
    weight_ptr,
    bias_ptr,
    out_ptr,
    N, C, D, H, W,
    groups,
    eps,
    stride_xn, stride_xc, stride_xd, stride_xh, stride_xw,
    BLOCK_C: tl.constexpr,
    BLOCK_SIZE: tl.constexpr,
):
    """
    Fused kernel: Swish + GroupNorm + HardSwish
    Input: [N, C, D, H, W], groups for GroupNorm
    Output: [N, C, D, H, W]
    
    Each program processes one group in one sample across all spatial positions.
    We use program_id(0) for batch, program_id(1) for groups.
    """
    pid_n = tl.program_id(0)  # batch index
    pid_g = tl.program_id(1)  # group index
    
    if pid_n >= N or pid_g >= groups:
        return
    
    # Calculate group parameters
    group_size = C // groups
    start_c = pid_g * group_size
    end_c = start_c + group_size
    
    # Offsets for channels in this group
    offs_c = tl.arange(0, BLOCK_C)
    c_mask = (start_c + offs_c < end_c)
    
    # Precompute spatial dimensions
    DHW = D * H * W
    offs_spatial = tl.arange(0, BLOCK_SIZE)
    
    # Step 1: Compute mean and variance for the group
    # We accumulate across all spatial positions and channels in the group
    sum_val = tl.zeros((BLOCK_C,), dtype=tl.float32)
    sum_sq = tl.zeros((BLOCK_C,), dtype=tl.float32)
    count = 0
    
    for spatial_base in range(0, DHW, BLOCK_SIZE):
        spatial_offs = spatial_base + offs_spatial
        spatial_mask = spatial_offs < DHW
        
        # Reconstruct spatial indices
        d_idx = spatial_offs // (H * W)
        h_idx = (spatial_offs % (H * W)) // W
        w_idx = spatial_offs % W
        
        # Accumulate across all channels in the group
        for c in range(group_size):
            c_idx = start_c + c
            if c_idx >= C:
                break
                
            # Calculate pointer offsets
            x_ptrs = (x_ptr + 
                     pid_n * stride_xn + 
                     c_idx * stride_xc +
                     d_idx * stride_xd +
                     h_idx * stride_xh +
                     w_idx * stride_xw)
            
            # Load and apply Swish
            x_vals = tl.load(x_ptrs, mask=spatial_mask, other=0.0)
            x_swish = swish_activation(x_vals)
            
            # Accumulate for mean and variance
            sum_val = tl.where(c_mask & (offs_c == c), sum_val + tl.sum(x_swish, axis=0), sum_val)
            sum_sq = tl.where(c_mask & (offs_c == c), sum_sq + tl.sum(x_swish * x_swish, axis=0), sum_sq)
        
        count += tl.sum(spatial_mask) * tl.sum(c_mask)
    
    # Compute mean and variance for the group
    group_mean = tl.sum(sum_val) / (count + eps)
    group_var = tl.sum(sum_sq) / (count + eps) - group_mean * group_mean
    
    # Step 2: Apply normalization and activations
    for spatial_base in range(0, DHW, BLOCK_SIZE):
        spatial_offs = spatial_base + offs_spatial
        spatial_mask = spatial_offs < DHW
        
        # Reconstruct spatial indices
        d_idx = spatial_offs // (H * W)
        h_idx = (spatial_offs % (H * W)) // W
        w_idx = spatial_offs % W
        
        for c in range(group_size):
            c_idx = start_c + c
            if c_idx >= C:
                break
                
            # Load input
            x_ptrs = (x_ptr + 
                     pid_n * stride_xn + 
                     c_idx * stride_xc +
                     d_idx * stride_xd +
                     h_idx * stride_xh +
                     w_idx * stride_xw)
            
            x_vals = tl.load(x_ptrs, mask=spatial_mask, other=0.0)
            
            # Apply Swish
            x_swish = swish_activation(x_vals)
            
            # Apply GroupNorm
            x_norm = (x_swish - group_mean) / tl.sqrt(group_var + eps)
            
            # Load weight and bias for this channel
            weight = tl.load(weight_ptr + c_idx)
            bias = tl.load(bias_ptr + c_idx)
            
            # Apply affine transformation
            x_gn = x_norm * weight + bias
            
            # Apply HardSwish
            x_final = hardswish_activation(x_gn)
            
            # Store result
            out_ptrs = (out_ptr + 
                       pid_n * stride_xn + 
                       c_idx * stride_xc +
                       d_idx * stride_xd +
                       h_idx * stride_xh +
                       w_idx * stride_xw)
            
            tl.store(out_ptrs, x_final, mask=spatial_mask)


def fused_post_convtranspose_3d(x, weight, bias, groups, eps):
    """
    Fused: Swish + GroupNorm + HardSwish for 5D tensors
    """
    N, C, D, H, W = x.shape
    out = torch.empty_like(x)
    
    # Choose block sizes (powers of 2)
    BLOCK_C = triton.next_power_of_2(C // groups)
    BLOCK_SIZE = 128  # Spatial block size
    
    # Grid: (batch, groups)
    grid = (N, groups)
    
    fused_swish_groupnorm_hardswish_kernel[grid](
        x, weight, bias, out,
        N, C, D, H, W,
        groups, eps,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),
        BLOCK_C=BLOCK_C,
        BLOCK_SIZE=BLOCK_SIZE,
    )
    
    return out


class ModelNew(nn.Module):
    """
    ConvTranspose3d (PyTorch native) + Fused post-ops (Triton)
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, eps, bias=True):
        super(ModelNew, self).__init__()
        # Keep ConvTranspose3d as PyTorch native
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size, 
            stride=stride, padding=padding, bias=bias
        )
        
        # GroupNorm parameters - we'll use these in the fused kernel
        self.group_norm = nn.GroupNorm(num_groups=groups, num_channels=out_channels, eps=eps)
        self.groups = groups
        self.eps = eps

    def forward(self, x):
        # Step 1: PyTorch native ConvTranspose3d
        x = self.conv_transpose(x)
        
        # Step 2: Fused post-ops in Triton
        x = fused_post_convtranspose_3d(
            x, 
            self.group_norm.weight, 
            self.group_norm.bias,
            self.groups,
            self.eps
        )
        
        return x
```

---

## Your Task

Identify the **single most critical issue** that causes the error above.

### Analysis Guidelines

1. **Focus on root cause**, not symptoms
   - Bad: "Output is wrong"
   - Good: "BLOCK_K loop missing, only processes first 32 elements of K dimension"

2. **Be specific about WHAT and WHERE**
   - Bad: "Memory access issue"
   - Good: "Line 45: tl.atomic_add(c_block_ptr, acc) - atomic_add requires scalar pointer, not block_ptr"

3. **Prioritize by impact**
   - Correctness bugs > Performance issues > Style problems
   - Algorithm errors > Implementation details

### Output Format

**CRITICAL: You MUST output ONLY valid JSON. No other text allowed.**

```json
{
  "critical_issue": "<Concise description of THE root cause, max 30 words>",
  "why_it_matters": "<Why this causes the observed error, max 35 words>",
  "minimal_fix_hint": "<What needs to change (not how), max 30 words>"
}
```

**Remember**: Output ONLY the JSON block. No explanations, no commentary, no additional text.
