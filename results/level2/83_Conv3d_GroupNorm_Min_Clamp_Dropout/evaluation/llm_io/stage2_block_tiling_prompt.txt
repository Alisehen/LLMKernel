You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU: 4090

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: BLOCK_SIZE with register pressure awareness.

Key Principle:
- Fusion increases register usage (intermediates stay in registers)
- Spill to local memory kills fusion benefit

Register Pressure Signals (from NCU):
- launch__registers_per_thread > 128 → likely spilling
- launch__occupancy_limit_registers < other limits → register-bound

Rules:
- Start conservative: BLOCK_SIZE ∈ {256, 512} for element-wise
- For matmul fusion: BLOCK_M/N ∈ {32, 64}, BLOCK_K ∈ {32}
- If registers > 128: reduce BLOCK_* by half
- Trade-off: recompute cheap ops (e.g., x*0.5) vs store intermediate

When to Recompute vs Keep:
- Keep: expensive ops (exp, log, div, sqrt)
- Recompute: cheap ops (add, mul, max) if register pressure high
- Example: `y = relu(x); z = y * scale` → keep y
- Example: `y = x * 0.5; z = y + bias` → can recompute y if needed

Autotune:
- 2-3 BLOCK_SIZE configs, always include smaller fallback



[CURRENT CODE]
```python
# corrected code

import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_SIZE": 64}, num_warps=2),
        triton.Config({"BLOCK_SIZE": 128}, num_warps=4),
        triton.Config({"BLOCK_SIZE": 256}, num_warps=8),
    ],
    key=["numel"],
)
@triton.jit
def fill_min_value_kernel(
    y_ptr,              # *T
    numel,              # int32
    min_value,          # scalar (same dtype as *y_ptr)
    BLOCK_SIZE: tl.constexpr,
):
    """
    Kernel that fills the output tensor with a constant `min_value`.
    """
    pid = tl.program_id(0)
    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = offs < numel

    # Single, coalesced, masked store of the constant value
    tl.store(y_ptr + offs, min_value, mask=mask)


def groupnorm_min_clamp(x, weight, bias, groups, min_value, max_value, eps=1e-5):
    """
    Fused GroupNorm + torch.min + torch.clamp specialized for the identity:

        y = GroupNorm(x, weight, bias, groups, eps)
        y = torch.min(y, min_value)
        y = torch.clamp(y, min=min_value, max=max_value)

    For all inputs, the result of min+clamp is identically `min_value`,
    independent of GroupNorm output. We therefore bypass all computation and
    directly fill the output tensor with `min_value` using an optimized Triton
    kernel.
    """
    assert x.is_cuda, "Input must be on CUDA for Triton kernels."

    # Ensure a predictable, contiguous layout for linear indexing
    x = x.contiguous()

    # Allocate output tensor with same shape and dtype as x
    y = torch.empty_like(x)

    numel = y.numel()

    # 1D grid over all elements in the output tensor; ensure grid > 0
    def grid(meta):
        return (max(1, triton.cdiv(numel, meta["BLOCK_SIZE"])),)

    # Normalize min_value to a Python scalar so Triton sees a scalar, not a tensor
    if isinstance(min_value, torch.Tensor):
        min_scalar = float(min_value.item())
    else:
        min_scalar = float(min_value)

    # Launch kernel; autotuner selects best BLOCK_SIZE / num_warps
    fill_min_value_kernel[grid](
        y,            # y_ptr
        numel,        # numel
        min_scalar,   # scalar min_value (cast inside kernel to y.dtype)
    )
    return y


class ModelNew(nn.Module):
    """
    Model:
        x -> Conv3d -> (conceptual) GroupNorm -> min -> clamp -> Dropout

    The GroupNorm + min + clamp sequence is algebraically simplified:
      - For any input, the output of min+clamp is exactly `min_value`.
      - We implement this as a single Triton kernel that fills the tensor
        with `min_value`, achieving maximal throughput.

    Dropout is then applied using PyTorch's implementation.
    """
    def __init__(
        self,
        in_channels,
        out_channels,
        kernel_size,
        groups,
        min_value,
        max_value,
        dropout_p,
    ):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.groups = groups

        # Kept for API compatibility; they do not affect the optimized path
        self.gn_weight = nn.Parameter(torch.ones(out_channels))
        self.gn_bias = nn.Parameter(torch.zeros(out_channels))

        self.min_value = float(min_value)
        self.max_value = float(max_value)
        self.eps = 1e-5
        self.dropout = nn.Dropout(dropout_p)

    def forward(self, x):
        x = self.conv(x)

        # Fused & algebraically simplified GroupNorm + min + clamp
        x = groupnorm_min_clamp(
            x,
            self.gn_weight,
            self.gn_bias,
            self.groups,
            self.min_value,
            self.max_value,
            self.eps,
        )

        # Dropout applied after the constant activation
        x = self.dropout(x)
        return x
```

[NCU PROFILING METRICS]
No NCU metrics available

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

## CRITICAL — Code MUST compile and run:
1. EVERY kernel function MUST have `@triton.jit` decorator
2. Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
3. BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
4. `tl.program_id(axis)` only supports axis = 0, 1, 2
5. No `continue`, `break`, `return` inside loops — use masking
6. No tensor indexing with loop vars: `x[:, i]` is INVALID
7. mask shape MUST match data shape in tl.load/tl.store

## Missing Triton Functions (implement manually):
- tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

## OUTPUT FORMAT (STRICT):
1. Imports: torch, torch.nn, triton, triton.language as tl
2. @triton.jit decorated kernel function(s)
3. Wrapper function(s) for grid calculation and kernel launch
4. class ModelNew(nn.Module) that calls your kernels

Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
