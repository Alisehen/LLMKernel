Fix the Triton kernel errors. Generate correct code.

## ERROR LOG
```
Traceback (most recent call last):
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 43, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 2192, in store
    return _semantic.store(pointer, value, mask, boundary_check, cache_modifier, eviction_policy)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/semantic.py", line 1297, in store
    return self._store_legacy(ptr, val, mask, boundary_check, cache, eviction)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/semantic.py", line 1273, in _store_legacy
    val = self.cast(val, elt_ty)
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/semantic.py", line 899, in cast
    assert False, f'cannot cast {input} to {dst_ty}'
           ^^^^^
AssertionError: cannot cast pointer<fp32>[constexpr[64]] to <['64'], fp32>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 555, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251228_161811_batch_range79to85_openai_deepseek/83_Conv3d_GroupNorm_Min_Clamp_Dropout/code/kernel_20251228_172522.py", line 120, in forward
    x = groupnorm_min_clamp(
        ^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251228_161811_batch_range79to85_openai_deepseek/83_Conv3d_GroupNorm_Min_Clamp_Dropout/code/kernel_20251228_172522.py", line 73, in groupnorm_min_clamp
    fill_min_value_kernel[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 419, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 238, in run
    benchmark()
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 227, in benchmark
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 227, in <dictcomp>
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 162, in _bench
    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/testing.py", line 149, in do_bench
    fn()
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 148, in kernel_call
    self.fn.run(
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 733, in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 861, in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 300, in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 80, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 20:4:

    Grid & indexing:
      - 1D grid over flattened output tensor
      - offs: shared offsets for all fused ops (here only a constant store)
      - mask: single boundary mask shared by all accesses
    """
    pid = tl.program_id(0)
    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = offs < numel

    # Single, coalesced, masked store of the constant value
    tl.store(y_ptr + offs, min_value, mask=mask)
    ^
cannot cast pointer<fp32>[constexpr[64]] to <['64'], fp32>
```

## Broken Code
```python
# optimized Triton code

import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_SIZE": 64}, num_warps=2),
        triton.Config({"BLOCK_SIZE": 128}, num_warps=4),
        triton.Config({"BLOCK_SIZE": 256}, num_warps=8),
    ],
    key=["numel"],
)
@triton.jit
def fill_min_value_kernel(
    y_ptr,              # *T
    numel,              # int32
    min_value,          # scalar (same dtype as y_ptr)
    BLOCK_SIZE: tl.constexpr,
):
    """
    Kernel that fills the output tensor with a constant `min_value`.

    Grid & indexing:
      - 1D grid over flattened output tensor
      - offs: shared offsets for all fused ops (here only a constant store)
      - mask: single boundary mask shared by all accesses
    """
    pid = tl.program_id(0)
    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = offs < numel

    # Single, coalesced, masked store of the constant value
    tl.store(y_ptr + offs, min_value, mask=mask)


def groupnorm_min_clamp(x, weight, bias, groups, min_value, max_value, eps=1e-5):
    """
    Fused GroupNorm + torch.min + torch.clamp specialized for the identity:

        y = GroupNorm(x, weight, bias, groups, eps)
        y = torch.min(y, min_value)
        y = torch.clamp(y, min=min_value, max=max_value)

    For all inputs, the result of min+clamp is identically `min_value`,
    independent of GroupNorm output. We therefore bypass all computation and
    directly fill the output tensor with `min_value` using an optimized Triton
    kernel.

    All (implicit) fused ops share:
      - same 1D grid over the flattened output
      - same offsets `offs`
      - same boundary mask `mask`
    """
    assert x.is_cuda, "Input must be on CUDA for Triton kernels."

    # Ensure a predictable, contiguous layout for linear indexing
    x = x.contiguous()

    # Allocate output tensor with same shape and dtype as x
    y = torch.empty_like(x)

    numel = y.numel()

    # 1D grid over all elements in the output tensor
    def grid(meta):
        return (triton.cdiv(numel, meta["BLOCK_SIZE"]),)

    # Launch kernel; autotuner selects best BLOCK_SIZE / num_warps for this 4090
    fill_min_value_kernel[grid](
        y,                           # y_ptr
        numel,                       # numel
        x.new_tensor(min_value),     # min_value in correct dtype/device
    )
    return y


class ModelNew(nn.Module):
    """
    Model:
        x -> Conv3d -> (conceptual) GroupNorm -> min -> clamp -> Dropout

    The GroupNorm + min + clamp sequence is algebraically simplified:
      - For any input, the output of min+clamp is exactly `min_value`.
      - We implement this as a single Triton kernel that fills the tensor
        with `min_value`, achieving maximal throughput on the 4090.

    Dropout is then applied using PyTorch's implementation.
    """
    def __init__(
        self,
        in_channels,
        out_channels,
        kernel_size,
        groups,
        min_value,
        max_value,
        dropout_p,
    ):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.groups = groups

        # Kept for API compatibility; they do not affect the optimized path
        self.gn_weight = nn.Parameter(torch.ones(out_channels))
        self.gn_bias = nn.Parameter(torch.zeros(out_channels))

        self.min_value = float(min_value)
        self.max_value = float(max_value)
        self.eps = 1e-5
        self.dropout = nn.Dropout(dropout_p)

    def forward(self, x):
        x = self.conv(x)

        # Fused & algebraically simplified GroupNorm + min + clamp
        x = groupnorm_min_clamp(
            x,
            self.gn_weight,
            self.gn_bias,
            self.groups,
            self.min_value,
            self.max_value,
            self.eps,
        )

        # Dropout applied after the constant activation
        x = self.dropout(x)
        return x
```

## CRITICAL — These cause 60%+ of failures:
1. EVERY kernel function MUST have `@triton.jit` decorator — MANDATORY
2. Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
3. BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
4. `tl.program_id(axis)` only supports axis = 0, 1, 2
5. No `continue`, `break`, `return` inside loops — use masking
6. No tensor indexing with loop vars: `x[:, i]` is INVALID
7. mask shape MUST match data shape in tl.load/tl.store

## Missing Triton Functions (implement manually):
- tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

## OUTPUT FORMAT (STRICT):
1. Imports: torch, torch.nn, triton, triton.language as tl (and math if needed)
2. @triton.jit decorated kernel function(s)
3. Wrapper function(s) for grid calculation and kernel launch
4. class ModelNew(nn.Module) — REQUIRED

Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <corrected code>
```
