You are optimizing a Triton kernel based on algorithmic analysis.

# PyTorch Reference (Target Behavior)

```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Model that performs a 3D convolution, applies Group Normalization, minimum, clamp, and dropout.
    """
    def __init__(self, in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p):
        super(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.norm = nn.GroupNorm(groups, out_channels)
        self.dropout = nn.Dropout(dropout_p)

    def forward(self, x):
        x = self.conv(x)
        x = self.norm(x)
        x = torch.min(x, torch.tensor(min_value, device=x.device))
        x = torch.clamp(x, min=min_value, max=max_value)
        x = self.dropout(x)
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 64, 64
kernel_size = 3
groups = 8
min_value = 0.0
max_value = 1.0
dropout_p = 0.2

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p]
```

**CRITICAL**: Study the PyTorch code carefully to understand:
- What does `forward()` return? (full output sequence vs final hidden state only)
- What is the computational pattern?
- What are the input/output shapes?

Your optimized kernel MUST match this exact behavior.

---

# Analysis Results

**Bottleneck**: The kernel spends time computing mean/variance, normalizing, and applying affine parameters, even though `torch.min(x, min_value)` followed by `torch.clamp(x, min=min_value, max=max_value)` guarantees that every output element becomes exactly `min_value` (since it must be simultaneously ≤ min_value and ≥ min_value). Thus all GroupNorm work is wasted for this specific composition.

**Optimization Strategy**: Exploit the algebraic simplification of the activation sequence and replace the whole GroupNorm + min + clamp pipeline with a constant write of `min_value`, letting only dropout act afterwards (or equivalently, replace the fused kernel with a trivial kernel that ignores its input and just fills the output with `min_value`).

**Implementation Plan**: In the fused Triton kernel, detect the pattern where the same `min_value` is used for both `torch.min` and the lower bound of `torch.clamp`, and directly write `min_value` to `y_ptr` without reading `x_ptr` or computing GroupNorm statistics/affine transforms. At the PyTorch level, you can even bypass the Triton kernel entirely for this configuration, replacing `groupnorm_min_clamp(x, ...)` with `torch.full_like(x, min_value)` so that only dropout runs on the constant tensor, preserving mathematical behavior while eliminating all GroupNorm work.

**Expected Speedup**: 20-30%

---

# Current Kernel (needs optimization)

```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def groupnorm_min_clamp_kernel(
    x_ptr, y_ptr,
    gamma_ptr, beta_ptr,
    N, C,
    groups,
    channels_per_group,
    DHW,
    group_elems,
    eps, min_value, max_value,
    stride_N, stride_C,
    BLOCK_SIZE: tl.constexpr,
):
    # program id over (batch, group) pairs
    pid = tl.program_id(0)
    n = pid // groups
    g = pid % groups

    # offsets within flattened spatial dimension
    offs = tl.arange(0, BLOCK_SIZE)

    # -----------------------------
    # First pass: compute mean/var
    # -----------------------------
    sum_x = tl.zeros((), dtype=tl.float32)
    sum_x2 = tl.zeros((), dtype=tl.float32)

    # Base offset for this batch n
    base_n = n * stride_N

    # iterate over channels in this group
    for c_local in range(0, channels_per_group):
        c = g * channels_per_group + c_local
        channel_base = x_ptr + base_n + c * stride_C

        # iterate over spatial positions in tiles
        for offset in range(0, DHW, BLOCK_SIZE):
            idx = offset + offs
            mask = idx < DHW

            ptrs = channel_base + idx
            x_vals = tl.load(ptrs, mask=mask, other=0.0).to(tl.float32)

            sum_x += tl.sum(x_vals, axis=0)
            sum_x2 += tl.sum(x_vals * x_vals, axis=0)

    group_elems_f = tl.full((), group_elems, dtype=tl.float32)
    mean = sum_x / group_elems_f
    var = sum_x2 / group_elems_f - mean * mean
    rstd = 1.0 / tl.sqrt(var + eps)

    # -----------------------------
    # Second pass: normalize + affine + min + clamp
    # -----------------------------
    for c_local in range(0, channels_per_group):
        c = g * channels_per_group + c_local
        channel_base_x = x_ptr + base_n + c * stride_C
        channel_base_y = y_ptr + base_n + c * stride_C

        gamma = tl.load(gamma_ptr + c).to(tl.float32)
        beta = tl.load(beta_ptr + c).to(tl.float32)

        for offset in range(0, DHW, BLOCK_SIZE):
            idx = offset + offs
            mask = idx < DHW

            ptrs_x = channel_base_x + idx
            x_vals = tl.load(ptrs_x, mask=mask, other=0.0).to(tl.float32)

            # GroupNorm: normalize then affine
            norm = (x_vals - mean) * rstd
            out = norm * gamma + beta

            # torch.min(x, min_value)
            out = tl.minimum(out, min_value)

            # torch.clamp(out, min=min_value, max=max_value)
            out = tl.maximum(out, min_value)
            out = tl.minimum(out, max_value)

            ptrs_y = channel_base_y + idx
            tl.store(ptrs_y, out.to(tl.float32), mask=mask)


def groupnorm_min_clamp(x, weight, bias, groups, min_value, max_value, eps=1e-5):
    """
    Fused GroupNorm (with affine) + torch.min(x, min_value) + torch.clamp(x, min=min_value, max=max_value)
    for 5D tensors in NCDHW layout.
    """
    assert x.is_cuda, "Input must be on CUDA for Triton kernels."
    x = x.contiguous()
    weight = weight.contiguous()
    bias = bias.contiguous()

    N, C, D, H, W = x.shape
    DHW = D * H * W
    channels_per_group = C // groups
    group_elems = channels_per_group * DHW

    y = torch.empty_like(x)

    stride_N, stride_C, stride_D, stride_H, stride_W = x.stride()

    BLOCK_SIZE = 256

    grid = lambda META: (max(1, N * groups),)

    groupnorm_min_clamp_kernel[grid](
        x, y,
        weight, bias,
        N, C,
        groups,
        channels_per_group,
        DHW,
        group_elems,
        eps, float(min_value), float(max_value),
        stride_N, stride_C,
        BLOCK_SIZE=BLOCK_SIZE,
    )
    return y


class ModelNew(nn.Module):
    """
    Model that performs a 3D convolution, applies Group Normalization,
    then torch.min(x, min_value), torch.clamp(x, min=min_value, max=max_value),
    and finally dropout. GroupNorm + min + clamp are fused in a Triton kernel.
    """
    def __init__(self, in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.groups = groups
        # GroupNorm affine parameters
        self.gn_weight = nn.Parameter(torch.ones(out_channels))
        self.gn_bias = nn.Parameter(torch.zeros(out_channels))
        self.min_value = float(min_value)
        self.max_value = float(max_value)
        self.eps = 1e-5
        self.dropout = nn.Dropout(dropout_p)

    def forward(self, x):
        x = self.conv(x)
        x = groupnorm_min_clamp(
            x,
            self.gn_weight,
            self.gn_bias,
            self.groups,
            self.min_value,
            self.max_value,
            self.eps,
        )
        x = self.dropout(x)
        return x
```

---

# Your Task

Implement the optimization strategy above. Focus on the specific bottleneck identified.

## Key Requirements

1. **Preserve correctness**: Maintain the same input/output behavior
2. **Apply the optimization**: Follow the implementation plan exactly
3. **Use valid Triton syntax**:
   - Every kernel MUST have `@triton.jit` decorator
   - Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
   - BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
   - No `continue`, `break`, `return` inside kernels (use masking)
   - Prefer `tl.dot(a, b, allow_tf32=True)` for matmul operations

4. **CRITICAL for RNN/GRU/LSTM Persistent Kernels**:
   - Time loop MUST be inside @triton.jit kernel, NOT in Python forward()
   - **HYBRID computation strategy** (CRITICAL for performance):
     * Precompute input-side gates OUTSIDE kernel: `gates_x = (T*B, In) @ W_ih` (ONE large GEMM)
     * INSIDE kernel: only recurrent-side: `for t: gates_h = h @ W_hh` (T small GEMMs)
   - CORRECT (FAST - use this):
     ```python
     # Python forward():
     gates_x_all = x.reshape(T*B, In) @ W_ih + b_ih  # ONE large GEMM
     gates_x_all = gates_x_all.view(T, B, 3*H)
     gru_persistent_kernel[grid](gates_x_all, h0, W_hh, ...)  # Launch ONCE

     @triton.jit
     def gru_persistent_kernel(gates_x_ptr, h_ptr, W_hh_ptr, ...):
         for t in range(T):  # Inside kernel
             gates_x_t = tl.load(gates_x_ptr + t*...)  # Precomputed
             gates_h = h @ W_hh  # Only recurrent GEMM
             h = (1-z)*n + z*h   # Fuse and update
     ```

5. **Output format**:
   - Imports: `import torch, torch.nn as nn, triton, triton.language as tl`
   - `@triton.jit` kernel(s)
   - Wrapper function(s)
   - `class ModelNew(nn.Module)` — REQUIRED
   - NO testing code, NO `if __name__ == "__main__"`

---

Generate the optimized kernel now. Output ONLY the complete Python code.
