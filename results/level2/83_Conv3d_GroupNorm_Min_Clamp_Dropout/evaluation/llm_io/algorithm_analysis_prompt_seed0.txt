You are a GPU kernel optimization architect. Analyze the kernel and identify **ONE high-level algorithmic optimization**.

# PyTorch Reference
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Model that performs a 3D convolution, applies Group Normalization, minimum, clamp, and dropout.
    """
    def __init__(self, in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p):
        super(Model, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.norm = nn.GroupNorm(groups, out_channels)
        self.dropout = nn.Dropout(dropout_p)

    def forward(self, x):
        x = self.conv(x)
        x = self.norm(x)
        x = torch.min(x, torch.tensor(min_value, device=x.device))
        x = torch.clamp(x, min=min_value, max=max_value)
        x = self.dropout(x)
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 64, 64
kernel_size = 3
groups = 8
min_value = 0.0
max_value = 1.0
dropout_p = 0.2

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p]
```

# Current Triton Kernel
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def groupnorm_min_clamp_kernel(
    x_ptr, y_ptr,
    gamma_ptr, beta_ptr,
    N, C,
    groups,
    channels_per_group,
    DHW,
    group_elems,
    eps, min_value, max_value,
    stride_N, stride_C,
    BLOCK_SIZE: tl.constexpr,
):
    # program id over (batch, group) pairs
    pid = tl.program_id(0)
    n = pid // groups
    g = pid % groups

    # offsets within flattened spatial dimension
    offs = tl.arange(0, BLOCK_SIZE)

    # -----------------------------
    # First pass: compute mean/var
    # -----------------------------
    sum_x = tl.zeros((), dtype=tl.float32)
    sum_x2 = tl.zeros((), dtype=tl.float32)

    # Base offset for this batch n
    base_n = n * stride_N

    # iterate over channels in this group
    for c_local in range(0, channels_per_group):
        c = g * channels_per_group + c_local
        channel_base = x_ptr + base_n + c * stride_C

        # iterate over spatial positions in tiles
        for offset in range(0, DHW, BLOCK_SIZE):
            idx = offset + offs
            mask = idx < DHW

            ptrs = channel_base + idx
            x_vals = tl.load(ptrs, mask=mask, other=0.0).to(tl.float32)

            sum_x += tl.sum(x_vals, axis=0)
            sum_x2 += tl.sum(x_vals * x_vals, axis=0)

    group_elems_f = tl.full((), group_elems, dtype=tl.float32)
    mean = sum_x / group_elems_f
    var = sum_x2 / group_elems_f - mean * mean
    rstd = 1.0 / tl.sqrt(var + eps)

    # -----------------------------
    # Second pass: normalize + affine + min + clamp
    # -----------------------------
    for c_local in range(0, channels_per_group):
        c = g * channels_per_group + c_local
        channel_base_x = x_ptr + base_n + c * stride_C
        channel_base_y = y_ptr + base_n + c * stride_C

        gamma = tl.load(gamma_ptr + c).to(tl.float32)
        beta = tl.load(beta_ptr + c).to(tl.float32)

        for offset in range(0, DHW, BLOCK_SIZE):
            idx = offset + offs
            mask = idx < DHW

            ptrs_x = channel_base_x + idx
            x_vals = tl.load(ptrs_x, mask=mask, other=0.0).to(tl.float32)

            # GroupNorm: normalize then affine
            norm = (x_vals - mean) * rstd
            out = norm * gamma + beta

            # torch.min(x, min_value)
            out = tl.minimum(out, min_value)

            # torch.clamp(out, min=min_value, max=max_value)
            out = tl.maximum(out, min_value)
            out = tl.minimum(out, max_value)

            ptrs_y = channel_base_y + idx
            tl.store(ptrs_y, out.to(tl.float32), mask=mask)


def groupnorm_min_clamp(x, weight, bias, groups, min_value, max_value, eps=1e-5):
    """
    Fused GroupNorm (with affine) + torch.min(x, min_value) + torch.clamp(x, min=min_value, max=max_value)
    for 5D tensors in NCDHW layout.
    """
    assert x.is_cuda, "Input must be on CUDA for Triton kernels."
    x = x.contiguous()
    weight = weight.contiguous()
    bias = bias.contiguous()

    N, C, D, H, W = x.shape
    DHW = D * H * W
    channels_per_group = C // groups
    group_elems = channels_per_group * DHW

    y = torch.empty_like(x)

    stride_N, stride_C, stride_D, stride_H, stride_W = x.stride()

    BLOCK_SIZE = 256

    grid = lambda META: (max(1, N * groups),)

    groupnorm_min_clamp_kernel[grid](
        x, y,
        weight, bias,
        N, C,
        groups,
        channels_per_group,
        DHW,
        group_elems,
        eps, float(min_value), float(max_value),
        stride_N, stride_C,
        BLOCK_SIZE=BLOCK_SIZE,
    )
    return y


class ModelNew(nn.Module):
    """
    Model that performs a 3D convolution, applies Group Normalization,
    then torch.min(x, min_value), torch.clamp(x, min=min_value, max=max_value),
    and finally dropout. GroupNorm + min + clamp are fused in a Triton kernel.
    """
    def __init__(self, in_channels, out_channels, kernel_size, groups, min_value, max_value, dropout_p):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.groups = groups
        # GroupNorm affine parameters
        self.gn_weight = nn.Parameter(torch.ones(out_channels))
        self.gn_bias = nn.Parameter(torch.zeros(out_channels))
        self.min_value = float(min_value)
        self.max_value = float(max_value)
        self.eps = 1e-5
        self.dropout = nn.Dropout(dropout_p)

    def forward(self, x):
        x = self.conv(x)
        x = groupnorm_min_clamp(
            x,
            self.gn_weight,
            self.gn_bias,
            self.groups,
            self.min_value,
            self.max_value,
            self.eps,
        )
        x = self.dropout(x)
        return x
```

# Performance
- **PyTorch baseline**: 6.96 ms
- **Current Triton**: 5.41 ms
- **Current speedup**: 1.29x (+22.3% vs baseline)


---

## Analysis Steps

1. **Code Analysis**: Count kernels, identify operations, check for inefficiencies
2. **Performance Diagnosis**: Use metrics/latency to identify bottleneck type
3. **Root Cause**: Combine code + performance to find the core issue

## Optimization Categories (pick ONE if worth optimizing):

### 1. Operator Fusion
Fuse consecutive ops into fewer kernels to reduce memory traffic and launch overhead.

### 2. Algorithm Replacement
Replace naive algorithm with optimized variant.
- For Attention: Flash Attention, online softmax
- For Convolution: Winograd, im2col
- **For RNN/GRU/LSTM**: Persistent kernel with HYBRID computation
  - **CRITICAL**: Use hybrid approach for best performance:
    * Precompute input-side gates ONCE (outside kernel): `gates_x = (T*B, In) @ W_ih`
    * Persistent kernel (inside): only recurrent-side: `for t: gates_h = h @ W_hh`
  - Time loop `for t in range(T)` must be inside kernel, NOT in Python
  - Launch kernel once per layer, not once per timestep
  - Expected speedup: 10-100x (vs per-timestep launches)

### 3. Kernel Launch Reduction
Combine multiple small kernels to reduce overhead.
- **For RNN/GRU/LSTM**: See "Algorithm Replacement" above for persistent kernel approach

### 4. Memory Layout Optimization
Use in-place operations, buffer reuse, or better layouts.

## Should We Optimize?

Before proposing optimization, determine if it's worthwhile:
- **Not worth optimizing** if:
  - Code is already near-optimal (expected speedup < 10%)
  - Bottleneck cannot be addressed (hardware limited, already optimal algorithm)
  - Optimization would add significant complexity with minimal gain

- **Worth optimizing** if:
  - Clear algorithmic inefficiency exists (multiple kernels, suboptimal algorithm)
  - Expected speedup >= 20%
  - Concrete optimization path available

## Output (JSON)

```json
{
  "worth_optimizing": "yes/no",
  "reason": "<Why worth or not worth optimizing, 1 sentence>",
  "bottleneck": "<Root cause in 1-2 sentences, empty if not worth optimizing>",
  "optimisation method": "<Specific optimization in 1-2 sentences, empty if not worth optimizing>",
  "modification plan": "<Implementation steps in 2-3 sentences, empty if not worth optimizing>",
  "expected_speedup": "<e.g., '30-40%', empty if not worth optimizing>"
}
```

Return JSON only.
