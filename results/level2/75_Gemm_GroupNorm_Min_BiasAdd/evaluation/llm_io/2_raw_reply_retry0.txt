```python
# Optimized Triton code

import torch
import torch.nn as nn
import triton
import triton.language as tl


# -----------------------------------------------
# 1. GEMM + bias: [B, K] @ [N, K]^T -> [B, N]
#    Tuned for lower register pressure / higher occupancy
# -----------------------------------------------

@triton.autotune(
    configs=[
        # Main high-throughput config
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4, num_stages=3),
        # Skewed tiles for tall / wide matrices
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=2, num_stages=2),
    ],
    key=['M', 'N', 'K'],
)
@triton.jit
def linear_kernel(
    a_ptr, w_ptr, bias_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_wn, stride_wk,
    stride_cm, stride_cn,
    # meta-parameters
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    """
    Compute: C = A @ W^T + bias
    A: [M, K], row-major
    W: [N, K]  (we treat it as [K, N] via (stride_wk, stride_wn))
    bias: [N]
    C: [M, N]
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn

    # Accumulator in FP32
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    tl.multiple_of(offs_k, BLOCK_K)
    tl.max_contiguous(offs_m, BLOCK_M)
    tl.max_contiguous(offs_n, BLOCK_N)

    for k in range(0, K, BLOCK_K):
        k_mask = (k + offs_k) < K

        a = tl.load(
            a_ptrs,
            mask=(offs_m[:, None] < M) & k_mask[None, :],
            other=0.0,
        )
        w = tl.load(
            w_ptrs,
            mask=k_mask[:, None] & (offs_n[None, :] < N),
            other=0.0,
        )

        acc += tl.dot(a, w, allow_tf32=True)

        a_ptrs += BLOCK_K * stride_ak
        w_ptrs += BLOCK_K * stride_wk

    # Fuse bias add + store
    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)
    acc += bias[None, :]

    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    tl.store(c_ptrs, acc, mask=c_mask)


# -----------------------------------------------
# 2. GroupNorm + per-(row,group) min:
#    x: [B, C], G groups, channels_per_group = C // G
#    Produces mins_group: [B, G] = min over normalized+affine y within each group
#    Two-pass over channels to keep register pressure low.
# -----------------------------------------------

@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE': 256}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_SIZE': 128}, num_warps=2, num_stages=2),
    ],
    key=['channels_per_group'],
)
@triton.jit
def groupnorm_rowmin_kernel(
    x_ptr,            # [B, C]
    gamma_ptr,        # [C]
    beta_ptr,         # [C]
    mins_group_ptr,   # [B, G]
    B, C, G,
    channels_per_group,
    stride_xb, stride_xc,
    stride_gamma, stride_beta,
    stride_mgb, stride_mgg,
    eps,
    BLOCK_SIZE: tl.constexpr,
):
    """
    For each sample b and group g:
      - Compute GroupNorm over channels in that group
      - Compute min over y = normalized * gamma + beta within that group
      - Store result to mins_group[b, g]
    """
    pid = tl.program_id(0)
    b = pid // G
    g = pid % G

    valid_b = b < B
    if not valid_b:
        return

    group_offset = g * channels_per_group

    # -------------------------
    # Pass 1: compute mean and variance
    # -------------------------
    sum_x = tl.zeros((), dtype=tl.float32)
    sum_x2 = tl.zeros((), dtype=tl.float32)

    offs = tl.arange(0, BLOCK_SIZE)

    for ch_start in range(0, channels_per_group, BLOCK_SIZE):
        c_offset = ch_start + offs
        c_idx = group_offset + c_offset

        mask = (c_offset < channels_per_group) & (c_idx < C)

        x = tl.load(
            x_ptr + b * stride_xb + c_idx * stride_xc,
            mask=mask,
            other=0.0,
        )

        sum_x += tl.sum(x, axis=0)
        sum_x2 += tl.sum(x * x, axis=0)

    group_size = channels_per_group
    mean = sum_x / group_size
    mean2 = sum_x2 / group_size
    var = mean2 - mean * mean
    inv_std = 1.0 / tl.sqrt(var + eps)

    # -------------------------
    # Pass 2: compute y and per-group min
    # -------------------------
    large = 1e30
    curr_min = tl.full((BLOCK_SIZE,), large, dtype=tl.float32)

    for ch_start in range(0, channels_per_group, BLOCK_SIZE):
        c_offset = ch_start + offs
        c_idx = group_offset + c_offset

        mask = (c_offset < channels_per_group) & (c_idx < C)

        x = tl.load(
            x_ptr + b * stride_xb + c_idx * stride_xc,
            mask=mask,
            other=0.0,
        )
        gamma = tl.load(
            gamma_ptr + c_idx * stride_gamma,
            mask=mask,
            other=1.0,
        )
        beta = tl.load(
            beta_ptr + c_idx * stride_beta,
            mask=mask,
            other=0.0,
        )

        norm = (x - mean) * inv_std
        y = norm * gamma + beta

        curr_min = tl.where(mask, tl.minimum(curr_min, y), curr_min)

    group_min = tl.min(curr_min, axis=0)

    tl.store(
        mins_group_ptr + b * stride_mgb + g * stride_mgg,
        group_min,
        mask=valid_b,
    )


# -----------------------------------------------
# 3. Row-wise min over groups: [B, G] -> [B]
#    Much smaller than original [B, C] reduction
# -----------------------------------------------

@triton.autotune(
    configs=[
        triton.Config({'BLOCK_G': 32}, num_warps=2, num_stages=1),
        triton.Config({'BLOCK_G': 64}, num_warps=2, num_stages=1),
    ],
    key=['G'],
)
@triton.jit
def row_min_groups_kernel(
    x_ptr,       # [B, G]
    out_ptr,     # [B]
    B, G,
    stride_xb, stride_xg,
    stride_out,
    BLOCK_G: tl.constexpr,
):
    """
    Compute per-row minimum over groups:
      out[b] = min_g x[b, g]
    """
    b = tl.program_id(0)
    row_valid = b < B

    offs_g = tl.arange(0, BLOCK_G)
    large = 1e30
    curr_min = tl.full((BLOCK_G,), large, dtype=tl.float32)

    for g_start in range(0, G, BLOCK_G):
        idx_g = g_start + offs_g
        mask = row_valid & (idx_g < G)
        x = tl.load(
            x_ptr + b * stride_xb + idx_g * stride_xg,
            mask=mask,
            other=large,
        )
        curr_min = tl.minimum(curr_min, x)

    row_min = tl.min(curr_min, axis=0)
    tl.store(out_ptr + b * stride_out, row_min, mask=row_valid)


# -----------------------------------------------
# 4. Broadcast outer-sum: [B] + [C] -> [1, C, B, 1]
# -----------------------------------------------

@triton.autotune(
    configs=[
        triton.Config({'BLOCK_C': 64,  'BLOCK_B': 64},  num_warps=4, num_stages=2),
        triton.Config({'BLOCK_C': 128, 'BLOCK_B': 32},  num_warps=4, num_stages=2),
        triton.Config({'BLOCK_C': 32,  'BLOCK_B': 128}, num_warps=4, num_stages=2),
    ],
    key=['C', 'B'],
)
@triton.jit
def broadcast_add_bias_kernel(
    min_ptr,     # [B]
    bias_ptr,    # [C]
    out_ptr,     # [1, C, B, 1]
    B, C,
    stride_min,
    stride_bias,
    stride_o0, stride_o1, stride_o2, stride_o3,
    BLOCK_C: tl.constexpr, BLOCK_B: tl.constexpr,
):
    """
    Compute outer sum:
      out[0, c, b, 0] = min_vals[b] + bias[c]
    """
    pid_c = tl.program_id(0)
    pid_b = tl.program_id(1)

    offs_c = pid_c * BLOCK_C + tl.arange(0, BLOCK_C)
    offs_b = pid_b * BLOCK_B + tl.arange(0, BLOCK_B)

    mask_c = offs_c < C
    mask_b = offs_b < B
    mask = mask_c[:, None] & mask_b[None, :]

    bias_vals = tl.load(
        bias_ptr + offs_c * stride_bias,
        mask=mask_c,
        other=0.0,
    )
    min_vals = tl.load(
        min_ptr + offs_b * stride_min,
        mask=mask_b,
        other=0.0,
    )

    res = bias_vals[:, None] + min_vals[None, :]

    out_ptrs = (
        out_ptr
        + offs_c[:, None] * stride_o1
        + offs_b[None, :] * stride_o2
    )

    tl.store(
        out_ptrs,
        res,
        mask=mask,
    )


# -----------------------------------------------
# 5. Python wrapper to launch kernels
# -----------------------------------------------

def fused_linear_groupnorm_min_bias(
    x: torch.Tensor,
    weight: torch.Tensor,
    linear_bias: torch.Tensor,
    gn_weight: torch.Tensor,
    gn_bias: torch.Tensor,
    num_groups: int,
    final_bias: torch.Tensor,
    eps: float,
) -> torch.Tensor:
    """
    x:          [B, in_features]
    weight:     [out_features, in_features]
    linear_bias:[out_features]
    gn_weight:  [out_features]
    gn_bias:    [out_features]
    final_bias: [1, out_features, 1, 1]

    Returns:
      out: [1, out_features, B, 1]
    """
    assert x.is_cuda and weight.is_cuda and final_bias.is_cuda
    B, K = x.shape
    N = weight.shape[0]
    C = N
    assert gn_weight.shape[0] == C
    assert gn_bias.shape[0] == C
    assert C % num_groups == 0
    channels_per_group = C // num_groups

    # -----------------------------
    # 1) GEMM + bias: [B, K] @ [K, N] -> [B, N]
    # -----------------------------
    y = torch.empty((B, N), device=x.device, dtype=torch.float32)

    def grid_linear(meta):
        return (
            triton.cdiv(B, meta['BLOCK_M']),
            triton.cdiv(N, meta['BLOCK_N']),
        )

    linear_kernel[grid_linear](
        x, weight, linear_bias, y,
        B, N, K,
        x.stride(0), x.stride(1),
        weight.stride(0), weight.stride(1),
        y.stride(0), y.stride(1),
    )

    # -----------------------------
    # 2) GroupNorm + per-(row,group) min: [B, C] -> [B, G]
    #    (no full normalized tensor stored; minimizes bandwidth)
    # -----------------------------
    mins_group = torch.empty((B, num_groups), device=x.device, dtype=torch.float32)

    grid_gn = (B * num_groups,)

    groupnorm_rowmin_kernel[grid_gn](
        y, gn_weight, gn_bias, mins_group,
        B, C, num_groups,
        channels_per_group,
        y.stride(0), y.stride(1),
        gn_weight.stride(0), gn_bias.stride(0),
        mins_group.stride(0), mins_group.stride(1),
        eps,
    )

    # -----------------------------
    # 3) Min over groups dim=1: [B, G] -> [B]
    # -----------------------------
    mins = torch.empty((B,), device=x.device, dtype=torch.float32)

    grid_min = (B,)

    row_min_groups_kernel[grid_min](
        mins_group,
        mins,
        B, num_groups,
        mins_group.stride(0), mins_group.stride(1),
        mins.stride(0),
    )

    # -----------------------------
    # 4) Broadcast add with final bias: [1, C, B, 1]
    # -----------------------------
    out = torch.empty((1, C, B, 1), device=x.device, dtype=torch.float32)
    bias_vec = final_bias.view(C)

    def grid_bcast(meta):
        return (
            triton.cdiv(C, meta['BLOCK_C']),
            triton.cdiv(B, meta['BLOCK_B']),
        )

    broadcast_add_bias_kernel[grid_bcast](
        mins,
        bias_vec,
        out,
        B, C,
        mins.stride(0),
        bias_vec.stride(0),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3),
    )

    return out


# -----------------------------------------------
# 6. nn.Module wrapper
# -----------------------------------------------

class ModelNew(nn.Module):
    """
    Model that performs:
      Linear (GEMM) -> GroupNorm -> min over channels -> bias add (broadcast)
    Implemented with optimized Triton kernels.
    """

    def __init__(self, in_features, out_features, num_groups, bias_shape):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.group_norm = nn.GroupNorm(num_groups, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return fused_linear_groupnorm_min_bias(
            x,
            self.gemm.weight,
            self.gemm.bias,
            self.group_norm.weight,
            self.group_norm.bias,
            self.group_norm.num_groups,
            self.bias,
            self.group_norm.eps,
        )
```