You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Fine-tuning fused kernel parameters.

Params:
- num_warps ∈ {4, 8}
- num_stages ∈ {2, 3}

Conditional Rules (NOT one-size-fits-all):

IF register pressure LOW (regs < 96, no spill):
  - Try num_warps=8 for compute-bound fusion
  - num_stages=3 may help hide latency

IF register pressure HIGH (regs > 128 or occupancy_limit_registers):
  - Use num_warps=4 (fewer warps = more registers per warp)
  - Keep num_stages=2 (higher stages need more registers)

IF multi-input fusion (3+ distinct loads):
  - num_stages=2 preferred (each stage buffers all inputs)
  - num_warps=4 often better than 8

Autotune:
- Max 2-3 configs to reduce compilation time
- Always include conservative baseline (num_warps=4, num_stages=2)
- Test before/after: revert if gain < 2%



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


# -----------------------------------------------
# 1. GEMM + bias: [B, K] @ [N, K]^T -> [B, N]
#    Optimized for SM89 (Ada, e.g. RTX 4090)
# -----------------------------------------------

@triton.autotune(
    configs=[
        # Favor lower num_stages to reduce register pressure
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 64,  'BLOCK_N': 256, 'BLOCK_K': 32}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64,  'BLOCK_K': 32}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64,  'BLOCK_K': 64}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 64,  'BLOCK_N': 128, 'BLOCK_K': 64}, num_warps=4, num_stages=2),
        # A couple of higher-parallelism variants
        triton.Config({'BLOCK_M': 64,  'BLOCK_N': 64,  'BLOCK_K': 32}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32,  'BLOCK_K': 64}, num_warps=8, num_stages=2),
    ],
    key=['M', 'N', 'K'],
)
@triton.jit
def linear_kernel(
    a_ptr, w_ptr, bias_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_wn, stride_wk,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    """
    Compute: C = A @ W^T + bias
    A: [M, K], row-major
    W: [N, K]  (we treat it as [K, N] via (stride_wk, stride_wn))
    bias: [N]
    C: [M, N]
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    # Pointers for the first K-tile
    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn

    # Accumulator in FP32
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Main K loop
    for k in range(0, K, BLOCK_K):
        k_mask = (k + offs_k) < K

        a = tl.load(
            a_ptrs,
            mask=(offs_m[:, None] < M) & k_mask[None, :],
            other=0.0,
        )
        w = tl.load(
            w_ptrs,
            mask=k_mask[:, None] & (offs_n[None, :] < N),
            other=0.0,
        )

        acc += tl.dot(a, w, allow_tf32=True)

        a_ptrs += BLOCK_K * stride_ak
        w_ptrs += BLOCK_K * stride_wk

    # ----- FUSED elementwise: bias add + store -----
    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)

    # Bias is 1D over N; use same offs_n, boundary on N
    bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)
    acc += bias[None, :]

    # Single store for GEMM output
    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    tl.store(c_ptrs, acc, mask=c_mask)


# -----------------------------------------------
# 2. GroupNorm over channels for [B, C]
#    FUSED with row-wise min over channels: [B, C] -> [B]
#
#    This completely removes the separate row_min kernel and
#    eliminates all intermediate groupnorm output writes.
# -----------------------------------------------

@triton.jit
def groupnorm_rowmin_kernel(
    x_ptr,        # [B, C]  (output of GEMM)
    gamma_ptr,    # [C]
    beta_ptr,     # [C]
    mins_ptr,     # [B]     (output: per-row minimum after GroupNorm+affine)
    B, C, G,
    channels_per_group,
    stride_xb, stride_xc,
    stride_gamma, stride_beta,
    eps,
    BLOCK_SIZE: tl.constexpr,
):
    """
    GroupNorm over channels for 2D input [B, C], fused with row-wise minimum.

    For each sample b and group g:
      - group size = channels_per_group = C // G
      - normalize x[b, g*group:(g+1)*group] using mean/var over that group
      - apply affine: y = x_hat * gamma + beta
      - compute group's contribution to row-wise min over channels:
            row_min[b] = min_c y[b, c]

    Grid:
      pid in [0, B*G)
      b = pid // G, g = pid % G
      One program handles one (b, g) group.

    Output:
      mins_ptr[b] holds min_c y[b, c] after processing all groups.
      This kernel uses atomic_min to accumulate per-row minima.
    """
    pid = tl.program_id(0)
    b = pid // G
    g = pid % G

    # Guard against out-of-bounds (in case grid is slightly over-provisioned)
    if b >= B:
        return

    offs = tl.arange(0, BLOCK_SIZE)
    c_idx = g * channels_per_group + offs

    # Mask over channels within this group
    valid_ch = (offs < channels_per_group) & (c_idx < C)

    # Load group slice
    x = tl.load(
        x_ptr + b * stride_xb + c_idx * stride_xc,
        mask=valid_ch,
        other=0.0,
    )

    group_size = channels_per_group

    # Compute mean and variance over the group (E[x], E[x^2])
    mean = tl.sum(x, axis=0) / group_size
    mean2 = tl.sum(x * x, axis=0) / group_size
    var = mean2 - mean * mean
    inv_std = 1.0 / tl.sqrt(var + eps)

    x_hat = (x - mean) * inv_std

    # Affine parameters
    gamma = tl.load(
        gamma_ptr + c_idx * stride_gamma,
        mask=valid_ch,
        other=1.0,
    )
    beta = tl.load(
        beta_ptr + c_idx * stride_beta,
        mask=valid_ch,
        other=0.0,
    )

    y = x_hat * gamma + beta

    # Compute this group's contribution to the row-wise minimum.
    # Ignore padded/invalid lanes by setting them to a large value.
    large = 1e30
    y_masked = tl.where(valid_ch, y, large)
    group_min = tl.min(y_masked, axis=0)

    # Atomically accumulate into per-row minima.
    # mins_ptr[b] should be initialized to a large value on the host.
    tl.atomic_min(mins_ptr + b, group_min)


# -----------------------------------------------
# 3. Broadcast outer-sum: [B] + [C] -> [1, C, B, 1]
# -----------------------------------------------

@triton.autotune(
    configs=[
        triton.Config({'BLOCK_C': 64,  'BLOCK_B': 64},  num_warps=4, num_stages=2),
        triton.Config({'BLOCK_C': 128, 'BLOCK_B': 32},  num_warps=4, num_stages=2),
        triton.Config({'BLOCK_C': 32,  'BLOCK_B': 128}, num_warps=4, num_stages=2),
    ],
    key=['C', 'B'],
)
@triton.jit
def broadcast_add_bias_kernel(
    min_ptr,     # [B]
    bias_ptr,    # [C]
    out_ptr,     # [1, C, B, 1]
    B, C,
    stride_min,
    stride_bias,
    stride_o0, stride_o1, stride_o2, stride_o3,
    BLOCK_C: tl.constexpr, BLOCK_B: tl.constexpr,
):
    """
    Compute outer sum:
      out[0, c, b, 0] = min_vals[b] + bias[c]
    Shapes:
      min_vals: [B]
      bias:     [C]
      out:      [1, C, B, 1]
    """
    pid_c = tl.program_id(0)
    pid_b = tl.program_id(1)

    offs_c = pid_c * BLOCK_C + tl.arange(0, BLOCK_C)
    offs_b = pid_b * BLOCK_B + tl.arange(0, BLOCK_B)

    mask_c = offs_c < C
    mask_b = offs_b < B
    mask = mask_c[:, None] & mask_b[None, :]

    bias_vals = tl.load(
        bias_ptr + offs_c * stride_bias,
        mask=mask_c,
        other=0.0,
    )
    min_vals = tl.load(
        min_ptr + offs_b * stride_min,
        mask=mask_b,
        other=0.0,
    )

    # Outer sum: [BLOCK_C, BLOCK_B]
    res = bias_vals[:, None] + min_vals[None, :]

    out_ptrs = (
        out_ptr
        + offs_c[:, None] * stride_o1
        + offs_b[None, :] * stride_o2
    )

    # Single store for broadcast output
    tl.store(out_ptrs, res, mask=mask)


# -----------------------------------------------
# 4. Python wrapper to launch kernels
# -----------------------------------------------

def fused_linear_groupnorm_min_bias(
    x: torch.Tensor,
    weight: torch.Tensor,
    linear_bias: torch.Tensor,
    gn_weight: torch.Tensor,
    gn_bias: torch.Tensor,
    num_groups: int,
    final_bias: torch.Tensor,
    eps: float,
) -> torch.Tensor:
    """
    x:          [B, in_features]
    weight:     [out_features, in_features]
    linear_bias:[out_features]
    gn_weight:  [out_features]
    gn_bias:    [out_features]
    final_bias: [1, out_features, 1, 1]

    Returns:
      out: [1, out_features, B, 1]

    Pipeline:
      1) GEMM + bias       (Triton, tensor-core, tf32)
      2) GroupNorm + min   (fused in one Triton kernel; no y_norm writes)
      3) Broadcast + bias  (Triton)
    """
    assert x.is_cuda and weight.is_cuda and final_bias.is_cuda
    B, K = x.shape
    N = weight.shape[0]
    C = N
    assert gn_weight.shape[0] == C
    assert gn_bias.shape[0] == C
    assert C % num_groups == 0
    channels_per_group = C // num_groups

    # -----------------------------
    # 1) GEMM + bias: [B, K] @ [K, N] -> [B, N]
    # -----------------------------
    y = torch.empty((B, N), device=x.device, dtype=torch.float32)

    def grid_linear(meta):
        return (
            triton.cdiv(B, meta['BLOCK_M']),
            triton.cdiv(N, meta['BLOCK_N']),
        )

    linear_kernel[grid_linear](
        x, weight, linear_bias, y,
        B, N, K,
        x.stride(0), x.stride(1),
        weight.stride(0), weight.stride(1),
        y.stride(0), y.stride(1),
    )

    # -----------------------------
    # 2) GroupNorm over channels + fused row-wise min: [B, C] -> [B]
    # -----------------------------
    # Initialize row-wise minima with a large value for atomic_min
    mins = torch.full(
        (B,),
        1e30,
        device=x.device,
        dtype=torch.float32,
    )

    # Choose BLOCK_SIZE as next power-of-two ≥ channels_per_group for efficient reductions
    BLOCK_SIZE = min(
        1024,  # Triton max vector length
        1 << (channels_per_group - 1).bit_length(),
    )

    grid_gn = (B * num_groups,)

    groupnorm_rowmin_kernel[grid_gn](
        y,
        gn_weight,
        gn_bias,
        mins,
        B, C, num_groups,
        channels_per_group,
        y.stride(0), y.stride(1),
        gn_weight.stride(0), gn_bias.stride(0),
        eps,
        BLOCK_SIZE=BLOCK_SIZE,
        num_warps=4,
        num_stages=2,
    )

    # -----------------------------
    # 3) Broadcast add with final bias: [1, C, B, 1]
    # -----------------------------
    out = torch.empty((1, C, B, 1), device=x.device, dtype=torch.float32)
    bias_vec = final_bias.view(C)

    def grid_bcast(meta):
        return (
            triton.cdiv(C, meta['BLOCK_C']),
            triton.cdiv(B, meta['BLOCK_B']),
        )

    broadcast_add_bias_kernel[grid_bcast](
        mins,
        bias_vec,
        out,
        B, C,
        mins.stride(0),
        bias_vec.stride(0),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3),
    )

    return out


# -----------------------------------------------
# 5. nn.Module wrapper
# -----------------------------------------------

class ModelNew(nn.Module):
    """
    Model that performs:
      Linear (GEMM) -> GroupNorm -> min over channels -> bias add (broadcast)
    Implemented with aggressively optimized Triton kernels.

    Key optimizations:
      - GEMM uses autotuned tiling targeted for SM89, reduced num_stages
        to increase occupancy on RTX 4090.
      - GroupNorm and row-wise min are fused into a single kernel, removing
        all intermediate groupnorm output stores and the separate row_min kernel.
      - Broadcast add is tiled for coalesced accesses along the B dimension.
    """

    def __init__(self, in_features, out_features, num_groups, bias_shape):
        super(ModelNew, self).__init__()
        # Maintain same submodules for compatibility with existing state_dicts
        self.gemm = nn.Linear(in_features, out_features)
        self.group_norm = nn.GroupNorm(num_groups, out_features)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return fused_linear_groupnorm_min_bias(
            x,
            self.gemm.weight,
            self.gemm.bias,
            self.group_norm.weight,
            self.group_norm.bias,
            self.group_norm.num_groups,
            self.bias,
            self.group_norm.eps,
        )
```

[NCU PROFILING METRICS]
{
  "linear_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 42.76,
    "launch__grid_size": 1024,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 15.78,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 19.0,
    "lts__t_sector_hit_rate.pct": 94.1
  },
  "groupnorm_rowmin_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 62.13,
    "launch__grid_size": 524288,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 74.68,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 8.64,
    "lts__t_sector_hit_rate.pct": 79.62
  },
  "broadcast_add_bias_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 6.76,
    "launch__grid_size": 2048,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 65.33,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 1.89,
    "lts__t_sector_hit_rate.pct": 99.87
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

TRITON API CONSTRAINTS (CRITICAL):
- Triton has NO: tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
