You are a Triton kernel debugging expert. Analyze the error and identify the root cause.

## ERROR LOG
```
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 626, in compare_and_bench
    raise ValueError(
ValueError: Outputs are not close (atol=0.001, rtol=0.01). max_abs_err=3.125e+07, mean_abs_err=3.125e+07
```

## Expected Behavior (PyTorch Reference)
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    This model performs a convolution, average pooling, applies sigmoid, and sums the result.
    """
    def __init__(self, in_channels, out_channels, kernel_size, pool_kernel_size):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.avg_pool = nn.AvgPool2d(pool_kernel_size)

    def forward(self, x):
        x = self.conv(x)
        x = self.avg_pool(x)
        x = torch.sigmoid(x)
        x = torch.sum(x, dim=[1,2,3]) # Sum over all spatial dimensions
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 384, 384
kernel_size = 3
pool_kernel_size = 4

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, pool_kernel_size]
```

## Current Implementation (Broken Triton Kernel)
```python
# <complete ModelNew code with optimized Triton kernels>
import torch
import torch.nn as nn
import triton
import triton.language as tl


# ============================
# Conv2D forward kernel
# ============================

@triton.autotune(
    configs=[
        triton.Config(
            {'BLOCK_M': 32, 'BLOCK_N': 32},
            num_warps=4,
            num_stages=2,
        ),
        triton.Config(
            {'BLOCK_M': 64, 'BLOCK_N': 32},
            num_warps=4,
            num_stages=2,
        ),
        triton.Config(
            {'BLOCK_M': 32, 'BLOCK_N': 64},
            num_warps=4,
            num_stages=2,
        ),
    ],
    key=['H_out', 'W_out', 'C_out'],
)
@triton.jit
def conv2d_forward_kernel(
    x_ptr, w_ptr, b_ptr, y_ptr,
    N, H, W, H_out, W_out, C_out,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_wn, stride_wc, stride_wh, stride_ww,
    stride_yn, stride_yc, stride_yh, stride_yw,
    C_IN: tl.constexpr, K: tl.constexpr,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,
):
    # pid_m: flattened output positions (n, oh, ow)
    # pid_n: output channels
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    P = N * H_out * W_out

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    mask_m = offs_m < P
    mask_n = offs_n < C_out

    # Decode flattened index -> (n, oh, ow)
    HW_out = H_out * W_out
    n_idx = offs_m // HW_out
    rem = offs_m % HW_out
    oh_idx = rem // W_out
    ow_idx = rem % W_out

    # Precompute base input offsets (top-left of receptive field for each output)
    # x[n, ic, oh + kh, ow + kw] index = base + ic*stride_xc + kh*stride_xh + kw*stride_xw
    x_base_offsets = (
        n_idx * stride_xn +
        oh_idx * stride_xh +
        ow_idx * stride_xw
    )

    # Accumulator in fp32
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Convolution: sum over input channels and kernel window
    # Inner loops kept small; BLOCK_M/BLOCK_N autotuned to keep register pressure in check.
    for ic in range(0, C_IN):
        ic_offset = ic * stride_xc
        for kh in tl.static_range(0, K):
            kh_offset = kh * stride_xh
            for kw in tl.static_range(0, K):
                kw_offset = kw * stride_xw

                # Input pointers: shape [BLOCK_M]
                x_offsets = x_base_offsets + ic_offset + kh_offset + kw_offset
                x_ptrs = x_ptr + x_offsets
                x_vals = tl.load(x_ptrs, mask=mask_m, other=0.0).to(tl.float32)

                # Weight pointers: shape [BLOCK_N]
                w_offsets = (
                    offs_n * stride_wn +
                    ic * stride_wc +
                    kh * stride_wh +
                    kw * stride_ww
                )
                w_ptrs = w_ptr + w_offsets
                w_vals = tl.load(w_ptrs, mask=mask_n, other=0.0).to(tl.float32)

                # Outer product update
                acc += x_vals[:, None] * w_vals[None, :]

    # Add bias (fused, uses same offs_n/masks)
    bias_vals = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0).to(tl.float32)
    acc += bias_vals[None, :]

    # Store result
    y_ptrs = (
        y_ptr +
        n_idx[:, None] * stride_yn +
        offs_n[None, :] * stride_yc +
        oh_idx[:, None] * stride_yh +
        ow_idx[:, None] * stride_yw
    )
    out_mask = mask_m[:, None] & mask_n[None, :]
    tl.store(y_ptrs, acc, mask=out_mask)


# ============================
# AvgPool + Sigmoid + Sum kernel
# ============================

@triton.autotune(
    configs=[
        triton.Config(
            {'BLOCK_HW': 64, 'BLOCK_C': 4},
            num_warps=4,
            num_stages=2,
        ),
        triton.Config(
            {'BLOCK_HW': 128, 'BLOCK_C': 4},
            num_warps=4,
            num_stages=2,
        ),
        triton.Config(
            {'BLOCK_HW': 64, 'BLOCK_C': 8},
            num_warps=4,
            num_stages=2,
        ),
        triton.Config(
            {'BLOCK_HW': 32, 'BLOCK_C': 4},  # conservative fallback for high register pressure
            num_warps=2,
            num_stages=2,
        ),
    ],
    key=['H_out', 'W_out', 'C_OUT', 'POOL_K'],
)
@triton.jit
def avgpool_sigmoid_sum_kernel(
    x_ptr, out_ptr,
    N, H_out, W_out, Hp, Wp,
    total_hw,
    stride_xn, stride_xc, stride_xh, stride_xw,
    C_OUT: tl.constexpr, POOL_K: tl.constexpr,
    BLOCK_HW: tl.constexpr, BLOCK_C: tl.constexpr,
):
    """
    Layout:
      - 3D grid: [pooled_hw_tiles, channel_tiles, batch]
      - Tile over pooled spatial positions (HW) and channels (C).
      - One atomic_add per (n, tile_hw, tile_c) program.
    """
    pid_hw = tl.program_id(0)  # tile over pooled H*W
    pid_c = tl.program_id(1)   # tile over channels
    pid_n = tl.program_id(2)   # batch index

    # Tiled pooled spatial indices
    offs_hw = pid_hw * BLOCK_HW + tl.arange(0, BLOCK_HW)
    hw_mask = offs_hw < total_hw

    # Decode flattened pooled index -> (ohp, owp) in pooled space
    ohp = offs_hw // Wp
    owp = offs_hw % Wp

    # Tiled channels
    offs_c = pid_c * BLOCK_C + tl.arange(0, BLOCK_C)
    c_mask = offs_c < C_OUT

    # Accumulate pooled values for this (n, tile_hw, tile_c)
    acc = tl.zeros((BLOCK_HW, BLOCK_C), dtype=tl.float32)

    base_n = pid_n * stride_xn

    for ph in tl.static_range(0, POOL_K):
        for pw in tl.static_range(0, POOL_K):
            ih = ohp * POOL_K + ph  # [BLOCK_HW]
            iw = owp * POOL_K + pw  # [BLOCK_HW]

            x_offsets = (
                base_n +
                ih[:, None] * stride_xh +
                iw[:, None] * stride_xw +
                offs_c[None, :] * stride_xc
            )
            mask = hw_mask[:, None] & c_mask[None, :]
            x_vals = tl.load(
                x_ptr + x_offsets,
                mask=mask,
                other=0.0,
            ).to(tl.float32)
            acc += x_vals

    # Average pooling
    pool_area = 1.0 / (POOL_K * POOL_K)
    acc = acc * pool_area

    # Sigmoid: 1 / (1 + exp(-x))
    exp_neg = tl.exp(-acc)
    sig = 1.0 / (1.0 + exp_neg)

    # Zero-out inactive lanes (to avoid polluting reduction)
    mask = hw_mask[:, None] & c_mask[None, :]
    sig = sig * mask.to(sig.dtype)

    # Reduce over tile (HW, C) -> scalar contribution for this (n)
    tile_sum = tl.sum(sig, axis=0)
    tile_sum = tl.sum(tile_sum, axis=0)

    # Atomic add into output[n]
    tl.atomic_add(out_ptr + pid_n, tile_sum)


# ============================
# Wrapper functions
# ============================

def conv2d_triton(x, weight, bias, kernel_size: int):
    """
    x:      [N, C_in, H, W]
    weight: [C_out, C_in, K, K]
    bias:   [C_out]
    """
    assert x.ndim == 4
    assert weight.ndim == 4
    assert bias.ndim == 1

    N, C_in, H, W = x.shape
    C_out, C_in_w, K, K_w = weight.shape
    assert C_in_w == C_in and K_w == kernel_size and K == kernel_size
    assert bias.shape[0] == C_out

    # Valid conv2d (stride=1, padding=0)
    H_out = H - kernel_size + 1
    W_out = W - kernel_size + 1
    assert H_out > 0 and W_out > 0

    x_contig = x.contiguous()
    w_contig = weight.contiguous()
    b_contig = bias.contiguous()

    y = torch.empty((N, C_out, H_out, W_out), device=x.device, dtype=x.dtype)

    P = N * H_out * W_out

    def grid(meta):
        return (
            triton.cdiv(P, meta['BLOCK_M']),
            triton.cdiv(C_out, meta['BLOCK_N']),
        )

    conv2d_forward_kernel[grid](
        x_contig, w_contig, b_contig, y,
        N, H, W, H_out, W_out, C_out,
        x_contig.stride(0), x_contig.stride(1),
        x_contig.stride(2), x_contig.stride(3),
        w_contig.stride(0), w_contig.stride(1),
        w_contig.stride(2), w_contig.stride(3),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3),
        C_IN=C_in,
        K=kernel_size,
    )

    return y


def avgpool_sigmoid_sum_triton(conv_out, pool_kernel_size: int):
    """
    conv_out: [N, C_out, H_out, W_out]
    Returns:  [N] (sum over channels and spatial dims of sigmoid(avg_pool(conv_out)))
    """
    assert conv_out.ndim == 4
    N, C_out, H_out, W_out = conv_out.shape

    stride = pool_kernel_size
    Hp = (H_out - pool_kernel_size) // stride + 1
    Wp = (W_out - pool_kernel_size) // stride + 1
    assert Hp > 0 and Wp > 0

    x_contig = conv_out.contiguous()
    out = torch.zeros((N,), device=conv_out.device, dtype=conv_out.dtype)

    total_hw = Hp * Wp

    def grid(meta):
        return (
            triton.cdiv(total_hw, meta['BLOCK_HW']),  # pooled H*W tiles
            triton.cdiv(C_out, meta['BLOCK_C']),      # channel tiles
            N,                                        # batch dimension
        )

    avgpool_sigmoid_sum_kernel[grid](
        x_contig,
        out,
        N, H_out, W_out, Hp, Wp,
        total_hw,
        x_contig.stride(0), x_contig.stride(1),
        x_contig.stride(2), x_contig.stride(3),
        C_OUT=C_out,
        POOL_K=pool_kernel_size,
    )

    return out


# ============================
# Model wrapper
# ============================

class ModelNew(nn.Module):
    """
    Triton implementation of:
      Conv2d -> AvgPool2d -> Sigmoid -> Sum over [C, H, W]
    """

    def __init__(self, in_channels, out_channels, kernel_size, pool_kernel_size):
        super(ModelNew, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.pool_kernel_size = pool_kernel_size

        # Match PyTorch Conv2d parameter shapes (bias enabled)
        self.weight = nn.Parameter(
            torch.randn(out_channels, in_channels, kernel_size, kernel_size)
        )
        self.bias = nn.Parameter(torch.randn(out_channels))

    def forward(self, x):
        # x: [N, C_in, H, W]
        y_conv = conv2d_triton(x, self.weight, self.bias, self.kernel_size)
        out = avgpool_sigmoid_sum_triton(y_conv, self.pool_kernel_size)
        return out
```

---

## Your Task

Identify the **single most critical issue** that causes the error above.

### Analysis Guidelines

1. **Focus on root cause**, not symptoms
   - Bad: "Output is wrong"
   - Good: "BLOCK_K loop missing, only processes first 32 elements of K dimension"

2. **Be specific about WHAT and WHERE**
   - Bad: "Memory access issue"
   - Good: "Line 45: tl.atomic_add(c_block_ptr, acc) - atomic_add requires scalar pointer, not block_ptr"

3. **Prioritize by impact**
   - Correctness bugs > Performance issues > Style problems
   - Algorithm errors > Implementation details

### Output Format

**CRITICAL: You MUST output ONLY valid JSON. No other text allowed.**

```json
{
  "critical_issue": "<Concise description of THE root cause, max 30 words>",
  "why_it_matters": "<Why this causes the observed error, max 35 words>",
  "minimal_fix_hint": "<What needs to change (not how), max 30 words>"
}
```

**Remember**: Output ONLY the JSON block. No explanations, no commentary, no additional text.
