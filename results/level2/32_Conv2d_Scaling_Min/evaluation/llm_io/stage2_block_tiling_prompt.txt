You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU: 4090

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: BLOCK_SIZE with register pressure awareness.

Key Principle:
- Fusion increases register usage (intermediates stay in registers)
- Spill to local memory kills fusion benefit

Register Pressure Signals (from NCU):
- launch__registers_per_thread > 128 → likely spilling
- launch__occupancy_limit_registers < other limits → register-bound

Rules:
- Start conservative: BLOCK_SIZE ∈ {256, 512} for element-wise
- For matmul fusion: BLOCK_M/N ∈ {32, 64}, BLOCK_K ∈ {32}
- If registers > 128: reduce BLOCK_* by half
- Trade-off: recompute cheap ops (e.g., x*0.5) vs store intermediate

When to Recompute vs Keep:
- Keep: expensive ops (exp, log, div, sqrt)
- Recompute: cheap ops (add, mul, max) if register pressure high
- Example: `y = relu(x); z = y * scale` → keep y
- Example: `y = x * 0.5; z = y + bias` → can recompute y if needed

Autotune:
- 2-3 BLOCK_SIZE configs, always include smaller fallback



[CURRENT CODE]
```python
# Optimized Triton code for fused scale + channel-wise min on RTX 4090

import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_M': 128, 'BLOCK_C': 32}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 128, 'BLOCK_C': 64}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 256, 'BLOCK_C': 32}, num_warps=8, num_stages=3),
    ],
    key=['M', 'C'],
)
@triton.jit
def scale_channel_min_kernel(
    x_ptr,  # *const T, (N, C, H, W)
    y_ptr,  # *T,       (N, 1, H, W)
    N, C, H, W, M,      # M = N * H * W
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_yn, stride_yc, stride_yh, stride_yw,
    scale,              # float scalar
    IS_POS: tl.constexpr,          # True: use min, False: use max
    BLOCK_M: tl.constexpr,         # number of (n, h, w) elements per program
    BLOCK_C: tl.constexpr,         # channel tile size
):
    # Program id along the flattened (N, H, W) dimension
    pid = tl.program_id(0)
    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)
    mask_m = offs_m < M  # (BLOCK_M,)

    # Decode offs_m -> (n, h, w)
    hw = H * W
    n_idx = offs_m // hw
    rem = offs_m % hw
    h_idx = rem // W
    w_idx = rem % W

    # Base offset for each (n, h, w) pixel in x and y
    base_x = (
        n_idx * stride_xn
        + h_idx * stride_xh
        + w_idx * stride_xw
    )
    base_y = (
        n_idx * stride_yn
        + h_idx * stride_yh
        + w_idx * stride_yw
    )

    # Initialize accumulator with +/- inf in fp32
    pos_inf = 3.402823e38
    neg_inf = -3.402823e38
    if IS_POS:
        acc = tl.full((BLOCK_M,), pos_inf, dtype=tl.float32)
        other = pos_inf
    else:
        acc = tl.full((BLOCK_M,), neg_inf, dtype=tl.float32)
        other = neg_inf

    # Loop over channel dimension in BLOCK_C tiles
    # All loads share the same (offs_m, mask_m) and broadcasted channel offsets.
    for c_start in range(0, C, BLOCK_C):
        offs_c = c_start + tl.arange(0, BLOCK_C)          # (BLOCK_C,)
        mask_c = offs_c < C                               # (BLOCK_C,)

        # 2D pointer matrix: [BLOCK_M, BLOCK_C]
        x_ptrs = x_ptr + base_x[:, None] + offs_c[None, :] * stride_xc
        mask = mask_m[:, None] & mask_c[None, :]          # (BLOCK_M, BLOCK_C)

        vals = tl.load(x_ptrs, mask=mask, other=other)
        vals = vals.to(tl.float32)

        if IS_POS:
            tile_red = tl.min(vals, axis=1)               # (BLOCK_M,)
            acc = tl.minimum(acc, tile_red)
        else:
            tile_red = tl.max(vals, axis=1)               # (BLOCK_M,)
            acc = tl.maximum(acc, tile_red)

    # Apply scale after reduction (handles negative scale correctly)
    acc = acc * scale

    # Store result at channel 0: y[n, 0, h, w]
    y_ptrs = y_ptr + base_y
    tl.store(y_ptrs, acc, mask=mask_m)


def scale_min_triton(conv_out: torch.Tensor, scale_factor: float) -> torch.Tensor:
    """
    Fused scale + channel-wise min over Conv2d output using Triton.

    Args:
        conv_out: (N, C, H, W) CUDA tensor
        scale_factor: float scalar

    Returns:
        y: (N, 1, H, W) where
           y[n, 0, h, w] = min_c(conv_out[n, c, h, w] * scale_factor)
    """
    assert conv_out.is_cuda, "Triton kernel requires CUDA tensor"
    conv_out = conv_out.contiguous()

    N, C, H, W = conv_out.shape
    M = N * H * W

    y = torch.empty((N, 1, H, W), device=conv_out.device, dtype=conv_out.dtype)

    stride_xn, stride_xc, stride_xh, stride_xw = conv_out.stride()
    stride_yn, stride_yc, stride_yh, stride_yw = y.stride()

    is_pos = float(scale_factor) >= 0.0

    # 1D grid over flattened (N, H, W) dimension
    def grid(meta):
        return (triton.cdiv(M, meta["BLOCK_M"]),)

    scale_channel_min_kernel[grid](
        conv_out,
        y,
        N, C, H, W, M,
        stride_xn, stride_xc, stride_xh, stride_xw,
        stride_yn, stride_yc, stride_yh, stride_yw,
        float(scale_factor),
        IS_POS=is_pos,
    )
    return y


class ModelNew(nn.Module):
    """
    Optimized model:
    - Uses cuDNN (torch.nn.Conv2d) for convolution
    - Uses a Triton kernel to fuse scaling and channel-wise minimum:
        y[n, 0, h, w] = min_c( conv(x)[n, c, h, w] * scale_factor )
    """
    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scale_factor = float(scale_factor)

    def forward(self, x):
        x = self.conv(x)
        if x.is_cuda:
            return scale_min_triton(x, self.scale_factor)
        else:
            x = x * self.scale_factor
            return torch.min(x, dim=1, keepdim=True)[0]
```

[NCU PROFILING METRICS]
{
  "scale_channel_min_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 4.76,
    "launch__grid_size": 32258,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 54.09,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 87.96,
    "lts__t_sector_hit_rate.pct": 6.39
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

## CRITICAL — Code MUST compile and run:
1. EVERY kernel function MUST have `@triton.jit` decorator
2. Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
3. BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
4. `tl.program_id(axis)` only supports axis = 0, 1, 2
5. No `continue`, `break`, `return` inside loops — use masking
6. No tensor indexing with loop vars: `x[:, i]` is INVALID
7. mask shape MUST match data shape in tl.load/tl.store

## Missing Triton Functions (implement manually):
- tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

## OUTPUT FORMAT (STRICT):
1. Imports: torch, torch.nn, triton, triton.language as tl
2. @triton.jit decorated kernel function(s)
3. Wrapper function(s) for grid calculation and kernel launch
4. class ModelNew(nn.Module) that calls your kernels

Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
