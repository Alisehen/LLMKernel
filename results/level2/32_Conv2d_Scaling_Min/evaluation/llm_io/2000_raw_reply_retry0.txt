```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import triton
import triton.language as tl


@triton.jit
def scale_channel_min_kernel(
    x_ptr,  # (N, C, H, W)
    y_ptr,  # (N, 1, H, W)
    N, C, H, W,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_yn, stride_yc, stride_yh, stride_yw,
    scale,
    BLOCK_M: tl.constexpr,  # number of (n, h, w) elements per program
    BLOCK_C: tl.constexpr,  # channel tile
):
    # Flattened index over (N * H * W)
    pid = tl.program_id(0)
    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)
    M = N * H * W
    mask_m = offs_m < M

    # Decode offs_m -> (n, h, w)
    hw = H * W
    n_idx = offs_m // hw
    rem = offs_m % hw
    h_idx = rem // W
    w_idx = rem % W

    # Decide whether to compute min or max depending on scale sign.
    # For scale >= 0: min(scale * x) = scale * min(x)
    # For scale < 0:  min(scale * x) = scale * max(x)
    is_pos = scale >= 0

    # Initialize accumulator
    pos_inf = 3.402823e38
    neg_inf = -3.402823e38

    acc = tl.where(is_pos, tl.full((BLOCK_M,), pos_inf, dtype=tl.float32),
                           tl.full((BLOCK_M,), neg_inf, dtype=tl.float32))
    other_val = tl.where(is_pos, pos_inf, neg_inf)

    # Loop over channel dimension in BLOCK_C tiles
    for c_start in range(0, C, BLOCK_C):
        offs_c = c_start + tl.arange(0, BLOCK_C)
        mask_c = offs_c < C

        x_ptrs = (
            x_ptr
            + n_idx[:, None] * stride_xn
            + offs_c[None, :] * stride_xc
            + h_idx[:, None] * stride_xh
            + w_idx[:, None] * stride_xw
        )
        mask = mask_m[:, None] & mask_c[None, :]

        vals = tl.load(x_ptrs, mask=mask, other=other_val)

        if is_pos:
            tile_red = tl.min(vals, axis=1)
            acc = tl.minimum(acc, tile_red)
        else:
            tile_red = tl.max(vals, axis=1)
            acc = tl.maximum(acc, tile_red)

    # Apply scale after reduction for correct semantics with negative scale
    acc = acc * scale

    # Store result at channel 0
    y_ptrs = (
        y_ptr
        + n_idx * stride_yn
        + 0 * stride_yc
        + h_idx * stride_yh
        + w_idx * stride_yw
    )
    tl.store(y_ptrs, acc, mask=mask_m)


def scale_min_triton(conv_out: torch.Tensor, scale_factor: float) -> torch.Tensor:
    """
    Fused scale + channel-wise min over a Conv2d output.

    Args:
        conv_out: (N, C, H, W) CUDA tensor
        scale_factor: float scalar

    Returns:
        y: (N, 1, H, W) where
           y[n, 0, h, w] = min_c(conv_out[n, c, h, w] * scale_factor)
    """
    assert conv_out.is_cuda, "Triton kernel requires CUDA tensor"
    conv_out = conv_out.contiguous()

    N, C, H, W = conv_out.shape
    y = torch.empty((N, 1, H, W), device=conv_out.device, dtype=conv_out.dtype)

    stride_xn, stride_xc, stride_xh, stride_xw = conv_out.stride()
    stride_yn, stride_yc, stride_yh, stride_yw = y.stride()

    M = N * H * W
    BLOCK_M = 128
    BLOCK_C = 64

    grid = lambda META: (triton.cdiv(M, META["BLOCK_M"]),)

    scale_channel_min_kernel[grid](
        conv_out,
        y,
        N, C, H, W,
        stride_xn, stride_xc, stride_xh, stride_xw,
        stride_yn, stride_yc, stride_yh, stride_yw,
        float(scale_factor),
        BLOCK_M=BLOCK_M,
        BLOCK_C=BLOCK_C,
        num_warps=4,
        num_stages=2,
    )
    return y


class ModelNew(nn.Module):
    """
    Optimized model:
    - Uses cuDNN (torch.nn.Conv2d) for convolution
    - Uses a Triton kernel to fuse scaling and channel-wise minimum:
        y[n, 0, h, w] = min_c( conv(x)[n, c, h, w] * scale_factor )
    """
    def __init__(self, in_channels, out_channels, kernel_size, scale_factor):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scale_factor = float(scale_factor)

    def forward(self, x):
        # First do convolution using cuDNN-optimized Conv2d
        x = self.conv(x)

        # If on CUDA, use Triton to fuse scale + min; otherwise fall back to PyTorch
        if x.is_cuda:
            return scale_min_triton(x, self.scale_factor)
        else:
            x = x * self.scale_factor
            return torch.min(x, dim=1, keepdim=True)[0]
```