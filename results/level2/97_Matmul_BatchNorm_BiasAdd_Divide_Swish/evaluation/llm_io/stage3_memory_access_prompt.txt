You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Memory pattern for fused operations.

Key Principle:
- Fusion benefit = eliminated INTERMEDIATE stores
- Multiple input loads are OK; intermediate stores are NOT

Rules:
- ✅ Multiple tl.load() for different inputs (x, weight, bias) - OK
- ❌ tl.store() for intermediate results - NEVER (this is what fusion eliminates)
- ✅ Single tl.store() for final output - required

Verification:
- Count tl.store() calls: should equal number of OUTPUT tensors (usually 1)
- Intermediate values: must stay in registers between ops
- If you see store-then-load pattern for same data → BUG, refactor

Multi-input Fusion Pattern:
```
x = tl.load(input_ptr + offs, mask=mask)
w = tl.load(weight_ptr + ..., mask=...)  # OK: different input
b = tl.load(bias_ptr + ..., mask=...)    # OK: different input
y = op1(x, w)  # in registers
z = op2(y, b)  # in registers
tl.store(out_ptr + offs, z, mask=mask)   # single output store
```

num_stages: start with 2, only increase if memory stalls high AND registers OK



[CURRENT CODE]
```python
# <complete ModelNew code with optimized Triton kernels>

import math
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config(
            {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32},
            num_warps=4,
        ),
        triton.Config(
            {'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32},
            num_warps=8,
        ),
    ],
    key=['M', 'N', 'K'],
)
@triton.jit
def fused_linear_bn_bias_div_swish_kernel(
    x_ptr,                # [M, K]
    w_ptr,                # [K, N]  (weight^T)
    lin_bias_ptr,         # [N]
    bn_weight_ptr,        # [N]
    bn_bias_ptr,          # [N]
    running_mean_ptr,     # [N]
    running_var_ptr,      # [N]
    extra_bias_ptr,       # scalar (numel == 1)
    y_ptr,                # [M, N] output

    M, N, K,
    stride_xm, stride_xk,
    stride_wk, stride_wn,
    stride_ym, stride_yn,
    eps,                  # batchnorm epsilon
    divide_value,         # scalar division value

    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    # Program IDs for 2D tiling over M and N
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    # Offsets within the overall matrices
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    # Pointers for the current tile of X and W
    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk
    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn

    # Accumulator
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # K-loop
    for k in range(0, K, BLOCK_K):
        k_remaining = K - k

        x_mask = (offs_m[:, None] < M) & (offs_k[None, :] < k_remaining)
        w_mask = (offs_k[:, None] < k_remaining) & (offs_n[None, :] < N)

        x_tile = tl.load(x_ptrs, mask=x_mask, other=0.0)
        w_tile = tl.load(w_ptrs, mask=w_mask, other=0.0)

        acc += tl.dot(x_tile, w_tile)

        x_ptrs += BLOCK_K * stride_xk
        w_ptrs += BLOCK_K * stride_wk

    # ---- Fused epilogue: Linear bias + BatchNorm + extra bias + division + Swish ----

    # Load per-feature parameters (length N)
    n_mask = offs_n < N

    lin_bias = tl.load(lin_bias_ptr + offs_n, mask=n_mask, other=0.0)

    gamma = tl.load(bn_weight_ptr + offs_n, mask=n_mask, other=0.0)
    beta = tl.load(bn_bias_ptr + offs_n, mask=n_mask, other=0.0)
    running_mean = tl.load(running_mean_ptr + offs_n, mask=n_mask, other=0.0)
    running_var = tl.load(running_var_ptr + offs_n, mask=n_mask, other=0.0)

    # Add linear bias
    acc = acc + lin_bias[None, :]

    # BatchNorm (inference-style) per feature:
    # y = gamma * (x - mean) / sqrt(var + eps) + beta
    var_eps = running_var + eps
    inv_std = 1.0 / tl.sqrt(var_eps)
    scale = gamma * inv_std
    shift = beta - running_mean * scale

    acc = acc * scale[None, :] + shift[None, :]

    # Extra bias (scalar)
    extra_bias_val = tl.load(extra_bias_ptr)
    acc = acc + extra_bias_val

    # Division
    acc = acc / divide_value

    # Swish: x * sigmoid(x) = x / (1 + exp(-x))
    neg_acc = -acc
    exp_neg = tl.exp(neg_acc)
    sig = 1.0 / (1.0 + exp_neg)
    acc = acc * sig

    # Store
    y_ptrs = y_ptr + offs_m[:, None] * stride_ym + offs_n[None, :] * stride_yn
    y_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(y_ptrs, acc, mask=y_mask)


def fused_linear_bn_bias_div_swish(
    x: torch.Tensor,
    weight: torch.Tensor,
    lin_bias: torch.Tensor,
    bn_weight: torch.Tensor,
    bn_bias: torch.Tensor,
    running_mean: torch.Tensor,
    running_var: torch.Tensor,
    extra_bias: torch.Tensor,
    eps: float,
    divide_value: float,
) -> torch.Tensor:
    """
    Fused implementation of:
        y = x @ weight.T + lin_bias
        y = BatchNorm1d(y; running_mean, running_var, bn_weight, bn_bias, eps)  (inference-style)
        y = y + extra_bias          # scalar bias
        y = y / divide_value
        y = y * sigmoid(y)          # Swish

    Shapes:
        x:          [M, K]
        weight:     [N, K]
        lin_bias:   [N]
        bn_weight:  [N]
        bn_bias:    [N]
        running_*:  [N]
        extra_bias: [1] (scalar)
    """
    assert x.is_cuda, "Input must be on CUDA device"
    assert weight.is_cuda, "Weight must be on CUDA device"
    assert x.dtype == torch.float32, "Kernel currently supports float32 only"
    assert weight.dtype == torch.float32, "Kernel currently supports float32 only"

    M, K = x.shape
    N, Kw = weight.shape
    assert Kw == K, "in_features mismatch between input and weight"

    assert lin_bias.shape[0] == N
    assert bn_weight.shape[0] == N
    assert bn_bias.shape[0] == N
    assert running_mean.shape[0] == N
    assert running_var.shape[0] == N
    assert extra_bias.numel() == 1, "Kernel currently supports scalar extra_bias only"

    # Make sure tensors are contiguous where it matters
    x_contig = x.contiguous()
    weight_contig = weight.contiguous()
    lin_bias_contig = lin_bias.contiguous()
    bn_weight_contig = bn_weight.contiguous()
    bn_bias_contig = bn_bias.contiguous()
    running_mean_contig = running_mean.contiguous()
    running_var_contig = running_var.contiguous()
    extra_bias_contig = extra_bias.contiguous()

    # We use weight^T inside the kernel: [K, N]
    w_t = weight_contig.t().contiguous()

    y = torch.empty((M, N), device=x.device, dtype=x.dtype)

    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_M']),
        triton.cdiv(N, META['BLOCK_N']),
    )

    fused_linear_bn_bias_div_swish_kernel[grid](
        x_contig,
        w_t,
        lin_bias_contig,
        bn_weight_contig,
        bn_bias_contig,
        running_mean_contig,
        running_var_contig,
        extra_bias_contig,
        y,
        M, N, K,
        x_contig.stride(0), x_contig.stride(1),
        w_t.stride(0), w_t.stride(1),
        y.stride(0), y.stride(1),
        eps,
        float(divide_value),
    )

    return y


class ModelNew(nn.Module):
    """
    Triton-accelerated model that performs:
        Linear -> BatchNorm1d (inference-style) -> bias add -> division -> Swish
    """

    def __init__(
        self,
        in_features: int,
        out_features: int,
        bn_eps: float = 1e-5,
        bn_momentum: float = 0.1,  # unused, kept for API compatibility
        bias_shape=(1,),
        divide_value: float = 1.0,
    ):
        super(ModelNew, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.bn_eps = bn_eps
        self.bn_momentum = bn_momentum  # not used (no running-stat updates in this fused version)
        self.divide_value = float(divide_value)

        # Linear layer parameters
        self.weight = nn.Parameter(torch.empty(out_features, in_features))
        self.lin_bias = nn.Parameter(torch.zeros(out_features))

        # Initialize like nn.Linear
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
        bound = 1 / math.sqrt(fan_in)
        nn.init.uniform_(self.lin_bias, -bound, bound)

        # BatchNorm1d parameters (inference-style)
        self.bn_weight = nn.Parameter(torch.ones(out_features))
        self.bn_bias = nn.Parameter(torch.zeros(out_features))
        self.register_buffer("running_mean", torch.zeros(out_features))
        self.register_buffer("running_var", torch.ones(out_features))

        # Extra bias (broadcasted scalar)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: [batch_size, in_features]
        assert x.dim() == 2, "Expected input of shape [batch_size, in_features]"
        assert x.shape[1] == self.in_features, "in_features mismatch"

        return fused_linear_bn_bias_div_swish(
            x,
            self.weight,
            self.lin_bias,
            self.bn_weight,
            self.bn_bias,
            self.running_mean,
            self.running_var,
            self.bias,
            self.bn_eps,
            self.divide_value,
        )
```

[NCU PROFILING METRICS]
{
  "fused_linear_bn_bias_div_swish_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 44.64,
    "launch__grid_size": 2048,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 24.01,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 16.8,
    "lts__t_sector_hit_rate.pct": 96.3
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

TRITON API CONSTRAINTS (CRITICAL):
- Triton has NO: tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
