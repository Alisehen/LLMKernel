```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


# ---------------------------------------------------------------------------
# Kernel 1: Mean over depth (D)  ->  mean[b, c, h, w] in fp32
# ---------------------------------------------------------------------------

@triton.jit
def mean_over_depth_kernel(
    x_ptr,         # [B, C, D, H, W]
    mean_ptr,      # [B, C, H, W] (fp32)
    B, C, D, H, W,
    stride_xb, stride_xc, stride_xd, stride_xh, stride_xw,
    stride_mb, stride_mc, stride_mh, stride_mw,
    BLOCK_W: tl.constexpr,
):
    """
    Each program computes mean over D for a tile of W for fixed (b, c, h).

    Grid:
      pid0 = b * (C*H) + c * H + h   (0 .. B*C*H - 1)
      pid1 = w-tile index            (0 .. ceil_div(W, BLOCK_W) - 1)
    """
    pid_bch = tl.program_id(0)
    pid_w = tl.program_id(1)

    # Decode (b, c, h)
    bc_h = C * H
    b = pid_bch // bc_h
    tmp = pid_bch % bc_h
    c = tmp // H
    h = tmp % H

    # Guard
    if b >= B:
        return
    if c >= C:
        return
    if h >= H:
        return

    # Tile of W this program handles
    offs_w = pid_w * BLOCK_W + tl.arange(0, BLOCK_W)
    mask_w = offs_w < W

    # Accumulate over D in fp32
    acc = tl.zeros((BLOCK_W,), dtype=tl.float32)

    for d in range(0, D):
        x_ptrs = (
            x_ptr
            + b * stride_xb
            + c * stride_xc
            + d * stride_xd
            + h * stride_xh
            + offs_w * stride_xw
        )
        x_vals = tl.load(x_ptrs, mask=mask_w, other=0.0)
        acc += x_vals.to(tl.float32)

    mean_vals = acc / D

    mean_ptrs = (
        mean_ptr
        + b * stride_mb
        + c * stride_mc
        + h * stride_mh
        + offs_w * stride_mw
    )
    tl.store(mean_ptrs, mean_vals, mask=mask_w)


# ---------------------------------------------------------------------------
# Kernel 2: Bias + softmax over C + tanh + scaling
#           Input:  mean[b, c, h, w] (fp32)
#           Output: out[b, c, 0, h, w] (x.dtype)
# ---------------------------------------------------------------------------

@triton.autotune(
    configs=[
        triton.Config({'BLOCK_C': 64}, num_warps=2, num_stages=2),
        triton.Config({'BLOCK_C': 128}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_C': 256}, num_warps=8, num_stages=2),
    ],
    key=['C'],
)
@triton.jit
def fused_bias_softmax_tanh_scale_kernel(
    mean_ptr,      # [B, C, H, W] (fp32)
    bias_ptr,      # [C]          (same dtype as out)
    out_ptr,       # [B, C, 1, H, W]
    B, C, H, W,
    stride_mb, stride_mc, stride_mh, stride_mw,
    stride_ob, stride_oc, stride_od, stride_oh, stride_ow,
    scaling_factor,
    BLOCK_C: tl.constexpr,
):
    """
    For each (b, h, w), compute:
      v_c = mean[b, c, h, w] + bias[c]
      softmax over c: s_c = exp(v_c - max_v) / sum_c exp(v_c - max_v)
      y_c = scaling_factor * tanh(s_c)

    Grid:
      pid = b * (H*W) + h * W + w  (0 .. B*H*W - 1)
      Each program handles all C (in BLOCK_C tiles) for one (b, h, w).
    """
    pid = tl.program_id(0)
    bhw = H * W
    b = pid // bhw
    hw = pid % bhw
    h = hw // W
    w = hw % W

    if b >= B:
        return
    if h >= H:
        return
    if w >= W:
        return

    offs_c = tl.arange(0, BLOCK_C)
    neg_inf = -float('inf')

    # ------------------------ Pass 1: max over C ------------------------- #
    max_val = neg_inf

    c0 = 0
    while c0 < C:
        c_idxs = c0 + offs_c
        mask_c = c_idxs < C

        mean_ptrs = (
            mean_ptr
            + b * stride_mb
            + c_idxs * stride_mc
            + h * stride_mh
            + w * stride_mw
        )
        mean_vals = tl.load(mean_ptrs, mask=mask_c, other=0.0)  # fp32

        bias_vals = tl.load(bias_ptr + c_idxs, mask=mask_c, other=0.0)
        bias_vals = bias_vals.to(tl.float32)

        v = mean_vals + bias_vals
        v = tl.where(mask_c, v, neg_inf)
        local_max = tl.max(v, axis=0)
        max_val = tl.maximum(max_val, local_max)

        c0 += BLOCK_C

    # ------------------------ Pass 2: sum of exp ------------------------ #
    sum_exp = tl.zeros((), dtype=tl.float32)

    c0 = 0
    while c0 < C:
        c_idxs = c0 + offs_c
        mask_c = c_idxs < C

        mean_ptrs = (
            mean_ptr
            + b * stride_mb
            + c_idxs * stride_mc
            + h * stride_mh
            + w * stride_mw
        )
        mean_vals = tl.load(mean_ptrs, mask=mask_c, other=0.0)

        bias_vals = tl.load(bias_ptr + c_idxs, mask=mask_c, other=0.0)
        bias_vals = bias_vals.to(tl.float32)

        v = mean_vals + bias_vals
        v = tl.where(mask_c, v, neg_inf)

        exp_vals = tl.exp(v - max_val)
        exp_vals = tl.where(mask_c, exp_vals, 0.0)
        sum_exp += tl.sum(exp_vals, axis=0)

        c0 += BLOCK_C

    inv_denom = 1.0 / sum_exp

    # ---------------- Pass 3: softmax + tanh + scale + store ------------ #
    c0 = 0
    while c0 < C:
        c_idxs = c0 + offs_c
        mask_c = c_idxs < C

        mean_ptrs = (
            mean_ptr
            + b * stride_mb
            + c_idxs * stride_mc
            + h * stride_mh
            + w * stride_mw
        )
        mean_vals = tl.load(mean_ptrs, mask=mask_c, other=0.0)

        bias_vals = tl.load(bias_ptr + c_idxs, mask=mask_c, other=0.0)
        bias_vals = bias_vals.to(tl.float32)

        v = mean_vals + bias_vals
        v = tl.where(mask_c, v, neg_inf)

        exp_vals = tl.exp(v - max_val)
        softmax_vals = exp_vals * inv_denom

        # tanh(x) = (1 - e^{-2x}) / (1 + e^{-2x})
        e_neg_2x = tl.exp(-2.0 * softmax_vals)
        tanh_vals = (1.0 - e_neg_2x) / (1.0 + e_neg_2x)

        out_vals = tanh_vals * scaling_factor

        out_ptrs = (
            out_ptr
            + b * stride_ob
            + c_idxs * stride_oc
            + h * stride_oh
            + w * stride_ow
        )
        tl.store(out_ptrs, out_vals, mask=mask_c)

        c0 += BLOCK_C


# ---------------------------------------------------------------------------
# Kernel 3 (NEW): Fully fused mean(D) + bias + softmax(C) + tanh + scale
#                 No intermediate global stores; reads x directly.
#
#   Input:  x[b, c, d, h, w]
#   Output: out[b, c, 0, h, w]
#
# This path is used when C fits in a single BLOCK_C tile (typical fast case).
# ---------------------------------------------------------------------------

@triton.autotune(
    configs=[
        triton.Config({'BLOCK_C': 64,  'BLOCK_W': 8},  num_warps=4, num_stages=2),
        triton.Config({'BLOCK_C': 128, 'BLOCK_W': 4},  num_warps=4, num_stages=2),
        triton.Config({'BLOCK_C': 128, 'BLOCK_W': 8},  num_warps=8, num_stages=2),
        triton.Config({'BLOCK_C': 256, 'BLOCK_W': 4},  num_warps=8, num_stages=2),
    ],
    key=['C'],
)
@triton.jit
def fused_mean_bias_softmax_tanh_scale_kernel(
    x_ptr,         # [B, C, D, H, W]
    bias_ptr,      # [C]
    out_ptr,       # [B, C, 1, H, W]
    B, C, D, H, W,
    stride_xb, stride_xc, stride_xd, stride_xh, stride_xw,
    stride_ob, stride_oc, stride_od, stride_oh, stride_ow,
    scaling_factor,
    BLOCK_C: tl.constexpr,
    BLOCK_W: tl.constexpr,
):
    """
    Fully fused kernel:

      For each (b, h, w) (tiled over w as BLOCK_W), compute:

        mean_c(b,h,w) = mean over d of x[b,c,d,h,w]
        v_c = mean_c + bias[c]
        s_c = softmax over c of v_c
        y_c = scaling_factor * tanh(s_c)

      and write y_c -> out[b,c,0,h,w].

    Constraints: C <= BLOCK_C in the calling code for this kernel.
    Grid:
      pid0 = b * H + h          (0 .. B*H-1)
      pid1 = w-tile index       (0 .. ceil_div(W, BLOCK_W)-1)
    """
    pid_bh = tl.program_id(0)
    pid_w  = tl.program_id(1)

    b = pid_bh // H
    h = pid_bh % H

    if b >= B:
        return
    if h >= H:
        return

    # Channel and width indices
    offs_c = tl.arange(0, BLOCK_C)[:, None]  # (BLOCK_C, 1)
    offs_w = pid_w * BLOCK_W + tl.arange(0, BLOCK_W)[None, :]  # (1, BLOCK_W)

    mask_c = offs_c < C
    mask_w = offs_w < W
    mask = mask_c & mask_w

    # --------------------- Mean over D in fp32 -------------------------- #
    acc = tl.zeros((BLOCK_C, BLOCK_W), dtype=tl.float32)

    for d in range(0, D):
        x_ptrs = (
            x_ptr
            + b * stride_xb
            + offs_c * stride_xc
            + d * stride_xd
            + h * stride_xh
            + offs_w * stride_xw
        )
        x_vals = tl.load(x_ptrs, mask=mask, other=0.0)
        acc += x_vals.to(tl.float32)

    mean_vals = acc / D  # (BLOCK_C, BLOCK_W), fp32

    # ------------------------ Add bias ---------------------------------- #
    bias_vals = tl.load(bias_ptr + offs_c, mask=mask_c, other=0.0)
    bias_vals = bias_vals.to(tl.float32)  # (BLOCK_C, 1) broadcasts along W

    logits = mean_vals + bias_vals  # (BLOCK_C, BLOCK_W)
    neg_inf = -float('inf')
    logits = tl.where(mask, logits, neg_inf)

    # ------------------------ Softmax over C ---------------------------- #
    # Reduce along C (axis=0), per w-column.
    max_vals = tl.max(logits, axis=0)                          # (BLOCK_W,)
    logits_shifted = logits - max_vals[None, :]                # (BLOCK_C, BLOCK_W)
    exp_vals = tl.exp(logits_shifted)
    exp_vals = tl.where(mask, exp_vals, 0.0)
    sum_exp = tl.sum(exp_vals, axis=0)                         # (BLOCK_W,)
    softmax_vals = exp_vals / sum_exp[None, :]                 # (BLOCK_C, BLOCK_W)

    # ------------------------ tanh + scale ------------------------------ #
    # tanh(x) = (1 - e^{-2x}) / (1 + e^{-2x})
    e_neg_2x = tl.exp(-2.0 * softmax_vals)
    tanh_vals = (1.0 - e_neg_2x) / (1.0 + e_neg_2x)

    out_vals = tanh_vals * scaling_factor

    # ------------------------ Store result ------------------------------ #
    out_ptrs = (
        out_ptr
        + b * stride_ob
        + offs_c * stride_oc
        + h * stride_oh
        + offs_w * stride_ow
    )
    tl.store(out_ptrs, out_vals, mask=mask)


# ---------------------------------------------------------------------------
# Wrapper for launching kernels
# ---------------------------------------------------------------------------

def fused_post_convtranspose_3d(x: torch.Tensor, bias: torch.Tensor, scaling_factor: float):
    """
    x:    [B, C, D, H, W] - output of ConvTranspose3d
    bias: [1, C, 1, 1, 1] - broadcastable bias
    return: [B, C, 1, H, W] after mean(D) + bias + softmax(C) + tanh + scaling

    Fast path:
      - If C <= MAX_FUSED_C, use fully fused kernel that reads x directly and
        performs all ops without intermediate global stores.

    Fallback path (generic C):
      - Mean over D kernel -> mean[b,c,h,w] (fp32)
      - Fused bias + softmax + tanh + scale on mean.
    """
    assert x.is_cuda, "Input must be on CUDA for Triton kernels"
    B, C, D, H, W = x.shape

    # Per-channel bias as [C]
    bias_flat = bias.view(-1)  # [C]

    # Final output (same dtype as x)
    out = torch.empty((B, C, 1, H, W), device=x.device, dtype=x.dtype)

    # Threshold for fully fused path (must not exceed max BLOCK_C config)
    MAX_FUSED_C = 256

    if C <= MAX_FUSED_C:
        # ---------------------- Fully fused fast path --------------------
        def grid(meta):
            return (
                B * H,
                triton.cdiv(W, meta['BLOCK_W']),
            )

        fused_mean_bias_softmax_tanh_scale_kernel[grid](
            x,
            bias_flat,
            out,
            B, C, D, H, W,
            x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),
            out.stride(0), out.stride(1), out.stride(2), out.stride(3), out.stride(4),
            float(scaling_factor),
        )
    else:
        # ------------------------- Fallback path -------------------------
        # Intermediate mean over depth in fp32
        mean = torch.empty((B, C, H, W), device=x.device, dtype=torch.float32)

        # Mean over D kernel (memory-bandwidth optimized)
        BLOCK_W = 64

        grid_mean = (B * C * H, triton.cdiv(W, BLOCK_W))
        mean_over_depth_kernel[grid_mean](
            x,
            mean,
            B, C, D, H, W,
            x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),
            mean.stride(0), mean.stride(1), mean.stride(2), mean.stride(3),
            BLOCK_W=BLOCK_W,
        )

        # Fused bias + softmax + tanh + scale on mean
        grid_softmax = (B * H * W,)

        fused_bias_softmax_tanh_scale_kernel[grid_softmax](
            mean,
            bias_flat,
            out,
            B, C, H, W,
            mean.stride(0), mean.stride(1), mean.stride(2), mean.stride(3),
            out.stride(0), out.stride(1), out.stride(2), out.stride(3), out.stride(4),
            float(scaling_factor),
        )

    return out


# ---------------------------------------------------------------------------
# PyTorch module integrating ConvTranspose3d with optimized Triton kernels
# ---------------------------------------------------------------------------

class ModelNew(nn.Module):
    """
    PyTorch ConvTranspose3d + Triton-optimized post-ops:

      1. ConvTranspose3d  (PyTorch native)
      2. Mean over depth (D)
      3. Bias add
      4. Softmax over channels (C)
      5. Tanh
      6. Scaling

    Fast path fuses (2-6) into a single Triton kernel with only one global
    write of the final output; no intermediate tensors are written to memory.
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
        )
        self.bias = nn.Parameter(torch.randn(1, out_channels, 1, 1, 1))
        self.scaling_factor = float(scaling_factor)

    def forward(self, x):
        x = self.conv_transpose(x)  # [B, C, D, H, W]
        x = fused_post_convtranspose_3d(x, self.bias, self.scaling_factor)
        return x
```