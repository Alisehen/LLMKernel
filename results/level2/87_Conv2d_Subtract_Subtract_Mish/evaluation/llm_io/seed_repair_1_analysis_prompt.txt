You are a Triton kernel debugging expert. Analyze the error and identify the root cause.

## ERROR LOG
```
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 538, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251215_024837_batch_range56to88_openai_deepseek/87_Conv2d_Subtract_Subtract_Mish/code/kernel_20251215_050439.py", line 183, in forward
    return conv2d_sub_sub_mish(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251215_024837_batch_range56to88_openai_deepseek/87_Conv2d_Subtract_Subtract_Mish/code/kernel_20251215_050439.py", line 152, in conv2d_sub_sub_mish
    conv2d_sub_sub_mish_kernel[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 278, in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 81, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 71:7:
                    + kh * stride_wkh
                    + kw * stride_wkw
                )
                w_ptrs = w_ptr + w_offsets

                w_vals = tl.load(w_ptrs, mask=n_valid, other=0.0)  # [BLOCK_N]

                # Outer product accumulate
                acc += x_vals[:, None] * w_vals[None, :]

    # Add bias
    if b_ptr != 0:
       ^
IncompatibleTypeErrorImpl('invalid operands of type pointer<fp32> and triton.language.int32')
```

## Expected Behavior (PyTorch Reference)
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Model that performs a convolution, subtracts two values, applies Mish activation.
    """
    def __init__(self, in_channels, out_channels, kernel_size, subtract_value_1, subtract_value_2):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.subtract_value_1 = subtract_value_1
        self.subtract_value_2 = subtract_value_2

    def forward(self, x):
        x = self.conv(x)
        x = x - self.subtract_value_1
        x = x - self.subtract_value_2
        x = torch.nn.functional.mish(x)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 256, 256
kernel_size = 3
subtract_value_1 = 0.5
subtract_value_2 = 0.2

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, subtract_value_1, subtract_value_2]
```

## Current Implementation (Broken Triton Kernel)
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def conv2d_sub_sub_mish_kernel(
    x_ptr, w_ptr, b_ptr, out_ptr,
    subtract1, subtract2,
    N, C_in, H, W,
    OC, KH, KW, H_out, W_out,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_wo, stride_wc, stride_wkh, stride_wkw,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,
):
    # Flatten output [N, OC, H_out, W_out] into:
    #   P = N * H_out * W_out  (rows, M dimension)
    #   OC                      (cols, N dimension)
    P = N * H_out * W_out

    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)  # [BLOCK_M] over P
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)  # [BLOCK_N] over OC

    m_valid = offs_m < P
    n_valid = offs_n < OC

    # Decode flattened spatial+batch index into (n, oh, ow)
    HW_out = H_out * W_out
    n_idx = offs_m // HW_out
    rem = offs_m % HW_out
    oh_idx = rem // W_out
    ow_idx = rem % W_out

    # Accumulator for conv result
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Convolution: sum over input channels and kernel spatial positions
    for c in range(0, C_in):
        for kh in range(0, KH):
            for kw in range(0, KW):
                # Input spatial positions
                ih = oh_idx + kh  # [BLOCK_M]
                iw = ow_idx + kw  # [BLOCK_M]

                # Compute input pointer offsets for this (c, kh, kw)
                x_offsets = (
                    n_idx * stride_xn
                    + c * stride_xc
                    + ih * stride_xh
                    + iw * stride_xw
                )
                x_ptrs = x_ptr + x_offsets

                # Mask: only valid for in-bounds and valid m indices
                x_mask = m_valid & (ih < H) & (iw < W)

                x_vals = tl.load(x_ptrs, mask=x_mask, other=0.0)  # [BLOCK_M]

                # Weight offsets for this (c, kh, kw) over oc dimension
                w_offsets = (
                    offs_n * stride_wo
                    + c * stride_wc
                    + kh * stride_wkh
                    + kw * stride_wkw
                )
                w_ptrs = w_ptr + w_offsets

                w_vals = tl.load(w_ptrs, mask=n_valid, other=0.0)  # [BLOCK_N]

                # Outer product accumulate
                acc += x_vals[:, None] * w_vals[None, :]

    # Add bias
    if b_ptr != 0:
        bias = tl.load(b_ptr + offs_n, mask=n_valid, other=0.0)  # [BLOCK_N]
        acc += bias[None, :]

    # Subtractions
    acc = acc - subtract1
    acc = acc - subtract2

    # Mish activation: x * tanh(softplus(x))
    # softplus(x) = log(1 + exp(x))
    softplus = tl.log(1.0 + tl.exp(acc))
    t = tl.exp(2.0 * softplus)
    tanh_sp = (t - 1.0) / (t + 1.0)
    out_vals = acc * tanh_sp

    # Store result to out[N, OC, H_out, W_out]
    # out layout: NCHW with strides derived in wrapper
    # Reuse decoded indices and offs_n
    out_offsets = (
        n_idx[:, None] * (OC * H_out * W_out)
        + offs_n[None, :] * (H_out * W_out)
        + oh_idx[:, None] * W_out
        + ow_idx[:, None]
    )

    out_ptrs = out_ptr + out_offsets
    out_mask = m_valid[:, None] & n_valid[None, :]

    tl.store(out_ptrs, out_vals, mask=out_mask)


def conv2d_sub_sub_mish(x, weight, bias, subtract1: float, subtract2: float):
    """
    x:      [N, C_in, H, W] (CUDA tensor)
    weight: [OC, C_in, KH, KW]
    bias:   [OC]
    Performs conv2d (stride=1, padding=0, dilation=1), then
      y = conv(x, weight, bias) - subtract1 - subtract2
      y = mish(y)
    """
    assert x.is_cuda, "Input must be on CUDA"
    assert x.ndim == 4
    assert weight.ndim == 4

    N, C_in, H, W = x.shape
    OC, Cw, KH, KW = weight.shape
    assert Cw == C_in, "Weight in_channels must match input"

    # Only support standard conv2d with stride=1, padding=0, dilation=1
    H_out = H - KH + 1
    W_out = W - KW + 1
    assert H_out > 0 and W_out > 0, "Invalid H/W for given kernel size (no padding assumed)"

    x_contig = x.contiguous()
    w_contig = weight.contiguous()
    b_contig = bias.contiguous() if bias is not None else None

    out = torch.empty((N, OC, H_out, W_out), device=x.device, dtype=torch.float32)

    # Strides (PyTorch gives strides in element units)
    stride_xn, stride_xc, stride_xh, stride_xw = x_contig.stride()
    stride_wo, stride_wc, stride_wkh, stride_wkw = w_contig.stride()

    # Flattened spatial dimension for grid
    P = N * H_out * W_out

    BLOCK_M = 64
    BLOCK_N = 64

    def cdiv(a, b):
        return (a + b - 1) // b

    grid = (cdiv(P, BLOCK_M), cdiv(OC, BLOCK_N))

    conv2d_sub_sub_mish_kernel[grid](
        x_contig, w_contig, b_contig if b_contig is not None else 0, out,
        float(subtract1), float(subtract2),
        N, C_in, H, W,
        OC, KH, KW, H_out, W_out,
        stride_xn, stride_xc, stride_xh, stride_xw,
        stride_wo, stride_wc, stride_wkh, stride_wkw,
        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N,
    )

    # Cast back to original dtype if needed
    if out.dtype != x.dtype:
        out = out.to(x.dtype)
    return out


class ModelNew(nn.Module):
    """
    Triton-accelerated version:
      y = mish(conv2d(x) - subtract_value_1 - subtract_value_2)
    with conv2d: stride=1, padding=0, dilation=1.
    """
    def __init__(self, in_channels, out_channels, kernel_size, subtract_value_1, subtract_value_2):
        super(ModelNew, self).__init__()
        # Match nn.Conv2d parameter shapes (no padding/stride handling here)
        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))
        self.bias = nn.Parameter(torch.randn(out_channels))
        self.subtract_value_1 = float(subtract_value_1)
        self.subtract_value_2 = float(subtract_value_2)

    def forward(self, x):
        return conv2d_sub_sub_mish(
            x,
            self.weight,
            self.bias,
            self.subtract_value_1,
            self.subtract_value_2,
        )
```

---

## Your Task

Identify the **single most critical issue** that causes the error above.

### Analysis Guidelines

1. **Focus on root cause**, not symptoms
   - Bad: "Output is wrong"
   - Good: "BLOCK_K loop missing, only processes first 32 elements of K dimension"

2. **Be specific about WHAT and WHERE**
   - Bad: "Memory access issue"
   - Good: "Line 45: tl.atomic_add(c_block_ptr, acc) - atomic_add requires scalar pointer, not block_ptr"

3. **Prioritize by impact**
   - Correctness bugs > Performance issues > Style problems
   - Algorithm errors > Implementation details

### Output Format

**CRITICAL: You MUST output ONLY valid JSON. No other text allowed.**

```json
{
  "critical_issue": "<Concise description of THE root cause, max 30 words>",
  "why_it_matters": "<Why this causes the observed error, max 35 words>",
  "minimal_fix_hint": "<What needs to change (not how), max 30 words>"
}
```

**Remember**: Output ONLY the JSON block. No explanations, no commentary, no additional text.
