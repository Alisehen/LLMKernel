Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 626, in compare_and_bench
    raise ValueError(
ValueError: Outputs are not close (atol=0.001, rtol=0.01). max_abs_err=1.444e+01, mean_abs_err=1.139e+01

Main Critical Problem Analysis:
Problem Analysis (from expert diagnosis):
critical_issue: Kernel incorrectly sums height dimension without proper full reduction across entire H dimension.
why_it_matters: Only processes first BLOCK_H elements (max 32) of height dimension, missing most of H=128, causing large numerical error.
minimal_fix_hint: Add tiling/looping over H dimension to process entire height, not just BLOCK_H window.

Focus your fix on addressing the identified critical issue.


Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl
import math


@triton.jit
def fused_min_sum_gelu_bias_kernel(
    x_ptr,
    bias_ptr,
    out_ptr,
    N, C, H, W,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_on, stride_o1, stride_o2, stride_ow,
    BLOCK_H: tl.constexpr,
    BLOCK_C: tl.constexpr,
    # Optimal parameters for Ada Lovelace based on register pressure
    num_warps: tl.constexpr = 4,
    num_stages: tl.constexpr = 2,
):
    """
    Optimized fused kernel for: min(dim=1) + sum(dim=2) + GELU + bias
    Input: [N, C, H, W] -> Output: [N, 1, 1, W]
    
    Optimizations applied:
    1. Vectorized loads for better memory throughput
    2. Warp-level parallel reductions
    3. Improved GELU approximation with FMA
    4. Optimized for Ada Lovelace architecture with proper warp/stage config
    """
    pid_n = tl.program_id(0)
    pid_w = tl.program_id(1)
    
    if pid_n >= N or pid_w >= W:
        return
    
    # Initialize with vectorized operations
    h_offsets = tl.arange(0, BLOCK_H)
    h_mask = h_offsets < H
    
    # Pre-allocate registers efficiently
    min_vals = tl.full((BLOCK_H,), float('inf'), dtype=tl.float32)
    
    # Process channels in blocks with vectorized loads
    for c_block in range(0, C, BLOCK_C):
        c_offsets = c_block + tl.arange(0, BLOCK_C)
        c_mask = c_offsets < C
        
        # Vectorized load: [BLOCK_C, BLOCK_H]
        # Use proper masking to avoid out-of-bounds
        x_ptrs = (
            x_ptr +
            pid_n * stride_xn +
            (c_offsets[:, None] * stride_xc) +
            (h_offsets[None, :] * stride_xh) +
            pid_w * stride_xw
        )
        
        # Load with proper masking
        x_block = tl.load(
            x_ptrs,
            mask=c_mask[:, None] & h_mask[None, :],
            other=float('inf'),
            eviction_policy="evict_first"
        )
        
        # Warp-level parallel reduction for min
        # Each thread handles BLOCK_H elements, reduction across channels
        block_min = tl.min(x_block, axis=0)
        min_vals = tl.minimum(min_vals, block_min)
    
    # Sum across H dimension with warp shuffle reduction
    # Mask invalid positions first
    valid_min_vals = tl.where(h_mask, min_vals, 0.0)
    
    # Efficient sum reduction using tree reduction within warp
    sum_val = tl.sum(valid_min_vals, axis=0)
    
    # Optimized GELU approximation with FMA operations
    # GELU(x) = 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x^3)))
    # Using more accurate polynomial approximation for tanh
    x = sum_val
    x3 = x * x * x
    
    # FMA operations for better throughput
    sqrt_2_over_pi = 0.7978845608028654
    gelu_coef = 0.044715
    
    # inner = sqrt_2_over_pi * (x + gelu_coef * x3)
    inner = tl.fma(gelu_coef, x3, x)
    inner = inner * sqrt_2_over_pi
    
    # Optimized tanh approximation: tanh(x) ≈ x * (1 - x²/3) for |x| < 1
    # For better accuracy, use piecewise approximation
    inner_sq = inner * inner
    
    # Use Chebyshev approximation for tanh
    # tanh(x) ≈ x * (0.9999999995 + x² * (-0.3333332147 + x² * (0.1333243841 - 0.0533686743 * x²)))
    tanh_approx = inner * (
        0.9999999995 +
        inner_sq * (-0.3333332147 +
        inner_sq * (0.1333243841 -
        0.0533686743 * inner_sq))
    )
    
    # Clamp for stability
    tanh_approx = tl.minimum(tl.maximum(tanh_approx, -1.0), 1.0)
    
    # Compute GELU
    gelu_result = 0.5 * x * (1.0 + tanh_approx)
    
    # Add bias
    bias_val = tl.load(bias_ptr)
    result = gelu_result + bias_val
    
    # Store result
    out_ptr_base = pid_n * stride_on + pid_w * stride_ow
    tl.store(out_ptr + out_ptr_base, result)


def fused_post_convtranspose(x, bias):
    """
    Fused: min(dim=1) + sum(dim=2) + GELU + bias
    Input shape: [N, C, H, W]
    Output shape: [N, 1, 1, W]
    """
    N, C, H, W = x.shape
    
    # Output tensor
    out = torch.empty((N, 1, 1, W), device=x.device, dtype=x.dtype)
    
    # Auto-tune block sizes based on input dimensions
    # Use power-of-2 for optimal memory access
    BLOCK_H = triton.next_power_of_2(H)
    BLOCK_H = min(BLOCK_H, 32)  # Cap for register efficiency
    
    # Channel block size tuned for L2 cache efficiency
    # Ada Lovelace has 72MB L2, so we can use larger blocks
    BLOCK_C = 128 if C >= 128 else triton.next_power_of_2(C)
    
    # Grid: N x W - matches output dimensions
    grid = (N, W)
    
    # Launch kernel with autotuned configurations
    # Based on register pressure analysis:
    # - 4 warps for better register allocation
    # - 2 stages for multi-input fusion (3+ loads: x, bias, out)
    fused_min_sum_gelu_bias_kernel[grid](
        x, bias, out,
        N, C, H, W,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3),
        BLOCK_H=BLOCK_H,
        BLOCK_C=BLOCK_C,
        num_warps=4,
        num_stages=2,
    )
    
    return out


class ModelNew(nn.Module):
    """
    ConvTranspose2d (PyTorch native) + Fused min + sum + GELU + bias (Triton)
    Optimized for Ada Lovelace architecture
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):
        super(ModelNew, self).__init__()
        # Keep ConvTranspose as PyTorch native
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size, 
            stride, padding, output_padding,
            bias=False  # Bias handled in fused kernel
        )
        # Bias is scalar (1, 1, 1) -> store as 1-element tensor
        self.bias = nn.Parameter(torch.randn(1))
        
        # Initialize weights for better convergence
        nn.init.kaiming_normal_(self.conv_transpose.weight, mode='fan_out', nonlinearity='relu')
        nn.init.zeros_(self.bias)
        
    def forward(self, x):
        # Step 1: PyTorch native ConvTranspose2d
        x = self.conv_transpose(x)
        # Step 2: Fused post-ops in Triton (optimized kernel)
        x = fused_post_convtranspose(x, self.bias)
        return x
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl, AND any other modules used (e.g., import math if using math.sqrt)
   2. @triton.jit decorated kernel function(s) — NO continue/break/return inside loops (use masking)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels — THIS CLASS IS REQUIRED
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes
4. Ensure ALL imports are included at the top (common mistake: forgetting `import math`)

```python
# <corrected code>
```
