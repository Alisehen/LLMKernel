You are a Triton kernel debugging expert. Analyze the error and identify the root cause.

## ERROR LOG
```
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 538, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_114657_batch_range25to55_deepseek_deepseek/36_ConvTranspose2d_Min_Sum_GELU_Add/code/kernel_20251214_175021.py", line 155, in forward
    x = fused_post_convtranspose(x, self.bias)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_114657_batch_range25to55_deepseek_deepseek/36_ConvTranspose2d_Min_Sum_GELU_Add/code/kernel_20251214_175021.py", line 126, in fused_post_convtranspose
    fused_min_sum_gelu_bias_kernel[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 192, in run
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 192, in <dictcomp>
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 170, in _bench
    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/testing.py", line 145, in do_bench
    fn()
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 156, in kernel_call
    self.fn.run(
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 278, in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 81, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 44:15:

    # Process channels in blocks
    # Optimize memory access: load contiguous blocks for better coalescing
    c_range = tl.arange(0, BLOCK_C)

    for c_start in range(0, C, BLOCK_C):
        c_mask = (c_start + c_range) < C

        # Load block of [BLOCK_H, BLOCK_C] with transpose-friendly access
        # This pattern improves memory coalescing on Ada Lovelace
        for h_idx in range(0, BLOCK_H):
            if h_mask[h_idx]:
               ^
ValueError('Did you forget to add @triton.jit ? (`_builder` argument must be provided outside of JIT functions.)')
```

## Expected Behavior (PyTorch Reference)
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    A model that performs a convolution transpose, minimum operation, sum operation, GELU activation and addition.
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        x = self.conv_transpose(x)
        x = torch.min(x, dim=1, keepdim=True)[0]  # Minimum operation along channel dimension
        x = torch.sum(x, dim=2, keepdim=True)  # Sum operation along height dimension
        x = torch.nn.functional.gelu(x)  # GELU activation
        x = x + self.bias
        return x

batch_size = 16
in_channels = 64
out_channels = 128
height, width = 128, 128
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
bias_shape = (1, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]
```

## Current Implementation (Broken Triton Kernel)
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl
import math


@triton.autotune(
    configs=[
        # Conservative configs for register pressure
        triton.Config({'BLOCK_H': 32, 'BLOCK_C': 32}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_H': 16, 'BLOCK_C': 64}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_H': 16, 'BLOCK_C': 32}, num_warps=4, num_stages=2),
        # Aggressive config for smaller tensors
        triton.Config({'BLOCK_H': 32, 'BLOCK_C': 64}, num_warps=4, num_stages=1),
    ],
    key=['N', 'C', 'H', 'W'],
)
@triton.jit
def fused_min_sum_gelu_bias_kernel(
    x_ptr,
    bias_ptr,
    out_ptr,
    N, C, H, W,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_on, stride_o1, stride_o2, stride_ow,
    BLOCK_H: tl.constexpr,
    BLOCK_C: tl.constexpr,
):
    """
    Optimized fused kernel for: min(dim=1) + sum(dim=2) + GELU + bias
    Input: [N, C, H, W] -> Output: [N, 1, 1, W]
    Key optimizations:
    - Register-aware blocking to avoid spilling
    - Optimized GELU approximation
    - Memory access pattern optimization
    """
    pid_n = tl.program_id(0)
    pid_w = tl.program_id(1)
    
    if pid_n >= N or pid_w >= W:
        return
    
    # Initialize min accumulators for H dimension
    # Use registers only, avoid local memory
    h_offsets = tl.arange(0, BLOCK_H)
    h_mask = h_offsets < H
    
    # Initialize min values to INF for valid positions, 0 for invalid
    # Use f32 for better precision in reductions
    min_vals = tl.where(h_mask, tl.full((BLOCK_H,), float('inf'), dtype=tl.float32), 0.0)
    
    # Process channels in blocks
    # Optimize memory access: load contiguous blocks for better coalescing
    c_range = tl.arange(0, BLOCK_C)
    
    for c_start in range(0, C, BLOCK_C):
        c_mask = (c_start + c_range) < C
        
        # Load block of [BLOCK_H, BLOCK_C] with transpose-friendly access
        # This pattern improves memory coalescing on Ada Lovelace
        for h_idx in range(0, BLOCK_H):
            if h_mask[h_idx]:
                # Vectorized load across channels for this H position
                h_idx_val = tl.multiple_of(h_offsets[h_idx], 8)  # Hint for compiler
                
                # Compute base pointer once
                base_ptr = (
                    x_ptr +
                    pid_n * stride_xn +
                    h_idx_val * stride_xh +
                    pid_w * stride_xw
                )
                
                # Vectorized load across C dimension
                x_ptrs = base_ptr + (c_start + c_range) * stride_xc
                x_vals = tl.load(x_ptrs, mask=c_mask, other=float('inf'))
                
                # Update min for this H position
                current_min = tl.min(x_vals)
                min_vals = tl.where(h_mask[h_idx], 
                                   tl.minimum(min_vals[h_idx], current_min),
                                   min_vals[h_idx])
    
    # Sum across H dimension (only valid positions)
    # Use efficient reduction with tree-based sum
    sum_val = 0.0
    for h_idx in range(BLOCK_H):
        if h_mask[h_idx] and min_vals[h_idx] != float('inf'):
            sum_val += min_vals[h_idx]
    
    # Optimized GELU approximation for Ada Tensor Cores
    # Using polynomial approximation for better performance
    x = sum_val
    
    # Fast GELU approximation: x * sigmoid(1.702 * x)
    # This uses fewer operations than tanh-based version
    sigmoid_input = x * 1.702  # Optimized constant for GELU
    sigmoid = 1.0 / (1.0 + tl.exp(-sigmoid_input))  # Fast sigmoid
    gelu_result = x * sigmoid
    
    # Add bias (scalar)
    bias_val = tl.load(bias_ptr)
    result = gelu_result + bias_val
    
    # Store result [N, 1, 1, W]
    out_ptr_base = pid_n * stride_on + pid_w * stride_ow
    tl.store(out_ptr + out_ptr_base, result)


def fused_post_convtranspose(x, bias):
    """
    Fused: min(dim=1) + sum(dim=2) + GELU + bias
    Input shape: [N, C, H, W]
    Output shape: [N, 1, 1, W]
    """
    N, C, H, W = x.shape
    
    # Output tensor
    out = torch.empty((N, 1, 1, W), device=x.device, dtype=x.dtype)
    
    # Use autotune to select best block sizes
    # Grid: N x W (2D grid for better occupancy)
    grid = (N, W)
    
    fused_min_sum_gelu_bias_kernel[grid](
        x, bias, out,
        N, C, H, W,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3),
    )
    
    return out


class ModelNew(nn.Module):
    """
    ConvTranspose2d (PyTorch native) + Fused min + sum + GELU + bias (Triton)
    Optimized for Ada Lovelace architecture
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):
        super(ModelNew, self).__init__()
        # Keep ConvTranspose as PyTorch native
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size, 
            stride, padding, output_padding
        )
        # Initialize bias with small values for GELU stability
        self.bias = nn.Parameter(torch.randn(1) * 0.02)
        
    def forward(self, x):
        # Step 1: PyTorch native ConvTranspose2d
        x = self.conv_transpose(x)
        # Step 2: Fused post-ops in Triton
        x = fused_post_convtranspose(x, self.bias)
        return x
```

---

## Your Task

Identify the **single most critical issue** that causes the error above.

### Analysis Guidelines

1. **Focus on root cause**, not symptoms
   - Bad: "Output is wrong"
   - Good: "BLOCK_K loop missing, only processes first 32 elements of K dimension"

2. **Be specific about WHAT and WHERE**
   - Bad: "Memory access issue"
   - Good: "Line 45: tl.atomic_add(c_block_ptr, acc) - atomic_add requires scalar pointer, not block_ptr"

3. **Prioritize by impact**
   - Correctness bugs > Performance issues > Style problems
   - Algorithm errors > Implementation details

### Output Format

**CRITICAL: You MUST output ONLY valid JSON. No other text allowed.**

```json
{
  "critical_issue": "<Concise description of THE root cause, max 30 words>",
  "why_it_matters": "<Why this causes the observed error, max 35 words>",
  "minimal_fix_hint": "<What needs to change (not how), max 30 words>"
}
```

**Remember**: Output ONLY the JSON block. No explanations, no commentary, no additional text.
