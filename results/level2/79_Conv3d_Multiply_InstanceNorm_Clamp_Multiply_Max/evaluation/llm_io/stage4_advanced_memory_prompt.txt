You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Fine-tuning fused kernel parameters.

Params:
- num_warps ∈ {4, 8}
- num_stages ∈ {2, 3}

Conditional Rules (NOT one-size-fits-all):

IF register pressure LOW (regs < 96, no spill):
  - Try num_warps=8 for compute-bound fusion
  - num_stages=3 may help hide latency

IF register pressure HIGH (regs > 128 or occupancy_limit_registers):
  - Use num_warps=4 (fewer warps = more registers per warp)
  - Keep num_stages=2 (higher stages need more registers)

IF multi-input fusion (3+ distinct loads):
  - num_stages=2 preferred (each stage buffers all inputs)
  - num_warps=4 often better than 8

Autotune:
- Max 2-3 configs to reduce compilation time
- Always include conservative baseline (num_warps=4, num_stages=2)
- Test before/after: revert if gain < 2%



[CURRENT CODE]
```python
# <optimized Triton code>
import torch
import torch.nn as nn
import triton
import triton.language as tl


# ------------------------------------------------------------------------------------
# 1) Conv3D kernel (NCDHW, stride=1, padding=0) with autotuning
# ------------------------------------------------------------------------------------


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_M": 32, "BLOCK_N": 16}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 32}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 64}, num_warps=8, num_stages=2),
        triton.Config({"BLOCK_M": 128, "BLOCK_N": 32}, num_warps=8, num_stages=3),
    ],
    key=["D_out", "H_out", "W_out", "C_out"],
)
@triton.jit
def conv3d_ncdhw_kernel(
    x_ptr,
    w_ptr,
    b_ptr,
    y_ptr,
    N,
    C_in,
    D_in,
    H_in,
    W_in,
    C_out,
    D_out,
    H_out,
    W_out,
    DHW_out,
    stride_xn,
    stride_xc,
    stride_xd,
    stride_xh,
    stride_xw,
    stride_wn,
    stride_wc,
    stride_wd,
    stride_wh,
    stride_ww,
    stride_yn,
    stride_yc,
    stride_yd,
    stride_yh,
    stride_yw,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    K_D: tl.constexpr,
    K_H: tl.constexpr,
    K_W: tl.constexpr,
):
    # program ids
    pid_m = tl.program_id(0)  # over output positions (flattened N * D_out * H_out * W_out)
    pid_n = tl.program_id(1)  # over output channels

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    P = N * DHW_out
    mask_m = offs_m < P
    mask_n = offs_n < C_out

    HW_out = H_out * W_out

    # Decode flattened position -> (n, od, oh, ow)
    n_idx = offs_m // DHW_out
    rem = offs_m % DHW_out
    od_idx = rem // HW_out
    rem2 = rem % HW_out
    oh_idx = rem2 // W_out
    ow_idx = rem2 % W_out

    # Accumulator in FP32 for better precision
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Convolution: loops over input channels and kernel volume
    # K_* are tl.constexpr -> loops over kd/kh/kw are fully unrolled
    for ic in range(0, C_in):
        for kd in range(0, K_D):
            id_idx = od_idx + kd
            for kh in range(0, K_H):
                ih_idx = oh_idx + kh
                for kw in range(0, K_W):
                    iw_idx = ow_idx + kw

                    # Input: [N, C_in, D_in, H_in, W_in] (NCDHW)
                    x_offsets = (
                        n_idx * stride_xn
                        + ic * stride_xc
                        + id_idx * stride_xd
                        + ih_idx * stride_xh
                        + iw_idx * stride_xw
                    )
                    x_ptrs = x_ptr + x_offsets
                    x_vals = tl.load(x_ptrs, mask=mask_m, other=0.0)  # [BLOCK_M]

                    # Weights: [C_out, C_in, K_D, K_H, K_W]
                    w_offsets = (
                        offs_n * stride_wn
                        + ic * stride_wc
                        + kd * stride_wd
                        + kh * stride_wh
                        + kw * stride_ww
                    )
                    w_ptrs = w_ptr + w_offsets
                    w_vals = tl.load(w_ptrs, mask=mask_n, other=0.0)  # [BLOCK_N]

                    # FMA in registers
                    acc += x_vals[:, None] * w_vals[None, :]

    # Add bias per output channel
    bias = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0)
    acc += bias[None, :]

    # Store result in y [N, C_out, D_out, H_out, W_out]
    y_ptrs = (
        y_ptr
        + n_idx[:, None] * stride_yn
        + offs_n[None, :] * stride_yc
        + od_idx[:, None] * stride_yd
        + oh_idx[:, None] * stride_yh
        + ow_idx[:, None] * stride_yw
    )
    tl.store(y_ptrs, acc, mask=mask_m[:, None] & mask_n[None, :])


# ------------------------------------------------------------------------------------
# 2) InstanceNorm3d + clamp + mul kernel with autotuning
# ------------------------------------------------------------------------------------


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_S": 64}, num_warps=2, num_stages=2),
        triton.Config({"BLOCK_S": 128}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_S": 256}, num_warps=4, num_stages=2),
    ],
    key=["S"],
)
@triton.jit
def instancenorm_clamp_mul_kernel(
    x_ptr,
    multiplier_ptr,
    y_ptr,
    N,
    C,
    D,
    H,
    W,
    S,
    HW,
    stride_xn,
    stride_xc,
    stride_xd,
    stride_xh,
    stride_xw,
    stride_mc,
    stride_md,
    stride_mh,
    stride_mw,
    stride_yn,
    stride_yc,
    stride_yd,
    stride_yh,
    stride_yw,
    eps,
    clamp_min,
    clamp_max,
    BLOCK_S: tl.constexpr,
):
    # One program per (n, c)
    pid = tl.program_id(0)
    nc = pid
    n = nc // C
    c = nc % C

    offs = tl.arange(0, BLOCK_S)

    # Per-channel multiplier scalar (multiplier shape [C, 1, 1, 1])
    m_ptr = multiplier_ptr + c * stride_mc
    m_val = tl.load(m_ptr)

    # First pass: compute mean and variance over spatial positions for (n, c)
    mean = tl.zeros((), dtype=tl.float32)
    mean_sq = tl.zeros((), dtype=tl.float32)

    base_x_nc = x_ptr + n * stride_xn + c * stride_xc

    for s_start in range(0, S, BLOCK_S):
        s_idx = s_start + offs
        mask = s_idx < S

        d = s_idx // HW
        rem = s_idx % HW
        h = rem // W
        w = rem % W

        x_ptrs = (
            base_x_nc
            + d * stride_xd
            + h * stride_xh
            + w * stride_xw
        )
        x = tl.load(x_ptrs, mask=mask, other=0.0)
        x_scaled = x * m_val

        mean += tl.sum(x_scaled, axis=0)
        mean_sq += tl.sum(x_scaled * x_scaled, axis=0)

    # Normalize statistics
    mean = mean / S
    mean_sq = mean_sq / S
    var = mean_sq - mean * mean
    inv_std = 1.0 / tl.sqrt(var + eps)

    # Second pass: normalize, clamp, and multiply again
    for s_start in range(0, S, BLOCK_S):
        s_idx = s_start + offs
        mask = s_idx < S

        d = s_idx // HW
        rem = s_idx % HW
        h = rem // W
        w = rem % W

        x_ptrs = (
            base_x_nc
            + d * stride_xd
            + h * stride_xh
            + w * stride_xw
        )
        x = tl.load(x_ptrs, mask=mask, other=0.0)
        x_scaled = x * m_val
        x_norm = (x_scaled - mean) * inv_std
        x_clamped = tl.maximum(tl.minimum(x_norm, clamp_max), clamp_min)
        y_vals = x_clamped * m_val

        y_ptrs = (
            y_ptr
            + n * stride_yn
            + c * stride_yc
            + d * stride_yd
            + h * stride_yh
            + w * stride_yw
        )
        tl.store(y_ptrs, y_vals, mask=mask)


# ------------------------------------------------------------------------------------
# 3) Channel-wise max kernel with autotuning
# ------------------------------------------------------------------------------------


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_P": 64}, num_warps=2, num_stages=2),
        triton.Config({"BLOCK_P": 128}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_P": 256}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_P": 512}, num_warps=8, num_stages=2),
    ],
    key=["P", "C"],
)
@triton.jit
def channel_max_kernel(
    x_ptr,
    y_ptr,
    N,
    C,
    D,
    H,
    W,
    P,
    DHW,
    HW,
    stride_xn,
    stride_xc,
    stride_xd,
    stride_xh,
    stride_xw,
    stride_yn,
    stride_yd,
    stride_yh,
    stride_yw,
    BLOCK_P: tl.constexpr,
):
    pid = tl.program_id(0)
    offs = pid * BLOCK_P + tl.arange(0, BLOCK_P)
    mask = offs < P

    n = offs // DHW
    rem = offs % DHW
    d = rem // HW
    rem2 = rem % HW
    h = rem2 // W
    w = rem2 % W

    max_vals = tl.full((BLOCK_P,), -float("inf"), dtype=tl.float32)

    base_y = (
        y_ptr
        + n * stride_yn
        + d * stride_yd
        + h * stride_yh
        + w * stride_yw
    )

    # Reduce over channel dimension
    for c in range(0, C):
        x_ptrs = (
            x_ptr
            + n * stride_xn
            + c * stride_xc
            + d * stride_xd
            + h * stride_xh
            + w * stride_xw
        )
        x_vals = tl.load(x_ptrs, mask=mask, other=-float("inf"))
        max_vals = tl.maximum(max_vals, x_vals)

    tl.store(base_y, max_vals, mask=mask)


# ------------------------------------------------------------------------------------
# 4) Python wrappers for kernel launch / grid computation
# ------------------------------------------------------------------------------------


def conv3d_triton(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:
    """
    Direct 3D convolution in NCDHW with kernel [C_out, C_in, Kd, Kh, Kw],
    stride=1, padding=0, dilation=1, groups=1.
    """
    assert x.is_cuda and weight.is_cuda and bias.is_cuda
    x = x.contiguous()
    weight = weight.contiguous()
    bias = bias.contiguous()

    N, C_in, D_in, H_in, W_in = x.shape
    C_out, C_in_w, K_D, K_H, K_W = weight.shape
    assert C_in == C_in_w, "Inconsistent in_channels between input and weight."
    assert bias.numel() == C_out

    D_out = D_in - K_D + 1
    H_out = H_in - K_H + 1
    W_out = W_in - K_W + 1

    y = torch.empty((N, C_out, D_out, H_out, W_out), device=x.device, dtype=x.dtype)

    DHW_out = D_out * H_out * W_out
    P = N * DHW_out

    def grid(meta):
        return (
            triton.cdiv(P, meta["BLOCK_M"]),
            triton.cdiv(C_out, meta["BLOCK_N"]),
        )

    conv3d_ncdhw_kernel[grid](
        x,
        weight,
        bias,
        y,
        N,
        C_in,
        D_in,
        H_in,
        W_in,
        C_out,
        D_out,
        H_out,
        W_out,
        DHW_out,
        x.stride(0),
        x.stride(1),
        x.stride(2),
        x.stride(3),
        x.stride(4),
        weight.stride(0),
        weight.stride(1),
        weight.stride(2),
        weight.stride(3),
        weight.stride(4),
        y.stride(0),
        y.stride(1),
        y.stride(2),
        y.stride(3),
        y.stride(4),
        K_D=K_D,
        K_H=K_H,
        K_W=K_W,
    )
    return y


def instancenorm_clamp_mul_triton(
    x: torch.Tensor,
    multiplier: torch.Tensor,
    clamp_min: float,
    clamp_max: float,
    eps: float = 1e-5,
) -> torch.Tensor:
    """
    x: [N, C, D, H, W]
    multiplier: [C, 1, 1, 1]
    Applies:
      x1 = x * multiplier
      x2 = InstanceNorm3d(x1) (no affine, eps)
      x3 = clamp(x2, clamp_min, clamp_max)
      y  = x3 * multiplier
    """
    assert x.is_cuda and multiplier.is_cuda
    x = x.contiguous()
    multiplier = multiplier.contiguous()

    N, C, D, H, W = x.shape
    assert multiplier.shape[0] == C

    y = torch.empty_like(x)

    S = D * H * W
    HW = H * W

    def grid(meta):
        return (N * C,)

    instancenorm_clamp_mul_kernel[grid](
        x,
        multiplier,
        y,
        N,
        C,
        D,
        H,
        W,
        S,
        HW,
        x.stride(0),
        x.stride(1),
        x.stride(2),
        x.stride(3),
        x.stride(4),
        multiplier.stride(0),
        multiplier.stride(1),
        multiplier.stride(2),
        multiplier.stride(3),
        y.stride(0),
        y.stride(1),
        y.stride(2),
        y.stride(3),
        y.stride(4),
        eps,
        clamp_min,
        clamp_max,
    )
    return y


def channel_max_triton(x: torch.Tensor) -> torch.Tensor:
    """
    x: [N, C, D, H, W]
    returns: [N, D, H, W] = max over channel dimension
    """
    assert x.is_cuda
    x = x.contiguous()

    N, C, D, H, W = x.shape
    y = torch.empty((N, D, H, W), device=x.device, dtype=x.dtype)

    P = N * D * H * W
    DHW = D * H * W
    HW = H * W

    def grid(meta):
        return (triton.cdiv(P, meta["BLOCK_P"]),)

    channel_max_kernel[grid](
        x,
        y,
        N,
        C,
        D,
        H,
        W,
        P,
        DHW,
        HW,
        x.stride(0),
        x.stride(1),
        x.stride(2),
        x.stride(3),
        x.stride(4),
        y.stride(0),
        y.stride(1),
        y.stride(2),
        y.stride(3),
    )
    return y


def fused_conv3d_instancenorm_clamp_max(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    multiplier: torch.Tensor,
    clamp_min: float,
    clamp_max: float,
    eps: float = 1e-5,
) -> torch.Tensor:
    """
    Full fusion of the original PyTorch forward:
      x = conv3d(x, weight, bias)
      x = x * multiplier
      x = InstanceNorm3d(x)
      x = clamp(x, clamp_min, clamp_max)
      x = x * multiplier
      x = max(x, dim=1)
    Implemented as three Triton kernels with aggressive fusion inside each.
    """
    y = conv3d_triton(x, weight, bias)
    y = instancenorm_clamp_mul_triton(y, multiplier, clamp_min, clamp_max, eps)
    y = channel_max_triton(y)
    return y


# ------------------------------------------------------------------------------------
# 5) ModelNew using the optimized Triton kernels
# ------------------------------------------------------------------------------------


class ModelNew(nn.Module):
    """
    Triton implementation of:
      Conv3d -> *multiplier -> InstanceNorm3d -> clamp -> *multiplier -> max over channel

    Uses an nn.Conv3d module so that its weights/bias match the reference
    model's Conv3d parameters (enabling correct state_dict loading and
    identical convolutions).
    """

    def __init__(self, in_channels, out_channels, kernel_size, multiplier_shape, clamp_min, clamp_max):
        super(ModelNew, self).__init__()
        k = kernel_size
        self.conv = nn.Conv3d(
            in_channels,
            out_channels,
            kernel_size=k,
            stride=1,
            padding=0,
            dilation=1,
            groups=1,
            bias=True,
        )
        self.multiplier = nn.Parameter(torch.randn(*multiplier_shape))
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max
        self.eps = 1e-5

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return fused_conv3d_instancenorm_clamp_max(
            x,
            self.conv.weight,
            self.conv.bias,
            self.multiplier,
            self.clamp_min,
            self.clamp_max,
            self.eps,
        )
```

[NCU PROFILING METRICS]
{
  "conv3d_ncdhw_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 67.32,
    "launch__grid_size": 25200,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 32.97,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 6.25,
    "lts__t_sector_hit_rate.pct": 89.58
  },
  "instancenorm_clamp_mul_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 31.31,
    "launch__grid_size": 2048,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 61.55,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 68.83,
    "lts__t_sector_hit_rate.pct": 33.41
  },
  "channel_max_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 9.13,
    "launch__grid_size": 12600,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 92.15,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 93.82,
    "lts__t_sector_hit_rate.pct": 6.07
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

TRITON API CONSTRAINTS (CRITICAL):
- Triton has NO: tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
