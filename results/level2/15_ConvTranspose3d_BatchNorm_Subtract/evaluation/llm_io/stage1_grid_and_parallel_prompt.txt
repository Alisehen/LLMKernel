You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Grid layout & indexing for FUSED operations.

⚠️ FUSION EXCLUSIONS (do NOT apply fusion rules to these):
- Reduction ops (sum, mean, softmax along axis)
- Atomic operations
- Irregular/data-dependent access patterns
- Cross-block dependencies

Key Principle:
- All fused ops share the SAME grid AND the SAME (offsets, mask) tuple
- Grid covers OUTPUT tensor dimensions

Hard Rules:
- Every fused op MUST use identical offset calculation
- Every fused op MUST use identical boundary mask
- If broadcast needed: explicit `[None, :]` or `[:, None]`, NOT different offsets
- Element-wise: 1D grid, single `offs = pid * BLOCK + tl.arange(0, BLOCK)`
- Matmul fusion: 2D grid, `offs_m/offs_n` shared by bias add & activation

Verification:
- Check: all tl.load/tl.store use same `offsets` variable
- Check: all masks derived from same boundary condition
- If ANY op needs different indexing → do NOT fuse, split kernel



[CURRENT CODE]
```python
# Imports
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def subtract_spatial_mean_kernel(
    x_ptr, out_ptr,
    N, C, D, H, W,
    stride_xn, stride_xc, stride_xd, stride_xh, stride_xw,
    stride_on, stride_oc, stride_od, stride_oh, stride_ow,
    BLOCK_T: tl.constexpr,  # number of spatial elements processed per kernel instance in a vectorized chunk
):
    """
    For each (n, c) pair, compute the mean over spatial dimensions (D, H, W) and subtract it:
        out[n, c, d, h, w] = x[n, c, d, h, w] - mean_{d,h,w}(x[n, c, :, :, :])

    x, out: [N, C, D, H, W]
    """
    pid = tl.program_id(axis=0)
    NC = N * C

    # Guard against out-of-range programs
    if pid >= NC:
        return

    # Map program id to (n, c)
    n = pid // C
    c = pid % C

    # Base pointers for this (n, c) slice
    base_x = x_ptr + n * stride_xn + c * stride_xc
    base_out = out_ptr + n * stride_on + c * stride_oc

    # Total number of spatial elements
    S = D * H * W

    # Vectorized offsets over the flattened spatial dimension
    offs = tl.arange(0, BLOCK_T)

    # -------------------------------------------------------------------------
    # Pass 1: compute mean over spatial dims for this (n, c)
    # -------------------------------------------------------------------------
    sum_val = 0.0  # scalar accumulator in fp32

    for start in range(0, S, BLOCK_T):
        idx = start + offs
        mask = idx < S

        # Map flattened index idx -> (d, h, w)
        w = idx % W
        tmp = idx // W
        h = tmp % H
        d = tmp // H

        x_ptrs = base_x + d * stride_xd + h * stride_xh + w * stride_xw
        x_vals = tl.load(x_ptrs, mask=mask, other=0.0)
        # Reduce this block's values into a scalar and accumulate
        sum_val += tl.sum(x_vals, axis=0)

    mean = sum_val / S

    # -------------------------------------------------------------------------
    # Pass 2: subtract mean and write result
    # -------------------------------------------------------------------------
    for start in range(0, S, BLOCK_T):
        idx = start + offs
        mask = idx < S

        w = idx % W
        tmp = idx // W
        h = tmp % H
        d = tmp // H

        x_ptrs = base_x + d * stride_xd + h * stride_xh + w * stride_xw
        out_ptrs = base_out + d * stride_od + h * stride_oh + w * stride_ow

        x_vals = tl.load(x_ptrs, mask=mask, other=0.0)
        # Cast mean to the input dtype to match PyTorch's behavior as closely as possible
        mean_cast = tl.cast(mean, x_vals.dtype)
        y = x_vals - mean_cast
        tl.store(out_ptrs, y, mask=mask)


def subtract_spatial_mean_triton(x: torch.Tensor) -> torch.Tensor:
    """
    Wrapper around the Triton kernel to subtract the spatial mean over (D, H, W)
    from each (N, C) slice of a 5D tensor x: [N, C, D, H, W].

    This matches:
        x - torch.mean(x, dim=(2, 3, 4), keepdim=True)
    """
    assert x.ndim == 5, "Input must be a 5D tensor [N, C, D, H, W]"
    assert x.is_cuda, "Input tensor must be on CUDA device"

    N, C, D, H, W = x.shape
    out = torch.empty_like(x)

    # BLOCK_T must be a power-of-two constexpr. 256 is a good default here.
    BLOCK_T = 256

    grid = (N * C,)

    subtract_spatial_mean_kernel[grid](
        x, out,
        N, C, D, H, W,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3), out.stride(4),
        BLOCK_T=BLOCK_T,
    )
    return out


class ModelNew(nn.Module):
    """
    ConvTranspose3d (PyTorch native) + BatchNorm3d (PyTorch native) +
    Triton-accelerated spatial mean subtraction.

    Matches:
        x = conv_transpose(x)
        x = batch_norm(x)
        x = x - torch.mean(x, dim=(2, 3, 4), keepdim=True)
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True):
        super(ModelNew, self).__init__()
        # Keep ConvTranspose3d in PyTorch as required
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            bias=bias,
        )
        # BatchNorm3d in PyTorch (stateful, handles training/eval correctly)
        self.batch_norm = nn.BatchNorm3d(out_channels)

    def forward(self, x):
        # ConvTranspose3d (PyTorch)
        x = self.conv_transpose(x)
        # BatchNorm3d (PyTorch)
        x = self.batch_norm(x)
        # Subtract spatial mean over (D, H, W) using Triton
        x = subtract_spatial_mean_triton(x)
        return x
```

[NCU PROFILING METRICS]
{
  "subtract_spatial_mean_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 20.78,
    "launch__grid_size": 512,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 31.24,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 75.92,
    "lts__t_sector_hit_rate.pct": 38.94
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

TRITON API CONSTRAINTS (CRITICAL):
- Triton has NO: tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
