{
  "critical_issue": "Kernel implements tanh-approximate GELU while PyTorch reference uses default erf-based GELU, so activation formulas differ.",
  "why_it_matters": "Different GELU definitions cause systematic output deviations up to 1e-2, exceeding atol/rtol thresholds even if matmul and other activations are correct.",
  "minimal_fix_hint": "Match PyTorch's GELU formula and approximation setting (use erf-based variant or pass approximate='tanh' in the reference model)."
}