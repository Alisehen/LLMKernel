You are a GPU kernel optimization architect. Analyze the kernel and identify **ONE high-level algorithmic optimization**.

# PyTorch Reference
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Simple model that performs a convolution, applies Batch Normalization, and scales the output.
    """
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bn = nn.BatchNorm2d(out_channels)
        self.scaling_factor = scaling_factor

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = x * self.scaling_factor
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 128, 128
kernel_size = 3
scaling_factor = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scaling_factor]
```

# Current Triton Kernel
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def fused_conv_bn_scale_kernel(
    x_ptr,            # *f32,   [B, Cin, H, W]
    w_ptr,            # *f32,   [Cout, Cin, K, K]
    bias_ptr,         # *f32,   [Cout]
    bn_scale_ptr,     # *f32,   [Cout]  (alpha)
    bn_shift_ptr,     # *f32,   [Cout]  (beta')
    out_ptr,          # *f32,   [B, Cout, Ho, Wo]
    Ho, Wo,           # int32
    M, Kdim,          # int32,  M = B*Ho*Wo, Kdim = Cin*K*K
    Cout, K,          # int32
    stride_x_n, stride_x_c, stride_x_h, stride_x_w,
    stride_w_oc, stride_w_ic, stride_w_kh, stride_w_kw,
    stride_out_n, stride_out_c, stride_out_h, stride_out_w,
    BLOCK_M: tl.constexpr,  # tile size in "M" (N * Ho * Wo)
    BLOCK_N: tl.constexpr,  # tile size in Cout
    BLOCK_K: tl.constexpr,  # tile size in Kdim
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k_init = tl.arange(0, BLOCK_K)

    mask_m = offs_m < M
    mask_n = offs_n < Cout

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    nhw = Ho * Wo
    kk = K * K

    # Loop over Kdim (Cin * K * K)
    for k_start in range(0, Kdim, BLOCK_K):
        offs_k = k_start + offs_k_init
        mask_k = offs_k < Kdim

        # ----- A tile (input-im2col) : [BLOCK_M, BLOCK_K] -----
        # m decomposition: [BM,1]
        m_b = offs_m[:, None]
        n_idx = m_b // nhw
        rem_m = m_b - n_idx * nhw
        ho_idx = rem_m // Wo
        wo_idx = rem_m - ho_idx * Wo

        # k decomposition for A: [1,BK]
        k_a = offs_k[None, :]
        cin_a = k_a // kk
        rem_ka = k_a - cin_a * kk
        kh_a = rem_ka // K
        kw_a = rem_ka - kh_a * K

        h_in = ho_idx + kh_a
        w_in = wo_idx + kw_a

        x_ptrs = (
            x_ptr
            + n_idx * stride_x_n
            + cin_a * stride_x_c
            + h_in * stride_x_h
            + w_in * stride_x_w
        )

        a_mask = mask_m[:, None] & mask_k[None, :]

        a = tl.load(x_ptrs, mask=a_mask, other=0.0)

        # ----- B tile (weights) : [BLOCK_K, BLOCK_N] -----
        k_b = offs_k[:, None]
        out_b = offs_n[None, :]

        cin_b = k_b // kk
        rem_kb = k_b - cin_b * kk
        kh_b = rem_kb // K
        kw_b = rem_kb - kh_b * K

        w_ptrs = (
            w_ptr
            + out_b * stride_w_oc
            + cin_b * stride_w_ic
            + kh_b * stride_w_kh
            + kw_b * stride_w_kw
        )

        b_mask = mask_k[:, None] & mask_n[None, :]

        b = tl.load(w_ptrs, mask=b_mask, other=0.0)

        acc += tl.dot(a, b, allow_tf32=True)

    # Add convolution bias per output channel
    bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)
    acc = acc + bias[None, :]

    # Apply fused BatchNorm + scaling: y = alpha * conv_out + beta'
    bn_scale = tl.load(bn_scale_ptr + offs_n, mask=mask_n, other=0.0)
    bn_shift = tl.load(bn_shift_ptr + offs_n, mask=mask_n, other=0.0)
    acc = acc * bn_scale[None, :] + bn_shift[None, :]

    # Store result to [B, Cout, Ho, Wo]
    m_b = offs_m[:, None]
    n_idx = m_b // nhw
    rem_m = m_b - n_idx * nhw
    ho_idx = rem_m // Wo
    wo_idx = rem_m - ho_idx * Wo

    out_ptrs = (
        out_ptr
        + n_idx * stride_out_n
        + offs_n[None, :] * stride_out_c
        + ho_idx * stride_out_h
        + wo_idx * stride_out_w
    )

    out_mask = mask_m[:, None] & mask_n[None, :]
    tl.store(out_ptrs, acc, mask=out_mask)


def fused_conv_bn_scale(x, weight, bias, bn_scale, bn_shift, scaling_factor):
    # x:        [B, Cin, H, W]
    # weight:   [Cout, Cin, K, K]
    # bias:     [Cout]
    # bn_scale: [Cout]  (gamma / sqrt(var+eps) * scaling_factor)
    # bn_shift: [Cout]  ((beta - mu*gamma/sqrt(var+eps)) * scaling_factor)
    assert x.is_cuda, "Input must be on CUDA for Triton kernels."
    assert x.dtype == torch.float32, "This fused kernel currently supports float32."

    B, Cin, H, W = x.shape
    Cout, Cin_w, K, K_w = weight.shape
    assert Cin == Cin_w and K == K_w, "Incompatible input/weight shapes."

    # Only stride=1, padding=0, dilation=1, groups=1 are handled
    Ho = H - K + 1
    Wo = W - K + 1
    assert Ho > 0 and Wo > 0, "Input too small for valid convolution with given kernel size."

    # Ensure contiguous layouts for best performance
    x_c = x.contiguous()
    w_c = weight.contiguous()
    bias_c = bias.contiguous()
    bn_scale_c = bn_scale.contiguous()
    bn_shift_c = bn_shift.contiguous()

    out = torch.empty((B, Cout, Ho, Wo), device=x.device, dtype=x.dtype)

    M = B * Ho * Wo
    Kdim = Cin * K * K

    grid = lambda META: (
        triton.cdiv(M, META["BLOCK_M"]),
        triton.cdiv(Cout, META["BLOCK_N"]),
    )

    fused_conv_bn_scale_kernel[grid](
        x_c, w_c, bias_c, bn_scale_c, bn_shift_c, out,
        Ho, Wo, M, Kdim, Cout, K,
        x_c.stride(0), x_c.stride(1), x_c.stride(2), x_c.stride(3),
        w_c.stride(0), w_c.stride(1), w_c.stride(2), w_c.stride(3),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3),
        BLOCK_M=64,
        BLOCK_N=64,
        BLOCK_K=32,
        num_warps=4,
        num_stages=2,
    )

    return out


class ModelNew(nn.Module):
    """
    Fused Triton implementation of:
      Conv2d -> BatchNorm2d (inference) -> scaling_factor
    The BatchNorm is applied in inference mode using running_mean/var.
    """

    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor):
        super(ModelNew, self).__init__()
        # Match original module structure for parameter compatibility
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bn = nn.BatchNorm2d(out_channels)
        self.scaling_factor = float(scaling_factor)

    def forward(self, x):
        assert x.ndim == 4, "Input must be NCHW"
        assert self.conv.stride == (1, 1), "Kernel only supports stride=1"
        assert self.conv.padding == (0, 0), "Kernel only supports padding=0"
        assert self.conv.dilation == (1, 1), "Kernel only supports dilation=1"
        assert self.conv.groups == 1, "Kernel only supports groups=1"

        # Prepare parameters
        weight = self.conv.weight
        if self.conv.bias is None:
            bias = torch.zeros(
                weight.size(0),
                device=weight.device,
                dtype=weight.dtype,
            )
        else:
            bias = self.conv.bias

        bn = self.bn
        running_mean = bn.running_mean.to(weight.device, dtype=weight.dtype)
        running_var = bn.running_var.to(weight.device, dtype=weight.dtype)
        gamma = bn.weight.to(weight.device, dtype=weight.dtype)
        beta = bn.bias.to(weight.device, dtype=weight.dtype)
        eps = bn.eps

        # Precompute fused BatchNorm (inference) + scaling parameters
        # out = scaling_factor * [ (x - mu)/sqrt(var+eps) * gamma + beta ]
        inv_std = torch.rsqrt(running_var + eps)
        bn_scale = gamma * inv_std * self.scaling_factor
        bn_shift = (beta - running_mean * inv_std * gamma) * self.scaling_factor

        return fused_conv_bn_scale(x, weight, bias, bn_scale, bn_shift, self.scaling_factor)
```

# Performance
- **PyTorch baseline**: 4.74 ms
- **Current Triton**: 1.73 ms
- **Current speedup**: 2.75x (+63.6% vs baseline)


---

## Analysis Steps

1. **Code Analysis**: Count kernels, identify operations, check for inefficiencies
2. **Performance Diagnosis**: Use metrics/latency to identify bottleneck type
3. **Root Cause**: Combine code + performance to find the core issue

## Optimization Categories (pick ONE if worth optimizing):

### 1. Operator Fusion
Fuse consecutive ops into fewer kernels to reduce memory traffic and launch overhead.

### 2. Algorithm Replacement
Replace naive algorithm with optimized variant.
- For Attention: Flash Attention, online softmax
- For Convolution: Winograd, im2col
- **For RNN/GRU/LSTM**: Persistent kernel with HYBRID computation
  - **CRITICAL**: Use hybrid approach for best performance:
    * Precompute input-side gates ONCE (outside kernel): `gates_x = (T*B, In) @ W_ih`
    * Persistent kernel (inside): only recurrent-side: `for t: gates_h = h @ W_hh`
  - Time loop `for t in range(T)` must be inside kernel, NOT in Python
  - Launch kernel once per layer, not once per timestep
  - Expected speedup: 10-100x (vs per-timestep launches)

### 3. Kernel Launch Reduction
Combine multiple small kernels to reduce overhead.
- **For RNN/GRU/LSTM**: See "Algorithm Replacement" above for persistent kernel approach

### 4. Memory Layout Optimization
Use in-place operations, buffer reuse, or better layouts.

## Should We Optimize?

Before proposing optimization, determine if it's worthwhile:
- **Not worth optimizing** if:
  - Code is already near-optimal (expected speedup < 10%)
  - Bottleneck cannot be addressed (hardware limited, already optimal algorithm)
  - Optimization would add significant complexity with minimal gain

- **Worth optimizing** if:
  - Clear algorithmic inefficiency exists (multiple kernels, suboptimal algorithm)
  - Expected speedup >= 20%
  - Concrete optimization path available

## Output (JSON)

```json
{
  "worth_optimizing": "yes/no",
  "reason": "<Why worth or not worth optimizing, 1 sentence>",
  "bottleneck": "<Root cause in 1-2 sentences, empty if not worth optimizing>",
  "optimisation method": "<Specific optimization in 1-2 sentences, empty if not worth optimizing>",
  "modification plan": "<Implementation steps in 2-3 sentences, empty if not worth optimizing>",
  "expected_speedup": "<e.g., '30-40%', empty if not worth optimizing>"
}
```

Return JSON only.
