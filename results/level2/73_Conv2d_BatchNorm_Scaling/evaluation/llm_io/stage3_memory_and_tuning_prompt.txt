You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU: 4090

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Memory pattern and parameter tuning for fused operations.

Key Principle:
- Fusion benefit = eliminated INTERMEDIATE stores
- Multiple input loads are OK; intermediate stores are NOT

Memory Rules:
- ✅ Multiple tl.load() for different inputs (x, weight, bias) - OK
- ❌ tl.store() for intermediate results - NEVER (this is what fusion eliminates)
- ✅ Single tl.store() for final output - required

Verification:
- Count tl.store() calls: should equal number of OUTPUT tensors (usually 1)
- Intermediate values: must stay in registers between ops
- If you see store-then-load pattern for same data → BUG, refactor

Parameters to tune:
- num_warps ∈ {4, 8}
- num_stages ∈ {2, 3}

Conditional Tuning Rules:

IF register pressure LOW (regs < 96, no spill):
  - Try num_warps=8 for compute-bound fusion
  - num_stages=3 may help hide latency

IF register pressure HIGH (regs > 128 or occupancy_limit_registers):
  - Use num_warps=4 (fewer warps = more registers per warp)
  - Keep num_stages=2 (higher stages need more registers)

IF multi-input fusion (3+ distinct loads):
  - num_stages=2 preferred (each stage buffers all inputs)
  - num_warps=4 often better than 8

Autotune:
- Max 3-4 configs combining num_stages and num_warps
- Always include conservative baseline (num_warps=4, num_stages=2)
- Revert if gain < 2%



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        # Main high-throughput config (may use more registers)
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 32},
            num_warps=4,
            num_stages=2,
        ),
        # More conservative along N – helps if register bound
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 32, "BLOCK_K": 32},
            num_warps=4,
            num_stages=2,
        ),
        # More conservative along M – increases occupancy when M large
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 64, "BLOCK_K": 32},
            num_warps=2,
            num_stages=2,
        ),
    ],
    key=["M", "Cout"],
)
@triton.jit
def fused_conv_bn_scale_kernel(
    x_ptr,            # *f32,   [B, Cin, H, W]
    w_ptr,            # *f32,   [Cout, Cin, K, K]
    bias_ptr,         # *f32,   [Cout]
    bn_scale_ptr,     # *f32,   [Cout]  (alpha)
    bn_shift_ptr,     # *f32,   [Cout]  (beta')
    out_ptr,          # *f32,   [B, Cout, Ho, Wo]
    Ho, Wo,           # int32
    M, Kdim,          # int32,  M = B*Ho*Wo, Kdim = Cin*K*K
    Cout, K,          # int32
    stride_x_n, stride_x_c, stride_x_h, stride_x_w,
    stride_w_oc, stride_w_ic, stride_w_kh, stride_w_kw,
    stride_out_n, stride_out_c, stride_out_h, stride_out_w,
    BLOCK_M: tl.constexpr,  # tile size in "M" (N * Ho * Wo)
    BLOCK_N: tl.constexpr,  # tile size in Cout
    BLOCK_K: tl.constexpr,  # tile size in Kdim
):
    # -------------------------------------------------------------------------
    # 2D grid over output: (M, Cout)
    #   - pid_m tiles over flattened [B, Ho, Wo]
    #   - pid_n tiles over Cout
    # -------------------------------------------------------------------------
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    mask_m = offs_m < M
    mask_n = offs_n < Cout

    tl.multiple_of(offs_m, BLOCK_M)
    tl.multiple_of(offs_n, BLOCK_N)

    # -------------------------------------------------------------------------
    # Decompose M index -> (n, ho, wo)
    # -------------------------------------------------------------------------
    nhw = Ho * Wo
    m_b = offs_m

    n_idx = m_b // nhw
    rem_m = m_b - n_idx * nhw
    ho_idx = rem_m // Wo
    wo_idx = rem_m - ho_idx * Wo

    # Shape them as [BM, 1] for broadcasting with K & N dimensions
    n_idx = n_idx[:, None]
    ho_idx = ho_idx[:, None]
    wo_idx = wo_idx[:, None]

    # Base input pointer for each output position (no Cin / K yet)
    x_base = (
        x_ptr
        + n_idx * stride_x_n
        + ho_idx * stride_x_h
        + wo_idx * stride_x_w
    )  # [BM, 1]

    # Base output pointer for each output position (no Cout yet)
    out_base = (
        out_ptr
        + n_idx * stride_out_n
        + ho_idx * stride_out_h
        + wo_idx * stride_out_w
    )  # [BM, 1]

    # Base weight pointer for each output channel
    w_oc_base = w_ptr + offs_n[None, :] * stride_w_oc  # [1, BN]

    # -------------------------------------------------------------------------
    # Main GEMM loop over Kdim (Cin * K * K)
    # -------------------------------------------------------------------------
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    kk = K * K
    offs_k_init = tl.arange(0, BLOCK_K)
    tl.multiple_of(offs_k_init, BLOCK_K)

    for k_start in range(0, Kdim, BLOCK_K):
        offs_k = k_start + offs_k_init  # [BK]
        mask_k = offs_k < Kdim          # [BK]

        # ---- Decompose K index once (1D) and broadcast as needed ----
        cin_1d = offs_k // kk
        rem_k = offs_k - cin_1d * kk
        kh_1d = rem_k // K
        kw_1d = rem_k - kh_1d * K

        # ----- A tile: input im2col [BM, BK] -----
        x_ptrs = (
            x_base
            + cin_1d[None, :] * stride_x_c
            + kh_1d[None, :] * stride_x_h
            + kw_1d[None, :] * stride_x_w
        )  # [BM, BK]

        a_mask = mask_m[:, None] & mask_k[None, :]
        a = tl.load(x_ptrs, mask=a_mask, other=0.0)

        # ----- B tile: weights [BK, BN] -----
        w_ptrs = (
            w_oc_base
            + cin_1d[:, None] * stride_w_ic
            + kh_1d[:, None] * stride_w_kh
            + kw_1d[:, None] * stride_w_kw
        )  # [BK, BN]

        b_mask = mask_k[:, None] & mask_n[None, :]
        b = tl.load(w_ptrs, mask=b_mask, other=0.0)

        # GEMM accumulate: [BM, BK] x [BK, BN] -> [BM, BN]
        acc += tl.dot(a, b, allow_tf32=True)

    # -------------------------------------------------------------------------
    # Fused bias + BatchNorm (inference) + scaling
    # -------------------------------------------------------------------------
    bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)         # [BN]
    bn_scale = tl.load(bn_scale_ptr + offs_n, mask=mask_n, other=0.0) # [BN]
    bn_shift = tl.load(bn_shift_ptr + offs_n, mask=mask_n, other=0.0) # [BN]

    acc = acc + bias[None, :]
    acc = acc * bn_scale[None, :] + bn_shift[None, :]

    # -------------------------------------------------------------------------
    # Store result to [B, Cout, Ho, Wo]
    # -------------------------------------------------------------------------
    out_ptrs = out_base + offs_n[None, :] * stride_out_c  # [BM, BN]
    out_mask = mask_m[:, None] & mask_n[None, :]

    tl.store(out_ptrs, acc, mask=out_mask)


def fused_conv_bn_scale(x, weight, bias, bn_scale, bn_shift, scaling_factor):
    # x:        [B, Cin, H, W]
    # weight:   [Cout, Cin, K, K]
    # bias:     [Cout]
    # bn_scale: [Cout]  (gamma / sqrt(var+eps) * scaling_factor)
    # bn_shift: [Cout]  ((beta - mu*gamma/sqrt(var+eps)) * scaling_factor)
    assert x.is_cuda, "Input must be on CUDA for Triton kernels."
    assert x.dtype == torch.float32, "This fused kernel currently supports float32."

    B, Cin, H, W = x.shape
    Cout, Cin_w, K, K_w = weight.shape
    assert Cin == Cin_w and K == K_w, "Incompatible input/weight shapes."

    # Only stride=1, padding=0, dilation=1, groups=1 are handled
    Ho = H - K + 1
    Wo = W - K + 1
    assert Ho > 0 and Wo > 0, "Input too small for valid convolution with given kernel size."

    x_c = x.contiguous()
    w_c = weight.contiguous()
    bias_c = bias.contiguous()
    bn_scale_c = bn_scale.contiguous()
    bn_shift_c = bn_shift.contiguous()

    out = torch.empty((B, Cout, Ho, Wo), device=x.device, dtype=x.dtype)

    M = B * Ho * Wo
    Kdim = Cin * K * K

    def grid(meta):
        return (
            triton.cdiv(M, meta["BLOCK_M"]),
            triton.cdiv(Cout, meta["BLOCK_N"]),
        )

    fused_conv_bn_scale_kernel[grid](
        x_c, w_c, bias_c, bn_scale_c, bn_shift_c, out,
        Ho, Wo, M, Kdim, Cout, K,
        x_c.stride(0), x_c.stride(1), x_c.stride(2), x_c.stride(3),
        w_c.stride(0), w_c.stride(1), w_c.stride(2), w_c.stride(3),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3),
    )

    return out


class ModelNew(nn.Module):
    """
    Fused Triton implementation of:
      Conv2d -> BatchNorm2d (inference) -> scaling_factor
    The BatchNorm is applied in inference mode using running_mean/var.
    """

    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bn = nn.BatchNorm2d(out_channels)
        self.scaling_factor = float(scaling_factor)

    def forward(self, x):
        assert x.ndim == 4, "Input must be NCHW"
        assert self.conv.stride == (1, 1), "Kernel only supports stride=1"
        assert self.conv.padding == (0, 0), "Kernel only supports padding=0"
        assert self.conv.dilation == (1, 1), "Kernel only supports dilation=1"
        assert self.conv.groups == 1, "Kernel only supports groups=1"

        weight = self.conv.weight
        if self.conv.bias is None:
            bias = torch.zeros(
                weight.size(0),
                device=weight.device,
                dtype=weight.dtype,
            )
        else:
            bias = self.conv.bias

        bn = self.bn
        running_mean = bn.running_mean.to(weight.device, dtype=weight.dtype)
        running_var = bn.running_var.to(weight.device, dtype=weight.dtype)
        gamma = bn.weight.to(weight.device, dtype=weight.dtype)
        beta = bn.bias.to(weight.device, dtype=weight.dtype)
        eps = bn.eps

        # Precompute fused BatchNorm (inference) + scaling parameters
        # out = scaling_factor * [ (x - mu)/sqrt(var+eps) * gamma + beta ]
        inv_std = torch.rsqrt(running_var + eps)
        bn_scale = gamma * inv_std * self.scaling_factor
        bn_shift = (beta - running_mean * inv_std * gamma) * self.scaling_factor

        return fused_conv_bn_scale(x, weight, bias, bn_scale, bn_shift, self.scaling_factor)
```

[NCU PROFILING METRICS]
{
  "fused_conv_bn_scale_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 50.19,
    "launch__grid_size": 31752,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 24.65,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 54.83,
    "lts__t_sector_hit_rate.pct": 93.24
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

## CRITICAL — Code MUST compile and run:
1. EVERY kernel function MUST have `@triton.jit` decorator
2. Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
3. BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
4. `tl.program_id(axis)` only supports axis = 0, 1, 2
5. No `continue`, `break`, `return` inside loops — use masking
6. No tensor indexing with loop vars: `x[:, i]` is INVALID
7. mask shape MUST match data shape in tl.load/tl.store

## Missing Triton Functions (implement manually):
- tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

## OUTPUT FORMAT (STRICT):
1. Imports: torch, torch.nn, triton, triton.language as tl
2. @triton.jit decorated kernel function(s)
3. Wrapper function(s) for grid calculation and kernel launch
4. class ModelNew(nn.Module) that calls your kernels

Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
