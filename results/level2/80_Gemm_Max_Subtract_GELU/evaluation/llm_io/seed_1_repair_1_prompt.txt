Fix the Triton kernel errors. Generate correct code.

## ERROR LOG
```
Traceback (most recent call last):
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 43, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 2192, in store
    return _semantic.store(pointer, value, mask, boundary_check, cache_modifier, eviction_policy)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/semantic.py", line 1297, in store
    return self._store_legacy(ptr, val, mask, boundary_check, cache, eviction)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/semantic.py", line 1253, in _store_legacy
    raise ValueError("Value argument cannot be block type if pointer argument is not a block")
ValueError: Value argument cannot be block type if pointer argument is not a block

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 555, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251228_161811_batch_range79to85_openai_deepseek/80_Gemm_Max_Subtract_GELU/code/kernel_20251228_164013.py", line 318, in forward
    x = triton_max_reduce(x, self.max_dim)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251228_161811_batch_range79to85_openai_deepseek/80_Gemm_Max_Subtract_GELU/code/kernel_20251228_164013.py", line 173, in triton_max_reduce
    rowwise_max_kernel[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 419, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 733, in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 861, in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 300, in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 80, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 30:4:
    for start_n in range(0, N, BLOCK_N):
        offs_n = start_n + cols
        ptrs = row_in_ptr + offs_n * stride_in
        vals = tl.load(
            ptrs,
            mask=row_mask & (offs_n < N),
            other=-float("inf"),
        )
        block_max = tl.max(vals, axis=0)
        max_val = tl.maximum(max_val, block_max)

    tl.store(row_out_ptr + 0 * stride_on, max_val, mask=row_mask)
    ^
Value argument cannot be block type if pointer argument is not a block
```

## Broken Code
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


# ---------------------------
# 1) GEMM (Linear) kernel: y = x @ W^T + b
# ---------------------------
@triton.jit
def linear_gemm_kernel(
    a_ptr, w_ptr, b_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_wn, stride_wk,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    for k in range(0, K, BLOCK_K):
        k_mask = offs_k[None, :] < (K - k)
        a = tl.load(
            a_ptrs,
            mask=(offs_m[:, None] < M) & k_mask,
            other=0.0,
        )
        w = tl.load(
            w_ptrs,
            mask=(offs_k[:, None] < (K - k)) & (offs_n[None, :] < N),
            other=0.0,
        )
        acc += tl.dot(a, w, allow_tf32=True)
        a_ptrs += BLOCK_K * stride_ak
        w_ptrs += BLOCK_K * stride_wk

    # add bias: shape [N], broadcast over rows
    bias = tl.load(b_ptr + offs_n, mask=offs_n < N, other=0.0)
    acc += bias[None, :]

    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    mask_out = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(c_ptrs, acc, mask=mask_out)


def triton_linear(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:
    """
    x:      [M, K]
    weight: [N, K]  (nn.Linear weight)
    bias:   [N]
    returns y = x @ weight.T + bias  with shape [M, N]
    """
    assert x.is_cuda and weight.is_cuda and bias.is_cuda
    M, K = x.shape
    N = weight.shape[0]

    # W^T is [K, N], contiguous for better memory coalescing
    w_t = weight.t().contiguous()

    y = torch.empty((M, N), device=x.device, dtype=x.dtype)

    def grid(meta):
        return (
            max(1, triton.cdiv(M, meta["BLOCK_M"])),
            max(1, triton.cdiv(N, meta["BLOCK_N"])),
        )

    linear_gemm_kernel[grid](
        x, w_t, bias, y,
        M, N, K,
        x.stride(0), x.stride(1),
        w_t.stride(1), w_t.stride(0),
        y.stride(0), y.stride(1),
        BLOCK_M=128, BLOCK_N=128, BLOCK_K=32,
        num_warps=8, num_stages=2,
    )
    return y


# ---------------------------
# 2) Max-reduction kernels
# ---------------------------
@triton.jit
def rowwise_max_kernel(
    in_ptr, out_ptr,
    M, N,
    stride_im, stride_in,
    stride_om, stride_on,
    BLOCK_N: tl.constexpr,
):
    # Each program processes one row (reduce over columns)
    pid = tl.program_id(0)
    row = pid
    row_mask = row < M

    row_in_ptr = in_ptr + row * stride_im
    row_out_ptr = out_ptr + row * stride_om

    max_val = tl.full((1,), -float("inf"), dtype=tl.float32)

    cols = tl.arange(0, BLOCK_N)
    for start_n in range(0, N, BLOCK_N):
        offs_n = start_n + cols
        ptrs = row_in_ptr + offs_n * stride_in
        vals = tl.load(
            ptrs,
            mask=row_mask & (offs_n < N),
            other=-float("inf"),
        )
        block_max = tl.max(vals, axis=0)
        max_val = tl.maximum(max_val, block_max)

    tl.store(row_out_ptr + 0 * stride_on, max_val, mask=row_mask)


@triton.jit
def colwise_max_kernel(
    in_ptr, out_ptr,
    M, N,
    stride_im, stride_in,
    stride_om, stride_on,
    BLOCK_M: tl.constexpr,
):
    # Each program processes one column (reduce over rows)
    pid = tl.program_id(0)
    col = pid
    col_mask = col < N

    col_out_ptr = out_ptr + col * stride_on

    max_val = tl.full((1,), -float("inf"), dtype=tl.float32)

    rows = tl.arange(0, BLOCK_M)
    for start_m in range(0, M, BLOCK_M):
        offs_m = start_m + rows
        ptrs = in_ptr + offs_m * stride_im + col * stride_in
        vals = tl.load(
            ptrs,
            mask=col_mask & (offs_m < M),
            other=-float("inf"),
        )
        block_max = tl.max(vals, axis=0)
        max_val = tl.maximum(max_val, block_max)

    tl.store(out_ptr + 0 * stride_om + col * stride_on, max_val, mask=col_mask)


def triton_max_reduce(x: torch.Tensor, dim: int) -> torch.Tensor:
    """
    x: [M, N]
    dim: 0 or 1
    returns torch.max(x, dim=dim, keepdim=True).values
    """
    assert x.is_cuda
    M, N = x.shape
    if dim == 1:
        # Reduce over columns: output [M, 1]
        out = torch.empty((M, 1), device=x.device, dtype=x.dtype)

        def grid(meta):
            return (max(1, triton.cdiv(M, 1)),)

        rowwise_max_kernel[grid](
            x, out,
            M, N,
            x.stride(0), x.stride(1),
            out.stride(0), out.stride(1),
            BLOCK_N=128,
            num_warps=4, num_stages=2,
        )
        return out
    elif dim == 0:
        # Reduce over rows: output [1, N]
        out = torch.empty((1, N), device=x.device, dtype=x.dtype)

        def grid(meta):
            return (max(1, triton.cdiv(N, 1)),)

        colwise_max_kernel[grid](
            x, out,
            M, N,
            x.stride(0), x.stride(1),
            out.stride(0), out.stride(1),
            BLOCK_M=128,
            num_warps=4, num_stages=2,
        )
        return out
    else:
        raise NotImplementedError("Only dim=0 or dim=1 are supported for max reduction.")


# ---------------------------
# 3) Mean-subtract + GELU kernel
#    out = GELU(z - z.mean(dim=1, keepdim=True))
# ---------------------------
@triton.jit
def mean_sub_gelu_kernel(
    in_ptr, out_ptr,
    M, N,
    stride_im, stride_in,
    stride_om, stride_on,
    BLOCK_N: tl.constexpr,
):
    pid = tl.program_id(0)
    row = pid
    row_mask = row < M

    row_in_ptr = in_ptr + row * stride_im
    row_out_ptr = out_ptr + row * stride_om

    # ---- First pass: compute row-wise mean over dim=1 ----
    sum_val = tl.zeros((1,), dtype=tl.float32)

    cols = tl.arange(0, BLOCK_N)
    for start_n in range(0, N, BLOCK_N):
        offs_n = start_n + cols
        ptrs = row_in_ptr + offs_n * stride_in
        vals = tl.load(
            ptrs,
            mask=row_mask & (offs_n < N),
            other=0.0,
        )
        vals_f32 = vals.to(tl.float32)
        block_sum = tl.sum(vals_f32, axis=0)
        sum_val += block_sum

    n_f32 = tl.full((1,), N, dtype=tl.float32)
    mean = sum_val / n_f32

    # ---- Second pass: subtract mean and apply GELU ----
    for start_n in range(0, N, BLOCK_N):
        offs_n = start_n + cols
        ptrs = row_in_ptr + offs_n * stride_in
        vals = tl.load(
            ptrs,
            mask=row_mask & (offs_n < N),
            other=0.0,
        )
        x = vals.to(tl.float32) - mean  # center

        # GELU approximation:
        # gelu(x) = 0.5 * x * (1 + tanh( sqrt(2/pi) * (x + 0.044715 * x^3) ))
        x3 = x * x * x
        u = 0.7978845608028654 * (x + 0.044715 * x3)  # sqrt(2/pi) ≈ 0.79788456
        e = tl.exp(2.0 * u)
        tanh_u = (e - 1.0) / (e + 1.0)
        gelu = 0.5 * x * (1.0 + tanh_u)

        out_vals = gelu.to(out_ptr.dtype.element_ty)
        out_ptrs = row_out_ptr + offs_n * stride_on
        tl.store(out_ptrs, out_vals, mask=row_mask & (offs_n < N))


def triton_mean_sub_gelu(z: torch.Tensor) -> torch.Tensor:
    """
    z: 2D tensor
       In this model, z is either [B, 1] (if max_dim=1) or [1, F] (if max_dim=0).
    returns GELU(z - z.mean(dim=1, keepdim=True))
    """
    assert z.is_cuda
    M, N = z.shape
    out = torch.empty_like(z)

    def grid(meta):
        return (max(1, triton.cdiv(M, 1)),)

    mean_sub_gelu_kernel[grid](
        z, out,
        M, N,
        z.stride(0), z.stride(1),
        out.stride(0), out.stride(1),
        BLOCK_N=128,
        num_warps=4, num_stages=2,
    )
    return out


# ---------------------------
# 4) High-performance replacement model
# ---------------------------
class ModelNew(nn.Module):
    """
    Model that performs:
      1) GEMM (Linear)
      2) torch.max(x, dim=max_dim, keepdim=True).values
      3) subtraction of mean along dim=1 (keepdim=True)
      4) GELU activation

    All heavy ops are implemented with Triton kernels.
    """

    def __init__(self, in_features, out_features, max_dim):
        super(ModelNew, self).__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.bias = nn.Parameter(torch.randn(out_features))
        self.max_dim = max_dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Move parameters to same device as input if needed
        if self.weight.device != x.device:
            self.weight.data = self.weight.data.to(x.device)
            self.bias.data = self.bias.data.to(x.device)

        # 1) GEMM
        x = triton_linear(x, self.weight, self.bias)

        # 2) Max reduction with keepdim
        x = triton_max_reduce(x, self.max_dim)

        # 3) Subtract mean over dim=1 and 4) GELU
        x = triton_mean_sub_gelu(x)
        return x
```

## CRITICAL — These cause 60%+ of failures:
1. EVERY kernel function MUST have `@triton.jit` decorator — MANDATORY
2. Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
3. BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
4. `tl.program_id(axis)` only supports axis = 0, 1, 2
5. No `continue`, `break`, `return` inside loops — use masking
6. No tensor indexing with loop vars: `x[:, i]` is INVALID
7. mask shape MUST match data shape in tl.load/tl.store

## Missing Triton Functions (implement manually):
- tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

## OUTPUT FORMAT (STRICT):
1. Imports: torch, torch.nn, triton, triton.language as tl (and math if needed)
2. @triton.jit decorated kernel function(s)
3. Wrapper function(s) for grid calculation and kernel launch
4. class ModelNew(nn.Module) — REQUIRED

Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <corrected code>
```
