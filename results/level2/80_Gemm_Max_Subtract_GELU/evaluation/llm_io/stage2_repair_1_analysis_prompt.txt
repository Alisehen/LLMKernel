You are a Triton kernel debugging expert. Analyze the error and identify the root cause.

## ERROR LOG
```
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 538, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_114735_batch_range56to80_deepseek_deepseek/80_Gemm_Max_Subtract_GELU/code/kernel_20251215_022728.py", line 244, in forward
    return fused_gemm_max_sub_gelu(x, self.weight, self.max_dim)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_114735_batch_range56to80_deepseek_deepseek/80_Gemm_Max_Subtract_GELU/code/kernel_20251215_022728.py", line 208, in fused_gemm_max_sub_gelu
    fused_gemm_max_sub_gelu_kernel[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 230, in compile
    key = f"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}"
                            ^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 77, in hash
    key = f"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}"
             ^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 665, in cache_key
    dependencies_finder.visit(self.parse())
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 149, in visit_FunctionDef
    self.generic_visit(node)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 203, in visit_Assign
    self.generic_visit(node)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 428, in generic_visit
    self.visit(value)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 428, in generic_visit
    self.visit(value)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 142, in visit_Attribute
    ret = getattr(lhs, node.attr)
          ^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dtype' object has no attribute 'inf'
```

## Expected Behavior (PyTorch Reference)
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Model that performs a GEMM, followed by a max operation, subtraction, and GELU activation.
    """
    def __init__(self, in_features, out_features, max_dim):
        super(Model, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.max_dim = max_dim

    def forward(self, x):
        """
        Args:
            x: Input tensor of shape (batch_size, in_features)

        Returns:
            Output tensor of shape (batch_size, out_features)
        """
        x = self.gemm(x)
        x = torch.max(x, dim=self.max_dim, keepdim=True).values
        x = x - x.mean(dim=1, keepdim=True)
        x = torch.nn.functional.gelu(x)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192
max_dim = 1

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features, max_dim]
```

## Current Implementation (Broken Triton Kernel)
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def fused_gemm_max_sub_gelu_kernel(
    # Pointers to matrices
    a_ptr, b_ptr, output_ptr,
    # Matrix dimensions
    M, N, K,
    # Strides
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_out_m, stride_out_n,
    # Kernel parameters
    max_dim: tl.constexpr,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
    GROUP_M: tl.constexpr = 8,
    USE_TF32: tl.constexpr = True,
):
    """
    Fully fused kernel for:
    1. GEMM: x @ weight.T
    2. Max reduction along specified dimension
    3. Subtract mean along dim=1
    4. GELU activation
    
    Optimized for Ada Lovelace with register pressure awareness.
    """
    pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_M)
    num_pid_n = tl.cdiv(N, BLOCK_N)
    num_pid_in_group = GROUP_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)
    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m

    # ----------------------------------------------------------
    # Create pointers for the first blocks of A and B
    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)
    
    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)
    
    # ----------------------------------------------------------
    # Accumulator
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    # ----------------------------------------------------------
    # Inner product with K dimension blocking
    for k in range(0, tl.cdiv(K, BLOCK_K)):
        a = tl.load(a_ptrs, 
                   mask=(offs_am[:, None] < M) & (offs_k[None, :] < K - k * BLOCK_K),
                   other=0.0)
        b = tl.load(b_ptrs,
                   mask=(offs_k[:, None] < K - k * BLOCK_K) & (offs_bn[None, :] < N),
                   other=0.0)
        
        if USE_TF32:
            acc += tl.dot(a, b, allow_tf32=True)
        else:
            acc += tl.dot(a, b, allow_tf32=False)
        
        a_ptrs += BLOCK_K * stride_ak
        b_ptrs += BLOCK_K * stride_bk
    
    # ----------------------------------------------------------
    # Handle max reduction, subtract mean, and GELU
    # Based on profiling, we recompute cheap ops to save registers
    if max_dim == 1:  # Reduce over columns (dim=1)
        # Each row produces one value
        row_mask = offs_am < M
        col_mask = offs_bn < N
        
        # Compute max per row within this block
        if BLOCK_N > 1:
            # Use efficient reduction for larger blocks
            max_vals = tl.max(acc, axis=1)
        else:
            # Direct value for single column per thread
            max_vals = tl.where(col_mask, acc[:, 0], -tl.float32.inf)
        
        # Use shared memory for block-level reduction
        max_shared = tl.zeros((BLOCK_M,), dtype=tl.float32) - tl.float32.inf
        
        # Store initial max values
        if tl.program_id(0) == 0:  # First thread in block
            max_shared = tl.maximum(max_shared, max_vals)
        
        # Synchronize (Triton handles this implicitly for shared memory)
        max_vals = max_shared
        
        # Broadcast max to all threads in block
        max_vals = tl.broadcast_to(max_vals[:, None], (BLOCK_M, BLOCK_N))
        
        # Subtract max (element-wise)
        centered = acc - max_vals
        
        # Apply GELU: 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))
        # Optimized for Ada Lovelace TF32 tensor cores
        sqrt_2_over_pi = 0.7978845608028654
        gelu_coeff = 0.044715
        
        x = centered
        # Compute x^3 using FMA where possible
        x_sq = x * x
        x_cubed = x * x_sq
        inner = sqrt_2_over_pi * (x + gelu_coeff * x_cubed)
        
        # tanh approximation using exp
        exp_2x = tl.exp(2.0 * inner)
        tanh_inner = (exp_2x - 1.0) / (exp_2x + 1.0)
        
        gelu = 0.5 * x * (1.0 + tanh_inner)
        
        # Store results
        out_ptrs = output_ptr + offs_am[:, None] * stride_out_m + offs_bn[None, :] * stride_out_n
        tl.store(out_ptrs, gelu, mask=row_mask[:, None] & col_mask[None, :])
        
    else:  # max_dim == 0, reduce over rows (dim=0)
        # Each column produces one value
        row_mask = offs_am < M
        col_mask = offs_bn < N
        
        # Compute max per column within this block
        if BLOCK_M > 1:
            max_vals = tl.max(acc, axis=0)
        else:
            max_vals = tl.where(row_mask, acc[0, :], -tl.float32.inf)
        
        # Use shared memory for block-level reduction
        max_shared = tl.zeros((BLOCK_N,), dtype=tl.float32) - tl.float32.inf
        
        # Store initial max values
        if tl.program_id(0) == 0:  # First thread in block
            max_shared = tl.maximum(max_shared, max_vals)
        
        # Synchronize
        max_vals = max_shared
        
        # Broadcast max to all threads in block
        max_vals = tl.broadcast_to(max_vals[None, :], (BLOCK_M, BLOCK_N))
        
        # Subtract max (element-wise)
        centered = acc - max_vals
        
        # Apply GELU
        sqrt_2_over_pi = 0.7978845608028654
        gelu_coeff = 0.044715
        
        x = centered
        x_sq = x * x
        x_cubed = x * x_sq
        inner = sqrt_2_over_pi * (x + gelu_coeff * x_cubed)
        exp_2x = tl.exp(2.0 * inner)
        tanh_inner = (exp_2x - 1.0) / (exp_2x + 1.0)
        gelu = 0.5 * x * (1.0 + tanh_inner)
        
        # Store results
        out_ptrs = output_ptr + offs_am[:, None] * stride_out_m + offs_bn[None, :] * stride_out_n
        tl.store(out_ptrs, gelu, mask=row_mask[:, None] & col_mask[None, :])

def fused_gemm_max_sub_gelu(
    x: torch.Tensor,
    weight: torch.Tensor,
    max_dim: int,
    configs: list = None
):
    """
    Wrapper for the fully fused kernel.
    """
    M, K = x.shape
    N = weight.shape[0]
    
    # Transpose weight for GEMM
    weight_t = weight.t().contiguous()
    
    # Output shape depends on max_dim
    if max_dim == 1:
        output_shape = (M, N)  # Full output for GELU
    else:  # max_dim == 0
        output_shape = (M, N)  # Full output for GELU
    
    # Allocate output
    output = torch.empty(output_shape, device=x.device, dtype=x.dtype)
    
    # Default configurations optimized for Ada Lovelace
    if configs is None:
        configs = [
            {'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32, 'GROUP_M': 8},  # High occupancy
            {'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'GROUP_M': 8},   # Memory-friendly
            {'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 32, 'GROUP_M': 8},   # Compute-heavy
        ]
    
    # Grid calculation
    def grid(META):
        return (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']),)
    
    # Use autotuning - start with conservative configuration
    config = configs[0]
    
    # Launch kernel
    fused_gemm_max_sub_gelu_kernel[grid](
        x, weight_t, output,
        M, N, K,
        x.stride(0), x.stride(1),
        weight_t.stride(0), weight_t.stride(1),
        output.stride(0), output.stride(1),
        max_dim,
        BLOCK_M=config['BLOCK_M'],
        BLOCK_N=config['BLOCK_N'],
        BLOCK_K=config['BLOCK_K'],
        GROUP_M=config['GROUP_M'],
        USE_TF32=True,
    )
    
    return output

class ModelNew(nn.Module):
    """
    Optimized model that performs a GEMM, max reduction,
    subtraction, and GELU activation in a single fused kernel.
    """
    def __init__(self, in_features, out_features, max_dim):
        super(ModelNew, self).__init__()
        # Initialize weight with proper scaling for stable training
        std = (2.0 / in_features) ** 0.5
        self.weight = nn.Parameter(torch.randn(out_features, in_features) * std)
        self.max_dim = max_dim
        
    def forward(self, x):
        """
        Args:
            x: Input tensor of shape (batch_size, in_features)
            
        Returns:
            Output tensor after fused operations
        """
        return fused_gemm_max_sub_gelu(x, self.weight, self.max_dim)
```

---

## Your Task

Identify the **single most critical issue** that causes the error above.

### Analysis Guidelines

1. **Focus on root cause**, not symptoms
   - Bad: "Output is wrong"
   - Good: "BLOCK_K loop missing, only processes first 32 elements of K dimension"

2. **Be specific about WHAT and WHERE**
   - Bad: "Memory access issue"
   - Good: "Line 45: tl.atomic_add(c_block_ptr, acc) - atomic_add requires scalar pointer, not block_ptr"

3. **Prioritize by impact**
   - Correctness bugs > Performance issues > Style problems
   - Algorithm errors > Implementation details

### Output Format

**CRITICAL: You MUST output ONLY valid JSON. No other text allowed.**

```json
{
  "critical_issue": "<Concise description of THE root cause, max 30 words>",
  "why_it_matters": "<Why this causes the observed error, max 35 words>",
  "minimal_fix_hint": "<What needs to change (not how), max 30 words>"
}
```

**Remember**: Output ONLY the JSON block. No explanations, no commentary, no additional text.
