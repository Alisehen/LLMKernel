You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Grid layout & indexing for FUSED operations.

⚠️ FUSION EXCLUSIONS (do NOT apply fusion rules to these):
- Reduction ops (sum, mean, softmax along axis)
- Atomic operations
- Irregular/data-dependent access patterns
- Cross-block dependencies

Key Principle:
- All fused ops share the SAME grid AND the SAME (offsets, mask) tuple
- Grid covers OUTPUT tensor dimensions

Hard Rules:
- Every fused op MUST use identical offset calculation
- Every fused op MUST use identical boundary mask
- If broadcast needed: explicit `[None, :]` or `[:, None]`, NOT different offsets
- Element-wise: 1D grid, single `offs = pid * BLOCK + tl.arange(0, BLOCK)`
- Matmul fusion: 2D grid, `offs_m/offs_n` shared by bias add & activation

Verification:
- Check: all tl.load/tl.store use same `offsets` variable
- Check: all masks derived from same boundary condition
- If ANY op needs different indexing → do NOT fuse, split kernel



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def fused_gemm_max_sub_gelu_kernel(
    a_ptr, b_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,
    max_dim: tl.constexpr,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    """
    Fused kernel for:
    1. GEMM: x @ weight.T
    2. Max reduction along specified dimension
    3. Subtract mean along dim=1
    4. GELU activation
    
    For max_dim=1: output shape (M, 1) after max reduction
    For max_dim=0: output shape (1, N) after max reduction
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)
    
    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn
    
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    # Matrix multiplication with blocking
    for k in range(0, K, BLOCK_K):
        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] < K - k), other=0.0)
        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k) & (offs_n[None, :] < N), other=0.0)
        acc += tl.dot(a, b, allow_tf32=True)
        a_ptrs += BLOCK_K * stride_ak
        b_ptrs += BLOCK_K * stride_bk
    
    # Max reduction along specified dimension
    if max_dim == 1:  # Reduce over columns (dim=1)
        # Each row produces one output
        if BLOCK_N == 1:
            # Direct load if BLOCK_N=1 (each thread handles one column)
            max_val = tl.load(c_ptr + offs_m * stride_cm)
            new_max = tl.maximum(max_val, tl.max(acc, axis=1))
            tl.store(c_ptr + offs_m * stride_cm, new_max, mask=offs_m < M)
        else:
            # Reduce within block first, then atomic max
            block_max = tl.max(acc, axis=1)  # shape: (BLOCK_M,)
            for i in range(0, BLOCK_N):
                col_idx = pid_n * BLOCK_N + i
                if col_idx < N:
                    # Extract column i from acc matrix properly using masking
                    col_mask = tl.arange(0, BLOCK_N) == i
                    acc_col = tl.where(col_mask, acc, -float('inf'))
                    col_max = tl.max(acc_col, axis=1)
                    val = tl.maximum(block_max, col_max)  # FIXED: use tl.maximum for elementwise
                    tl.atomic_max(c_ptr + offs_m * stride_cm + col_idx * stride_cn, 
                                 val, mask=offs_m < M)
    
    else:  # max_dim == 0, reduce over rows (dim=0)
        # Each column produces one output
        if BLOCK_M == 1:
            # Direct load if BLOCK_M=1 (each thread handles one row)
            max_val = tl.load(c_ptr + offs_n * stride_cn)
            new_max = tl.maximum(max_val, tl.max(acc, axis=0))
            tl.store(c_ptr + offs_n * stride_cn, new_max, mask=offs_n < N)
        else:
            # Reduce within block first, then atomic max
            block_max = tl.max(acc, axis=0)  # shape: (BLOCK_N,)
            for i in range(0, BLOCK_M):
                row_idx = pid_m * BLOCK_M + i
                if row_idx < M:
                    # Extract row i from acc matrix properly using masking
                    row_mask = tl.arange(0, BLOCK_M) == i
                    acc_row = tl.where(row_mask[:, None], acc, -float('inf'))
                    row_max = tl.max(acc_row, axis=0)
                    val = tl.maximum(block_max, row_max)  # FIXED: use tl.maximum for elementwise
                    tl.atomic_max(c_ptr + row_idx * stride_cm + offs_n * stride_cn,
                                 val, mask=offs_n < N)

@triton.jit
def subtract_mean_gelu_kernel(
    input_ptr, output_ptr,
    M, N,
    stride_in_m, stride_in_n, stride_out_m, stride_out_n,
    max_dim: tl.constexpr,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,
):
    """
    Kernel for:
    1. Subtract mean along dim=1
    2. GELU activation
    
    For max_dim=1: input shape (M, 1), mean over dim=1 is trivial
    For max_dim=0: input shape (1, N), need to compute mean
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    
    if max_dim == 1:
        # Input is (M, 1), mean along dim=1 is just the value itself
        val = tl.load(input_ptr + offs_m * stride_in_m, 
                     mask=offs_m < M, other=0.0)
        # Subtract mean (which is the value itself) results in 0
        # GELU(0) = 0, so we can just store 0
        result = 0.0
        tl.store(output_ptr + offs_m * stride_out_m,
                result, mask=offs_m < M)
    
    else:  # max_dim == 0
        # Input is (1, N), need to compute mean over rows (dim=1)
        # Since there's only one row, mean is just that row's mean
        # We compute mean by loading all values and using atomics
        
        # Load value for this block
        val = tl.load(input_ptr + offs_n * stride_in_n,
                     mask=offs_n < N, other=0.0)
        
        # For mean computation, we accumulate sum using atomics
        # and count of elements
        sum_ptr = output_ptr + M * N  # Use extra memory for sum
        count_ptr = output_ptr + M * N + 1  # Use extra memory for count
        
        if pid_m == 0 and pid_n == 0:
            # Initialize sum and count
            tl.store(sum_ptr, 0.0)
            tl.store(count_ptr, 0.0)
        
        tl.atomic_add(sum_ptr, tl.sum(val))
        tl.atomic_add(count_ptr, tl.sum(tl.where(offs_n < N, 1.0, 0.0)))
        
        # Synchronize (implicit in Triton)
        # Wait for all threads to complete accumulation
        
        # Compute mean
        total_sum = tl.load(sum_ptr)
        total_count = tl.load(count_ptr)
        mean = total_sum / total_count
        
        # Subtract mean and apply GELU
        val_centered = val - mean
        
        # GELU approximation: 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))
        # Using float32 constants for accuracy
        sqrt_2_over_pi = 0.7978845608028654
        gelu_coeff = 0.044715
        
        x = val_centered
        x_cubed = x * x * x
        inner = sqrt_2_over_pi * (x + gelu_coeff * x_cubed)
        tanh_inner = (tl.exp(2 * inner) - 1) / (tl.exp(2 * inner) + 1)  # tanh approximation
        gelu = 0.5 * x * (1 + tanh_inner)
        
        # Store result
        tl.store(output_ptr + offs_n * stride_out_n,
                gelu, mask=offs_n < N)

def fused_gemm_max_sub_gelu(x, weight, max_dim):
    """
    Wrapper function for fused GEMM + Max + Subtract + GELU
    """
    M, K = x.shape
    N = weight.shape[0]
    
    # Transpose weight for GEMM
    weight_t = weight.t().contiguous()
    
    if max_dim == 1:
        # Output shape: (M, 1) after max reduction
        output_shape = (M, 1)
        temp_shape = (M, N)  # Need full matrix for max reduction
    else:  # max_dim == 0
        # Output shape: (1, N) after max reduction
        output_shape = (1, N)
        temp_shape = (M, N)  # Need full matrix for max reduction
    
    # Allocate output tensor
    output = torch.empty(output_shape, device=x.device, dtype=x.dtype)
    
    # Allocate temporary tensor for max reduction
    temp_max = torch.full(temp_shape, float('-inf'), 
                         device=x.device, dtype=x.dtype)
    
    # Configuration for GEMM + Max kernel
    configs = [
        {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32},
        {'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32},
    ]
    
    # Launch GEMM + Max kernel
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_M']),
        triton.cdiv(N, META['BLOCK_N']),
    )
    
    # Use first config for stable compilation
    config = configs[0]
    fused_gemm_max_sub_gelu_kernel[grid](
        x, weight_t, temp_max,
        M, N, K,
        x.stride(0), x.stride(1),
        weight_t.stride(0), weight_t.stride(1),
        temp_max.stride(0), temp_max.stride(1),
        max_dim,
        BLOCK_M=config['BLOCK_M'],
        BLOCK_N=config['BLOCK_N'],
        BLOCK_K=config['BLOCK_K'],
    )
    
    # Launch Subtract Mean + GELU kernel
    if max_dim == 1:
        grid_sub = lambda META: (triton.cdiv(M, META['BLOCK_M']), 1)
        config_sub = {'BLOCK_M': 128, 'BLOCK_N': 1}
    else:
        grid_sub = lambda META: (1, triton.cdiv(N, META['BLOCK_N']))
        config_sub = {'BLOCK_M': 1, 'BLOCK_N': 128}
    
    # Allocate additional memory for mean computation if needed
    if max_dim == 0:
        extra_memory = torch.zeros(M * N + 2, device=x.device, dtype=x.dtype)
        output_with_extra = torch.cat([output.view(-1), extra_memory])
    else:
        output_with_extra = output
    
    subtract_mean_gelu_kernel[grid_sub](
        temp_max if max_dim == 1 else temp_max[0:1, :],  # Only need first row for max_dim=0
        output_with_extra,
        output_shape[0], output_shape[1],
        temp_max.stride(0) if max_dim == 1 else temp_max.stride(1),
        temp_max.stride(1) if max_dim == 1 else 0,
        output.stride(0), output.stride(1),
        max_dim,
        BLOCK_M=config_sub['BLOCK_M'],
        BLOCK_N=config_sub['BLOCK_N'],
    )
    
    return output

class ModelNew(nn.Module):
    """
    Model that performs a GEMM, followed by a max operation, 
    subtraction, and GELU activation.
    """
    def __init__(self, in_features, out_features, max_dim):
        super(ModelNew, self).__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.max_dim = max_dim
        
    def forward(self, x):
        """
        Args:
            x: Input tensor of shape (batch_size, in_features)
            
        Returns:
            Output tensor of shape (batch_size, out_features) for max_dim=1
            or (1, out_features) for max_dim=0
        """
        return fused_gemm_max_sub_gelu(x, self.weight, self.max_dim)
```

[NCU PROFILING METRICS]
{
  "fused_gemm_max_sub_gelu_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 40.67,
    "launch__grid_size": 2048,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 24.13,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 16.72,
    "lts__t_sector_hit_rate.pct": 96.11
  },
  "subtract_mean_gelu_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 0.03,
    "launch__grid_size": 8,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 8.29,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 0.17,
    "lts__t_sector_hit_rate.pct": 94.49
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

TRITON API CONSTRAINTS (CRITICAL):
- Triton has NO: tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
