```python
import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def fused_gemm_max_sub_gelu_kernel(
    a_ptr, b_ptr, c_ptr, output_ptr,
    M, N, K,
    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,
    stride_out_m, stride_out_n,
    max_dim: tl.constexpr,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
    FUSED_ELEMWISE: tl.constexpr,
):
    """
    Fused kernel for:
    1. GEMM: x @ weight.T
    2. Max reduction along specified dimension
    3. Subtract mean along dim=1
    4. GELU activation
    
    All operations use SAME grid layout and SAME indexing.
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    # Shared indexing - same for ALL operations
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)
    
    # Same mask for ALL operations
    mask_m = offs_m < M
    mask_n = offs_n < N
    
    # GEMM: Standard blocked matrix multiplication
    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn
    
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    for k in range(0, K, BLOCK_K):
        # CRITICAL FIX: Use 1D k_mask then broadcast to avoid dimension mismatch
        k_mask_1d = offs_k < (K - k)
        a_mask = mask_m[:, None] & k_mask_1d[None, :]
        b_mask = k_mask_1d[:, None] & mask_n[None, :]
        
        a = tl.load(a_ptrs, mask=a_mask, other=0.0)
        b = tl.load(b_ptrs, mask=b_mask, other=0.0)
        acc += tl.dot(a, b, allow_tf32=True)
        
        a_ptrs += BLOCK_K * stride_ak
        b_ptrs += BLOCK_K * stride_bk
    
    # Fused element-wise operations if enabled
    if FUSED_ELEMWISE:
        if max_dim == 1:  # Reduce over columns (dim=1)
            # Each row produces one output - compute mean over columns
            row_sum = tl.sum(acc, axis=1) / N  # [BLOCK_M]
            row_mean = tl.where(mask_m, row_sum, 0.0)
            
            # Subtract mean and apply GELU
            for i in range(BLOCK_N):
                col_idx = pid_n * BLOCK_N + i
                if col_idx < N:
                    # Subtract row mean from each element
                    val = acc[:, i] - row_mean
                    
                    # Fast GELU approximation optimized for Ada Lovelace
                    # 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))
                    x = val
                    x_cubed = x * x * x
                    sqrt_2_over_pi = 0.7978845608028654
                    gelu_coeff = 0.044715
                    inner = sqrt_2_over_pi * (x + gelu_coeff * x_cubed)
                    # tanh(x) = (exp(2x) - 1) / (exp(2x) + 1)
                    exp_2_inner = tl.exp(2.0 * inner)
                    tanh_inner = (exp_2_inner - 1.0) / (exp_2_inner + 1.0)
                    gelu = 0.5 * x * (1.0 + tanh_inner)
                    
                    # Store directly to output
                    tl.store(output_ptr + offs_m * stride_out_m + col_idx * stride_out_n,
                            gelu, mask=mask_m)
        
        else:  # max_dim == 0, reduce over rows (dim=0)
            # Each column produces one output - compute mean over rows
            col_sum = tl.sum(acc, axis=0) / M  # [BLOCK_N]
            col_mean = tl.where(mask_n, col_sum, 0.0)
            
            # Subtract mean and apply GELU
            for i in range(BLOCK_M):
                row_idx = pid_m * BLOCK_M + i
                if row_idx < M:
                    # Subtract column mean from each element
                    val = acc[i, :] - col_mean
                    
                    # Fast GELU approximation
                    x = val
                    x_cubed = x * x * x
                    sqrt_2_over_pi = 0.7978845608028654
                    gelu_coeff = 0.044715
                    inner = sqrt_2_over_pi * (x + gelu_coeff * x_cubed)
                    exp_2_inner = tl.exp(2.0 * inner)
                    tanh_inner = (exp_2_inner - 1.0) / (exp_2_inner + 1.0)
                    gelu = 0.5 * x * (1.0 + tanh_inner)
                    
                    # Store directly to output
                    tl.store(output_ptr + row_idx * stride_out_m + offs_n * stride_out_n,
                            gelu, mask=mask_n)
    else:
        # Only max reduction (legacy path)
        if max_dim == 1:
            for i in range(BLOCK_N):
                col_idx = pid_n * BLOCK_N + i
                if col_idx < N:
                    col_val = acc[:, i]
                    tl.atomic_max(c_ptr + offs_m * stride_cm + col_idx * stride_cn,
                                col_val, mask=mask_m)
        else:  # max_dim == 0
            for i in range(BLOCK_M):
                row_idx = pid_m * BLOCK_M + i
                if row_idx < M:
                    row_val = acc[i, :]
                    tl.atomic_max(c_ptr + row_idx * stride_cm + offs_n * stride_cn,
                                row_val, mask=mask_n)

@triton.jit
def fast_elementwise_kernel(
    input_ptr, output_ptr,
    M, N,
    stride_in_m, stride_in_n, stride_out_m, stride_out_n,
    max_dim: tl.constexpr,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,
):
    """
    Optimized elementwise kernel for subtract mean + GELU.
    Uses SAME grid layout as GEMM for optimal fusion.
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    # Same indexing as GEMM kernel
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    
    mask_m = offs_m < M
    mask_n = offs_n < N
    
    if max_dim == 1:
        # Input is (M, N) after max reduction along columns
        # Each row has same max value repeated
        
        # Load input values
        vals = tl.load(input_ptr + offs_m[:, None] * stride_in_m + offs_n[None, :] * stride_in_n,
                      mask=mask_m[:, None] & mask_n[None, :], other=0.0)
        
        # Compute row mean
        row_sum = tl.sum(vals, axis=1) / N
        row_mean = tl.where(mask_m, row_sum, 0.0)
        
        # Subtract mean and apply GELU
        centered = vals - row_mean[:, None]
        
        # Vectorized GELU computation
        x = centered
        x_cubed = x * x * x
        sqrt_2_over_pi = 0.7978845608028654
        gelu_coeff = 0.044715
        inner = sqrt_2_over_pi * (x + gelu_coeff * x_cubed)
        exp_2_inner = tl.exp(2.0 * inner)
        tanh_inner = (exp_2_inner - 1.0) / (exp_2_inner + 1.0)
        gelu = 0.5 * x * (1.0 + tanh_inner)
        
        # Store results
        tl.store(output_ptr + offs_m[:, None] * stride_out_m + offs_n[None, :] * stride_out_n,
                gelu, mask=mask_m[:, None] & mask_n[None, :])
    
    else:  # max_dim == 0
        # Input is (M, N) after max reduction along rows
        # Each column has same max value repeated
        
        # Load input values
        vals = tl.load(input_ptr + offs_m[:, None] * stride_in_m + offs_n[None, :] * stride_in_n,
                      mask=mask_m[:, None] & mask_n[None, :], other=0.0)
        
        # Compute column mean
        col_sum = tl.sum(vals, axis=0) / M
        col_mean = tl.where(mask_n, col_sum, 0.0)
        
        # Subtract mean and apply GELU
        centered = vals - col_mean[None, :]
        
        # Vectorized GELU computation
        x = centered
        x_cubed = x * x * x
        sqrt_2_over_pi = 0.7978845608028654
        gelu_coeff = 0.044715
        inner = sqrt_2_over_pi * (x + gelu_coeff * x_cubed)
        exp_2_inner = tl.exp(2.0 * inner)
        tanh_inner = (exp_2_inner - 1.0) / (exp_2_inner + 1.0)
        gelu = 0.5 * x * (1.0 + tanh_inner)
        
        # Store results
        tl.store(output_ptr + offs_m[:, None] * stride_out_m + offs_n[None, :] * stride_out_n,
                gelu, mask=mask_m[:, None] & mask_n[None, :])

def fused_gemm_max_sub_gelu(x, weight, max_dim, use_fused=True):
    """
    Wrapper function for fused GEMM + Max + Subtract + GELU.
    Optimized for Ada Lovelace architecture.
    """
    M, K = x.shape
    N = weight.shape[0]
    
    # Transpose weight for GEMM
    weight_t = weight.t().contiguous()
    
    # Allocate output tensor
    if max_dim == 1:
        output_shape = (M, N)
    else:  # max_dim == 0
        output_shape = (M, N)
    
    output = torch.empty(output_shape, device=x.device, dtype=x.dtype)
    
    # Configuration optimized for Ada Lovelace
    # Using larger blocks for better Tensor Core utilization
    configs = [
        {'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64},  # Best for TF32/BF16
        {'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32},   # Good for memory-bound
        {'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 32},   # Good for tall-skinny
    ]
    
    if use_fused:
        # Fully fused path: GEMM + max + subtract + GELU in one kernel
        config = configs[0]
        grid = (triton.cdiv(M, config['BLOCK_M']), triton.cdiv(N, config['BLOCK_N']))
        
        # For fused operation, we don't need intermediate tensor
        temp_max = torch.empty((M, N), device=x.device, dtype=x.dtype)
        
        fused_gemm_max_sub_gelu_kernel[grid](
            x, weight_t, temp_max, output,
            M, N, K,
            x.stride(0), x.stride(1),
            weight_t.stride(0), weight_t.stride(1),
            temp_max.stride(0), temp_max.stride(1),
            output.stride(0), output.stride(1),
            max_dim,
            BLOCK_M=config['BLOCK_M'],
            BLOCK_N=config['BLOCK_N'],
            BLOCK_K=config['BLOCK_K'],
            FUSED_ELEMWISE=True,
        )
    else:
        # Two-kernel path (legacy): GEMM + max, then subtract + GELU
        config = configs[0]
        grid = (triton.cdiv(M, config['BLOCK_M']), triton.cdiv(N, config['BLOCK_N']))
        
        # Allocate intermediate tensor for max reduction
        temp_max = torch.full((M, N), float('-inf'), device=x.device, dtype=x.dtype)
        
        # Launch GEMM + max kernel
        fused_gemm_max_sub_gelu_kernel[grid](
            x, weight_t, temp_max, output,
            M, N, K,
            x.stride(0), x.stride(1),
            weight_t.stride(0), weight_t.stride(1),
            temp_max.stride(0), temp_max.stride(1),
            output.stride(0), output.stride(1),
            max_dim,
            BLOCK_M=config['BLOCK_M'],
            BLOCK_N=config['BLOCK_N'],
            BLOCK_K=config['BLOCK_K'],
            FUSED_ELEMWISE=False,
        )
        
        # Launch elementwise kernel with SAME grid layout
        grid_elem = (triton.cdiv(M, config['BLOCK_M']), triton.cdiv(N, config['BLOCK_N']))
        fast_elementwise_kernel[grid_elem](
            temp_max, output,
            M, N,
            temp_max.stride(0), temp_max.stride(1),
            output.stride(0), output.stride(1),
            max_dim,
            BLOCK_M=config['BLOCK_M'],
            BLOCK_N=config['BLOCK_N'],
        )
    
    # Final reduction if needed
    if max_dim == 1:
        output = output.max(dim=1, keepdim=True)[0]
    else:  # max_dim == 0
        output = output.max(dim=0, keepdim=True)[0]
    
    return output

class ModelNew(nn.Module):
    """
    Model that performs a GEMM, followed by a max operation, 
    subtraction, and GELU activation.
    Optimized for Ada Lovelace architecture.
    """
    def __init__(self, in_features, out_features, max_dim):
        super(ModelNew, self).__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.max_dim = max_dim
        
    def forward(self, x):
        """
        Args:
            x: Input tensor of shape (batch_size, in_features)
            
        Returns:
            Output tensor after GEMM + max + subtract + GELU
        """
        return fused_gemm_max_sub_gelu(x, self.weight, self.max_dim, use_fused=True)
```