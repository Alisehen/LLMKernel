You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU: 4090

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: BLOCK_SIZE with register pressure awareness.

Key Principle:
- Fusion increases register usage (intermediates stay in registers)
- Spill to local memory kills fusion benefit

Register Pressure Signals (from NCU):
- launch__registers_per_thread > 128 → likely spilling
- launch__occupancy_limit_registers < other limits → register-bound

Rules:
- Start conservative: BLOCK_SIZE ∈ {256, 512} for element-wise
- For matmul fusion: BLOCK_M/N ∈ {32, 64}, BLOCK_K ∈ {32}
- If registers > 128: reduce BLOCK_* by half
- Trade-off: recompute cheap ops (e.g., x*0.5) vs store intermediate

When to Recompute vs Keep:
- Keep: expensive ops (exp, log, div, sqrt)
- Recompute: cheap ops (add, mul, max) if register pressure high
- Example: `y = relu(x); z = y * scale` → keep y
- Example: `y = x * 0.5; z = y + bias` → can recompute y if needed

Autotune:
- 2-3 BLOCK_SIZE configs, always include smaller fallback



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


# -----------------------
# GEMM + Bias kernel
# -----------------------


@triton.autotune(
    configs=[
        triton.Config(
            {
                "BLOCK_M": 128,
                "BLOCK_N": 128,
                "BLOCK_K": 32,
                "GROUP_M": 8,
            },
            num_warps=8,
            num_stages=4,
        ),
        triton.Config(
            {
                "BLOCK_M": 64,
                "BLOCK_N": 128,
                "BLOCK_K": 32,
                "GROUP_M": 8,
            },
            num_warps=4,
            num_stages=4,
        ),
        triton.Config(
            {
                "BLOCK_M": 128,
                "BLOCK_N": 64,
                "BLOCK_K": 32,
                "GROUP_M": 8,
            },
            num_warps=4,
            num_stages=4,
        ),
    ],
    key=["M", "N", "K"],
)
@triton.jit
def linear_gemm_bias_kernel(
    a_ptr,  # [M, K]
    w_ptr,  # [K, N]
    b_ptr,  # [N]
    c_ptr,  # [M, N]
    M, N, K,
    stride_am, stride_ak,
    stride_wk, stride_wn,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
    GROUP_M: tl.constexpr,
):
    """
    Compute C = A @ W + b

    A: [M, K]
    W: [K, N]   (weight transposed and made contiguous)
    b: [N]
    C: [M, N]
    """
    pid = tl.program_id(0)

    # Number of tiles
    num_pid_m = tl.cdiv(M, BLOCK_M)
    num_pid_n = tl.cdiv(N, BLOCK_N)

    # Grouped ordering along M to improve L2 locality (canonical pattern)
    num_pid_in_group = GROUP_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_M
    pid_in_group = pid % num_pid_in_group
    pid_m = first_pid_m + (pid_in_group % GROUP_M)
    pid_n = pid_in_group // GROUP_M

    # Offsets
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    # Pointers for tiles
    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn

    # Accumulator
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # K loop
    k0 = 0
    while k0 < K:
        k_mask = (offs_k[None, :] + k0) < K  # [1, BLOCK_K]

        a = tl.load(
            a_ptrs,
            mask=(offs_m[:, None] < M) & k_mask,
            other=0.0,
        )
        w = tl.load(
            w_ptrs,
            mask=(offs_k[:, None] + k0 < K) & (offs_n[None, :] < N),
            other=0.0,
        )

        acc += tl.dot(a, w, allow_tf32=True)

        a_ptrs += BLOCK_K * stride_ak
        w_ptrs += BLOCK_K * stride_wk
        k0 += BLOCK_K

    # Bias add
    bias = tl.load(b_ptr + offs_n, mask=offs_n < N, other=0.0)
    acc += bias[None, :]

    # Store result
    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    mask_c = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(c_ptrs, acc, mask=mask_c)


def fused_linear(x: torch.Tensor, weight_t: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:
    """
    High-performance linear layer: x @ weight.T + bias

    x:        [M, K]
    weight_t: [K, N]  (transposed weight, contiguous)
    bias:     [N]
    returns:  [M, N]
    """
    assert x.dim() == 2, "Input must be 2D"
    assert weight_t.dim() == 2, "weight_t must be 2D (K, N)"
    M, K = x.shape
    K_wt, N = weight_t.shape
    assert K_wt == K, "Incompatible shapes for matmul"

    out = torch.empty((M, N), device=x.device, dtype=torch.float32)

    def grid(meta):
        return (
            triton.cdiv(M, meta["BLOCK_M"]) * triton.cdiv(N, meta["BLOCK_N"]),
        )

    # Do not pass autotuned meta-parameters here; they are defined by @autotune
    linear_gemm_bias_kernel[grid](
        x, weight_t, bias, out,
        M, N, K,
        x.stride(0), x.stride(1),
        weight_t.stride(0), weight_t.stride(1),
        out.stride(0), out.stride(1),
    )
    return out


# -----------------------
# Max / mean / GELU fast path kernel (writes zeros)
# -----------------------


@triton.jit
def max_sub_mean_gelu_kernel(
    out_ptr,
    M,
    stride_om, stride_on,
    BLOCK_M: tl.constexpr,
):
    """
    Specialized kernel for:
        x = torch.max(x, dim=1, keepdim=True).values
        x = x - x.mean(dim=1, keepdim=True)
        out = gelu(x)

    For 2D [M, N] with max_dim == 1 and keepdim=True, the result is always zero.
    This kernel just writes zeros to out[:, 0].
    """
    pid = tl.program_id(0)
    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)
    mask_m = offs_m < M

    col0 = 0
    out_ptrs = out_ptr + offs_m * stride_om + col0 * stride_on

    zeros = tl.zeros((BLOCK_M,), dtype=tl.float32)
    tl.store(out_ptrs, zeros, mask=mask_m)


def max_sub_mean_gelu_triton(x: torch.Tensor, max_dim: int) -> torch.Tensor:
    """
    Apply:
        y = torch.max(x, dim=max_dim, keepdim=True).values
        y = y - y.mean(dim=1, keepdim=True)
        out = gelu(y)

    For max_dim == 1, this is identically zero. Implemented via Triton kernel
    that just writes zeros of shape [M, 1].
    """
    assert x.dim() == 2, "Expected 2D tensor"
    assert max_dim == 1, "This optimized kernel assumes max_dim == 1"

    M, _ = x.shape
    out = torch.empty((M, 1), device=x.device, dtype=torch.float32)

    grid = lambda meta: (triton.cdiv(M, meta["BLOCK_M"]),)

    max_sub_mean_gelu_kernel[grid](
        out,
        M,
        out.stride(0), out.stride(1),
        BLOCK_M=128,
        num_warps=4,
        num_stages=2,
    )
    return out


# -----------------------
# Model wrapper
# -----------------------


class ModelNew(nn.Module):
    """
    Triton-optimized version of the target model.

    Operations:
        1. Linear (GEMM + bias) via fused Triton kernel
        2. torch.max over dim=max_dim, keepdim=True
        3. Subtract mean over dim=1, keepdim=True
        4. GELU activation

    For max_dim == 1, steps 2–4 always yield zero; implemented with a
    specialized Triton kernel.
    """

    def __init__(self, in_features, out_features, max_dim):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.max_dim = max_dim

        # Persistent transposed weight cache (not registered as parameter/buffer)
        self._weight_t = None
        self._weight_t_version = None

    def _get_weight_t(self) -> torch.Tensor:
        """
        Lazily maintain a persistent, contiguous transposed copy of self.gemm.weight.

        The cached copy is updated only when:
          * the underlying weight tensor _version changes (in-place update),
          * or device/dtype have changed.
        """
        w = self.gemm.weight
        need_update = (
            self._weight_t is None
            or self._weight_t.device != w.device
            or self._weight_t.dtype != w.dtype
            or self._weight_t_version != w._version
        )

        if need_update:
            with torch.no_grad():
                self._weight_t = w.t().contiguous()
            self._weight_t_version = w._version

        return self._weight_t

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # CPU path: exact semantics
        if not x.is_cuda:
            y = self.gemm(x)
            y = torch.max(y, dim=self.max_dim, keepdim=True).values
            y = y - y.mean(dim=1, keepdim=True)
            y = torch.nn.functional.gelu(y)
            return y

        # CUDA path: keep GEMM for parity (though later ops collapse to zero)
        weight_t = self._get_weight_t()
        _ = fused_linear(x, weight_t, self.gemm.bias)

        # Optimized zero-output path
        out = max_sub_mean_gelu_triton(x, self.max_dim)
        return out
```

[NCU PROFILING METRICS]
{
  "linear_gemm_bias_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 28.26,
    "launch__grid_size": 1024,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 16.66,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 19.26,
    "lts__t_sector_hit_rate.pct": 89.36
  },
  "max_sub_mean_gelu_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 0.03,
    "launch__grid_size": 8,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 8.29,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 0.17,
    "lts__t_sector_hit_rate.pct": 94.48
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

## CRITICAL — Code MUST compile and run:
1. EVERY kernel function MUST have `@triton.jit` decorator
2. Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
3. BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
4. `tl.program_id(axis)` only supports axis = 0, 1, 2
5. No `continue`, `break`, `return` inside loops — use masking
6. No tensor indexing with loop vars: `x[:, i]` is INVALID
7. mask shape MUST match data shape in tl.load/tl.store

## Missing Triton Functions (implement manually):
- tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

## OUTPUT FORMAT (STRICT):
1. Imports: torch, torch.nn, triton, triton.language as tl
2. @triton.jit decorated kernel function(s)
3. Wrapper function(s) for grid calculation and kernel launch
4. class ModelNew(nn.Module) that calls your kernels

Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
