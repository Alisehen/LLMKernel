You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Fine-tuning fused kernel parameters.

Params:
- num_warps ∈ {4, 8}
- num_stages ∈ {2, 3}

Conditional Rules (NOT one-size-fits-all):

IF register pressure LOW (regs < 96, no spill):
  - Try num_warps=8 for compute-bound fusion
  - num_stages=3 may help hide latency

IF register pressure HIGH (regs > 128 or occupancy_limit_registers):
  - Use num_warps=4 (fewer warps = more registers per warp)
  - Keep num_stages=2 (higher stages need more registers)

IF multi-input fusion (3+ distinct loads):
  - num_stages=2 preferred (each stage buffers all inputs)
  - num_warps=4 often better than 8

Autotune:
- Max 2-3 configs to reduce compilation time
- Always include conservative baseline (num_warps=4, num_stages=2)
- Test before/after: revert if gain < 2%



[CURRENT CODE]
```python
# <optimized Triton code>
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_L": 64}, num_warps=2, num_stages=2),
        triton.Config({"BLOCK_L": 128}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_L": 256}, num_warps=4, num_stages=3),
        triton.Config({"BLOCK_L": 512}, num_warps=8, num_stages=3),
        triton.Config({"BLOCK_L": 1024}, num_warps=8, num_stages=4),
    ],
    key=["L"],
)
@triton.jit
def clamp_softmax_scale_kernel(
    x_ptr,          # *const T, [B, C, L]
    scale_ptr,      # *const T, [C]
    out_ptr,        # *T,       [B, C, L]
    stride_xb, stride_xc, stride_xl,
    stride_ob, stride_oc, stride_ol,
    L,              # int32
    clamp_min,      # scalar
    clamp_max,      # scalar
    OUTPUT_DTYPE: tl.constexpr,
    BLOCK_L: tl.constexpr,
):
    """
    Fused kernel over x of shape [B, C, L]:

      1) Clamp: y = clamp(x, clamp_min, clamp_max)
      2) Spatial softmax over dimension L (per (b, c) row) with numerically
         stable running-max + running-sum reduction.
      3) Multiply by per-channel scale: scale[c]

    Memory pattern:
      - Each element of x is loaded twice (first + second pass).
      - No intermediate stores; only a single store to out_ptr.
    """

    # 2D launch: one program per (b, c) row
    pid_b = tl.program_id(0)
    pid_c = tl.program_id(1)

    # Row base pointers
    row_x_ptr = x_ptr + pid_b * stride_xb + pid_c * stride_xc
    row_out_ptr = out_ptr + pid_b * stride_ob + pid_c * stride_oc

    # Per-channel scale (kept in fp32 for compute)
    scale_val = tl.load(scale_ptr + pid_c)
    scale_val = scale_val.to(tl.float32)

    clamp_min_f32 = clamp_min
    clamp_max_f32 = clamp_max

    # -------------------------------------------------------------------------
    # First pass: numerically-stable running computation of max and sum(exp).
    # -------------------------------------------------------------------------
    running_max = tl.full((), -float("inf"), tl.float32)
    running_sum = tl.zeros((), dtype=tl.float32)

    for start in range(0, L, BLOCK_L):
        offs_l = start + tl.arange(0, BLOCK_L)
        mask = offs_l < L

        # Load, convert, clamp
        x = tl.load(row_x_ptr + offs_l * stride_xl, mask=mask, other=0.0)
        x = x.to(tl.float32)
        x = tl.maximum(x, clamp_min_f32)
        x = tl.minimum(x, clamp_max_f32)

        # Ensure out-of-bounds lanes don't affect max/sum
        x = tl.where(mask, x, -float("inf"))

        # Block max
        block_max = tl.max(x, axis=0)
        new_max = tl.maximum(running_max, block_max)

        # Rescale running_sum to new_max
        scale_old = tl.exp(running_max - new_max)
        running_max = new_max

        # Sum of exp(x - running_max) for this block
        exp_x = tl.exp(x - running_max)
        block_sum = tl.sum(exp_x, axis=0)

        running_sum = running_sum * scale_old + block_sum

    inv_sum = 1.0 / running_sum
    norm_scale = inv_sum * scale_val

    # -------------------------------------------------------------------------
    # Second pass: compute normalized softmax * scale and store.
    # -------------------------------------------------------------------------
    for start in range(0, L, BLOCK_L):
        offs_l = start + tl.arange(0, BLOCK_L)
        mask = offs_l < L

        x = tl.load(row_x_ptr + offs_l * stride_xl, mask=mask, other=0.0)
        x = x.to(tl.float32)
        x = tl.maximum(x, clamp_min_f32)
        x = tl.minimum(x, clamp_max_f32)

        logits = x - running_max
        probs = tl.exp(logits) * norm_scale

        out = probs.to(OUTPUT_DTYPE)
        tl.store(row_out_ptr + offs_l * stride_ol, out, mask=mask)


def fused_clamp_softmax_scale(x, scale, clamp_min, clamp_max):
    """
    x:     [B, C, D, H, W]  (fp16 / bf16 / fp32, assumed contiguous)
    scale: [1, C, 1, 1, 1] or [C]
    Returns:
        y: [B, C, D, H, W]
        with y = softmax(clamp(x)) over spatial dims (D*H*W), then * scale (per-channel).
    """
    assert x.is_cuda, "Input must be on CUDA for Triton kernel"

    B, C, D, H, W = x.shape
    L = D * H * W

    # Flatten spatial dimensions: [B, C, L]
    x_flat = x.view(B, C, L)
    out_flat = torch.empty_like(x_flat)

    # Per-channel scale as 1D: [C]
    scale_1d = scale.view(-1)

    stride_xb, stride_xc, stride_xl = x_flat.stride()
    stride_ob, stride_oc, stride_ol = out_flat.stride()

    # 2D grid: one program per (b, c) row
    grid = (B, C)

    # Map torch dtype to Triton dtype (for output cast)
    if x.dtype == torch.float16:
        output_dtype = tl.float16
    elif x.dtype == torch.bfloat16:
        output_dtype = tl.bfloat16
    elif x.dtype == torch.float32:
        output_dtype = tl.float32
    else:
        raise NotImplementedError(f"Unsupported dtype for Triton kernel: {x.dtype}")

    # Pass clamp bounds as host scalars (not device tensors)
    clamp_min_val = float(clamp_min)
    clamp_max_val = float(clamp_max)

    clamp_softmax_scale_kernel[grid](
        x_flat,
        scale_1d,
        out_flat,
        stride_xb,
        stride_xc,
        stride_xl,
        stride_ob,
        stride_oc,
        stride_ol,
        L,
        clamp_min_val,
        clamp_max_val,
        OUTPUT_DTYPE=output_dtype,
    )

    return out_flat.view(B, C, D, H, W)


class ModelNew(nn.Module):
    """
    Model that performs:
      AvgPool3d (PyTorch) ->
      ConvTranspose3d (PyTorch) ->
      Clamp + Spatial Softmax + Per-channel Scale (Triton fused)
    """
    def __init__(
        self,
        in_channels,
        out_channels,
        kernel_size,
        stride,
        padding,
        output_padding,
        pool_kernel_size,
        clamp_min,
        clamp_max,
    ):
        super(ModelNew, self).__init__()
        self.avg_pool = nn.AvgPool3d(pool_kernel_size)
        # Keep ConvTranspose3d as PyTorch native
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
        )
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max
        # Per-channel learnable scale
        self.scale = nn.Parameter(torch.ones(1, out_channels, 1, 1, 1))

    def forward(self, x):
        """
        x: [B, in_channels, D, H, W]
        returns: [B, out_channels, D_out, H_out, W_out]
        """
        x = self.avg_pool(x)
        x = self.conv_transpose(x)
        x = fused_clamp_softmax_scale(x, self.scale, self.clamp_min, self.clamp_max)
        return x
```

[NCU PROFILING METRICS]
{
  "clamp_softmax_scale_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 34.23,
    "launch__grid_size": 2048,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 79.82,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 75.34,
    "lts__t_sector_hit_rate.pct": 33.34
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

TRITON API CONSTRAINTS (CRITICAL):
- Triton has NO: tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
