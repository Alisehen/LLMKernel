You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Fine-tuning fused kernel parameters.

Params:
- num_warps ∈ {4, 8}
- num_stages ∈ {2, 3}

Conditional Rules (NOT one-size-fits-all):

IF register pressure LOW (regs < 96, no spill):
  - Try num_warps=8 for compute-bound fusion
  - num_stages=3 may help hide latency

IF register pressure HIGH (regs > 128 or occupancy_limit_registers):
  - Use num_warps=4 (fewer warps = more registers per warp)
  - Keep num_stages=2 (higher stages need more registers)

IF multi-input fusion (3+ distinct loads):
  - num_stages=2 preferred (each stage buffers all inputs)
  - num_warps=4 often better than 8

Autotune:
- Max 2-3 configs to reduce compilation time
- Always include conservative baseline (num_warps=4, num_stages=2)
- Test before/after: revert if gain < 2%



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl
import math


@triton.autotune(
    configs=[
        # High-occupancy, low-register tiles (good generic choice)
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 64}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 16}, num_warps=4, num_stages=2),
        # Higher ILP variants for larger channel counts
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64}, num_warps=8, num_stages=2),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32}, num_warps=8, num_stages=2),
        # Conservative fallback (very low register pressure)
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 32}, num_warps=4, num_stages=2),
    ],
    key=['P', 'C_out', 'Kd', 'Kh', 'Kw'],
)
@triton.jit
def conv3d_ncdhw_kernel(
    x_ptr, w_ptr, b_ptr, y_ptr,
    N, C_in, C_out,
    D_in, H_in, W_in,
    Kd: tl.constexpr, Kh: tl.constexpr, Kw: tl.constexpr,
    D_out, H_out, W_out,
    P,  # N * D_out * H_out * W_out
    stride_x_n, stride_x_c, stride_x_d, stride_x_h, stride_x_w,
    stride_w_co, stride_w_ci, stride_w_kd, stride_w_kh, stride_w_kw,
    stride_y_n, stride_y_c, stride_y_d, stride_y_h, stride_y_w,
    HAS_BIAS: tl.constexpr,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,
):
    # Program ids: 2D grid over (spatial+batch) x output-channels
    pid_m = tl.program_id(0)  # over P = N * D_out * H_out * W_out
    pid_n = tl.program_id(1)  # over C_out

    # Tile offsets
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)  # [BLOCK_M]
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)  # [BLOCK_N]

    mask_m = offs_m < P
    mask_n = offs_n < C_out
    out_mask = mask_m[:, None] & mask_n[None, :]

    # Decode flattened spatial index: offs_m -> (n, od, oh, ow)
    DHW = D_out * H_out * W_out
    HW = H_out * W_out

    n_idx = offs_m // DHW
    rem = offs_m % DHW
    od_idx = rem // HW
    rem = rem % HW
    oh_idx = rem // W_out
    ow_idx = rem % W_out

    # Base input pointer for each (n, od, oh, ow)
    # These correspond to the top-left/front corner of the receptive field
    x_base = (
        x_ptr
        + n_idx * stride_x_n
        + od_idx * stride_x_d
        + oh_idx * stride_x_h
        + ow_idx * stride_x_w
    )  # [BLOCK_M]

    # Precompute per-output-channel offsets in weight tensor
    w_co_offs = offs_n * stride_w_co  # [BLOCK_N]

    # Accumulator: everything stays in registers until the final store
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Main reduction loop: iterate over input channels and kernel volume
    # All intermediate results remain in registers; no intermediate tl.store()
    ci = 0
    while ci < C_in:
        ci_x_off = ci * stride_x_c
        ci_w_off = ci * stride_w_ci

        # Base pointers for this input channel tile
        x_ci_base = x_base + ci_x_off                  # [BLOCK_M]
        w_ci_base = w_ptr + ci_w_off + w_co_offs[None, :]  # [1, BLOCK_N]

        # Static unrolling over kernel spatial dimensions for better ILP
        for kd in tl.static_range(0, Kd):
            x_kd_base = x_ci_base + kd * stride_x_d      # [BLOCK_M]
            w_kd_base = w_ci_base + kd * stride_w_kd     # [1, BLOCK_N]

            for kh in tl.static_range(0, Kh):
                x_kh_base = x_kd_base + kh * stride_x_h  # [BLOCK_M]
                w_kh_base = w_kd_base + kh * stride_w_kh # [1, BLOCK_N]

                for kw in tl.static_range(0, Kw):
                    x_ptrs = x_kh_base + kw * stride_x_w            # [BLOCK_M]
                    w_ptrs = w_kh_base + kw * stride_w_kw           # [1, BLOCK_N]

                    # Load input and weight tiles; multiple input loads are OK
                    x_vals = tl.load(x_ptrs, mask=mask_m, other=0.0).to(tl.float32)  # [BLOCK_M]
                    w_vals = tl.load(w_ptrs, mask=mask_n[None, :], other=0.0).to(tl.float32)  # [1, BLOCK_N]

                    # Rank-1 update: outer product in registers
                    acc += x_vals[:, None] * w_vals  # [BLOCK_M, BLOCK_N]

        ci += 1

    # Bias add in registers
    if HAS_BIAS:
        b_vals = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0).to(tl.float32)  # [BLOCK_N]
        acc += b_vals[None, :]

    # Compute output pointers and store final result
    y_ptrs = (
        y_ptr
        + n_idx[:, None] * stride_y_n
        + offs_n[None, :] * stride_y_c
        + od_idx[:, None] * stride_y_d
        + oh_idx[:, None] * stride_y_h
        + ow_idx[:, None] * stride_y_w
    )
    # Single tl.store for the final output tensor; no intermediate stores
    tl.store(y_ptrs, acc, mask=out_mask)


def conv3d_triton_ncdhw(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor):
    """
    3D convolution (N, C, D, H, W) * (C_out, C_in, Kd, Kh, Kw) -> (N, C_out, D_out, H_out, W_out)
    NCDHW layout, stride=1, padding=0, dilation=1.
    Accumulates in fp32; all intermediate results stay in registers.
    """
    assert x.dim() == 5, "Input must be NCDHW"
    assert weight.dim() == 5, "Weight must be (C_out, C_in, Kd, Kh, Kw)"
    assert x.is_cuda and weight.is_cuda, "Tensors must be on CUDA device"

    N, C_in, D_in, H_in, W_in = x.shape
    C_out, C_in_w, Kd, Kh, Kw = weight.shape
    assert C_in == C_in_w, "In-channel mismatch between input and weight"

    # Output dimensions for valid convolution (no padding, stride=1)
    D_out = D_in - Kd + 1
    H_out = H_in - Kh + 1
    W_out = W_in - Kw + 1
    assert D_out > 0 and H_out > 0 and W_out > 0, "Kernel larger than input"

    # Output tensor (fp32 for numerical stability)
    y = torch.empty(
        (N, C_out, D_out, H_out, W_out),
        device=x.device,
        dtype=torch.float32,
    )

    # Flatten spatial+batch into a single dimension P
    P = N * D_out * H_out * W_out

    # Strides
    stride_x_n, stride_x_c, stride_x_d, stride_x_h, stride_x_w = x.stride()
    stride_w_co, stride_w_ci, stride_w_kd, stride_w_kh, stride_w_kw = weight.stride()
    stride_y_n, stride_y_c, stride_y_d, stride_y_h, stride_y_w = y.stride()

    # 2D launch grid: tiles over spatial+batch (P) and output channels (C_out)
    def grid(meta):
        return (
            triton.cdiv(P, meta['BLOCK_M']),
            triton.cdiv(C_out, meta['BLOCK_N']),
        )

    conv3d_ncdhw_kernel[grid](
        x, weight, bias, y,
        N, C_in, C_out,
        D_in, H_in, W_in,
        Kd, Kh, Kw,
        D_out, H_out, W_out,
        P,
        stride_x_n, stride_x_c, stride_x_d, stride_x_h, stride_x_w,
        stride_w_co, stride_w_ci, stride_w_kd, stride_w_kh, stride_w_kw,
        stride_y_n, stride_y_c, stride_y_d, stride_y_h, stride_y_w,
        True,  # HAS_BIAS
    )

    return y


class ModelNew(nn.Module):
    """
    Triton-accelerated version of:
      Conv3d -> Softmax (dim=1) -> MaxPool3d -> MaxPool3d

    Conv3d is implemented in Triton with aggressive fusion of
    convolution and bias into a single kernel, using only a single
    global store for the final output. Softmax and pooling are
    dispatched to PyTorch.
    """

    def __init__(self, in_channels, out_channels, kernel_size, pool_kernel_size):
        super(ModelNew, self).__init__()

        # Normalize kernel_size to 3D (int -> (k,k,k))
        if isinstance(kernel_size, int):
            kd = kh = kw = kernel_size
        else:
            kd, kh, kw = kernel_size
        self.kernel_size = (kd, kh, kw)

        # Parameters for Conv3d: (C_out, C_in, Kd, Kh, Kw)
        self.weight = nn.Parameter(
            torch.empty(out_channels, in_channels, kd, kh, kw)
        )
        self.bias = nn.Parameter(torch.empty(out_channels))

        # Initialization similar to nn.Conv3d
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        fan_in = in_channels * kd * kh * kw
        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
        nn.init.uniform_(self.bias, -bound, bound)

        # Max pooling layers
        self.pool1 = nn.MaxPool3d(pool_kernel_size)
        self.pool2 = nn.MaxPool3d(pool_kernel_size)

    def forward(self, x):
        # x: (N, C_in, D, H, W)
        # Conv3d via Triton (single kernel, bias fused, no intermediate stores)
        y = conv3d_triton_ncdhw(x, self.weight, self.bias)

        # Softmax over channels (dim=1), then two max-pool layers
        y = torch.softmax(y, dim=1)
        y = self.pool1(y)
        y = self.pool2(y)
        return y
```

[NCU PROFILING METRICS]
{
  "conv3d_ncdhw_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 67.77,
    "launch__grid_size": 50400,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 24.69,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 6.29,
    "lts__t_sector_hit_rate.pct": 92.19
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

TRITON API CONSTRAINTS (CRITICAL):
- Triton has NO: tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
