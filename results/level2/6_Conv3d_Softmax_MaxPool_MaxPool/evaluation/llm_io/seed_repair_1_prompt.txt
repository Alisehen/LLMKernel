Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 527, in compare_and_bench
    test_model = ModelNew(*init_args, **init_kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251215_024708_batch_range6to30_openai_deepseek/6_Conv3d_Softmax_MaxPool_MaxPool/code/kernel_20251215_024756.py", line 191, in __init__
    nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
                                            ^^^^
NameError: name 'math' is not defined

Main Critical Problem Analysis:
Problem Analysis (from expert diagnosis):
critical_issue: The `math` module is used in ModelNew.__init__ but never imported.
why_it_matters: Python cannot resolve `math.sqrt(5)`, raising a NameError during ModelNew initialization before any Triton kernel runs.
minimal_fix_hint: Add a proper import for the math module at the top of the file.

Focus your fix on addressing the identified critical issue.


Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32}, num_warps=4),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64}, num_warps=4),
    ],
    key=['P', 'C_out'],
)
@triton.jit
def conv3d_ncdhw_kernel(
    x_ptr, w_ptr, b_ptr, y_ptr,
    N, C_in, C_out,
    D_in, H_in, W_in,
    Kd, Kh, Kw,
    D_out, H_out, W_out,
    P,  # N * D_out * H_out * W_out
    stride_x_n, stride_x_c, stride_x_d, stride_x_h, stride_x_w,
    stride_w_co, stride_w_ci, stride_w_kd, stride_w_kh, stride_w_kw,
    stride_y_n, stride_y_c, stride_y_d, stride_y_h, stride_y_w,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,
):
    # Program IDs: 2D grid over flattened output positions (M) and output channels (N)
    pid_m = tl.program_id(0)  # over P = N * D_out * H_out * W_out
    pid_n = tl.program_id(1)  # over C_out

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    # Masks for valid positions
    mask_m = offs_m < P
    mask_n = offs_n < C_out

    # Decode flattened index offs_m -> (n, od, oh, ow)
    DHW = D_out * H_out * W_out
    HW = H_out * W_out

    n_idx = offs_m // DHW
    rem = offs_m % DHW
    od_idx = rem // HW
    rem = rem % HW
    oh_idx = rem // W_out
    ow_idx = rem % W_out

    # Accumulator in fp32 for numerical stability
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Convolution: loop over input channels and kernel volume
    for ci in range(0, C_in):
        for kd in range(0, Kd):
            for kh in range(0, Kh):
                for kw in range(0, Kw):
                    # Input indices (within input volume, valid-range by construction)
                    d_in_vals = od_idx + kd
                    h_in_vals = oh_idx + kh
                    w_in_vals = ow_idx + kw

                    # Pointers for input: [BLOCK_M]
                    x_ptrs = (
                        x_ptr
                        + n_idx * stride_x_n
                        + ci * stride_x_c
                        + d_in_vals * stride_x_d
                        + h_in_vals * stride_x_h
                        + w_in_vals * stride_x_w
                    )
                    x_vals = tl.load(x_ptrs, mask=mask_m, other=0.0)
                    x_vals = x_vals.to(tl.float32)

                    # Pointers for weights: [BLOCK_N]
                    w_ptrs = (
                        w_ptr
                        + offs_n * stride_w_co
                        + ci * stride_w_ci
                        + kd * stride_w_kd
                        + kh * stride_w_kh
                        + kw * stride_w_kw
                    )
                    w_vals = tl.load(w_ptrs, mask=mask_n, other=0.0)
                    w_vals = w_vals.to(tl.float32)

                    # Outer product and accumulate
                    acc += x_vals[:, None] * w_vals[None, :]

    # Add bias if provided
    if b_ptr is not None:
        b_vals = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0)
        b_vals = b_vals.to(tl.float32)
        acc += b_vals[None, :]

    # Store result to output tensor
    y_ptrs = (
        y_ptr
        + n_idx[:, None] * stride_y_n
        + offs_n[None, :] * stride_y_c
        + od_idx[:, None] * stride_y_d
        + oh_idx[:, None] * stride_y_h
        + ow_idx[:, None] * stride_y_w
    )
    mask = mask_m[:, None] & mask_n[None, :]
    tl.store(y_ptrs, acc.to(tl.float32), mask=mask)


def conv3d_triton_ncdhw(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor):
    """
    3D convolution (N, C, D, H, W) * (C_out, C_in, Kd, Kh, Kw) -> (N, C_out, D_out, H_out, W_out)
    Implemented in Triton for stride=1, padding=0, dilation=1.
    """
    assert x.dim() == 5, "Input must be NCDHW"
    assert weight.dim() == 5, "Weight must be (C_out, C_in, Kd, Kh, Kw)"
    assert x.is_cuda and weight.is_cuda, "Tensors must be on CUDA device"

    N, C_in, D_in, H_in, W_in = x.shape
    C_out, C_in_w, Kd, Kh, Kw = weight.shape
    assert C_in == C_in_w, "In-channel mismatch between input and weight"

    # Output dimensions (valid convolution, no padding, stride=1)
    D_out = D_in - Kd + 1
    H_out = H_in - Kh + 1
    W_out = W_in - Kw + 1
    assert D_out > 0 and H_out > 0 and W_out > 0, "Kernel larger than input"

    y = torch.empty(
        (N, C_out, D_out, H_out, W_out),
        device=x.device,
        dtype=torch.float32,  # accumulator/output in fp32
    )

    # Flattened number of output positions
    P = N * D_out * H_out * W_out

    # Strides
    stride_x_n, stride_x_c, stride_x_d, stride_x_h, stride_x_w = x.stride()
    stride_w_co, stride_w_ci, stride_w_kd, stride_w_kh, stride_w_kw = weight.stride()
    stride_y_n, stride_y_c, stride_y_d, stride_y_h, stride_y_w = y.stride()

    # Grid over flattened spatial+batch (P) and output channels (C_out)
    def grid(meta):
        return (
            triton.cdiv(P, meta['BLOCK_M']),
            triton.cdiv(C_out, meta['BLOCK_N']),
        )

    # Bias pointer can be None in Python, but Triton expects a pointer,
    # so we handle None by passing a dummy and not using it in that case.
    b_ptr = bias if bias is not None else None

    conv3d_ncdhw_kernel[grid](
        x, weight, b_ptr, y,
        N, C_in, C_out,
        D_in, H_in, W_in,
        Kd, Kh, Kw,
        D_out, H_out, W_out,
        P,
        stride_x_n, stride_x_c, stride_x_d, stride_x_h, stride_x_w,
        stride_w_co, stride_w_ci, stride_w_kd, stride_w_kh, stride_w_kw,
        stride_y_n, stride_y_c, stride_y_d, stride_y_h, stride_y_w,
    )

    return y


class ModelNew(nn.Module):
    """
    Triton-accelerated version of:
      Conv3d -> Softmax (dim=1) -> MaxPool3d -> MaxPool3d
    Conv3d is implemented in Triton; softmax and pooling use PyTorch.
    """

    def __init__(self, in_channels, out_channels, kernel_size, pool_kernel_size):
        super(ModelNew, self).__init__()

        # Normalize kernel_size to 3D (int -> (k,k,k))
        if isinstance(kernel_size, int):
            kd = kh = kw = kernel_size
        else:
            kd, kh, kw = kernel_size
        self.kernel_size = (kd, kh, kw)

        # Parameters for Conv3d: (C_out, C_in, Kd, Kh, Kw)
        self.weight = nn.Parameter(
            torch.empty(out_channels, in_channels, kd, kh, kw)
        )
        self.bias = nn.Parameter(torch.empty(out_channels))

        # Use a similar initialization to nn.Conv3d
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        fan_in = in_channels * kd * kh * kw
        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
        nn.init.uniform_(self.bias, -bound, bound)

        # Max pooling layers (same as original)
        self.pool1 = nn.MaxPool3d(pool_kernel_size)
        self.pool2 = nn.MaxPool3d(pool_kernel_size)

    def forward(self, x):
        # x: (N, C_in, D, H, W)
        # Conv3d via Triton
        y = conv3d_triton_ncdhw(x, self.weight, self.bias)

        # Softmax over channels (dim=1), then two max-pool layers
        y = torch.softmax(y, dim=1)
        y = self.pool1(y)
        y = self.pool2(y)
        return y
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl, AND any other modules used (e.g., import math if using math.sqrt)
   2. @triton.jit decorated kernel function(s) — NO continue/break/return inside loops (use masking)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels — THIS CLASS IS REQUIRED
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes
4. Ensure ALL imports are included at the top (common mistake: forgetting `import math`)

```python
# <corrected code>
```
