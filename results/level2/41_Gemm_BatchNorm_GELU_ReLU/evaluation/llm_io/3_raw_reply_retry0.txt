```python
# <corrected code>

import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        # Main high-performance tile
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 32, "GROUP_M": 8},
            num_warps=4,
            num_stages=2,
        ),
        # Asymmetric tiles for tall / wide shapes
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 64, "BLOCK_K": 32, "GROUP_M": 8},
            num_warps=2,
            num_stages=2,
        ),
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 32, "BLOCK_K": 32, "GROUP_M": 8},
            num_warps=2,
            num_stages=2,
        ),
        # Smallest fallback to keep register pressure low on odd shapes
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 32, "BLOCK_K": 32, "GROUP_M": 8},
            num_warps=2,
            num_stages=2,
        ),
    ],
    key=["M", "N", "K"],
)
@triton.jit
def fused_gemm_bn_gelu_relu_kernel(
    a_ptr,            # *f32, (M, K)
    b_ptr,            # *f32, (K, N) = weight^T
    bias_lin_ptr,     # *f32, (N,)
    bn_scale_ptr,     # *f32, (N,)  == inv_std * gamma
    bn_shift_ptr,     # *f32, (N,)  == beta - mean * inv_std * gamma
    c_ptr,            # *f32, (M, N)

    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,

    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
    GROUP_M: tl.constexpr,
):
    # 1D launch grid, mapped to 2D tiles with M-grouping for better L2 reuse of B
    pid = tl.program_id(0)

    num_pid_m = tl.cdiv(M, BLOCK_M)
    num_pid_n = tl.cdiv(N, BLOCK_N)

    # Group program IDs along M to increase B tile reuse in L2
    num_pid_in_group = GROUP_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_M

    group_size_m = tl.minimum(num_pid_m - first_pid_m, GROUP_M)
    pid_m = first_pid_m + (pid % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m

    # Offsets within output tile
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    # Masks corresponding to output tile bounds
    mask_m = offs_m < M
    mask_n = offs_n < N

    # Alignment hints for better vectorization
    tl.multiple_of(offs_m, BLOCK_M)
    tl.multiple_of(offs_n, BLOCK_N)

    # GEMM accumulator in fp32
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # K loop
    offs_k = tl.arange(0, BLOCK_K)
    tl.multiple_of(offs_k, BLOCK_K)

    k = 0
    while k < K:
        k_offsets = k + offs_k
        k_mask = k_offsets < K

        a_ptrs = (
            a_ptr
            + offs_m[:, None] * stride_am
            + k_offsets[None, :] * stride_ak
        )
        b_ptrs = (
            b_ptr
            + k_offsets[:, None] * stride_bk
            + offs_n[None, :] * stride_bn
        )

        a = tl.load(
            a_ptrs,
            mask=mask_m[:, None] & k_mask[None, :],
            other=0.0,
        )
        b = tl.load(
            b_ptrs,
            mask=k_mask[:, None] & mask_n[None, :],
            other=0.0,
        )

        # Tensor-core friendly dot-product (TF32 on fp32 inputs)
        acc += tl.dot(a, b, allow_tf32=True)

        k += BLOCK_K

    # ---- FUSED ELEMENTWISE BLOCK (Linear bias + BN + GELU + ReLU) ----

    # Load per-channel parameters (broadcast over M)
    bias_lin = tl.load(
        bias_lin_ptr + offs_n,
        mask=mask_n,
        other=0.0,
    )[None, :]  # (1, BLOCK_N)

    bn_scale = tl.load(
        bn_scale_ptr + offs_n,
        mask=mask_n,
        other=1.0,
    )[None, :]
    bn_shift = tl.load(
        bn_shift_ptr + offs_n,
        mask=mask_n,
        other=0.0,
    )[None, :]

    # Linear + pre-fused BatchNorm: y = (acc + bias_lin) * bn_scale + bn_shift
    y = acc + bias_lin
    y = y * bn_scale + bn_shift

    # GELU using erf formulation to match torch.nn.functional.gelu (approximate='none'):
    # gelu(x) = 0.5 * x * (1 + erf(x / sqrt(2)))
    x = y
    inv_sqrt_2 = 0.7071067811865476  # 1 / sqrt(2), constant to avoid extra math ops
    y = 0.5 * x * (1.0 + tl.math.erf(x * inv_sqrt_2))

    # ReLU
    y = tl.maximum(y, 0.0)

    # Store result
    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    out_mask = mask_m[:, None] & mask_n[None, :]
    tl.store(c_ptrs, y, mask=out_mask)


def fused_gemm_bn_gelu_relu(x: torch.Tensor, linear: nn.Linear, bn: nn.BatchNorm1d):
    """
    Fused eval-mode path:
        y = ReLU(GELU(BN(x @ W^T + b)))
    Uses a single high-performance Triton kernel on CUDA.
    Falls back to standard PyTorch ops for CPU or training mode.
    """
    assert linear.bias is not None, "Linear layer must have bias for this fused kernel."

    # Fallback to PyTorch if not CUDA
    if x.device.type != "cuda":
        y = linear(x)
        y = bn(y)
        y = torch.nn.functional.gelu(y)
        y = torch.nn.functional.relu(y)
        return y

    M, K = x.shape
    N = linear.weight.shape[0]

    # Ensure explicit, performant layouts
    x_fp32 = x.to(device=x.device, dtype=torch.float32)
    W = linear.weight.to(device=x.device, dtype=torch.float32, non_blocking=True)
    B = W.t().contiguous()  # (K, N), row-major

    # Allocate output
    y = torch.empty((M, N), device=x.device, dtype=torch.float32)

    # BN parameters (eval mode uses running stats)
    bn_weight = bn.weight.to(device=x.device, dtype=torch.float32, non_blocking=True)
    bn_bias = bn.bias.to(device=x.device, dtype=torch.float32, non_blocking=True)
    bn_mean = bn.running_mean.to(device=x.device, dtype=torch.float32, non_blocking=True)
    bn_var = bn.running_var.to(device=x.device, dtype=torch.float32, non_blocking=True)
    eps = float(bn.eps)

    # Pre-fuse BatchNorm parameters on GPU:
    # inv_std = 1 / sqrt(var + eps)
    # scale  = inv_std * gamma
    # shift  = beta - mean * scale
    inv_std = torch.rsqrt(bn_var + eps)
    bn_scale = inv_std * bn_weight
    bn_shift = bn_bias - bn_mean * bn_scale

    bn_scale = bn_scale.contiguous()
    bn_shift = bn_shift.contiguous()

    # Grid: 1D over (M, N) tiles; kernel maps to 2D tiles internally
    def grid(meta):
        return (
            triton.cdiv(M, meta["BLOCK_M"]) * triton.cdiv(N, meta["BLOCK_N"]),
        )

    fused_gemm_bn_gelu_relu_kernel[grid](
        x_fp32,
        B,
        linear.bias.to(device=x.device, dtype=torch.float32, non_blocking=True),
        bn_scale,
        bn_shift,
        y,
        M,
        N,
        K,
        x_fp32.stride(0),
        x_fp32.stride(1),
        B.stride(0),
        B.stride(1),
        y.stride(0),
        y.stride(1),
    )

    return y


class ModelNew(nn.Module):
    """
    GEMM + BatchNorm + GELU + ReLU.
    Uses fused Triton kernel in eval mode on CUDA, and the standard PyTorch
    implementation (preserving BN training semantics) otherwise.
    """

    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.batch_norm = nn.BatchNorm1d(out_features)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Training mode or non-CUDA: use full PyTorch path for correct semantics
        if self.training or x.device.type != "cuda":
            x = self.gemm(x)
            x = self.batch_norm(x)
            x = torch.nn.functional.gelu(x)
            x = torch.nn.functional.relu(x)
            return x

        # Eval-mode fast path: fused Triton kernel
        return fused_gemm_bn_gelu_relu(x, self.gemm, self.batch_norm)
```