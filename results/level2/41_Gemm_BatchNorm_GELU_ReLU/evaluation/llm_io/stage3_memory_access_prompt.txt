You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Memory pattern for fused operations.

Key Principle:
- Fusion benefit = eliminated INTERMEDIATE stores
- Multiple input loads are OK; intermediate stores are NOT

Rules:
- ✅ Multiple tl.load() for different inputs (x, weight, bias) - OK
- ❌ tl.store() for intermediate results - NEVER (this is what fusion eliminates)
- ✅ Single tl.store() for final output - required

Verification:
- Count tl.store() calls: should equal number of OUTPUT tensors (usually 1)
- Intermediate values: must stay in registers between ops
- If you see store-then-load pattern for same data → BUG, refactor

Multi-input Fusion Pattern:
```
x = tl.load(input_ptr + offs, mask=mask)
w = tl.load(weight_ptr + ..., mask=...)  # OK: different input
b = tl.load(bias_ptr + ..., mask=...)    # OK: different input
y = op1(x, w)  # in registers
z = op2(y, b)  # in registers
tl.store(out_ptr + offs, z, mask=mask)   # single output store
```

num_stages: start with 2, only increase if memory stalls high AND registers OK



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        # Larger tiles for high arithmetic intensity
        triton.Config(
            {"BLOCK_M": 128, "BLOCK_N": 128, "BLOCK_K": 32, "GROUP_M": 8},
            num_warps=8,
            num_stages=3,
        ),
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 128, "BLOCK_K": 32, "GROUP_M": 8},
            num_warps=4,
            num_stages=3,
        ),
        triton.Config(
            {"BLOCK_M": 128, "BLOCK_N": 64, "BLOCK_K": 32, "GROUP_M": 8},
            num_warps=4,
            num_stages=3,
        ),
        # Smaller tiles as a fallback for odd shapes / low K
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 32, "GROUP_M": 8},
            num_warps=4,
            num_stages=2,
        ),
    ],
    key=["M", "N", "K"],
)
@triton.jit
def fused_gemm_bn_gelu_relu_kernel(
    a_ptr,              # *f32, (M, K)
    b_ptr,              # *f32, (K, N) = weight^T
    bias_lin_ptr,       # *f32, (N,)
    bn_weight_ptr,      # *f32, (N,)
    bn_bias_ptr,        # *f32, (N,)
    bn_mean_ptr,        # *f32, (N,)
    bn_var_ptr,         # *f32, (N,)
    c_ptr,              # *f32, (M, N)

    M, N, K,
    eps,                # f32, BN epsilon

    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,

    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
    GROUP_M: tl.constexpr,
):
    # 1D launch grid, mapped to 2D tiles with M-grouping for better L2 reuse of B
    pid = tl.program_id(0)

    num_pid_m = tl.cdiv(M, BLOCK_M)
    num_pid_n = tl.cdiv(N, BLOCK_N)

    # Group program IDs along M to increase B tile reuse in L2
    num_pid_in_group = GROUP_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_M

    group_size_m = tl.minimum(num_pid_m - first_pid_m, GROUP_M)
    pid_m = first_pid_m + (pid % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m

    # Offsets within output tile
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    # Masks corresponding to output tile bounds
    mask_m = offs_m < M
    mask_n = offs_n < N

    # Provide alignment hints to the compiler
    tl.multiple_of(offs_m, BLOCK_M)
    tl.multiple_of(offs_n, BLOCK_N)

    # Accumulator for GEMM (always in fp32)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # K loop
    offs_k = tl.arange(0, BLOCK_K)
    tl.multiple_of(offs_k, BLOCK_K)

    k = 0
    while k < K:
        k_offsets = k + offs_k
        k_mask = k_offsets < K

        # Shared offsets / masks for GEMM loads
        a_ptrs = (
            a_ptr
            + offs_m[:, None] * stride_am
            + k_offsets[None, :] * stride_ak
        )
        b_ptrs = (
            b_ptr
            + k_offsets[:, None] * stride_bk
            + offs_n[None, :] * stride_bn
        )

        # All GEMM loads use masks derived from mask_m and mask_n
        a = tl.load(
            a_ptrs,
            mask=mask_m[:, None] & k_mask[None, :],
            other=0.0,
        )
        b = tl.load(
            b_ptrs,
            mask=k_mask[:, None] & mask_n[None, :],
            other=0.0,
        )

        # Tensor-core friendly dot-product (TF32 on fp32 inputs)
        acc += tl.dot(a, b, allow_tf32=True)

        k += BLOCK_K

    # ---- FUSED ELEMENTWISE BLOCK (BN + GELU + ReLU) ----
    # All fused ops share the same (offs_m, offs_n) tile and output mask

    # Load per-channel parameters (broadcast over M)
    bias_lin = tl.load(
        bias_lin_ptr + offs_n,
        mask=mask_n,
        other=0.0,
    )[None, :]  # (1, BLOCK_N)

    bn_weight = tl.load(
        bn_weight_ptr + offs_n,
        mask=mask_n,
        other=1.0,
    )[None, :]
    bn_bias = tl.load(
        bn_bias_ptr + offs_n,
        mask=mask_n,
        other=0.0,
    )[None, :]
    bn_mean = tl.load(
        bn_mean_ptr + offs_n,
        mask=mask_n,
        other=0.0,
    )[None, :]
    bn_var = tl.load(
        bn_var_ptr + offs_n,
        mask=mask_n,
        other=1.0,
    )[None, :]

    # Linear bias
    y = acc + bias_lin

    # BatchNorm (eval mode)
    inv_std = tl.rsqrt(bn_var + eps)
    y = (y - bn_mean) * inv_std
    y = y * bn_weight + bn_bias

    # GELU via erf approximation (no libdevice GELU; uses exp + poly)
    sqrt_2_inv = 0.7071067811865476
    u = y * sqrt_2_inv

    sign = tl.where(u >= 0, 1.0, -1.0)
    absu = tl.abs(u)
    t = 1.0 / (1.0 + 0.3275911 * absu)

    a1 = 0.254829592
    a2 = -0.284496736
    a3 = 1.421413741
    a4 = -1.453152027
    a5 = 1.061405429

    poly = (((((a5 * t + a4) * t + a3) * t + a2) * t + a1) * t)
    erf_u = sign * (1.0 - poly * tl.exp(-absu * absu))

    gelu = 0.5 * y * (1.0 + erf_u)

    # ReLU
    out = tl.maximum(gelu, 0.0)

    # Store result using the same (offs_m, offs_n) tile / mask
    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    out_mask = mask_m[:, None] & mask_n[None, :]
    tl.store(c_ptrs, out, mask=out_mask)


def fused_gemm_bn_gelu_relu(x: torch.Tensor, linear: nn.Linear, bn: nn.BatchNorm1d):
    """
    Fused eval-mode path:
        y = ReLU(GELU(BN(x @ W^T + b)))
    Uses a single high-performance Triton kernel on CUDA.
    Falls back to standard PyTorch ops for CPU or training mode.
    """
    assert linear.bias is not None, "Linear layer must have bias for this fused kernel."

    # Fallback to PyTorch if not CUDA
    if x.device.type != "cuda":
        y = linear(x)
        y = bn(y)
        y = torch.nn.functional.gelu(y)
        y = torch.nn.functional.relu(y)
        return y

    M, K = x.shape
    N = linear.weight.shape[0]

    # Ensure explicit, performant layouts
    x_fp32 = x.to(device=x.device, dtype=torch.float32)
    W = linear.weight.to(device=x.device, dtype=torch.float32, non_blocking=True)
    B = W.t().contiguous()  # (K, N), row-major

    # Allocate output
    y = torch.empty((M, N), device=x.device, dtype=torch.float32)

    # BN parameters (eval mode uses running stats)
    bn_weight = bn.weight.to(device=x.device, dtype=torch.float32, non_blocking=True)
    bn_bias = bn.bias.to(device=x.device, dtype=torch.float32, non_blocking=True)
    bn_mean = bn.running_mean.to(device=x.device, dtype=torch.float32, non_blocking=True)
    bn_var = bn.running_var.to(device=x.device, dtype=torch.float32, non_blocking=True)
    eps = float(bn.eps)

    # Grid: 1D over (M, N) tiles; kernel maps to 2D tiles internally
    def grid(meta):
        return (
            triton.cdiv(M, meta["BLOCK_M"]) * triton.cdiv(N, meta["BLOCK_N"]),
        )

    fused_gemm_bn_gelu_relu_kernel[grid](
        x_fp32,
        B,
        linear.bias.to(device=x.device, dtype=torch.float32, non_blocking=True),
        bn_weight,
        bn_bias,
        bn_mean,
        bn_var,
        y,
        M,
        N,
        K,
        eps,
        x_fp32.stride(0),
        x_fp32.stride(1),
        B.stride(0),
        B.stride(1),
        y.stride(0),
        y.stride(1),
    )

    return y


class ModelNew(nn.Module):
    """
    GEMM + BatchNorm + GELU + ReLU.
    Uses fused Triton kernel in eval mode on CUDA, and the standard PyTorch
    implementation (preserving BN training semantics) otherwise.
    """

    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.batch_norm = nn.BatchNorm1d(out_features)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Training mode or non-CUDA: use full PyTorch path for correct semantics
        if self.training or x.device.type != "cuda":
            x = self.gemm(x)
            x = self.batch_norm(x)
            x = torch.nn.functional.gelu(x)
            x = torch.nn.functional.relu(x)
            return x

        # Eval-mode fast path: fused Triton kernel
        return fused_gemm_bn_gelu_relu(x, self.gemm, self.batch_norm)
```

[NCU PROFILING METRICS]
{
  "fused_gemm_bn_gelu_relu_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 35.48,
    "launch__grid_size": 4096,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 16.66,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 15.45,
    "lts__t_sector_hit_rate.pct": 92.23
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

TRITON API CONSTRAINTS (CRITICAL):
- Triton has NO: tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
