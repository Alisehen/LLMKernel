Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 626, in compare_and_bench
    raise ValueError(
ValueError: Outputs are not close (atol=0.001, rtol=0.01). max_abs_err=1.115e+01, mean_abs_err=9.318e-04

Main Critical Problem Analysis:
Problem Analysis (from expert diagnosis):
critical_issue: The kernel uses a custom tanh/exp GELU approximation instead of PyTorch’s default erf-based GELU, so activations don’t numerically match the reference.
why_it_matters: This different GELU formulation changes output values enough that some elements exceed the specified atol/rtol thresholds, triggering the max_abs_err mismatch in the comparison.
minimal_fix_hint: Replace the custom tanh/exp GELU block with an erf-based GELU matching torch.nn.functional.gelu’s exact formulation.

Focus your fix on addressing the identified critical issue.


Broken Code:
```python
# <optimized Triton code>

import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        # Main high-performance tile
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 32, "GROUP_M": 8},
            num_warps=4,
            num_stages=2,
        ),
        # Asymmetric tiles for tall / wide shapes
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 64, "BLOCK_K": 32, "GROUP_M": 8},
            num_warps=2,
            num_stages=2,
        ),
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 32, "BLOCK_K": 32, "GROUP_M": 8},
            num_warps=2,
            num_stages=2,
        ),
        # Smallest fallback to keep register pressure low on odd shapes
        triton.Config(
            {"BLOCK_M": 32, "BLOCK_N": 32, "BLOCK_K": 32, "GROUP_M": 8},
            num_warps=2,
            num_stages=2,
        ),
    ],
    key=["M", "N", "K"],
)
@triton.jit
def fused_gemm_bn_gelu_relu_kernel(
    a_ptr,            # *f32, (M, K)
    b_ptr,            # *f32, (K, N) = weight^T
    bias_lin_ptr,     # *f32, (N,)
    bn_scale_ptr,     # *f32, (N,)  == inv_std * gamma
    bn_shift_ptr,     # *f32, (N,)  == beta - mean * inv_std * gamma
    c_ptr,            # *f32, (M, N)

    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,

    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
    GROUP_M: tl.constexpr,
):
    # 1D launch grid, mapped to 2D tiles with M-grouping for better L2 reuse of B
    pid = tl.program_id(0)

    num_pid_m = tl.cdiv(M, BLOCK_M)
    num_pid_n = tl.cdiv(N, BLOCK_N)

    # Group program IDs along M to increase B tile reuse in L2
    num_pid_in_group = GROUP_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_M

    group_size_m = tl.minimum(num_pid_m - first_pid_m, GROUP_M)
    pid_m = first_pid_m + (pid % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m

    # Offsets within output tile
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    # Masks corresponding to output tile bounds
    mask_m = offs_m < M
    mask_n = offs_n < N

    # Alignment hints for better vectorization
    tl.multiple_of(offs_m, BLOCK_M)
    tl.multiple_of(offs_n, BLOCK_N)

    # GEMM accumulator in fp32
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # K loop
    offs_k = tl.arange(0, BLOCK_K)
    tl.multiple_of(offs_k, BLOCK_K)

    k = 0
    while k < K:
        k_offsets = k + offs_k
        k_mask = k_offsets < K

        a_ptrs = (
            a_ptr
            + offs_m[:, None] * stride_am
            + k_offsets[None, :] * stride_ak
        )
        b_ptrs = (
            b_ptr
            + k_offsets[:, None] * stride_bk
            + offs_n[None, :] * stride_bn
        )

        a = tl.load(
            a_ptrs,
            mask=mask_m[:, None] & k_mask[None, :],
            other=0.0,
        )
        b = tl.load(
            b_ptrs,
            mask=k_mask[:, None] & mask_n[None, :],
            other=0.0,
        )

        # Tensor-core friendly dot-product (TF32 on fp32 inputs)
        acc += tl.dot(a, b, allow_tf32=True)

        k += BLOCK_K

    # ---- FUSED ELEMENTWISE BLOCK (Linear bias + BN + GELU + ReLU) ----

    # Load per-channel parameters (broadcast over M)
    bias_lin = tl.load(
        bias_lin_ptr + offs_n,
        mask=mask_n,
        other=0.0,
    )[None, :]  # (1, BLOCK_N)

    bn_scale = tl.load(
        bn_scale_ptr + offs_n,
        mask=mask_n,
        other=1.0,
    )[None, :]
    bn_shift = tl.load(
        bn_shift_ptr + offs_n,
        mask=mask_n,
        other=0.0,
    )[None, :]

    # Linear + pre-fused BatchNorm: y = (acc + bias_lin) * bn_scale + bn_shift
    y = acc + bias_lin
    y = y * bn_scale + bn_shift

    # Fast approximate GELU (tanh-based, implemented via exp to avoid tl.tanh):
    # gelu(x) ≈ 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 x^3)))
    x = y
    x2 = x * x
    inner = 0.044715 * x2 * x          # 0.044715 * x^3
    inner = (x + inner) * 0.7978845608028654  # √(2/π) * (x + 0.044715*x^3)

    # tanh(inner) via exp: tanh(z) = (e^{2z} - 1) / (e^{2z} + 1)
    t = tl.exp(2.0 * inner)
    t = (t - 1.0) / (t + 1.0)

    y = 0.5 * x * (1.0 + t)  # GELU output reuses y

    # ReLU
    y = tl.maximum(y, 0.0)

    # Store result
    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    out_mask = mask_m[:, None] & mask_n[None, :]
    tl.store(c_ptrs, y, mask=out_mask)


def fused_gemm_bn_gelu_relu(x: torch.Tensor, linear: nn.Linear, bn: nn.BatchNorm1d):
    """
    Fused eval-mode path:
        y = ReLU(GELU(BN(x @ W^T + b)))
    Uses a single high-performance Triton kernel on CUDA.
    Falls back to standard PyTorch ops for CPU or training mode.
    """
    assert linear.bias is not None, "Linear layer must have bias for this fused kernel."

    # Fallback to PyTorch if not CUDA
    if x.device.type != "cuda":
        y = linear(x)
        y = bn(y)
        y = torch.nn.functional.gelu(y)
        y = torch.nn.functional.relu(y)
        return y

    M, K = x.shape
    N = linear.weight.shape[0]

    # Ensure explicit, performant layouts
    x_fp32 = x.to(device=x.device, dtype=torch.float32)
    W = linear.weight.to(device=x.device, dtype=torch.float32, non_blocking=True)
    B = W.t().contiguous()  # (K, N), row-major

    # Allocate output
    y = torch.empty((M, N), device=x.device, dtype=torch.float32)

    # BN parameters (eval mode uses running stats)
    bn_weight = bn.weight.to(device=x.device, dtype=torch.float32, non_blocking=True)
    bn_bias = bn.bias.to(device=x.device, dtype=torch.float32, non_blocking=True)
    bn_mean = bn.running_mean.to(device=x.device, dtype=torch.float32, non_blocking=True)
    bn_var = bn.running_var.to(device=x.device, dtype=torch.float32, non_blocking=True)
    eps = float(bn.eps)

    # Pre-fuse BatchNorm parameters on GPU to reduce work & register pressure in the kernel:
    # inv_std = 1 / sqrt(var + eps)
    # scale  = inv_std * gamma
    # shift  = beta - mean * scale
    inv_std = torch.rsqrt(bn_var + eps)
    bn_scale = inv_std * bn_weight
    bn_shift = bn_bias - bn_mean * bn_scale

    bn_scale = bn_scale.contiguous()
    bn_shift = bn_shift.contiguous()

    # Grid: 1D over (M, N) tiles; kernel maps to 2D tiles internally
    def grid(meta):
        return (
            triton.cdiv(M, meta["BLOCK_M"]) * triton.cdiv(N, meta["BLOCK_N"]),
        )

    fused_gemm_bn_gelu_relu_kernel[grid](
        x_fp32,
        B,
        linear.bias.to(device=x.device, dtype=torch.float32, non_blocking=True),
        bn_scale,
        bn_shift,
        y,
        M,
        N,
        K,
        x_fp32.stride(0),
        x_fp32.stride(1),
        B.stride(0),
        B.stride(1),
        y.stride(0),
        y.stride(1),
    )

    return y


class ModelNew(nn.Module):
    """
    GEMM + BatchNorm + GELU + ReLU.
    Uses fused Triton kernel in eval mode on CUDA, and the standard PyTorch
    implementation (preserving BN training semantics) otherwise.
    """

    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.batch_norm = nn.BatchNorm1d(out_features)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Training mode or non-CUDA: use full PyTorch path for correct semantics
        if self.training or x.device.type != "cuda":
            x = self.gemm(x)
            x = self.batch_norm(x)
            x = torch.nn.functional.gelu(x)
            x = torch.nn.functional.relu(x)
            return x

        # Eval-mode fast path: fused Triton kernel
        return fused_gemm_bn_gelu_relu(x, self.gemm, self.batch_norm)
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl, AND any other modules used (e.g., import math if using math.sqrt)
   2. @triton.jit decorated kernel function(s) — NO continue/break/return inside loops (use masking)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels — THIS CLASS IS REQUIRED
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes
4. Ensure ALL imports are included at the top (common mistake: forgetting `import math`)

```python
# <corrected code>
```
