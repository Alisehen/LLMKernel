You are a Triton kernel debugging expert. Analyze the error and identify the root cause.

## ERROR LOG
```
Traceback (most recent call last):
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 34, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 1490, in full
    shape = _shape_check_impl(shape)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 1474, in _shape_check_impl
    validate_block_shape(shape)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/_utils.py", line 14, in validate_block_shape
    raise TypeError(f"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d)}]")
TypeError: Shape element 0 must have type `constexpr[int]`, got `constexpr[<class 'triton.language.core.tensor'>]

The above exception was the direct cause of the following exception:

triton.compiler.errors.CompilationError: at 10:11:
def zeros(shape, dtype):
    """
    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.

    :param shape: Shape of the new array, e.g., (8, 16) or (8, )
    :type shape: tuple of ints
    :param dtype: Data-type of the new array, e.g., :code:`tl.float16`
    :type dtype: DType
    """
    return core.full(shape, 0, dtype)
           ^

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 538, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_114800_batch_range81to100_deepseek_deepseek/96_ConvTranspose3d_Multiply_Max_GlobalAvgPool_Clamp/code/kernel_20251214_193544.py", line 301, in forward
    x = fused_post_convtranspose(x, self.scale, self.maxpool_kernel_size)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_114800_batch_range81to100_deepseek_deepseek/96_ConvTranspose3d_Multiply_Max_GlobalAvgPool_Clamp/code/kernel_20251214_193544.py", line 264, in fused_post_convtranspose
    global_avg_pool_clamp_kernel[grid2](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 278, in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 81, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 33:19:

    # Initialize accumulators
    sum_acc = tl.zeros((BLOCK_C,), dtype=tl.float32)
    count_acc = tl.zeros((BLOCK_C,), dtype=tl.float32)

    # Total number of elements to reduce (spatial dimensions)
    total_elements = D * H * W

    if REDUCE_IN_SMEM:
        # Use shared memory for better reduction performance
        smem_size = BLOCK_C * BLOCK_W
        smem_sum = tl.zeros((smem_size,), dtype=tl.float32)
                   ^
```

## Expected Behavior (PyTorch Reference)
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Model that performs a transposed 3D convolution, multiplies by a scalar, applies max pooling, 
    global average pooling, and clamps the output.
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale, maxpool_kernel_size):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        self.scale = scale
        self.maxpool = nn.MaxPool3d(kernel_size=maxpool_kernel_size)
        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.clamp_min = 0
        self.clamp_max = 1

    def forward(self, x):
        x = self.conv_transpose(x)
        x = x * self.scale
        x = self.maxpool(x)
        x = self.global_avg_pool(x)
        x = torch.clamp(x, min=self.clamp_min, max=self.clamp_max)
        return x

batch_size = 128
in_channels = 3
out_channels = 16
depth, height, width = 16, 32, 32
kernel_size = 3
stride = 2
padding = 1
scale = 0.5
maxpool_kernel_size = 2

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, scale, maxpool_kernel_size]
```

## Current Implementation (Broken Triton Kernel)
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def scale_maxpool3d_kernel(
    x_ptr,                 # Input tensor pointer
    out_ptr,              # Output tensor pointer (maxpooled)
    N, C, D, H, W,        # Input dimensions
    D_out, H_out, W_out,  # Output dimensions
    scale,                # Scalar multiplier
    K,                    # MaxPool kernel size
    stride_xn, stride_xc, stride_xd, stride_xh, stride_xw,
    stride_on, stride_oc, stride_od, stride_oh, stride_ow,
    BLOCK_C: tl.constexpr,
    USE_TENSOR_CORES: tl.constexpr,
):
    """Scale + MaxPool3d: Output shape [N, C, D_out, H_out, W_out]"""
    pid_n = tl.program_id(0)
    pid_c = tl.program_id(1)
    pid_w = tl.program_id(2)
    
    if pid_n >= N:
        return
    
    # Reconstruct 3D window index from linearized pid_w
    total_windows = D_out * H_out * W_out
    if pid_w >= total_windows:
        return
    
    d_out = pid_w // (H_out * W_out)
    h_out = (pid_w % (H_out * W_out)) // W_out
    w_out = pid_w % W_out
    
    # Channel block processing
    channel_base = pid_c * BLOCK_C
    channel_offsets = tl.arange(0, BLOCK_C)
    channel_mask = channel_base + channel_offsets < C
    
    # Initialize max values for all channels in block
    max_vals = tl.full((BLOCK_C,), -float('inf'), dtype=tl.float32)
    
    # Compute max over KÂ³ window (optimized with precomputed offsets)
    # Precompute base offsets to reduce arithmetic
    base_n_offset = pid_n * stride_xn
    base_d_start = d_out * K
    base_h_start = h_out * K
    base_w_start = w_out * K
    
    # Unroll loops and use tensor cores if enabled
    for kd in range(K):
        d_in = base_d_start + kd
        d_offset = d_in * stride_xd
        for kh in range(K):
            h_in = base_h_start + kh
            h_offset = d_offset + h_in * stride_xh
            for kw in range(K):
                w_in = base_w_start + kw
                w_offset = h_offset + w_in * stride_xw
                
                # Load input values for all channels in block
                c_indices = channel_base + channel_offsets
                x_ptrs = x_ptr + base_n_offset + c_indices * stride_xc + w_offset
                x_vals = tl.load(x_ptrs, mask=channel_mask, other=-float('inf'))
                
                # Apply scaling
                x_vals = x_vals * scale
                
                # Use tensor cores for max operation if enabled
                if USE_TENSOR_CORES:
                    # Convert to tensor core compatible format (fp16)
                    x_vals_fp16 = x_vals.to(tl.float16)
                    max_vals_fp16 = max_vals.to(tl.float16)
                    max_vals_fp16 = tl.maximum(max_vals_fp16, x_vals_fp16)
                    max_vals = max_vals_fp16.to(tl.float32)
                else:
                    max_vals = tl.maximum(max_vals, x_vals)
    
    # Store max values to output tensor
    c_indices = channel_base + channel_offsets
    out_ptrs = (
        out_ptr + pid_n * stride_on + 
        c_indices * stride_oc + 
        d_out * stride_od + h_out * stride_oh + w_out * stride_ow
    )
    tl.store(out_ptrs, max_vals, mask=channel_mask)


@triton.jit
def global_avg_pool_clamp_kernel(
    x_ptr,                 # Input tensor pointer (maxpooled output)
    out_ptr,              # Output tensor pointer
    N, C, D, H, W,        # Input dimensions (maxpooled tensor)
    stride_xn, stride_xc, stride_xd, stride_xh, stride_xw,
    stride_on, stride_oc, stride_od, stride_oh, stride_ow,
    BLOCK_C: tl.constexpr,
    BLOCK_W: tl.constexpr,  # Number of windows per thread
    REDUCE_IN_SMEM: tl.constexpr,  # Whether to use shared memory for reduction
):
    """GlobalAvgPool + Clamp: Output shape [N, C, 1, 1, 1]"""
    pid_n = tl.program_id(0)
    pid_c = tl.program_id(1)
    
    if pid_n >= N:
        return
    
    # Channel block processing
    channel_base = pid_c * BLOCK_C
    channel_offsets = tl.arange(0, BLOCK_C)
    channel_mask = channel_base + channel_offsets < C
    
    # Initialize accumulators
    sum_acc = tl.zeros((BLOCK_C,), dtype=tl.float32)
    count_acc = tl.zeros((BLOCK_C,), dtype=tl.float32)
    
    # Total number of elements to reduce (spatial dimensions)
    total_elements = D * H * W
    
    if REDUCE_IN_SMEM:
        # Use shared memory for better reduction performance
        smem_size = BLOCK_C * BLOCK_W
        smem_sum = tl.zeros((smem_size,), dtype=tl.float32)
        smem_count = tl.zeros((smem_size,), dtype=tl.float32)
        
        # Each thread processes BLOCK_W elements
        for block_start in range(0, total_elements, BLOCK_W):
            elem_offsets = block_start + tl.arange(0, BLOCK_W)
            elem_mask = elem_offsets < total_elements
            
            if tl.sum(elem_mask) > 0:
                # Convert linear index to 3D indices
                d_idx = elem_offsets // (H * W)
                h_idx = (elem_offsets % (H * W)) // W
                w_idx = elem_offsets % W
                
                # Compute pointer offsets
                c_indices = channel_base + channel_offsets[:, None]
                ptr_offsets = (
                    pid_n * stride_xn +
                    c_indices * stride_xc +
                    d_idx[None, :] * stride_xd +
                    h_idx[None, :] * stride_xh +
                    w_idx[None, :] * stride_xw
                )
                
                x_ptrs = x_ptr + ptr_offsets
                load_mask = channel_mask[:, None] & elem_mask[None, :]
                x_vals = tl.load(x_ptrs, mask=load_mask, other=0.0)
                
                # Store to shared memory
                smem_idx = channel_offsets[:, None] * BLOCK_W + tl.arange(0, BLOCK_W)[None, :]
                tl.store(smem_sum + smem_idx, x_vals, mask=load_mask)
                tl.store(smem_count + smem_idx, load_mask.to(tl.float32), mask=load_mask)
                
                # Wait for all threads to write to shared memory
                tl.debug_barrier()
                
                # Reduce in shared memory
                for i in range(BLOCK_W):
                    elem_idx = channel_offsets * BLOCK_W + i
                    smem_mask = elem_idx < smem_size
                    val = tl.load(smem_sum + elem_idx, mask=smem_mask, other=0.0)
                    cnt = tl.load(smem_count + elem_idx, mask=smem_mask, other=0.0)
                    sum_acc += val
                    count_acc += cnt
                
                # Clear shared memory for next iteration
                tl.store(smem_sum + smem_idx, tl.zeros((BLOCK_C, BLOCK_W), dtype=tl.float32), mask=load_mask)
                tl.store(smem_count + smem_idx, tl.zeros((BLOCK_C, BLOCK_W), dtype=tl.float32), mask=load_mask)
                tl.debug_barrier()
    else:
        # Original reduction approach
        for block_start in range(0, total_elements, BLOCK_W):
            elem_offsets = block_start + tl.arange(0, BLOCK_W)
            elem_mask = elem_offsets < total_elements
            
            if tl.sum(elem_mask) > 0:
                # Convert linear index to 3D indices
                d_idx = elem_offsets // (H * W)
                h_idx = (elem_offsets % (H * W)) // W
                w_idx = elem_offsets % W
                
                # Compute pointer offsets
                c_indices = channel_base + channel_offsets[:, None]
                ptr_offsets = (
                    pid_n * stride_xn +
                    c_indices * stride_xc +
                    d_idx[None, :] * stride_xd +
                    h_idx[None, :] * stride_xh +
                    w_idx[None, :] * stride_xw
                )
                
                x_ptrs = x_ptr + ptr_offsets
                load_mask = channel_mask[:, None] & elem_mask[None, :]
                x_vals = tl.load(x_ptrs, mask=load_mask, other=0.0)
                
                # Accumulate sum and count
                sum_acc += tl.sum(x_vals, axis=1)
                count_acc += tl.sum(load_mask.to(tl.float32), axis=1)
    
    # Compute average and clamp
    avg_vals = tl.where(count_acc > 0, sum_acc / count_acc, 0.0)
    avg_vals = tl.minimum(tl.maximum(avg_vals, 0.0), 1.0)
    
    # Store results
    c_indices = channel_base + channel_offsets
    out_ptrs = out_ptr + pid_n * stride_on + c_indices * stride_oc
    tl.store(out_ptrs, avg_vals, mask=channel_mask)


def fused_post_convtranspose(x, scale, maxpool_kernel_size):
    """
    Optimized fused operations using two kernels with autotuned parameters.
    Based on NCU metrics, we optimize for:
    1. Better SM utilization for scale_maxpool3d_kernel
    2. Higher warp activity for global_avg_pool_clamp_kernel
    """
    N, C, D, H, W = x.shape
    K = maxpool_kernel_size
    
    # MaxPool3d output dimensions
    D_out = D // K
    H_out = H // K
    W_out = W // K
    
    # Intermediate tensor for maxpool output
    maxpool_out = torch.empty((N, C, D_out, H_out, W_out), 
                              device=x.device, dtype=x.dtype)
    
    # Kernel 1: Scale + MaxPool3d - Use tensor cores for Ada Lovelace
    # Register pressure analysis: BLOCK_C=64, num_warps=4, num_stages=2
    BLOCK_C1 = min(triton.next_power_of_2(C), 64)
    total_windows = D_out * H_out * W_out
    
    grid1 = (N, triton.cdiv(C, BLOCK_C1), total_windows)
    
    # Use tensor cores if input is FP16/BF16
    use_tensor_cores = x.dtype in [torch.float16, torch.bfloat16]
    
    scale_maxpool3d_kernel[grid1](
        x, maxpool_out,
        N, C, D, H, W,
        D_out, H_out, W_out,
        scale, K,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),
        maxpool_out.stride(0), maxpool_out.stride(1), maxpool_out.stride(2),
        maxpool_out.stride(3), maxpool_out.stride(4),
        BLOCK_C=BLOCK_C1,
        USE_TENSOR_CORES=use_tensor_cores,
        num_warps=4,  # Conservative for register pressure
        num_stages=2,  # Better for multi-input fusion
    )
    
    # Kernel 2: GlobalAvgPool + Clamp - Optimize for higher warp activity
    # Register pressure analysis: BLOCK_C=32, num_warps=8, num_stages=3
    out = torch.empty((N, C, 1, 1, 1), device=x.device, dtype=x.dtype)
    BLOCK_C2 = min(triton.next_power_of_2(C), 32)
    BLOCK_W = 256  # Increased for better memory throughput
    
    grid2 = (N, triton.cdiv(C, BLOCK_C2))
    
    global_avg_pool_clamp_kernel[grid2](
        maxpool_out, out,
        N, C, D_out, H_out, W_out,
        maxpool_out.stride(0), maxpool_out.stride(1), maxpool_out.stride(2),
        maxpool_out.stride(3), maxpool_out.stride(4),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3), out.stride(4),
        BLOCK_C=BLOCK_C2,
        BLOCK_W=BLOCK_W,
        REDUCE_IN_SMEM=True,  # Use shared memory for better reduction
        num_warps=8,  # Higher warp count for compute-bound
        num_stages=3,  # Higher stages to hide latency
    )
    
    return out


class ModelNew(nn.Module):
    """
    ConvTranspose3d (PyTorch native) + Optimized fused post-ops (Triton)
    Optimized for Ada Lovelace architecture with:
    - Tensor core utilization when available
    - Optimized warp counts and staging
    - Shared memory reduction for global average pool
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale, maxpool_kernel_size):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding
        )
        self.scale = scale
        self.maxpool_kernel_size = maxpool_kernel_size
        
    def forward(self, x):
        # Step 1: PyTorch native ConvTranspose3d
        x = self.conv_transpose(x)
        # Step 2: Optimized fused post-ops in Triton
        x = fused_post_convtranspose(x, self.scale, self.maxpool_kernel_size)
        return x
```

---

## Your Task

Identify the **single most critical issue** that causes the error above.

### Analysis Guidelines

1. **Focus on root cause**, not symptoms
   - Bad: "Output is wrong"
   - Good: "BLOCK_K loop missing, only processes first 32 elements of K dimension"

2. **Be specific about WHAT and WHERE**
   - Bad: "Memory access issue"
   - Good: "Line 45: tl.atomic_add(c_block_ptr, acc) - atomic_add requires scalar pointer, not block_ptr"

3. **Prioritize by impact**
   - Correctness bugs > Performance issues > Style problems
   - Algorithm errors > Implementation details

### Output Format

**CRITICAL: You MUST output ONLY valid JSON. No other text allowed.**

```json
{
  "critical_issue": "<Concise description of THE root cause, max 30 words>",
  "why_it_matters": "<Why this causes the observed error, max 35 words>",
  "minimal_fix_hint": "<What needs to change (not how), max 30 words>"
}
```

**Remember**: Output ONLY the JSON block. No explanations, no commentary, no additional text.
