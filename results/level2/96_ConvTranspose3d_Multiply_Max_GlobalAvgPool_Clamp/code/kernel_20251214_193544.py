import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def scale_maxpool3d_kernel(
    x_ptr,                 # Input tensor pointer
    out_ptr,              # Output tensor pointer (maxpooled)
    N, C, D, H, W,        # Input dimensions
    D_out, H_out, W_out,  # Output dimensions
    scale,                # Scalar multiplier
    K,                    # MaxPool kernel size
    stride_xn, stride_xc, stride_xd, stride_xh, stride_xw,
    stride_on, stride_oc, stride_od, stride_oh, stride_ow,
    BLOCK_C: tl.constexpr,
    USE_TENSOR_CORES: tl.constexpr,
):
    """Scale + MaxPool3d: Output shape [N, C, D_out, H_out, W_out]"""
    pid_n = tl.program_id(0)
    pid_c = tl.program_id(1)
    pid_w = tl.program_id(2)
    
    if pid_n >= N:
        return
    
    # Reconstruct 3D window index from linearized pid_w
    total_windows = D_out * H_out * W_out
    if pid_w >= total_windows:
        return
    
    d_out = pid_w // (H_out * W_out)
    h_out = (pid_w % (H_out * W_out)) // W_out
    w_out = pid_w % W_out
    
    # Channel block processing
    channel_base = pid_c * BLOCK_C
    channel_offsets = tl.arange(0, BLOCK_C)
    channel_mask = channel_base + channel_offsets < C
    
    # Initialize max values for all channels in block
    max_vals = tl.full((BLOCK_C,), -float('inf'), dtype=tl.float32)
    
    # Compute max over KÂ³ window (optimized with precomputed offsets)
    # Precompute base offsets to reduce arithmetic
    base_n_offset = pid_n * stride_xn
    base_d_start = d_out * K
    base_h_start = h_out * K
    base_w_start = w_out * K
    
    # Unroll loops and use tensor cores if enabled
    for kd in range(K):
        d_in = base_d_start + kd
        d_offset = d_in * stride_xd
        for kh in range(K):
            h_in = base_h_start + kh
            h_offset = d_offset + h_in * stride_xh
            for kw in range(K):
                w_in = base_w_start + kw
                w_offset = h_offset + w_in * stride_xw
                
                # Load input values for all channels in block
                c_indices = channel_base + channel_offsets
                x_ptrs = x_ptr + base_n_offset + c_indices * stride_xc + w_offset
                x_vals = tl.load(x_ptrs, mask=channel_mask, other=-float('inf'))
                
                # Apply scaling
                x_vals = x_vals * scale
                
                # Use tensor cores for max operation if enabled
                if USE_TENSOR_CORES:
                    # Convert to tensor core compatible format (fp16)
                    x_vals_fp16 = x_vals.to(tl.float16)
                    max_vals_fp16 = max_vals.to(tl.float16)
                    max_vals_fp16 = tl.maximum(max_vals_fp16, x_vals_fp16)
                    max_vals = max_vals_fp16.to(tl.float32)
                else:
                    max_vals = tl.maximum(max_vals, x_vals)
    
    # Store max values to output tensor
    c_indices = channel_base + channel_offsets
    out_ptrs = (
        out_ptr + pid_n * stride_on + 
        c_indices * stride_oc + 
        d_out * stride_od + h_out * stride_oh + w_out * stride_ow
    )
    tl.store(out_ptrs, max_vals, mask=channel_mask)


@triton.jit
def global_avg_pool_clamp_kernel(
    x_ptr,                 # Input tensor pointer (maxpooled output)
    out_ptr,              # Output tensor pointer
    N, C, D, H, W,        # Input dimensions (maxpooled tensor)
    stride_xn, stride_xc, stride_xd, stride_xh, stride_xw,
    stride_on, stride_oc, stride_od, stride_oh, stride_ow,
    BLOCK_C: tl.constexpr,
    BLOCK_W: tl.constexpr,  # Number of windows per thread
    REDUCE_IN_SMEM: tl.constexpr,  # Whether to use shared memory for reduction
):
    """GlobalAvgPool + Clamp: Output shape [N, C, 1, 1, 1]"""
    pid_n = tl.program_id(0)
    pid_c = tl.program_id(1)
    
    if pid_n >= N:
        return
    
    # Channel block processing
    channel_base = pid_c * BLOCK_C
    channel_offsets = tl.arange(0, BLOCK_C)
    channel_mask = channel_base + channel_offsets < C
    
    # Initialize accumulators
    sum_acc = tl.zeros((BLOCK_C,), dtype=tl.float32)
    count_acc = tl.zeros((BLOCK_C,), dtype=tl.float32)
    
    # Total number of elements to reduce (spatial dimensions)
    total_elements = D * H * W
    
    if REDUCE_IN_SMEM:
        # Use shared memory for better reduction performance
        smem_size = BLOCK_C * BLOCK_W
        smem_sum = tl.zeros((smem_size,), dtype=tl.float32)
        smem_count = tl.zeros((smem_size,), dtype=tl.float32)
        
        # Each thread processes BLOCK_W elements
        for block_start in range(0, total_elements, BLOCK_W):
            elem_offsets = block_start + tl.arange(0, BLOCK_W)
            elem_mask = elem_offsets < total_elements
            
            if tl.sum(elem_mask) > 0:
                # Convert linear index to 3D indices
                d_idx = elem_offsets // (H * W)
                h_idx = (elem_offsets % (H * W)) // W
                w_idx = elem_offsets % W
                
                # Compute pointer offsets
                c_indices = channel_base + channel_offsets[:, None]
                ptr_offsets = (
                    pid_n * stride_xn +
                    c_indices * stride_xc +
                    d_idx[None, :] * stride_xd +
                    h_idx[None, :] * stride_xh +
                    w_idx[None, :] * stride_xw
                )
                
                x_ptrs = x_ptr + ptr_offsets
                load_mask = channel_mask[:, None] & elem_mask[None, :]
                x_vals = tl.load(x_ptrs, mask=load_mask, other=0.0)
                
                # Store to shared memory
                smem_idx = channel_offsets[:, None] * BLOCK_W + tl.arange(0, BLOCK_W)[None, :]
                tl.store(smem_sum + smem_idx, x_vals, mask=load_mask)
                tl.store(smem_count + smem_idx, load_mask.to(tl.float32), mask=load_mask)
                
                # Wait for all threads to write to shared memory
                tl.debug_barrier()
                
                # Reduce in shared memory
                for i in range(BLOCK_W):
                    elem_idx = channel_offsets * BLOCK_W + i
                    smem_mask = elem_idx < smem_size
                    val = tl.load(smem_sum + elem_idx, mask=smem_mask, other=0.0)
                    cnt = tl.load(smem_count + elem_idx, mask=smem_mask, other=0.0)
                    sum_acc += val
                    count_acc += cnt
                
                # Clear shared memory for next iteration
                tl.store(smem_sum + smem_idx, tl.zeros((BLOCK_C, BLOCK_W), dtype=tl.float32), mask=load_mask)
                tl.store(smem_count + smem_idx, tl.zeros((BLOCK_C, BLOCK_W), dtype=tl.float32), mask=load_mask)
                tl.debug_barrier()
    else:
        # Original reduction approach
        for block_start in range(0, total_elements, BLOCK_W):
            elem_offsets = block_start + tl.arange(0, BLOCK_W)
            elem_mask = elem_offsets < total_elements
            
            if tl.sum(elem_mask) > 0:
                # Convert linear index to 3D indices
                d_idx = elem_offsets // (H * W)
                h_idx = (elem_offsets % (H * W)) // W
                w_idx = elem_offsets % W
                
                # Compute pointer offsets
                c_indices = channel_base + channel_offsets[:, None]
                ptr_offsets = (
                    pid_n * stride_xn +
                    c_indices * stride_xc +
                    d_idx[None, :] * stride_xd +
                    h_idx[None, :] * stride_xh +
                    w_idx[None, :] * stride_xw
                )
                
                x_ptrs = x_ptr + ptr_offsets
                load_mask = channel_mask[:, None] & elem_mask[None, :]
                x_vals = tl.load(x_ptrs, mask=load_mask, other=0.0)
                
                # Accumulate sum and count
                sum_acc += tl.sum(x_vals, axis=1)
                count_acc += tl.sum(load_mask.to(tl.float32), axis=1)
    
    # Compute average and clamp
    avg_vals = tl.where(count_acc > 0, sum_acc / count_acc, 0.0)
    avg_vals = tl.minimum(tl.maximum(avg_vals, 0.0), 1.0)
    
    # Store results
    c_indices = channel_base + channel_offsets
    out_ptrs = out_ptr + pid_n * stride_on + c_indices * stride_oc
    tl.store(out_ptrs, avg_vals, mask=channel_mask)


def fused_post_convtranspose(x, scale, maxpool_kernel_size):
    """
    Optimized fused operations using two kernels with autotuned parameters.
    Based on NCU metrics, we optimize for:
    1. Better SM utilization for scale_maxpool3d_kernel
    2. Higher warp activity for global_avg_pool_clamp_kernel
    """
    N, C, D, H, W = x.shape
    K = maxpool_kernel_size
    
    # MaxPool3d output dimensions
    D_out = D // K
    H_out = H // K
    W_out = W // K
    
    # Intermediate tensor for maxpool output
    maxpool_out = torch.empty((N, C, D_out, H_out, W_out), 
                              device=x.device, dtype=x.dtype)
    
    # Kernel 1: Scale + MaxPool3d - Use tensor cores for Ada Lovelace
    # Register pressure analysis: BLOCK_C=64, num_warps=4, num_stages=2
    BLOCK_C1 = min(triton.next_power_of_2(C), 64)
    total_windows = D_out * H_out * W_out
    
    grid1 = (N, triton.cdiv(C, BLOCK_C1), total_windows)
    
    # Use tensor cores if input is FP16/BF16
    use_tensor_cores = x.dtype in [torch.float16, torch.bfloat16]
    
    scale_maxpool3d_kernel[grid1](
        x, maxpool_out,
        N, C, D, H, W,
        D_out, H_out, W_out,
        scale, K,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),
        maxpool_out.stride(0), maxpool_out.stride(1), maxpool_out.stride(2),
        maxpool_out.stride(3), maxpool_out.stride(4),
        BLOCK_C=BLOCK_C1,
        USE_TENSOR_CORES=use_tensor_cores,
        num_warps=4,  # Conservative for register pressure
        num_stages=2,  # Better for multi-input fusion
    )
    
    # Kernel 2: GlobalAvgPool + Clamp - Optimize for higher warp activity
    # Register pressure analysis: BLOCK_C=32, num_warps=8, num_stages=3
    out = torch.empty((N, C, 1, 1, 1), device=x.device, dtype=x.dtype)
    BLOCK_C2 = min(triton.next_power_of_2(C), 32)
    BLOCK_W = 256  # Increased for better memory throughput
    
    grid2 = (N, triton.cdiv(C, BLOCK_C2))
    
    global_avg_pool_clamp_kernel[grid2](
        maxpool_out, out,
        N, C, D_out, H_out, W_out,
        maxpool_out.stride(0), maxpool_out.stride(1), maxpool_out.stride(2),
        maxpool_out.stride(3), maxpool_out.stride(4),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3), out.stride(4),
        BLOCK_C=BLOCK_C2,
        BLOCK_W=BLOCK_W,
        REDUCE_IN_SMEM=True,  # Use shared memory for better reduction
        num_warps=8,  # Higher warp count for compute-bound
        num_stages=3,  # Higher stages to hide latency
    )
    
    return out


class ModelNew(nn.Module):
    """
    ConvTranspose3d (PyTorch native) + Optimized fused post-ops (Triton)
    Optimized for Ada Lovelace architecture with:
    - Tensor core utilization when available
    - Optimized warp counts and staging
    - Shared memory reduction for global average pool
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scale, maxpool_kernel_size):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding
        )
        self.scale = scale
        self.maxpool_kernel_size = maxpool_kernel_size
        
    def forward(self, x):
        # Step 1: PyTorch native ConvTranspose3d
        x = self.conv_transpose(x)
        # Step 2: Optimized fused post-ops in Triton
        x = fused_post_convtranspose(x, self.scale, self.maxpool_kernel_size)
        return x
