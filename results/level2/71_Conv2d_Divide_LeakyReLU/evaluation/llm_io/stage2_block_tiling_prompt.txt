You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU: 4090

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: BLOCK_SIZE with register pressure awareness.

Key Principle:
- Fusion increases register usage (intermediates stay in registers)
- Spill to local memory kills fusion benefit

Register Pressure Signals (from NCU):
- launch__registers_per_thread > 128 → likely spilling
- launch__occupancy_limit_registers < other limits → register-bound

Rules:
- Start conservative: BLOCK_SIZE ∈ {256, 512} for element-wise
- For matmul fusion: BLOCK_M/N ∈ {32, 64}, BLOCK_K ∈ {32}
- If registers > 128: reduce BLOCK_* by half
- Trade-off: recompute cheap ops (e.g., x*0.5) vs store intermediate

When to Recompute vs Keep:
- Keep: expensive ops (exp, log, div, sqrt)
- Recompute: cheap ops (add, mul, max) if register pressure high
- Example: `y = relu(x); z = y * scale` → keep y
- Example: `y = x * 0.5; z = y + bias` → can recompute y if needed

Autotune:
- 2-3 BLOCK_SIZE configs, always include smaller fallback



[CURRENT CODE]
```python
# Optimized Triton conv2d + divide + LeakyReLU (implicit GEMM, fused, autotuned)

import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        # Baseline
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 32},
            num_warps=4,
            num_stages=3,
        ),
        # Wider N tile
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 128, "BLOCK_K": 32},
            num_warps=8,
            num_stages=3,
        ),
        # Taller M tile
        triton.Config(
            {"BLOCK_M": 128, "BLOCK_N": 64, "BLOCK_K": 32},
            num_warps=8,
            num_stages=3,
        ),
        # Large square tile
        triton.Config(
            {"BLOCK_M": 128, "BLOCK_N": 128, "BLOCK_K": 32},
            num_warps=8,
            num_stages=4,
        ),
        # Deeper K tile
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 64},
            num_warps=4,
            num_stages=4,
        ),
    ],
    key=["N", "C_out", "H_out", "W_out", "K_total"],
)
@triton.jit
def conv2d_div_leakyrelu_kernel(
    x_ptr,  # *f32 or *f16
    w_ptr,  # *f32 or *f16
    bias_ptr,  # *f32
    y_ptr,  # *f32 or *f16
    inv_div,  # scalar
    negative_slope,  # scalar
    N,
    C_in,
    H_in,
    W_in,
    C_out,
    H_out,
    W_out,
    stride_xn,
    stride_xc,
    stride_xh,
    stride_xw,
    stride_wn,
    stride_wc,
    stride_wk,
    stride_wl,
    stride_yn,
    stride_yc,
    stride_yh,
    stride_yw,
    K_total,
    KERNEL_SIZE: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """
    Implicit-GEMM conv2d (valid, stride=1, no padding) fused with:
      - bias add
      - division by constant
      - LeakyReLU

    Grid:
      pid_m over M = N * H_out * W_out
      pid_n over N = C_out

    All fused elementwise ops share the SAME (offs_m, offs_n) and mask.
    """
    # ----------------------------
    # Program IDs
    # ----------------------------
    pid_m = tl.program_id(0)  # flattened output positions
    pid_n = tl.program_id(1)  # output channels

    # ----------------------------
    # Common offsets for ALL fused ops
    # ----------------------------
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)  # [BLOCK_M]
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)  # [BLOCK_N]

    M = N * H_out * W_out

    mask_m = offs_m < M         # [BLOCK_M]
    mask_n = offs_n < C_out     # [BLOCK_N]
    mask_mn = mask_m[:, None] & mask_n[None, :]  # [BLOCK_M, BLOCK_N]

    # Decode flattened m -> (n_idx, oh_idx, ow_idx).
    # These are used for input and output addressing, and are shared
    # by all fused ops via (offs_m, offs_n).
    tmp = offs_m
    hw_out = H_out * W_out
    n_idx = tmp // hw_out
    tmp = tmp % hw_out
    oh_idx = tmp // W_out
    ow_idx = tmp % W_out

    # ----------------------------
    # Prepare K loop
    # ----------------------------
    offs_k = tl.arange(0, BLOCK_K)  # [BLOCK_K]
    KS = KERNEL_SIZE
    KS2 = KS * KS

    # Accumulator (always FP32 for numeric stability)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Precompute base input pointer for each output element (channel 0, kh=0, kw=0)
    # Shape: [BLOCK_M, 1]
    n_b = n_idx[:, None]
    oh_b = oh_idx[:, None]
    ow_b = ow_idx[:, None]
    x_base = (
        x_ptr
        + n_b * stride_xn
        + oh_b * stride_xh
        + ow_b * stride_xw
    )

    # Precompute base weight pointer for each output channel index
    # Shape: [1, BLOCK_N]
    w_base = w_ptr + offs_n[None, :] * stride_wn

    # ----------------------------
    # K reduction loop: implicit GEMM
    # ----------------------------
    for k_start in range(0, K_total, BLOCK_K):
        k_offsets = k_start + offs_k  # [BLOCK_K]
        mask_k = k_offsets < K_total  # [BLOCK_K]

        # Map flattened K -> (c_idx, kh_idx, kw_idx)
        c_idx = k_offsets // KS2
        rem = k_offsets % KS2
        kh_idx = rem // KS
        kw_idx = rem % KS

        # Input matrix A: [M, K]
        c_b = c_idx[None, :]   # [1, BLOCK_K]
        kh_b = kh_idx[None, :]  # [1, BLOCK_K]
        kw_b = kw_idx[None, :]  # [1, BLOCK_K]

        x_ptrs = (
            x_base
            + c_b * stride_xc
            + kh_b * stride_xh
            + kw_b * stride_xw
        )  # [BLOCK_M, BLOCK_K]

        mask_a = mask_m[:, None] & mask_k[None, :]  # [BLOCK_M, BLOCK_K]
        a = tl.load(x_ptrs, mask=mask_a, other=0.0)

        # Weight matrix B: [K, C_out]
        w_ptrs = (
            w_base
            + c_idx[:, None] * stride_wc
            + kh_idx[:, None] * stride_wk
            + kw_idx[:, None] * stride_wl
        )  # [BLOCK_K, BLOCK_N]

        mask_b = mask_k[:, None] & mask_n[None, :]  # [BLOCK_K, BLOCK_N]
        b = tl.load(w_ptrs, mask=mask_b, other=0.0)

        # GEMM accumulate (use TF32 tensor cores when possible)
        a_fp32 = a.to(tl.float32)
        b_fp32 = b.to(tl.float32)
        acc += tl.dot(a_fp32, b_fp32, allow_tf32=True)

    # ----------------------------
    # Fused post-ops
    # All use SAME (offs_m, offs_n) + mask_mn
    # ----------------------------

    # Bias add: broadcast over M dimension
    bias_vals = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)  # [BLOCK_N]
    acc += bias_vals[None, :]  # [BLOCK_M, BLOCK_N]

    # Divide by constant (inv_div precomputed on host)
    acc = acc * inv_div

    # LeakyReLU
    acc = tl.where(acc >= 0, acc, acc * negative_slope)

    # ----------------------------
    # Store result
    # ----------------------------
    # Output pointer for each (offs_m, offs_n) pair
    y_ptrs = (
        y_ptr
        + n_b * stride_yn
        + offs_n[None, :] * stride_yc
        + oh_b * stride_yh
        + ow_b * stride_yw
    )  # [BLOCK_M, BLOCK_N]

    tl.store(y_ptrs, acc, mask=mask_mn)


def conv2d_div_leakyrelu(x, weight, bias, divisor, negative_slope=0.01):
    """
    Compute:
        y = LeakyReLU( conv2d(x, weight, bias) / divisor )

    Constraints:
      - x: NCHW
      - weight: (C_out, C_in, K, K)
      - stride=1, padding=0, dilation=1, groups=1
    """
    assert x.dim() == 4, "Input must be NCHW"
    N, C_in, H_in, W_in = x.shape
    C_out, Cw_in, KH, KW = weight.shape
    assert C_in == Cw_in, "Incompatible in_channels between input and weight"
    assert KH == KW, "Only square kernels are supported"
    KS = KH

    # Valid conv: no padding, stride 1
    H_out = H_in - KS + 1
    W_out = W_in - KS + 1
    assert H_out > 0 and W_out > 0, "Kernel larger than input with no padding"

    y = torch.empty((N, C_out, H_out, W_out), device=x.device, dtype=x.dtype)

    K_total = C_in * KS * KS
    M = N * H_out * W_out

    # Precompute inverse divisor to avoid division in kernel
    inv_div = 1.0 / float(divisor)
    neg_slope = float(negative_slope)

    def grid(meta):
        return (
            triton.cdiv(M, meta["BLOCK_M"]),
            triton.cdiv(C_out, meta["BLOCK_N"]),
        )

    conv2d_div_leakyrelu_kernel[grid](
        x,
        weight,
        bias,
        y,
        inv_div,
        neg_slope,
        N,
        C_in,
        H_in,
        W_in,
        C_out,
        H_out,
        W_out,
        x.stride(0),
        x.stride(1),
        x.stride(2),
        x.stride(3),
        weight.stride(0),
        weight.stride(1),
        weight.stride(2),
        weight.stride(3),
        y.stride(0),
        y.stride(1),
        y.stride(2),
        y.stride(3),
        K_total,
        KERNEL_SIZE=KS,
    )
    return y


class ModelNew(nn.Module):
    """
    Triton-optimized:
        Conv2d -> divide by constant -> LeakyReLU
    (stride=1, padding=0, dilation=1, groups=1)
    """

    def __init__(self, in_channels, out_channels, kernel_size, divisor):
        super(ModelNew, self).__init__()
        if isinstance(kernel_size, tuple):
            assert kernel_size[0] == kernel_size[1], "Only square kernels supported"
            k = kernel_size[0]
        else:
            k = kernel_size

        self.weight = nn.Parameter(
            torch.randn(out_channels, in_channels, k, k)
        )
        self.bias = nn.Parameter(torch.randn(out_channels))
        self.divisor = float(divisor)
        self.negative_slope = 0.01

    def forward(self, x):
        return conv2d_div_leakyrelu(
            x, self.weight, self.bias, self.divisor, self.negative_slope
        )
```

[NCU PROFILING METRICS]
{
  "conv2d_div_leakyrelu_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 52.36,
    "launch__grid_size": 15876,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 32.87,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 61.32,
    "lts__t_sector_hit_rate.pct": 93.63
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

## CRITICAL — Code MUST compile and run:
1. EVERY kernel function MUST have `@triton.jit` decorator
2. Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
3. BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
4. `tl.program_id(axis)` only supports axis = 0, 1, 2
5. No `continue`, `break`, `return` inside loops — use masking
6. No tensor indexing with loop vars: `x[:, i]` is INVALID
7. mask shape MUST match data shape in tl.load/tl.store

## Missing Triton Functions (implement manually):
- tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

## OUTPUT FORMAT (STRICT):
1. Imports: torch, torch.nn, triton, triton.language as tl
2. @triton.jit decorated kernel function(s)
3. Wrapper function(s) for grid calculation and kernel launch
4. class ModelNew(nn.Module) that calls your kernels

Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
