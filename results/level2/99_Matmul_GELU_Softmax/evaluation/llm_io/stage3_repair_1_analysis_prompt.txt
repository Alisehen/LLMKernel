You are a Triton kernel debugging expert. Analyze the error and identify the root cause.

## ERROR LOG
```
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 626, in compare_and_bench
    raise ValueError(
ValueError: Outputs are not close (atol=0.001, rtol=0.01). max_abs_err=1.295e-02, mean_abs_err=1.343e-04
```

## Expected Behavior (PyTorch Reference)
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Simple model that performs a matrix multiplication, applies GELU, and then applies Softmax.
    """
    def __init__(self, in_features, out_features):
        super(Model, self).__init__()
        self.linear = nn.Linear(in_features, out_features)

    def forward(self, x):
        x = self.linear(x)
        x = torch.nn.functional.gelu(x)
        x = torch.nn.functional.softmax(x, dim=1)
        return x

batch_size = 1024
in_features = 8192
out_features = 8192

def get_inputs():
    return [torch.rand(batch_size, in_features)]

def get_init_inputs():
    return [in_features, out_features]
```

## Current Implementation (Broken Triton Kernel)
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl
import math


@triton.jit
def gemm_gelu_kernel(
    # Pointers to matrices
    a_ptr, b_ptr, c_ptr,
    # Matrix dimensions
    M, N, K,
    # Strides
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    # Block sizes
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
    num_stages: tl.constexpr = 2,
):
    # Program ID for 2D grid
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    # Offsets for this block
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)
    
    # Initialize accumulator in float32 for precision
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    # Precompute pointers for better ILP
    a_ptrs_base = a_ptr + (offs_m[:, None] * stride_am)
    b_ptrs_base = b_ptr + (offs_n[None, :] * stride_bn)
    
    # Main GEMM loop with software pipelining
    for k in range(0, K, BLOCK_K):
        # Compute masks once per iteration
        a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)
        b_mask = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)
        
        # Load blocks with mask
        a = tl.load(
            a_ptrs_base + offs_k[None, :] * stride_ak,
            mask=a_mask,
            other=0.0
        )
        b = tl.load(
            b_ptrs_base + offs_k[:, None] * stride_bk,
            mask=b_mask,
            other=0.0
        )
        
        # Fused accumulation with TF32 tensor cores
        acc += tl.dot(a, b, allow_tf32=True, out_dtype=tl.float32)
        
        # Update base pointers
        a_ptrs_base += BLOCK_K * stride_ak
        b_ptrs_base += BLOCK_K * stride_bk
    
    # Fused GELU activation using fast approximation
    # GELU(x) â‰ˆ x * sigmoid(1.702x) for faster computation
    x = acc
    sigmoid_arg = x * 1.702
    # Fast sigmoid: 1 / (1 + exp(-x))
    sigmoid = 1.0 / (1.0 + tl.exp(-sigmoid_arg))
    gelu_out = x * sigmoid
    
    # Store final output with mask
    mask_m = offs_m[:, None] < M
    mask_n = offs_n[None, :] < N
    mask = mask_m & mask_n
    
    c_ptrs = c_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn)
    tl.store(c_ptrs, gelu_out, mask=mask)


@triton.jit
def fused_softmax_kernel(
    # Pointers to input/output
    x_ptr, output_ptr,
    # Matrix dimensions
    M, N,
    # Strides
    stride_xm, stride_xn,
    stride_om, stride_on,
    # Block sizes
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,
    num_stages: tl.constexpr = 2,
):
    # 1D grid over rows
    pid = tl.program_id(0)
    
    # Row indices for this block
    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)
    row_mask = offs_m < M
    
    # Allocate shared memory for max and sum reductions
    # (BLOCK_M, ) arrays in registers for better locality
    row_max = tl.full((BLOCK_M,), -float('inf'), dtype=tl.float32)
    row_sum = tl.zeros((BLOCK_M,), dtype=tl.float32)
    
    # First pass: compute max and sum simultaneously with online algorithm
    for n_start in range(0, N, BLOCK_N):
        offs_n = n_start + tl.arange(0, BLOCK_N)
        col_mask = offs_n < N
        
        # Load block
        x_ptrs = x_ptr + (offs_m[:, None] * stride_xm + offs_n[None, :] * stride_xn)
        mask = row_mask[:, None] & col_mask[None, :]
        x_vals = tl.load(x_ptrs, mask=mask, other=-float('inf'))
        
        # Online max update
        row_max = tl.maximum(row_max, tl.max(x_vals, axis=1))
    
    # Second pass: compute sum of exponentials and normalize in one go
    for n_start in range(0, N, BLOCK_N):
        offs_n = n_start + tl.arange(0, BLOCK_N)
        col_mask = offs_n < N
        
        # Load block again
        x_ptrs = x_ptr + (offs_m[:, None] * stride_xm + offs_n[None, :] * stride_xn)
        mask = row_mask[:, None] & col_mask[None, :]
        x_vals = tl.load(x_ptrs, mask=mask, other=0.0)
        
        # Compute exp(x - max) and accumulate sum
        x_vals_minus_max = x_vals - row_max[:, None]
        exp_vals = tl.exp(x_vals_minus_max)
        
        # Accumulate sum for normalization
        row_sum += tl.sum(exp_vals, axis=1)
        
        # Store normalized values directly to output
        softmax_vals = exp_vals / row_sum[:, None]
        
        output_ptrs = output_ptr + (offs_m[:, None] * stride_om + offs_n[None, :] * stride_on)
        tl.store(output_ptrs, softmax_vals, mask=mask)


def fused_gemm_gelu_softmax(x, weight):
    """Fused GEMM + GELU + Softmax operation with optimized memory patterns."""
    # Ensure inputs are contiguous
    x = x.contiguous()
    weight = weight.contiguous()
    
    # Get dimensions
    M, K = x.shape
    N = weight.shape[0]  # out_features
    
    # Allocate output tensors
    # Use separate output for softmax to avoid in-place modification
    gemm_output = torch.empty((M, N), device=x.device, dtype=x.dtype)
    final_output = torch.empty((M, N), device=x.device, dtype=x.dtype)
    
    # Grid for GEMM+GELU kernel - tuned for Ada Lovelace
    grid_gemm = lambda META: (
        triton.cdiv(M, META['BLOCK_M']),
        triton.cdiv(N, META['BLOCK_N'])
    )
    
    # Launch GEMM+GELU kernel with optimized block sizes
    # Using larger blocks for better tensor core utilization
    gemm_gelu_kernel[grid_gemm](
        x, weight, gemm_output,
        M, N, K,
        x.stride(0), x.stride(1),
        weight.stride(1), weight.stride(0),  # Transposed access
        gemm_output.stride(0), gemm_output.stride(1),
        BLOCK_M=128, BLOCK_N=64, BLOCK_K=32,
        num_stages=2
    )
    
    # Grid for fused softmax kernel
    grid_softmax = lambda META: (triton.cdiv(M, META['BLOCK_M']),)
    
    # Launch fused softmax kernel - reads from gemm_output, writes to final_output
    fused_softmax_kernel[grid_softmax](
        gemm_output, final_output,
        M, N,
        gemm_output.stride(0), gemm_output.stride(1),
        final_output.stride(0), final_output.stride(1),
        BLOCK_M=64, BLOCK_N=256,
        num_stages=2
    )
    
    return final_output


class ModelNew(nn.Module):
    """Optimized model with fully fused GEMM + GELU + Softmax operations."""
    
    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
    
    def forward(self, x):
        return fused_gemm_gelu_softmax(x, self.weight)
```

---

## Your Task

Identify the **single most critical issue** that causes the error above.

### Analysis Guidelines

1. **Focus on root cause**, not symptoms
   - Bad: "Output is wrong"
   - Good: "BLOCK_K loop missing, only processes first 32 elements of K dimension"

2. **Be specific about WHAT and WHERE**
   - Bad: "Memory access issue"
   - Good: "Line 45: tl.atomic_add(c_block_ptr, acc) - atomic_add requires scalar pointer, not block_ptr"

3. **Prioritize by impact**
   - Correctness bugs > Performance issues > Style problems
   - Algorithm errors > Implementation details

### Output Format

**CRITICAL: You MUST output ONLY valid JSON. No other text allowed.**

```json
{
  "critical_issue": "<Concise description of THE root cause, max 30 words>",
  "why_it_matters": "<Why this causes the observed error, max 35 words>",
  "minimal_fix_hint": "<What needs to change (not how), max 30 words>"
}
```

**Remember**: Output ONLY the JSON block. No explanations, no commentary, no additional text.
