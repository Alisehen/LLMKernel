Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 527, in compare_and_bench
    test_model = ModelNew(*init_args, **init_kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251215_024837_batch_range56to88_openai_deepseek/85_Conv2d_GroupNorm_Scale_MaxPool_Clamp/code/kernel_20251215_045532.py", line 349, in __init__
    bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
                ^^^^
NameError: name 'math' is not defined

Main Critical Problem Analysis:
Problem Analysis (from expert diagnosis):
critical_issue: The Python math module is used in ModelNew.__init__ without being imported.
why_it_matters: Accessing math.sqrt and math in __init__ without importing math raises a NameError before any Triton kernel runs.
minimal_fix_hint: Add an import for the math module at the top of the file.

Focus your fix on addressing the identified critical issue.


Broken Code:
```python
# <optimized Triton code>

import torch
import torch.nn as nn
import triton
import triton.language as tl


# ------------------------------------------------------------
# 1. Conv2d NCHW kernel (stride=1, padding=0, dilation=1)
# ------------------------------------------------------------

@triton.autotune(
    configs=[
        # Baseline, conservative: good balance of occupancy and register usage
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 32}, num_warps=4, num_stages=2),
        # More aggressive: more warps + stages for higher ILP if register pressure allows
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 32}, num_warps=8, num_stages=3),
        # Smaller BLOCK_N tile with more warps for potentially lower reg pressure / higher occ
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 16}, num_warps=8, num_stages=2),
    ],
    key=["N", "C_out", "H_out", "W_out"],
)
@triton.jit
def conv2d_nchw_kernel(
    x_ptr, w_ptr, b_ptr, y_ptr,
    N, C_in, H_in, W_in,
    C_out,
    H_out, W_out,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_wo, stride_wi, stride_wkh, stride_wkw,
    stride_yn, stride_yc, stride_yh, stride_yw,
    # Meta-parameters
    BLOCK_M: tl.constexpr,  # over P = N * H_out * W_out
    BLOCK_N: tl.constexpr,  # over C_out
    K_H: tl.constexpr,
    K_W: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    P = N * H_out * W_out

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    mask_m = offs_m < P
    mask_n = offs_n < C_out

    HW_out = H_out * W_out
    n_idx = offs_m // HW_out
    rem = offs_m % HW_out
    oh_idx = rem // W_out
    ow_idx = rem % W_out

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Per-N base pointers
    x_n_base = x_ptr + n_idx * stride_xn
    # Per-output-channel base for weights
    w_oc_base = w_ptr + offs_n * stride_wo

    for ic in range(0, C_in):
        x_ic_base = x_n_base + ic * stride_xc
        w_ic_base = w_oc_base + ic * stride_wi
        for kh in tl.static_range(0, K_H):
            h_in = oh_idx + kh
            x_kh_base = x_ic_base + h_in * stride_xh
            w_kh_base = w_ic_base + kh * stride_wkh
            for kw in tl.static_range(0, K_W):
                w_in = ow_idx + kw

                x_ptrs = x_kh_base + w_in * stride_xw
                x_vals = tl.load(x_ptrs, mask=mask_m, other=0.0).to(tl.float32)

                w_ptrs = w_kh_base + kw * stride_wkw
                w_vals = tl.load(w_ptrs, mask=mask_n, other=0.0).to(tl.float32)

                acc += x_vals[:, None] * w_vals[None, :]

    # Add bias
    b_vals = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0).to(tl.float32)
    acc += b_vals[None, :]

    # Store output
    y_ptrs = (
        y_ptr
        + n_idx[:, None] * stride_yn
        + offs_n[None, :] * stride_yc
        + oh_idx[:, None] * stride_yh
        + ow_idx[:, None] * stride_yw
    )
    tl.store(y_ptrs, acc, mask=mask_m[:, None] & mask_n[None, :])


# ------------------------------------------------------------
# 2. Fused GroupNorm + per-channel scale + MaxPool2d + clamp
# ------------------------------------------------------------

@triton.autotune(
    configs=[
        # Conservative baseline
        triton.Config({"BLOCK": 128}, num_warps=4, num_stages=2),
        # Larger tile to better saturate memory BW
        triton.Config({"BLOCK": 256}, num_warps=4, num_stages=2),
        # More warps for additional latency hiding where registers allow
        triton.Config({"BLOCK": 256}, num_warps=8, num_stages=2),
    ],
    key=["group_elems", "group_elems_out"],
)
@triton.jit
def groupnorm_scale_maxpool_clamp_kernel(
    x_ptr, gamma_ptr, beta_ptr, scale_ptr, y_ptr,
    N, C, H, W,
    H_out, W_out,
    num_groups, group_size,
    HW, HW_out, group_elems, group_elems_out,
    stride_hw,
    eps, clamp_min, clamp_max,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_yn, stride_yc, stride_yh, stride_yw,
    BLOCK: tl.constexpr,
    kernel_size: tl.constexpr,
):
    pid = tl.program_id(0)  # over N * num_groups
    n = pid // num_groups
    g = pid % num_groups

    c0 = g * group_size

    x_n_base = x_ptr + n * stride_xn
    y_n_base = y_ptr + n * stride_yn

    # -------- Pass 1: compute mean and variance over this (n, group) --------
    mean = tl.zeros((), dtype=tl.float32)
    m2 = tl.zeros((), dtype=tl.float32)

    offset = 0
    while offset < group_elems:
        offs = offset + tl.arange(0, BLOCK)
        mask = offs < group_elems

        c_off = offs // HW
        rem = offs % HW
        h = rem // W
        w = rem % W
        c = c0 + c_off

        x_ptrs = x_n_base + c * stride_xc + h * stride_xh + w * stride_xw
        vals = tl.load(x_ptrs, mask=mask, other=0.0).to(tl.float32)

        mean += tl.sum(vals, axis=0)
        m2 += tl.sum(vals * vals, axis=0)

        offset += BLOCK

    L_f = tl.full((), group_elems, dtype=tl.float32)
    mean = mean / L_f
    var = m2 / L_f - mean * mean
    inv_std = 1.0 / tl.sqrt(var + eps)

    # -------- Pass 2: GroupNorm + per-channel scale + MaxPool + clamp --------
    offset = 0
    while offset < group_elems_out:
        offs = offset + tl.arange(0, BLOCK)
        mask = offs < group_elems_out

        c_off = offs // HW_out
        rem = offs % HW_out
        oh = rem // W_out
        ow = rem % W_out
        c = c0 + c_off

        gamma = tl.load(gamma_ptr + c, mask=mask, other=1.0).to(tl.float32)
        beta = tl.load(beta_ptr + c, mask=mask, other=0.0).to(tl.float32)
        sc = tl.load(scale_ptr + c, mask=mask, other=1.0).to(tl.float32)

        acc = tl.full((BLOCK,), -float("inf"), dtype=tl.float32)

        # Unrolled pooling window
        for kh in tl.static_range(0, kernel_size):
            h_in = oh * stride_hw + kh
            for kw in tl.static_range(0, kernel_size):
                w_in = ow * stride_hw + kw

                x_ptrs = x_n_base + c * stride_xc + h_in * stride_xh + w_in * stride_xw
                vals = tl.load(
                    x_ptrs,
                    mask=mask,
                    other=-float("inf"),
                ).to(tl.float32)

                norm = (vals - mean) * inv_std
                y_vals = (norm * gamma + beta) * sc
                acc = tl.maximum(acc, y_vals)

        # Clamp
        acc = tl.maximum(acc, clamp_min)
        acc = tl.minimum(acc, clamp_max)

        y_ptrs = y_n_base + c * stride_yc + oh * stride_yh + ow * stride_yw
        tl.store(y_ptrs, acc, mask=mask)

        offset += BLOCK


# ------------------------------------------------------------
# 3. Wrapper functions
# ------------------------------------------------------------

def conv2d_triton(x, weight, bias):
    """
    NCHW conv2d with stride=1, padding=0, dilation=1 implemented in Triton.
    Args:
        x: (N, C_in, H_in, W_in)
        weight: (C_out, C_in, K_H, K_W) with square K_H == K_W
        bias: (C_out,)
    Returns:
        y: (N, C_out, H_out, W_out)
    """
    assert x.ndim == 4 and weight.ndim == 4
    N, C_in, H_in, W_in = x.shape
    C_out, C_in_w, K_H, K_W = weight.shape
    assert C_in == C_in_w, "Input channels mismatch between x and weight"
    assert K_H == K_W, "This implementation assumes square kernels"

    H_out = H_in - K_H + 1
    W_out = W_in - K_W + 1
    assert H_out > 0 and W_out > 0, "Invalid conv output size"

    x_contig = x.contiguous()
    w_contig = weight.contiguous()
    if bias is None:
        b_contig = torch.zeros(C_out, device=x.device, dtype=x.dtype)
    else:
        b_contig = bias.contiguous()

    y = torch.empty((N, C_out, H_out, W_out), device=x.device, dtype=x.dtype)

    def grid(meta):
        return (
            triton.cdiv(N * H_out * W_out, meta["BLOCK_M"]),
            triton.cdiv(C_out, meta["BLOCK_N"]),
        )

    conv2d_nchw_kernel[grid](
        x_contig, w_contig, b_contig, y,
        N, C_in, H_in, W_in,
        C_out,
        H_out, W_out,
        x_contig.stride(0), x_contig.stride(1), x_contig.stride(2), x_contig.stride(3),
        w_contig.stride(0), w_contig.stride(1), w_contig.stride(2), w_contig.stride(3),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3),
        K_H=K_H, K_W=K_W,
    )
    return y


def groupnorm_scale_maxpool_clamp_triton(
    x,
    num_groups,
    weight,
    bias,
    scale,
    kernel_size,
    clamp_min,
    clamp_max,
    eps=1e-5,
    stride=None,
):
    """
    Fused GroupNorm + per-channel scale + MaxPool2d + clamp in Triton.

    Args:
        x: (N, C, H, W)
        num_groups: int
        weight: (C,)  GroupNorm gamma
        bias: (C,)    GroupNorm beta
        scale: (C, 1, 1) or broadcastable to (C, H, W)
        kernel_size: int (maxpool kernel size)
        clamp_min, clamp_max: scalar
        stride: int, defaults to kernel_size
    """
    if stride is None:
        stride = kernel_size

    assert x.ndim == 4
    N, C, H, W = x.shape
    assert C % num_groups == 0
    group_size = C // num_groups

    H_out = (H - kernel_size) // stride + 1
    W_out = (W - kernel_size) // stride + 1
    assert H_out > 0 and W_out > 0

    HW = H * W
    HW_out = H_out * W_out
    group_elems = group_size * HW
    group_elems_out = group_size * HW_out

    x_contig = x.contiguous()
    gamma = weight.contiguous().view(C)
    beta = bias.contiguous().view(C)
    scale_flat = scale.contiguous().view(C)

    y = torch.empty((N, C, H_out, W_out), device=x.device, dtype=x.dtype)

    def grid(meta):
        return (N * num_groups,)

    groupnorm_scale_maxpool_clamp_kernel[grid](
        x_contig, gamma, beta, scale_flat, y,
        N, C, H, W,
        H_out, W_out,
        num_groups, group_size,
        HW, HW_out, group_elems, group_elems_out,
        stride,
        eps, clamp_min, clamp_max,
        x_contig.stride(0), x_contig.stride(1), x_contig.stride(2), x_contig.stride(3),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3),
        kernel_size=kernel_size,
    )
    return y


# ------------------------------------------------------------
# 4. ModelNew module
# ------------------------------------------------------------

class ModelNew(nn.Module):
    """
    Triton-based implementation of:
      Conv2d -> GroupNorm -> per-channel scale -> MaxPool2d -> clamp
      with fused GroupNorm + scale + MaxPool + clamp.
    """

    def __init__(self, in_channels, out_channels, kernel_size,
                 num_groups, scale_shape, maxpool_kernel_size,
                 clamp_min, clamp_max):
        super(ModelNew, self).__init__()
        # Conv parameters
        self.weight = nn.Parameter(
            torch.empty(out_channels, in_channels, kernel_size, kernel_size)
        )
        self.bias = nn.Parameter(torch.zeros(out_channels))

        # Match PyTorch Conv2d default initialization
        fan_in = in_channels * kernel_size * kernel_size
        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
        if in_channels > 0:
            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        nn.init.uniform_(self.bias, -bound, bound)

        # GroupNorm parameters
        self.num_groups = num_groups
        self.group_norm_weight = nn.Parameter(torch.ones(out_channels))
        self.group_norm_bias = nn.Parameter(torch.zeros(out_channels))

        # Extra per-channel scale
        self.scale = nn.Parameter(torch.ones(scale_shape))

        # MaxPool and clamp params
        self.maxpool_kernel_size = maxpool_kernel_size
        self.clamp_min = clamp_min
        self.clamp_max = clamp_max

    def forward(self, x):
        # Conv2d
        x = conv2d_triton(x, self.weight, self.bias)
        # Fused GroupNorm + extra scale + MaxPool + clamp
        x = groupnorm_scale_maxpool_clamp_triton(
            x,
            self.num_groups,
            self.group_norm_weight,
            self.group_norm_bias,
            self.scale,
            self.maxpool_kernel_size,
            self.clamp_min,
            self.clamp_max,
            eps=1e-5,
            stride=self.maxpool_kernel_size,
        )
        return x
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl, AND any other modules used (e.g., import math if using math.sqrt)
   2. @triton.jit decorated kernel function(s) — NO continue/break/return inside loops (use masking)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels — THIS CLASS IS REQUIRED
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes
4. Ensure ALL imports are included at the top (common mistake: forgetting `import math`)

```python
# <corrected code>
```
