You are optimizing a Triton kernel based on algorithmic analysis.

# PyTorch Reference (Target Behavior)

```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    A model that performs a convolution, applies tanh, scaling, adds a bias term, and then max-pools.
    """
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scaling_factor = scaling_factor
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.max_pool = nn.MaxPool2d(pool_kernel_size)

    def forward(self, x):
        # Convolution
        x = self.conv(x)
        # Tanh activation
        x = torch.tanh(x)
        # Scaling
        x = x * self.scaling_factor
        # Bias addition
        x = x + self.bias
        # Max-pooling
        x = self.max_pool(x)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 256, 256
kernel_size = 3
scaling_factor = 2.0
bias_shape = (out_channels, 1, 1)
pool_kernel_size = 4

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size]
```

**CRITICAL**: Study the PyTorch code carefully to understand:
- What does `forward()` return? (full output sequence vs final hidden state only)
- What is the computational pattern?
- What are the input/output shapes?

Your optimized kernel MUST match this exact behavior.

---

# Analysis Results

**Bottleneck**: Each output value is computed via explicit loops over C_in and K^2 with scalar loads, repeatedly reloading the same input and weight elements for different output positions and channels. This naive direct-convolution structure has poor data reuse and cannot leverage Triton’s highly-optimized GEMM/tensor-core style tiling.

**Optimization Strategy**: Replace the hand-written direct convolution loops with a blocked GEMM-style convolution (implicit im2col) so that for each (OC_block, P_block) tile you compute conv outputs as a matrix multiply over K = C_in * KERNEL_SIZE^2, then immediately apply tanh, scaling, bias, and pooling in registers.

**Implementation Plan**: Reshape the convolution conceptually into Y_tile[OC, P] = W_tile[OC, K] @ X_tile[K, P], where K = C_in * KERNEL_SIZE^2 and P indexes spatial positions (including the POOL_KERNEL window), and implement this with a Triton matmul-like kernel that iterates over K in blocks, using shared/L2-friendly tiles and vectorized loads. Within each tile, accumulate the matmul result in registers, then apply tanh and scaling, perform max over the POOL_KERNEL×POOL_KERNEL positions for that P-block, add the extra bias, and finally store the pooled outputs. This keeps all intermediate conv activations on-chip while dramatically improving arithmetic intensity and memory coalescing.

**Expected Speedup**: 25-40%

---

# Current Kernel (needs optimization)

```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def fused_conv_tanh_scale_bias_maxpool_kernel(
    x_ptr,               # float*  [N, C_in, H, W]
    w_ptr,               # float*  [C_out, C_in, K, K]
    conv_bias_ptr,       # float*  [C_out]
    extra_bias_ptr,      # float*  [C_out, 1, 1] (we use [C_out])
    y_ptr,               # float*  [N, C_out, H_pool, W_pool]
    N, C_in, H, W,
    C_out,
    H_out, W_out,        # not strictly needed, but passed for generality
    H_pool, W_pool,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_wn, stride_wc, stride_wkh, stride_wkw,
    stride_yn, stride_yc, stride_yh, stride_yw,
    scaling_factor,
    KERNEL_SIZE: tl.constexpr,
    POOL_KERNEL: tl.constexpr,
    POOL_STRIDE: tl.constexpr,
    BLOCK_P: tl.constexpr,
    BLOCK_OC: tl.constexpr,
):
    # Program IDs
    pid_p = tl.program_id(0)   # pooled spatial tiles (flattened H_pool * W_pool)
    pid_co = tl.program_id(1)  # output channel tiles
    pid_n = tl.program_id(2)   # batch index

    # Offsets for pooled spatial positions and output channels
    offs_p = pid_p * BLOCK_P + tl.arange(0, BLOCK_P)
    offs_oc = pid_co * BLOCK_OC + tl.arange(0, BLOCK_OC)

    P = H_pool * W_pool
    mask_p = offs_p < P
    mask_oc = offs_oc < C_out

    # Map flattened pooled index -> (h_pool, w_pool)
    hp = offs_p // W_pool
    wp = offs_p % W_pool

    # Initialize max accumulator with very negative values
    acc_max = tl.full((BLOCK_OC, BLOCK_P), -1e30, dtype=tl.float32)

    # Load conv bias (per output channel), shape: (BLOCK_OC,)
    conv_bias = tl.load(conv_bias_ptr + offs_oc, mask=mask_oc, other=0.0)
    conv_bias = conv_bias.to(tl.float32)

    # Pooling over POOL_KERNEL x POOL_KERNEL
    for pk_h in range(POOL_KERNEL):
        for pk_w in range(POOL_KERNEL):
            # Output coordinates (in conv output space)
            h_out_vec = hp * POOL_STRIDE + pk_h  # (BLOCK_P,)
            w_out_vec = wp * POOL_STRIDE + pk_w  # (BLOCK_P,)

            # Accumulator for this single conv output within the pool window
            conv_acc = tl.zeros((BLOCK_OC, BLOCK_P), dtype=tl.float32)

            # Convolution over input channels and kernel window
            for ci in range(C_in):
                ci_idx = ci
                for kh in range(KERNEL_SIZE):
                    kh_idx = kh
                    for kw in range(KERNEL_SIZE):
                        kw_idx = kw

                        # Input coordinates
                        h_in = h_out_vec + kh_idx       # (BLOCK_P,)
                        w_in = w_out_vec + kw_idx       # (BLOCK_P,)

                        # Input loads: shape (BLOCK_P,)
                        inp_ptrs = (
                            x_ptr
                            + pid_n * stride_xn
                            + ci_idx * stride_xc
                            + h_in * stride_xh
                            + w_in * stride_xw
                        )
                        inp_vals = tl.load(inp_ptrs, mask=mask_p, other=0.0)
                        inp_vals = inp_vals.to(tl.float32)

                        # Weight loads: shape (BLOCK_OC,)
                        w_ptrs = (
                            w_ptr
                            + offs_oc * stride_wn
                            + ci_idx * stride_wc
                            + kh_idx * stride_wkh
                            + kw_idx * stride_wkw
                        )
                        w_vals = tl.load(w_ptrs, mask=mask_oc, other=0.0)
                        w_vals = w_vals.to(tl.float32)

                        # Outer product update: (BLOCK_OC, BLOCK_P)
                        conv_acc += w_vals[:, None] * inp_vals[None, :]

            # Add conv bias
            conv_acc = conv_acc + conv_bias[:, None]

            # Tanh activation: tanh(x) = (exp(2x) - 1) / (exp(2x) + 1)
            tmp = 2.0 * conv_acc
            e = tl.exp(tmp)
            tanh_out = (e - 1.0) / (e + 1.0)

            # Scaling
            conv_out = tanh_out * scaling_factor

            # Max-pooling accumulation
            acc_max = tl.maximum(acc_max, conv_out)

    # Add extra bias (broadcast over spatial dims)
    extra_bias = tl.load(extra_bias_ptr + offs_oc, mask=mask_oc, other=0.0)
    extra_bias = extra_bias.to(tl.float32)
    acc_max = acc_max + extra_bias[:, None]

    # Store results to y: shape [N, C_out, H_pool, W_pool]
    hp_store = hp
    wp_store = wp
    out_ptrs = (
        y_ptr
        + pid_n * stride_yn
        + offs_oc[:, None] * stride_yc
        + hp_store[None, :] * stride_yh
        + wp_store[None, :] * stride_yw
    )
    out_mask = mask_oc[:, None] & mask_p[None, :]

    tl.store(out_ptrs, acc_max, mask=out_mask)


def fused_conv_tanh_scale_bias_maxpool(x, weight, conv_bias, extra_bias, scaling_factor, pool_kernel_size):
    # Ensure contiguity for simple, fast indexing
    x = x.contiguous()
    weight = weight.contiguous()
    conv_bias = conv_bias.contiguous()
    # extra_bias is (C_out, 1, 1); make sure contiguous in memory
    extra_bias = extra_bias.contiguous()

    N, C_in, H, W = x.shape
    C_out = weight.shape[0]
    KERNEL_SIZE = weight.shape[2]

    # Conv output spatial size (no padding, stride=1, dilation=1)
    H_out = H - KERNEL_SIZE + 1
    W_out = W - KERNEL_SIZE + 1

    POOL_KERNEL = pool_kernel_size
    POOL_STRIDE = pool_kernel_size

    # MaxPool2d output size: floor((H_out - Kp)/Sp + 1)
    H_pool = (H_out - POOL_KERNEL) // POOL_STRIDE + 1
    W_pool = (W_out - POOL_KERNEL) // POOL_STRIDE + 1

    y = torch.empty((N, C_out, H_pool, W_pool), device=x.device, dtype=x.dtype)

    P = H_pool * W_pool

    def grid(meta):
        return (
            triton.cdiv(P, meta['BLOCK_P']),
            triton.cdiv(C_out, meta['BLOCK_OC']),
            N,
        )

    fused_conv_tanh_scale_bias_maxpool_kernel[grid](
        x, weight, conv_bias, extra_bias, y,
        N, C_in, H, W,
        C_out,
        H_out, W_out,
        H_pool, W_pool,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        weight.stride(0), weight.stride(1), weight.stride(2), weight.stride(3),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3),
        float(scaling_factor),
        KERNEL_SIZE=KERNEL_SIZE,
        POOL_KERNEL=POOL_KERNEL,
        POOL_STRIDE=POOL_STRIDE,
        BLOCK_P=64,
        BLOCK_OC=64,
    )
    return y


class ModelNew(nn.Module):
    """
    Fused Triton implementation of:
      Conv2d (no padding, stride=1) -> tanh -> scaling -> bias add -> MaxPool2d
    """
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):
        super(ModelNew, self).__init__()
        # Conv parameters (match nn.Conv2d layout: [out_channels, in_channels, kH, kW])
        self.weight = nn.Parameter(
            torch.randn(out_channels, in_channels, kernel_size, kernel_size)
        )
        self.conv_bias = nn.Parameter(torch.randn(out_channels))
        # Extra bias with shape (out_channels, 1, 1)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = float(scaling_factor)
        self.pool_kernel_size = pool_kernel_size

    def forward(self, x):
        return fused_conv_tanh_scale_bias_maxpool(
            x, self.weight, self.conv_bias, self.bias, self.scaling_factor, self.pool_kernel_size
        )
```

---

# Your Task

Implement the optimization strategy above. Focus on the specific bottleneck identified.

## Key Requirements

1. **Preserve correctness**: Maintain the same input/output behavior
2. **Apply the optimization**: Follow the implementation plan exactly
3. **Use valid Triton syntax**:
   - Every kernel MUST have `@triton.jit` decorator
   - Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
   - BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
   - No `continue`, `break`, `return` inside kernels (use masking)
   - Prefer `tl.dot(a, b, allow_tf32=True)` for matmul operations

4. **CRITICAL for RNN/GRU/LSTM Persistent Kernels**:
   - Time loop MUST be inside @triton.jit kernel, NOT in Python forward()
   - **HYBRID computation strategy** (CRITICAL for performance):
     * Precompute input-side gates OUTSIDE kernel: `gates_x = (T*B, In) @ W_ih` (ONE large GEMM)
     * INSIDE kernel: only recurrent-side: `for t: gates_h = h @ W_hh` (T small GEMMs)
   - CORRECT (FAST - use this):
     ```python
     # Python forward():
     gates_x_all = x.reshape(T*B, In) @ W_ih + b_ih  # ONE large GEMM
     gates_x_all = gates_x_all.view(T, B, 3*H)
     gru_persistent_kernel[grid](gates_x_all, h0, W_hh, ...)  # Launch ONCE

     @triton.jit
     def gru_persistent_kernel(gates_x_ptr, h_ptr, W_hh_ptr, ...):
         for t in range(T):  # Inside kernel
             gates_x_t = tl.load(gates_x_ptr + t*...)  # Precomputed
             gates_h = h @ W_hh  # Only recurrent GEMM
             h = (1-z)*n + z*h   # Fuse and update
     ```

5. **Output format**:
   - Imports: `import torch, torch.nn as nn, triton, triton.language as tl`
   - `@triton.jit` kernel(s)
   - Wrapper function(s)
   - `class ModelNew(nn.Module)` — REQUIRED
   - NO testing code, NO `if __name__ == "__main__"`

---

Generate the optimized kernel now. Output ONLY the complete Python code.
