```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def fused_conv_tanh_scale_bias_maxpool_kernel(
    x_ptr,               # float*  [N, C_in, H, W]
    w_ptr,               # float*  [C_out, C_in, K, K]
    conv_bias_ptr,       # float*  [C_out]
    extra_bias_ptr,      # float*  [C_out, 1, 1] (we use [C_out])
    y_ptr,               # float*  [N, C_out, H_pool, W_pool]
    N, C_in, H, W,
    C_out,
    H_out, W_out,        # not strictly needed, but passed for generality
    H_pool, W_pool,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_wn, stride_wc, stride_wkh, stride_wkw,
    stride_yn, stride_yc, stride_yh, stride_yw,
    scaling_factor,
    KERNEL_SIZE: tl.constexpr,
    POOL_KERNEL: tl.constexpr,
    POOL_STRIDE: tl.constexpr,
    BLOCK_P: tl.constexpr,
    BLOCK_OC: tl.constexpr,
):
    # Program IDs
    pid_p = tl.program_id(0)   # pooled spatial tiles (flattened H_pool * W_pool)
    pid_co = tl.program_id(1)  # output channel tiles
    pid_n = tl.program_id(2)   # batch index

    # Offsets for pooled spatial positions and output channels
    offs_p = pid_p * BLOCK_P + tl.arange(0, BLOCK_P)
    offs_oc = pid_co * BLOCK_OC + tl.arange(0, BLOCK_OC)

    P = H_pool * W_pool
    mask_p = offs_p < P
    mask_oc = offs_oc < C_out

    # Map flattened pooled index -> (h_pool, w_pool)
    hp = offs_p // W_pool
    wp = offs_p % W_pool

    # Initialize max accumulator with very negative values
    acc_max = tl.full((BLOCK_OC, BLOCK_P), -1e30, dtype=tl.float32)

    # Load conv bias (per output channel), shape: (BLOCK_OC,)
    conv_bias = tl.load(conv_bias_ptr + offs_oc, mask=mask_oc, other=0.0)
    conv_bias = conv_bias.to(tl.float32)

    # Pooling over POOL_KERNEL x POOL_KERNEL
    for pk_h in range(POOL_KERNEL):
        for pk_w in range(POOL_KERNEL):
            # Output coordinates (in conv output space)
            h_out_vec = hp * POOL_STRIDE + pk_h  # (BLOCK_P,)
            w_out_vec = wp * POOL_STRIDE + pk_w  # (BLOCK_P,)

            # Accumulator for this single conv output within the pool window
            conv_acc = tl.zeros((BLOCK_OC, BLOCK_P), dtype=tl.float32)

            # Convolution over input channels and kernel window
            for ci in range(C_in):
                ci_idx = ci
                for kh in range(KERNEL_SIZE):
                    kh_idx = kh
                    for kw in range(KERNEL_SIZE):
                        kw_idx = kw

                        # Input coordinates
                        h_in = h_out_vec + kh_idx       # (BLOCK_P,)
                        w_in = w_out_vec + kw_idx       # (BLOCK_P,)

                        # Input loads: shape (BLOCK_P,)
                        inp_ptrs = (
                            x_ptr
                            + pid_n * stride_xn
                            + ci_idx * stride_xc
                            + h_in * stride_xh
                            + w_in * stride_xw
                        )
                        inp_vals = tl.load(inp_ptrs, mask=mask_p, other=0.0)
                        inp_vals = inp_vals.to(tl.float32)

                        # Weight loads: shape (BLOCK_OC,)
                        w_ptrs = (
                            w_ptr
                            + offs_oc * stride_wn
                            + ci_idx * stride_wc
                            + kh_idx * stride_wkh
                            + kw_idx * stride_wkw
                        )
                        w_vals = tl.load(w_ptrs, mask=mask_oc, other=0.0)
                        w_vals = w_vals.to(tl.float32)

                        # Outer product update: (BLOCK_OC, BLOCK_P)
                        conv_acc += w_vals[:, None] * inp_vals[None, :]

            # Add conv bias
            conv_acc = conv_acc + conv_bias[:, None]

            # Tanh activation: tanh(x) = (exp(2x) - 1) / (exp(2x) + 1)
            tmp = 2.0 * conv_acc
            e = tl.exp(tmp)
            tanh_out = (e - 1.0) / (e + 1.0)

            # Scaling
            conv_out = tanh_out * scaling_factor

            # Max-pooling accumulation
            acc_max = tl.maximum(acc_max, conv_out)

    # Add extra bias (broadcast over spatial dims)
    extra_bias = tl.load(extra_bias_ptr + offs_oc, mask=mask_oc, other=0.0)
    extra_bias = extra_bias.to(tl.float32)
    acc_max = acc_max + extra_bias[:, None]

    # Store results to y: shape [N, C_out, H_pool, W_pool]
    hp_store = hp
    wp_store = wp
    out_ptrs = (
        y_ptr
        + pid_n * stride_yn
        + offs_oc[:, None] * stride_yc
        + hp_store[None, :] * stride_yh
        + wp_store[None, :] * stride_yw
    )
    out_mask = mask_oc[:, None] & mask_p[None, :]

    tl.store(out_ptrs, acc_max, mask=out_mask)


def fused_conv_tanh_scale_bias_maxpool(x, weight, conv_bias, extra_bias, scaling_factor, pool_kernel_size):
    # Ensure contiguity for simple, fast indexing
    x = x.contiguous()
    weight = weight.contiguous()
    conv_bias = conv_bias.contiguous()
    # extra_bias is (C_out, 1, 1); make sure contiguous in memory
    extra_bias = extra_bias.contiguous()

    N, C_in, H, W = x.shape
    C_out = weight.shape[0]
    KERNEL_SIZE = weight.shape[2]

    # Conv output spatial size (no padding, stride=1, dilation=1)
    H_out = H - KERNEL_SIZE + 1
    W_out = W - KERNEL_SIZE + 1

    POOL_KERNEL = pool_kernel_size
    POOL_STRIDE = pool_kernel_size

    # MaxPool2d output size: floor((H_out - Kp)/Sp + 1)
    H_pool = (H_out - POOL_KERNEL) // POOL_STRIDE + 1
    W_pool = (W_out - POOL_KERNEL) // POOL_STRIDE + 1

    y = torch.empty((N, C_out, H_pool, W_pool), device=x.device, dtype=x.dtype)

    P = H_pool * W_pool

    def grid(meta):
        return (
            triton.cdiv(P, meta['BLOCK_P']),
            triton.cdiv(C_out, meta['BLOCK_OC']),
            N,
        )

    fused_conv_tanh_scale_bias_maxpool_kernel[grid](
        x, weight, conv_bias, extra_bias, y,
        N, C_in, H, W,
        C_out,
        H_out, W_out,
        H_pool, W_pool,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        weight.stride(0), weight.stride(1), weight.stride(2), weight.stride(3),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3),
        float(scaling_factor),
        KERNEL_SIZE=KERNEL_SIZE,
        POOL_KERNEL=POOL_KERNEL,
        POOL_STRIDE=POOL_STRIDE,
        BLOCK_P=64,
        BLOCK_OC=64,
    )
    return y


class ModelNew(nn.Module):
    """
    Fused Triton implementation of:
      Conv2d (no padding, stride=1) -> tanh -> scaling -> bias add -> MaxPool2d
    """
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):
        super(ModelNew, self).__init__()
        # Conv parameters (match nn.Conv2d layout: [out_channels, in_channels, kH, kW])
        self.weight = nn.Parameter(
            torch.randn(out_channels, in_channels, kernel_size, kernel_size)
        )
        self.conv_bias = nn.Parameter(torch.randn(out_channels))
        # Extra bias with shape (out_channels, 1, 1)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = float(scaling_factor)
        self.pool_kernel_size = pool_kernel_size

    def forward(self, x):
        return fused_conv_tanh_scale_bias_maxpool(
            x, self.weight, self.conv_bias, self.bias, self.scaling_factor, self.pool_kernel_size
        )
```