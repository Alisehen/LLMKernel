```python
import torch
import torch.nn as nn
import triton
import triton.language as tl
from typing import Tuple


@triton.autotune(
    configs=[
        triton.Config({'num_warps': 4, 'num_stages': 3}),
        triton.Config({'num_warps': 8, 'num_stages': 3}),
        triton.Config({'num_warps': 4, 'num_stages': 4}),
        triton.Config({'num_warps': 8, 'num_stages': 4}),
        triton.Config({'num_warps': 2, 'num_stages': 3}),
        triton.Config({'num_warps': 4, 'num_stages': 2}),
    ],
    key=['n_elements', 'C', 'H', 'W'],
)
@triton.jit
def fused_activation_kernel(
    x_ptr,
    bias_ptr,
    output_ptr,
    scaling_factor,
    n_elements,
    C: tl.constexpr,
    H: tl.constexpr,
    W: tl.constexpr,
    BLOCK_SIZE: tl.constexpr,
    num_warps: tl.constexpr,
    num_stages: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    
    x = tl.load(x_ptr + offsets, mask=mask)
    
    # Optimized tanh computation with improved precision
    x2 = 2.0 * x
    # Use clamp to prevent overflow in exp
    x2_clamped = tl.minimum(tl.maximum(x2, -88.0), 88.0)
    exp_2x = tl.exp(x2_clamped)
    
    # tanh(x) = (exp(2x) - 1) / (exp(2x) + 1)
    tanh_x = (exp_2x - 1.0) / (exp_2x + 1.0)
    
    # Compute bias index with integer division
    HW = H * W
    c_idx = (offsets // HW) % C
    bias = tl.load(bias_ptr + c_idx)
    
    activated = tanh_x * scaling_factor + bias
    tl.store(output_ptr + offsets, activated, mask=mask)


@triton.autotune(
    configs=[
        triton.Config({'num_warps': 4, 'num_stages': 3}),
        triton.Config({'num_warps': 8, 'num_stages': 3}),
        triton.Config({'num_warps': 4, 'num_stages': 4}),
        triton.Config({'num_warps': 2, 'num_stages': 3}),
        triton.Config({'num_warps': 8, 'num_stages': 2}),
        triton.Config({'num_warps': 4, 'num_stages': 2}),
    ],
    key=['B', 'C', 'H', 'W', 'pool_h', 'pool_w'],
)
@triton.jit
def max_pool_2d_kernel(
    input_ptr,
    output_ptr,
    B, C, H, W,
    pool_h, pool_w,
    stride_h, stride_w,
    output_h, output_w,
    BLOCK_SIZE: tl.constexpr,
    num_warps: tl.constexpr,
    num_stages: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    num_programs = tl.num_programs(axis=0)
    
    output_size = B * C * output_h * output_w
    positions_per_program = tl.cdiv(output_size, num_programs)
    start_pos = pid * positions_per_program
    end_pos = min(start_pos + positions_per_program, output_size)
    
    # Pre-compute output dimensions
    output_cw = output_h * output_w
    input_hw = H * W
    input_chw = C * input_hw
    
    for pos in range(start_pos, end_pos, BLOCK_SIZE):
        offsets = pos + tl.arange(0, BLOCK_SIZE)
        mask = offsets < end_pos
        
        # Compute output indices with integer arithmetic
        w_idx = offsets % output_w
        h_idx = (offsets // output_w) % output_h
        c_idx = (offsets // output_cw) % C
        b_idx = offsets // (output_cw * C)
        
        # Input window indices
        h_start = h_idx * stride_h
        w_start = w_idx * stride_w
        h_end = min(h_start + pool_h, H)
        w_end = min(w_start + pool_w, W)
        
        # Initialize with negative infinity
        min_val = tl.full((BLOCK_SIZE,), float('-inf'), dtype=tl.float32)
        max_val = min_val
        
        # Optimized pooling with early exit when possible
        pool_area = pool_h * pool_w
        
        # Use vector operations when possible
        for ph in range(pool_h):
            ph_actual = h_start + ph
            if ph_actual < h_end:
                # Pre-compute row offset for this height
                row_offset = ph_actual * W
                for pw in range(pool_w):
                    pw_actual = w_start + pw
                    if pw_actual < w_end:
                        # Calculate input index with integer arithmetic
                        input_idx = (b_idx * input_chw + 
                                    c_idx * input_hw + 
                                    row_offset + 
                                    pw_actual)
                        
                        # Load value with bounds checking
                        val = tl.load(input_ptr + input_idx, mask=mask)
                        
                        # Vectorized max operation
                        max_val = tl.maximum(val, max_val)
        
        # Calculate output index and store
        output_idx = (b_idx * C * output_cw + 
                     c_idx * output_cw + 
                     h_idx * output_w + 
                     w_idx)
        tl.store(output_ptr + output_idx, max_val, mask=mask)


def triton_fused_activation(x: torch.Tensor, scaling_factor: float, bias: torch.Tensor) -> torch.Tensor:
    # Flatten bias to (C,) for proper broadcasting
    if bias.dim() > 1:
        bias = bias.view(-1)
    
    output = torch.empty_like(x)
    n_elements = output.numel()
    B, C, H, W = x.shape
    
    # Optimize BLOCK_SIZE for elementwise operations
    BLOCK_SIZE = min(1024, triton.next_power_of_2(H * W))
    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)
    
    fused_activation_kernel[grid](
        x, bias, output, scaling_factor, n_elements,
        C, H, W,
        BLOCK_SIZE=BLOCK_SIZE
    )
    return output


def triton_max_pool2d(x: torch.Tensor, kernel_size: int) -> torch.Tensor:
    B, C, H, W = x.shape
    stride = kernel_size
    pool_h = pool_w = kernel_size
    
    output_h = H // stride
    output_w = W // stride
    output = torch.empty((B, C, output_h, output_w), device=x.device, dtype=x.dtype)
    
    if output_h <= 0 or output_w <= 0:
        raise ValueError(f"Invalid output dimensions: {output_h}x{output_w}")
    
    # Use 1D grid with optimized block size
    BLOCK_SIZE = min(256, triton.next_power_of_2(output_h * output_w))
    output_size = B * C * output_h * output_w
    grid_size = min(65535, triton.cdiv(output_size, BLOCK_SIZE))
    grid = (max(1, grid_size),)
    
    max_pool_2d_kernel[grid](
        x, output,
        B, C, H, W,
        pool_h, pool_w,
        stride, stride,
        output_h, output_w,
        BLOCK_SIZE=BLOCK_SIZE
    )
    return output


class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scaling_factor = scaling_factor
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.pool_kernel_size = pool_kernel_size
        
    def forward(self, x):
        # Use PyTorch convolution for stability
        x = self.conv(x)
        
        # Fused activation (tanh + scaling + bias)
        x = triton_fused_activation(x, self.scaling_factor, self.bias)
        
        # Max pooling
        x = triton_max_pool2d(x, self.pool_kernel_size)
        
        return x
```

## Key Optimizations Applied:

1. **Added Autotuning**: Both kernels now have 6 configurations each exploring different `num_warps` (2,4,8) and `num_stages` (2,3,4) combinations to maximize SM throughput.

2. **Fused Activation Kernel**:
   - Added numerical stability with clamping to prevent overflow in `exp()`
   - Optimized integer division for bias index calculation
   - Multiple warp configurations to improve occupancy

3. **Max Pool Kernel**:
   - Optimized index calculations to minimize integer operations
   - Used `tl.maximum()` instead of `tl.where()` for cleaner max operations
   - Added bounds checking in pooling loops to avoid unnecessary iterations
   - More aggressive warp configurations (up to 8 warps) to handle memory-bound nature

4. **Performance Targeting**:
   - Both kernels target the memory-bound issues identified by NCU metrics
   - Increased warp counts where occupancy allows (especially for max pool)
   - Varied pipeline stages to balance register pressure and latency hiding

The autotuning will automatically select the best configuration based on the input sizes, targeting the observed bottlenecks in the NCU metrics.