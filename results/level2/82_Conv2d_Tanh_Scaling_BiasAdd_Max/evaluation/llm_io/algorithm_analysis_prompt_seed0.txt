You are a GPU kernel optimization architect. Analyze the kernel and identify **ONE high-level algorithmic optimization**.

# PyTorch Reference
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    A model that performs a convolution, applies tanh, scaling, adds a bias term, and then max-pools.
    """
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scaling_factor = scaling_factor
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.max_pool = nn.MaxPool2d(pool_kernel_size)

    def forward(self, x):
        # Convolution
        x = self.conv(x)
        # Tanh activation
        x = torch.tanh(x)
        # Scaling
        x = x * self.scaling_factor
        # Bias addition
        x = x + self.bias
        # Max-pooling
        x = self.max_pool(x)
        return x

batch_size = 128
in_channels = 8
out_channels = 64
height, width = 256, 256
kernel_size = 3
scaling_factor = 2.0
bias_shape = (out_channels, 1, 1)
pool_kernel_size = 4

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size]
```

# Current Triton Kernel
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def fused_conv_tanh_scale_bias_maxpool_kernel(
    x_ptr,               # float*  [N, C_in, H, W]
    w_ptr,               # float*  [C_out, C_in, K, K]
    conv_bias_ptr,       # float*  [C_out]
    extra_bias_ptr,      # float*  [C_out, 1, 1] (we use [C_out])
    y_ptr,               # float*  [N, C_out, H_pool, W_pool]
    N, C_in, H, W,
    C_out,
    H_out, W_out,        # not strictly needed, but passed for generality
    H_pool, W_pool,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_wn, stride_wc, stride_wkh, stride_wkw,
    stride_yn, stride_yc, stride_yh, stride_yw,
    scaling_factor,
    KERNEL_SIZE: tl.constexpr,
    POOL_KERNEL: tl.constexpr,
    POOL_STRIDE: tl.constexpr,
    BLOCK_P: tl.constexpr,
    BLOCK_OC: tl.constexpr,
):
    # Program IDs
    pid_p = tl.program_id(0)   # pooled spatial tiles (flattened H_pool * W_pool)
    pid_co = tl.program_id(1)  # output channel tiles
    pid_n = tl.program_id(2)   # batch index

    # Offsets for pooled spatial positions and output channels
    offs_p = pid_p * BLOCK_P + tl.arange(0, BLOCK_P)
    offs_oc = pid_co * BLOCK_OC + tl.arange(0, BLOCK_OC)

    P = H_pool * W_pool
    mask_p = offs_p < P
    mask_oc = offs_oc < C_out

    # Map flattened pooled index -> (h_pool, w_pool)
    hp = offs_p // W_pool
    wp = offs_p % W_pool

    # Initialize max accumulator with very negative values
    acc_max = tl.full((BLOCK_OC, BLOCK_P), -1e30, dtype=tl.float32)

    # Load conv bias (per output channel), shape: (BLOCK_OC,)
    conv_bias = tl.load(conv_bias_ptr + offs_oc, mask=mask_oc, other=0.0)
    conv_bias = conv_bias.to(tl.float32)

    # Pooling over POOL_KERNEL x POOL_KERNEL
    for pk_h in range(POOL_KERNEL):
        for pk_w in range(POOL_KERNEL):
            # Output coordinates (in conv output space)
            h_out_vec = hp * POOL_STRIDE + pk_h  # (BLOCK_P,)
            w_out_vec = wp * POOL_STRIDE + pk_w  # (BLOCK_P,)

            # Accumulator for this single conv output within the pool window
            conv_acc = tl.zeros((BLOCK_OC, BLOCK_P), dtype=tl.float32)

            # Convolution over input channels and kernel window
            for ci in range(C_in):
                ci_idx = ci
                for kh in range(KERNEL_SIZE):
                    kh_idx = kh
                    for kw in range(KERNEL_SIZE):
                        kw_idx = kw

                        # Input coordinates
                        h_in = h_out_vec + kh_idx       # (BLOCK_P,)
                        w_in = w_out_vec + kw_idx       # (BLOCK_P,)

                        # Input loads: shape (BLOCK_P,)
                        inp_ptrs = (
                            x_ptr
                            + pid_n * stride_xn
                            + ci_idx * stride_xc
                            + h_in * stride_xh
                            + w_in * stride_xw
                        )
                        inp_vals = tl.load(inp_ptrs, mask=mask_p, other=0.0)
                        inp_vals = inp_vals.to(tl.float32)

                        # Weight loads: shape (BLOCK_OC,)
                        w_ptrs = (
                            w_ptr
                            + offs_oc * stride_wn
                            + ci_idx * stride_wc
                            + kh_idx * stride_wkh
                            + kw_idx * stride_wkw
                        )
                        w_vals = tl.load(w_ptrs, mask=mask_oc, other=0.0)
                        w_vals = w_vals.to(tl.float32)

                        # Outer product update: (BLOCK_OC, BLOCK_P)
                        conv_acc += w_vals[:, None] * inp_vals[None, :]

            # Add conv bias
            conv_acc = conv_acc + conv_bias[:, None]

            # Tanh activation: tanh(x) = (exp(2x) - 1) / (exp(2x) + 1)
            tmp = 2.0 * conv_acc
            e = tl.exp(tmp)
            tanh_out = (e - 1.0) / (e + 1.0)

            # Scaling
            conv_out = tanh_out * scaling_factor

            # Max-pooling accumulation
            acc_max = tl.maximum(acc_max, conv_out)

    # Add extra bias (broadcast over spatial dims)
    extra_bias = tl.load(extra_bias_ptr + offs_oc, mask=mask_oc, other=0.0)
    extra_bias = extra_bias.to(tl.float32)
    acc_max = acc_max + extra_bias[:, None]

    # Store results to y: shape [N, C_out, H_pool, W_pool]
    hp_store = hp
    wp_store = wp
    out_ptrs = (
        y_ptr
        + pid_n * stride_yn
        + offs_oc[:, None] * stride_yc
        + hp_store[None, :] * stride_yh
        + wp_store[None, :] * stride_yw
    )
    out_mask = mask_oc[:, None] & mask_p[None, :]

    tl.store(out_ptrs, acc_max, mask=out_mask)


def fused_conv_tanh_scale_bias_maxpool(x, weight, conv_bias, extra_bias, scaling_factor, pool_kernel_size):
    # Ensure contiguity for simple, fast indexing
    x = x.contiguous()
    weight = weight.contiguous()
    conv_bias = conv_bias.contiguous()
    # extra_bias is (C_out, 1, 1); make sure contiguous in memory
    extra_bias = extra_bias.contiguous()

    N, C_in, H, W = x.shape
    C_out = weight.shape[0]
    KERNEL_SIZE = weight.shape[2]

    # Conv output spatial size (no padding, stride=1, dilation=1)
    H_out = H - KERNEL_SIZE + 1
    W_out = W - KERNEL_SIZE + 1

    POOL_KERNEL = pool_kernel_size
    POOL_STRIDE = pool_kernel_size

    # MaxPool2d output size: floor((H_out - Kp)/Sp + 1)
    H_pool = (H_out - POOL_KERNEL) // POOL_STRIDE + 1
    W_pool = (W_out - POOL_KERNEL) // POOL_STRIDE + 1

    y = torch.empty((N, C_out, H_pool, W_pool), device=x.device, dtype=x.dtype)

    P = H_pool * W_pool

    def grid(meta):
        return (
            triton.cdiv(P, meta['BLOCK_P']),
            triton.cdiv(C_out, meta['BLOCK_OC']),
            N,
        )

    fused_conv_tanh_scale_bias_maxpool_kernel[grid](
        x, weight, conv_bias, extra_bias, y,
        N, C_in, H, W,
        C_out,
        H_out, W_out,
        H_pool, W_pool,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        weight.stride(0), weight.stride(1), weight.stride(2), weight.stride(3),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3),
        float(scaling_factor),
        KERNEL_SIZE=KERNEL_SIZE,
        POOL_KERNEL=POOL_KERNEL,
        POOL_STRIDE=POOL_STRIDE,
        BLOCK_P=64,
        BLOCK_OC=64,
    )
    return y


class ModelNew(nn.Module):
    """
    Fused Triton implementation of:
      Conv2d (no padding, stride=1) -> tanh -> scaling -> bias add -> MaxPool2d
    """
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):
        super(ModelNew, self).__init__()
        # Conv parameters (match nn.Conv2d layout: [out_channels, in_channels, kH, kW])
        self.weight = nn.Parameter(
            torch.randn(out_channels, in_channels, kernel_size, kernel_size)
        )
        self.conv_bias = nn.Parameter(torch.randn(out_channels))
        # Extra bias with shape (out_channels, 1, 1)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = float(scaling_factor)
        self.pool_kernel_size = pool_kernel_size

    def forward(self, x):
        return fused_conv_tanh_scale_bias_maxpool(
            x, self.weight, self.conv_bias, self.bias, self.scaling_factor, self.pool_kernel_size
        )
```

# Performance
- **PyTorch baseline**: 24.05 ms
- **Current Triton**: 20.28 ms
- **Current speedup**: 1.19x (+15.7% vs baseline)


---

## Analysis Steps

1. **Code Analysis**: Count kernels, identify operations, check for inefficiencies
2. **Performance Diagnosis**: Use metrics/latency to identify bottleneck type
3. **Root Cause**: Combine code + performance to find the core issue

## Optimization Categories (pick ONE if worth optimizing):

### 1. Operator Fusion
Fuse consecutive ops into fewer kernels to reduce memory traffic and launch overhead.

### 2. Algorithm Replacement
Replace naive algorithm with optimized variant.
- For Attention: Flash Attention, online softmax
- For Convolution: Winograd, im2col
- **For RNN/GRU/LSTM**: Persistent kernel with HYBRID computation
  - **CRITICAL**: Use hybrid approach for best performance:
    * Precompute input-side gates ONCE (outside kernel): `gates_x = (T*B, In) @ W_ih`
    * Persistent kernel (inside): only recurrent-side: `for t: gates_h = h @ W_hh`
  - Time loop `for t in range(T)` must be inside kernel, NOT in Python
  - Launch kernel once per layer, not once per timestep
  - Expected speedup: 10-100x (vs per-timestep launches)

### 3. Kernel Launch Reduction
Combine multiple small kernels to reduce overhead.
- **For RNN/GRU/LSTM**: See "Algorithm Replacement" above for persistent kernel approach

### 4. Memory Layout Optimization
Use in-place operations, buffer reuse, or better layouts.

## Should We Optimize?

Before proposing optimization, determine if it's worthwhile:
- **Not worth optimizing** if:
  - Code is already near-optimal (expected speedup < 10%)
  - Bottleneck cannot be addressed (hardware limited, already optimal algorithm)
  - Optimization would add significant complexity with minimal gain

- **Worth optimizing** if:
  - Clear algorithmic inefficiency exists (multiple kernels, suboptimal algorithm)
  - Expected speedup >= 20%
  - Concrete optimization path available

## Output (JSON)

```json
{
  "worth_optimizing": "yes/no",
  "reason": "<Why worth or not worth optimizing, 1 sentence>",
  "bottleneck": "<Root cause in 1-2 sentences, empty if not worth optimizing>",
  "optimisation method": "<Specific optimization in 1-2 sentences, empty if not worth optimizing>",
  "modification plan": "<Implementation steps in 2-3 sentences, empty if not worth optimizing>",
  "expected_speedup": "<e.g., '30-40%', empty if not worth optimizing>"
}
```

Return JSON only.
