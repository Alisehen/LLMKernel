```python
import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def fused_activation_kernel(
    x_ptr,
    output_ptr,
    M,  # Batch * Channels
    N,  # Height * Width
    stride_m,
    stride_n,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    """Optimized 2D kernel with better grid layout for 4D tensors"""
    # 2D grid: (batch*channels, height*width)
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    # Create offsets for 2D block
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    
    # Create masks for boundaries
    mask_m = rm < M
    mask_n = rn < N
    
    # Load input with 2D indexing
    x_ptrs = x_ptr + rm[:, None] * stride_m + rn[None, :] * stride_n
    mask = mask_m[:, None] & mask_n[None, :]
    x = tl.load(x_ptrs, mask=mask, other=0.0)
    
    # Compute fused activation
    # Optimized: Use tl.where and tl.log1p for better numerical stability
    is_positive = x > 0
    exp_neg_x = tl.exp(-tl.where(is_positive, x, 0.0))
    exp_x = tl.exp(tl.where(~is_positive, x, 0.0))
    
    log1p_exp_x = tl.where(
        is_positive,
        x + tl.log1p(exp_neg_x),  # log1p is more accurate for small values
        tl.log1p(exp_x)
    )
    
    # Use built-in tanh for better performance
    tanh_val = tl.tanh(log1p_exp_x)
    
    # Multiply with original x
    output = x * tanh_val
    
    # Store result
    tl.store(x_ptrs, output, mask=mask)

def triton_fused_activation(x: torch.Tensor) -> torch.Tensor:
    """Wrapper with 2D grid for 4D tensors"""
    # Reshape 4D tensor to (batch*channels, height*width)
    B, C, H, W = x.shape
    M = B * C
    N = H * W
    
    # Create output tensor
    output = torch.empty_like(x)
    
    # Use optimized 2D grid layout
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_M']),
        triton.cdiv(N, META['BLOCK_N']),
    )
    
    # Launch kernel with optimized block sizes
    fused_activation_kernel[grid](
        x, output, M, N,
        x.stride(0) * C,  # stride for batch*channels dimension
        x.stride(2),      # stride for spatial dimensions
        BLOCK_M=32,       # Optimized for spatial locality
        BLOCK_N=64        # Good balance for Ada Lovelace
    )
    
    return output


class ModelNew(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, eps=1e-5, momentum=0.1):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bn = nn.BatchNorm2d(out_channels, eps=eps, momentum=momentum)

    def forward(self, x):
        # Apply convolution
        x = self.conv(x)
        
        # Apply fused activation using optimized Triton kernel
        x = triton_fused_activation(x)
        
        # Apply batch normalization
        x = self.bn(x)
        return x
```

## Optimization Rationale

Based on the NCU metrics showing **24.96% SM throughput** and **127,008 grid size**, here are the key optimizations:

1. **2D Grid Layout**: Changed from 1D to 2D grid to better match the 4D tensor structure (batch×channels × height×width). This improves cache locality and reduces grid size from 127,008 to more optimal 2D configuration.

2. **Block Size Optimization**: 
   - BLOCK_M=32: Good for spatial locality across batch/channels dimension
   - BLOCK_N=64: Balances register pressure and warp utilization for Ada Lovelace

3. **Performance Improvements**:
   - Reduced grid dimensions for better SM occupancy
   - Better memory access patterns with 2D blocking
   - Used `tl.log1p()` for numerical stability and performance
   - Used built-in `tl.tanh()` instead of manual computation

4. **Expected Impact**:
   - Higher SM throughput (>60% target) through better grid utilization
   - Reduced kernel launch overhead with optimal block sizes
   - Better cache locality through 2D memory access patterns

The 2D grid layout exploits the natural parallelism in 4D tensors, allowing better utilization of the 128 SMs on RTX 4090 while maintaining correctness through proper boundary masking.