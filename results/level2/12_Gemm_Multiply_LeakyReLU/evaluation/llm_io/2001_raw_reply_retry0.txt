```python
import torch, torch.nn as nn, triton, triton.language as tl
import math


@triton.jit
def epilogue_mul_bias_leakyrelu_kernel(
    c_ptr, bias_ptr,
    M, N,
    stride_cm, stride_cn,
    multiplier, negative_slope,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,
):
    """
    Epilogue kernel:
      C[M, N] = LeakyReLU((C[M, N] + bias[N]) * multiplier)

    C is assumed to be the output of a high-performance GEMM (cuBLAS/cuBLASLt via PyTorch).
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    # Pointers into C
    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)

    # Load C tile
    c = tl.load(c_ptrs, mask=mask, other=0.0)

    # Load bias[N] and broadcast across rows
    bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)
    c = c + bias[None, :]

    # Multiply by scalar
    c = c * multiplier

    # LeakyReLU: x if x >= 0 else negative_slope * x
    zero = 0.0
    pos = tl.maximum(c, zero)
    neg = tl.minimum(c, zero) * negative_slope
    c = pos + neg

    # Store result back to C
    tl.store(c_ptrs, c, mask=mask)


def fused_gemm_mul_leakyrelu(x, weight, bias, multiplier, negative_slope):
    """
    Compute:
      y = x @ weight
      y = (y + bias) * multiplier
      y = LeakyReLU(y, negative_slope)

    x:      [M, K]  (batch_size, in_features)
    weight: [K, N]  (in_features, out_features)
    bias:   [N]     (out_features)
    """
    assert x.is_cuda and weight.is_cuda and bias.is_cuda, "Inputs must be CUDA tensors"

    M, K = x.shape
    Kw, N = weight.shape
    assert Kw == K, "Incompatible shapes for matmul"

    # High-performance GEMM via cuBLAS/cuBLASLt (through PyTorch)
    # x @ weight -> [M, N]
    out = x @ weight

    # Fused epilogue (bias + multiplier + LeakyReLU) in a Triton kernel
    grid = lambda META: (
        triton.cdiv(M, META["BLOCK_M"]),
        triton.cdiv(N, META["BLOCK_N"]),
    )

    epilogue_mul_bias_leakyrelu_kernel[grid](
        out, bias,
        M, N,
        out.stride(0), out.stride(1),
        float(multiplier), float(negative_slope),
        BLOCK_M=128, BLOCK_N=128,
        num_warps=4,
        num_stages=1,
    )

    return out


class ModelNew(nn.Module):
    """
    High-performance replacement for:

        nn.Linear(in_features, out_features)  # with bias
        x = x * multiplier
        x = nn.LeakyReLU(negative_slope)(x)

    Forward:
        x: [batch_size, in_features]
        returns: [batch_size, out_features]
    """

    def __init__(self, in_features, out_features, multiplier, negative_slope):
        super(ModelNew, self).__init__()
        # Store weight as [in_features, out_features] so that we can call x @ weight
        # directly without transposing in the hot path.
        self.weight = nn.Parameter(torch.empty(in_features, out_features))
        self.bias = nn.Parameter(torch.empty(out_features))

        # Initialize like nn.Linear(in_features, out_features)
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
        nn.init.uniform_(self.bias, -bound, bound)

        self.multiplier = float(multiplier)
        self.negative_slope = float(negative_slope)

    def forward(self, x):
        # x: [batch_size, in_features], must be CUDA tensor
        return fused_gemm_mul_leakyrelu(
            x, self.weight, self.bias,
            self.multiplier, self.negative_slope,
        )
```