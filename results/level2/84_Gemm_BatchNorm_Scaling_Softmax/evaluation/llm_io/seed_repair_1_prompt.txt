Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 538, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_114800_batch_range81to100_deepseek_deepseek/84_Gemm_BatchNorm_Scaling_Softmax/code/kernel_20251214_122334.py", line 316, in forward
    x = fused_bn_scale_softmax_wrapper(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_114800_batch_range81to100_deepseek_deepseek/84_Gemm_BatchNorm_Scaling_Softmax/code/kernel_20251214_122334.py", line 253, in fused_bn_scale_softmax_wrapper
    fused_bn_scale_softmax_kernel[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 278, in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 81, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 55:23:
    mean = sum_val / count
    variance = (sum_sq / count) - (mean * mean)

    # Load normalization parameters
    gamma = tl.load(gamma_ptr + col_offs, mask=col_mask, other=1.0)
    beta = tl.load(beta_ptr + col_offs, mask=col_mask, other=0.0)
    running_mean = tl.load(running_mean_ptr + col_offs, mask=col_mask, other=0.0)
    running_var = tl.load(running_var_ptr + col_offs, mask=col_mask, other=1.0)
    scale = tl.load(scale_ptr)

    # Update running statistics (inference mode)
    running_mean_new = running_mean * (1 - momentum) + mean[None, :] * momentum
                       ^
ValueError('Cannot make_shape_compatible: incompatible dimensions at index 1: 256 and 64')

Main Critical Problem Analysis:
Problem Analysis (from expert diagnosis):
critical_issue: Running statistics update uses row-wise mean (BLOCK_M=64) with column-wise running_mean (BLOCK_N=256), causing shape mismatch.
why_it_matters: BatchNorm running statistics are per-feature (columns), but kernel incorrectly tries to update with per-row statistics.
minimal_fix_hint: Remove running statistics update from kernel (inference-only) or implement proper column-wise mean reduction.

Focus your fix on addressing the identified critical issue.


Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl
import math

# -----------------------------------------------------------------------------
# Optimized Gemm kernel
# -----------------------------------------------------------------------------
@triton.jit
def gemm_kernel(
    a_ptr, b_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
    USE_TF32: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    # Create offset pointers
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)
    
    # Pointer arithmetic for A and B matrices
    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn
    
    # Initialize accumulator
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    # Main computation loop
    for k in range(0, K, BLOCK_K):
        # Load A and B tiles with masking
        a = tl.load(a_ptrs, 
                    mask=(offs_m[:, None] < M) & (offs_k[None, :] < K - k), 
                    other=0.0)
        b = tl.load(b_ptrs,
                    mask=(offs_k[:, None] < K - k) & (offs_n[None, :] < N),
                    other=0.0)
        
        # Matrix multiplication with TF32 support
        acc += tl.dot(a, b, allow_tf32=USE_TF32)
        
        # Update pointers for next iteration
        a_ptrs += BLOCK_K * stride_ak
        b_ptrs += BLOCK_K * stride_bk
    
    # Store result with masking
    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    tl.store(c_ptrs, acc, 
             mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))

# -----------------------------------------------------------------------------
# Fused BatchNorm + Scale + Softmax kernel
# -----------------------------------------------------------------------------
@triton.jit
def fused_bn_scale_softmax_kernel(
    x_ptr, gamma_ptr, beta_ptr, running_mean_ptr, running_var_ptr,
    scale_ptr, output_ptr,
    M, N,
    stride_xm, stride_xn, stride_output_m, stride_output_n,
    eps, momentum,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,
):
    pid = tl.program_id(0)
    
    # Row block offsets
    row_start = pid * BLOCK_M
    row_end = tl.minimum(row_start + BLOCK_M, M)
    row_offs = row_start + tl.arange(0, BLOCK_M)
    row_mask = row_offs < M
    
    # Column offsets
    col_offs = tl.arange(0, BLOCK_N)
    col_mask = col_offs < N
    
    # Pointers for loading input
    x_ptrs = x_ptr + row_offs[:, None] * stride_xm + col_offs[None, :] * stride_xn
    
    # Initialize accumulators for mean and variance computation
    sum_val = tl.zeros((BLOCK_M,), dtype=tl.float32)
    sum_sq = tl.zeros((BLOCK_M,), dtype=tl.float32)
    count = N
    
    # First pass: compute mean and variance for each row
    for start_col in range(0, N, BLOCK_N):
        col_offs = start_col + tl.arange(0, BLOCK_N)
        col_mask = col_offs < N
        
        # Load tile
        x_tile = tl.load(x_ptrs + col_offs[None, :] * stride_xn,
                        mask=row_mask[:, None] & col_mask[None, :],
                        other=0.0)
        
        # Accumulate sum and sum of squares
        sum_val += tl.sum(x_tile, axis=1)
        sum_sq += tl.sum(x_tile * x_tile, axis=1)
    
    # Compute mean and variance
    mean = sum_val / count
    variance = (sum_sq / count) - (mean * mean)
    
    # Load normalization parameters
    gamma = tl.load(gamma_ptr + col_offs, mask=col_mask, other=1.0)
    beta = tl.load(beta_ptr + col_offs, mask=col_mask, other=0.0)
    running_mean = tl.load(running_mean_ptr + col_offs, mask=col_mask, other=0.0)
    running_var = tl.load(running_var_ptr + col_offs, mask=col_mask, other=1.0)
    scale = tl.load(scale_ptr)
    
    # Update running statistics (inference mode)
    running_mean_new = running_mean * (1 - momentum) + mean[None, :] * momentum
    running_var_new = running_var * (1 - momentum) + variance[None, :] * momentum
    
    # Compute normalization parameters
    inv_std = 1.0 / tl.sqrt(running_var_new + eps)
    norm_factor = gamma * inv_std
    norm_bias = beta - running_mean_new * norm_factor
    
    # Second pass: apply normalization, scaling, and compute max for softmax
    max_val = tl.full((BLOCK_M,), float('-inf'), dtype=tl.float32)
    
    for start_col in range(0, N, BLOCK_N):
        col_offs = start_col + tl.arange(0, BLOCK_N)
        col_mask = col_offs < N
        
        # Load tile
        x_tile = tl.load(x_ptrs + col_offs[None, :] * stride_xn,
                        mask=row_mask[:, None] & col_mask[None, :],
                        other=0.0)
        
        # Apply batch normalization and scaling
        norm_gamma = tl.load(gamma_ptr + col_offs, mask=col_mask, other=1.0)
        norm_beta = tl.load(beta_ptr + col_offs, mask=col_mask, other=0.0)
        r_mean = tl.load(running_mean_ptr + col_offs, mask=col_mask, other=0.0)
        r_var = tl.load(running_var_ptr + col_offs, mask=col_mask, other=1.0)
        
        inv_std = 1.0 / tl.sqrt(r_var + eps)
        normalized = (x_tile - r_mean[None, :]) * inv_std[None, :] * norm_gamma[None, :] + norm_beta[None, :]
        scaled = normalized * scale
        
        # Update max for softmax
        max_val = tl.maximum(max_val, tl.max(scaled, axis=1))
        
        # Store intermediate result (we'll need it for softmax)
        intermediate_ptrs = output_ptr + row_offs[:, None] * stride_output_m + col_offs[None, :] * stride_output_n
        tl.store(intermediate_ptrs, scaled,
                mask=row_mask[:, None] & col_mask[None, :])
    
    # Third pass: compute softmax
    sum_exp = tl.zeros((BLOCK_M,), dtype=tl.float32)
    
    for start_col in range(0, N, BLOCK_N):
        col_offs = start_col + tl.arange(0, BLOCK_N)
        col_mask = col_offs < N
        
        # Load scaled values
        scaled_ptrs = output_ptr + row_offs[:, None] * stride_output_m + col_offs[None, :] * stride_output_n
        scaled = tl.load(scaled_ptrs,
                        mask=row_mask[:, None] & col_mask[None, :],
                        other=0.0)
        
        # Compute exponentials (subtract max for numerical stability)
        exp_vals = tl.exp(scaled - max_val[:, None])
        
        # Accumulate sum for softmax denominator
        sum_exp += tl.sum(exp_vals, axis=1)
    
    # Fourth pass: apply softmax normalization
    for start_col in range(0, N, BLOCK_N):
        col_offs = start_col + tl.arange(0, BLOCK_N)
        col_mask = col_offs < N
        
        # Load scaled values
        scaled_ptrs = output_ptr + row_offs[:, None] * stride_output_m + col_offs[None, :] * stride_output_n
        scaled = tl.load(scaled_ptrs,
                        mask=row_mask[:, None] & col_mask[None, :],
                        other=0.0)
        
        # Compute final softmax
        exp_vals = tl.exp(scaled - max_val[:, None])
        softmax_out = exp_vals / sum_exp[:, None]
        
        # Store final result
        output_ptrs = output_ptr + row_offs[:, None] * stride_output_m + col_offs[None, :] * stride_output_n
        tl.store(output_ptrs, softmax_out,
                mask=row_mask[:, None] & col_mask[None, :])

# -----------------------------------------------------------------------------
# Wrapper functions
# -----------------------------------------------------------------------------
def gemm_wrapper(x, weight):
    """Optimized matrix multiplication wrapper."""
    M, K = x.shape
    N = weight.shape[0]
    
    # Prepare output tensor
    output = torch.empty((M, N), device=x.device, dtype=x.dtype)
    
    # Transpose weight for efficient access
    weight_t = weight.t().contiguous()
    
    # Grid configuration
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_M']),
        triton.cdiv(N, META['BLOCK_N']),
    )
    
    # Autotune configurations
    configs = [
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),
    ]
    
    # Kernel launch
    best_config = configs[1]  # Select a balanced configuration
    gemm_kernel[grid](
        x, weight_t, output,
        M, N, K,
        x.stride(0), x.stride(1),
        weight_t.stride(0), weight_t.stride(1),
        output.stride(0), output.stride(1),
        BLOCK_M=best_config.kwargs['BLOCK_M'],
        BLOCK_N=best_config.kwargs['BLOCK_N'],
        BLOCK_K=best_config.kwargs['BLOCK_K'],
        USE_TF32=True,
        num_warps=best_config.num_warps,
    )
    
    return output

def fused_bn_scale_softmax_wrapper(x, gamma, beta, running_mean, running_var, scale, eps=1e-5, momentum=0.1):
    """Fused BatchNorm + Scale + Softmax wrapper."""
    M, N = x.shape
    
    # Prepare output tensor (in-place computation)
    output = torch.empty_like(x)
    
    # Grid configuration
    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']),)
    
    # Autotune configurations
    configs = [
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256}, num_warps=4),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128}, num_warps=4),
        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64}, num_warps=4),
    ]
    
    # Kernel launch
    best_config = configs[0]  # Prefer wider columns for softmax
    fused_bn_scale_softmax_kernel[grid](
        x, gamma, beta, running_mean, running_var,
        scale, output,
        M, N,
        x.stride(0), x.stride(1),
        output.stride(0), output.stride(1),
        eps, momentum,
        BLOCK_M=best_config.kwargs['BLOCK_M'],
        BLOCK_N=best_config.kwargs['BLOCK_N'],
        num_warps=best_config.num_warps,
    )
    
    return output

# -----------------------------------------------------------------------------
# Main Model Class
# -----------------------------------------------------------------------------
class ModelNew(nn.Module):
    """
    Optimized model that performs matrix multiplication (Gemm), 
    Batch Normalization, scaling, and Softmax in fused kernels.
    """
    def __init__(self, in_features, out_features, bn_eps=1e-5, bn_momentum=0.1, scale_shape=(1,)):
        super(ModelNew, self).__init__()
        
        # Store parameters
        self.in_features = in_features
        self.out_features = out_features
        self.bn_eps = bn_eps
        self.bn_momentum = bn_momentum
        
        # Learnable parameters
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.gamma = nn.Parameter(torch.ones(out_features))
        self.beta = nn.Parameter(torch.zeros(out_features))
        self.scale = nn.Parameter(torch.ones(scale_shape))
        
        # BatchNorm buffers (running statistics)
        self.register_buffer('running_mean', torch.zeros(out_features))
        self.register_buffer('running_var', torch.ones(out_features))
        
        # Initialize parameters
        self._initialize_parameters()
    
    def _initialize_parameters(self):
        """Initialize weights using Kaiming initialization."""
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        
        # Initialize gamma and beta
        nn.init.ones_(self.gamma)
        nn.init.zeros_(self.beta)
    
    def forward(self, x):
        """
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_features).
        """
        # Step 1: Matrix multiplication with optimized kernel
        x = gemm_wrapper(x, self.weight)
        
        # Step 2: Fused BatchNorm + Scale + Softmax
        x = fused_bn_scale_softmax_wrapper(
            x, self.gamma, self.beta,
            self.running_mean, self.running_var,
            self.scale, self.bn_eps, self.bn_momentum
        )
        
        return x
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl, AND any other modules used (e.g., import math if using math.sqrt)
   2. @triton.jit decorated kernel function(s) — NO continue/break/return inside loops (use masking)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels — THIS CLASS IS REQUIRED
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes
4. Ensure ALL imports are included at the top (common mistake: forgetting `import math`)

```python
# <corrected code>
```
