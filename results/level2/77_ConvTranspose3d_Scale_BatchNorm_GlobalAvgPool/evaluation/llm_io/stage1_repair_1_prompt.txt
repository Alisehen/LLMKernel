Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 538, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_114735_batch_range56to80_deepseek_deepseek/77_ConvTranspose3d_Scale_BatchNorm_GlobalAvgPool/code/kernel_20251214_234152.py", line 235, in forward
    x = fused_scale_bn_gap_optimized(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_114735_batch_range56to80_deepseek_deepseek/77_ConvTranspose3d_Scale_BatchNorm_GlobalAvgPool/code/kernel_20251214_234152.py", line 163, in fused_scale_bn_gap_optimized
    fused_scale_bn_gap_kernel_optimized[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 278, in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 81, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 100:39:

                                # Apply scaled batch norm
                                x_norm = x_vals * scale_norm + bias_norm

                                # Accumulate in buffer (using relative tile indices)
                                tile_d = d_idx - d_start
                                tile_h = h_idx - h_start
                                tile_w = (w_vec - w_start) // VEC_SIZE

                                # Vectorized accumulation
                                for v in range(VEC_SIZE):
                                    if mask_w[v]:
                                       ^
ValueError('Did you forget to add @triton.jit ? (`_builder` argument must be provided outside of JIT functions.)')

Main Critical Problem Analysis:
Problem Analysis (from expert diagnosis):
critical_issue: Using Python control flow (if mask_w[v]) inside Triton kernel instead of tl.where for vectorized operations
why_it_matters: Triton kernels cannot use Python if statements with runtime values - must use tl.where or tl.if_ for conditional logic
minimal_fix_hint: Replace Python if mask_w[v] with tl.where or restructure using vectorized operations

Focus your fix on addressing the identified critical issue.


Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl
import math


@triton.jit
def fused_scale_bn_gap_kernel_optimized(
    x_ptr,  # Input tensor [N, C, D, H, W]
    scale_factor,  # Scalar scale factor
    running_mean_ptr,  # Running mean [C]
    running_var_ptr,  # Running variance [C]
    weight_ptr,  # Weight [C] (gamma)
    bias_ptr,  # Bias [C] (beta)
    out_ptr,  # Output tensor [N, C, 1, 1, 1]
    eps,  # Epsilon for batch norm
    N, C, D, H, W,  # Input dimensions
    stride_xn, stride_xc, stride_xd, stride_xh, stride_xw,  # Input strides
    stride_on, stride_oc, stride_od, stride_oh, stride_ow,  # Output strides
    BLOCK_C: tl.constexpr,  # Must be power of 2, for channel dimension
    VEC_SIZE: tl.constexpr,  # Vector size for memory loads/stores
    TILE_D: tl.constexpr,  # Tile size for D dimension
    TILE_H: tl.constexpr,  # Tile size for H dimension
    TILE_W: tl.constexpr,  # Tile size for W dimension
    USE_TMA: tl.constexpr,  # Whether to use tensor memory accelerator
):
    """
    Optimized fused kernel: Scale + BatchNorm3d (eval mode) + GlobalAvgPool3d
    Key optimizations:
    1. Vectorized memory operations (VEC_SIZE)
    2. 2D tiling for spatial dimensions (D, H, W)
    3. Prefetching for better memory throughput
    4. Efficient reduction with multiple accumulators
    """
    # Parallelize over batch and channel
    pid_n = tl.program_id(0)  # Batch index
    pid_c = tl.program_id(1)  # Channel index
    
    if pid_n >= N or pid_c >= C:
        return
    
    # Load batch norm parameters for this channel
    mean_val = tl.load(running_mean_ptr + pid_c)
    var_val = tl.load(running_var_ptr + pid_c)
    weight_val = tl.load(weight_ptr + pid_c) if weight_ptr is not None else 1.0
    bias_val = tl.load(bias_ptr + pid_c) if bias_ptr is not None else 0.0
    
    # Pre-compute batch norm scale factor (optimized to minimize operations)
    inv_std = 1.0 / tl.sqrt(var_val + eps)
    scale_norm = weight_val * inv_std
    bias_norm = bias_val - mean_val * scale_norm
    
    # Apply scale factor only to the input scaling term
    scale_norm = scale_norm * scale_factor
    # bias_norm remains unchanged (no scale_factor multiplication)
    
    # Initialize accumulators with multiple buffers for better throughput
    accum_buffer = tl.zeros((TILE_D, TILE_H, TILE_W), dtype=tl.float32)
    count_buffer = tl.zeros((TILE_D, TILE_H, TILE_W), dtype=tl.float32)
    
    # Precompute base pointer offsets for this (n, c) slice
    base_offset = pid_n * stride_xn + pid_c * stride_xc
    
    # Process spatial dimensions with 3D tiling
    for d_tile in range(0, D, TILE_D):
        d_start = d_tile
        d_end = tl.minimum(d_tile + TILE_D, D)
        
        for h_tile in range(0, H, TILE_H):
            h_start = h_tile
            h_end = tl.minimum(h_tile + TILE_H, H)
            
            # Vectorized processing of W dimension
            for w_tile in range(0, W, TILE_W * VEC_SIZE):
                w_start = w_tile
                w_end = tl.minimum(w_tile + TILE_W * VEC_SIZE, W)
                
                # Process vectorized chunks within the tile
                for w_vec in range(w_start, w_end, VEC_SIZE):
                    # Create vectorized indices
                    w_idx = w_vec + tl.arange(0, VEC_SIZE)
                    mask_w = w_idx < w_end
                    
                    if tl.sum(mask_w) > 0:
                        # Process all D, H positions in the current tile
                        for d_idx in range(d_start, d_end):
                            d_offset = d_idx * stride_xd
                            for h_idx in range(h_start, h_end):
                                h_offset = h_idx * stride_xh
                                
                                # Calculate pointer with vectorized access
                                x_offset = base_offset + d_offset + h_offset + w_idx * stride_xw
                                
                                # Vectorized load with mask
                                x_vals = tl.load(x_ptr + x_offset, mask=mask_w, other=0.0)
                                
                                # Apply scaled batch norm
                                x_norm = x_vals * scale_norm + bias_norm
                                
                                # Accumulate in buffer (using relative tile indices)
                                tile_d = d_idx - d_start
                                tile_h = h_idx - h_start
                                tile_w = (w_vec - w_start) // VEC_SIZE
                                
                                # Vectorized accumulation
                                for v in range(VEC_SIZE):
                                    if mask_w[v]:
                                        w_pos = tile_w * VEC_SIZE + v
                                        if w_pos < TILE_W:
                                            accum_buffer[tile_d, tile_h, w_pos] += x_norm[v]
                                            count_buffer[tile_d, tile_h, w_pos] += 1.0
    
    # Final reduction across spatial dimensions
    # Use tree reduction for better efficiency
    accum_total = tl.sum(accum_buffer)
    count_total = tl.sum(count_buffer)
    
    # Compute global average
    avg_val = accum_total / tl.maximum(count_total, 1.0)
    
    # Store to output
    out_offset = pid_n * stride_on + pid_c * stride_oc
    tl.store(out_ptr + out_offset, avg_val)


def fused_scale_bn_gap_optimized(
    x: torch.Tensor,
    scale_factor: float,
    running_mean: torch.Tensor,
    running_var: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    eps: float = 1e-5
) -> torch.Tensor:
    """
    Optimized wrapper for fused Scale + BatchNorm3d + GlobalAvgPool3d
    """
    N, C, D, H, W = x.shape
    
    # Output shape [N, C, 1, 1, 1]
    out = torch.empty((N, C, 1, 1, 1), device=x.device, dtype=x.dtype)
    
    # Optimized tile sizes based on GPU architecture
    # Ada Lovelace (RTX 4090) optimization:
    # - Use larger tiles to improve memory throughput
    # - Vector size of 4 for optimal memory coalescing
    # - Tiling that maximizes L2 cache utilization
    
    # Determine optimal tile sizes based on input dimensions
    TILE_D = min(4, D)  # Tile size for D dimension
    TILE_H = min(4, H)  # Tile size for H dimension
    TILE_W = min(16, W)  # Tile size for W dimension
    VEC_SIZE = 4  # Vector size for memory operations
    
    # Grid: (N, C) - each thread block handles one (n, c) pair
    grid = (N, C)
    
    # Choose block size for channel dimension (power of 2)
    BLOCK_C = triton.next_power_of_2(C)
    
    # Launch optimized kernel
    fused_scale_bn_gap_kernel_optimized[grid](
        x,
        scale_factor,
        running_mean,
        running_var,
        weight,
        bias,
        out,
        eps,
        N, C, D, H, W,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3), x.stride(4),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3), out.stride(4),
        BLOCK_C=BLOCK_C,
        VEC_SIZE=VEC_SIZE,
        TILE_D=TILE_D,
        TILE_H=TILE_H,
        TILE_W=TILE_W,
        USE_TMA=False,  # TMA not supported in current Triton version
    )
    
    return out


class ModelNew(nn.Module):
    """
    ConvTranspose3d (PyTorch native) + Fused Scale + BatchNorm3d + GlobalAvgPool3d
    
    IMPORTANT: This fused implementation only works in evaluation mode
    because it uses running statistics for batch normalization.
    
    For training, fall back to PyTorch implementation.
    """
    def __init__(self, in_channels, out_channels, kernel_size, scale_factor, eps=1e-5, momentum=0.1):
        super(ModelNew, self).__init__()
        # Keep ConvTranspose3d as PyTorch native - DO NOT reimplement in Triton
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size)
        
        # Batch norm parameters
        self.scale_factor = scale_factor
        self.batch_norm = nn.BatchNorm3d(out_channels, eps=eps, momentum=momentum)
        
        # For fused kernel (eval mode only)
        self.eps = eps
        
    def forward(self, x):
        # Step 1: PyTorch native ConvTranspose3d
        x = self.conv_transpose(x)
        
        # Step 2: Fused Scale + BatchNorm + GlobalAvgPool
        # Use fused kernel only in evaluation mode
        if not self.training:
            # Ensure batch norm is in eval mode
            self.batch_norm.eval()
            
            # Sync batch norm running statistics
            with torch.no_grad():
                # Update running statistics if needed
                if self.batch_norm.track_running_stats:
                    # Use current batch to update running stats
                    # This maintains compatibility with PyTorch's behavior
                    current_mean = x.mean(dim=(0, 2, 3, 4))
                    current_var = x.var(dim=(0, 2, 3, 4), unbiased=False)
                    
                    # Update running statistics
                    self.batch_norm.running_mean.mul_(1 - self.batch_norm.momentum).add_(
                        current_mean * self.batch_norm.momentum
                    )
                    self.batch_norm.running_var.mul_(1 - self.batch_norm.momentum).add_(
                        current_var * self.batch_norm.momentum
                    )
            
            # Use optimized fused kernel with running statistics
            x = fused_scale_bn_gap_optimized(
                x,
                self.scale_factor,
                self.batch_norm.running_mean,
                self.batch_norm.running_var,
                self.batch_norm.weight,
                self.batch_norm.bias,
                self.eps
            )
        else:
            # Training mode: Use PyTorch operations
            x = x * self.scale_factor
            x = self.batch_norm(x)
            
            # Global average pooling
            x = x.mean(dim=(2, 3, 4), keepdim=True)
        
        return x
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl, AND any other modules used (e.g., import math if using math.sqrt)
   2. @triton.jit decorated kernel function(s) — NO continue/break/return inside loops (use masking)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels — THIS CLASS IS REQUIRED
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes
4. Ensure ALL imports are included at the top (common mistake: forgetting `import math`)

```python
# <corrected code>
```
