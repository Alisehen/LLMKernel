You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]f

## Current Optimization Stage

Focus: BLOCK_SIZE with register pressure awareness.

Key Principle:
- Fusion increases register usage (intermediates stay in registers)
- Spill to local memory kills fusion benefit

Register Pressure Signals (from NCU):
- launch__registers_per_thread > 128 → likely spilling
- launch__occupancy_limit_registers < other limits → register-bound

Rules:
- Start conservative: BLOCK_SIZE ∈ {256, 512} for element-wise
- For matmul fusion: BLOCK_M/N ∈ {32, 64}, BLOCK_K ∈ {32}
- If registers > 128: reduce BLOCK_* by half
- Trade-off: recompute cheap ops (e.g., x*0.5) vs store intermediate

When to Recompute vs Keep:
- Keep: expensive ops (exp, log, div, sqrt)
- Recompute: cheap ops (add, mul, max) if register pressure high
- Example: `y = relu(x); z = y * scale` → keep y
- Example: `y = x * 0.5; z = y + bias` → can recompute y if needed

Autotune:
- 2-3 BLOCK_SIZE configs, always include smaller fallback



[CURRENT CODE]
```python
# <optimized Triton code>

import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_NC': 32,  'BLOCK_HW': 32},  num_warps=4),
        triton.Config({'BLOCK_NC': 64,  'BLOCK_HW': 16},  num_warps=4),
        triton.Config({'BLOCK_NC': 16,  'BLOCK_HW': 64},  num_warps=4),
        triton.Config({'BLOCK_NC': 128, 'BLOCK_HW': 8},   num_warps=4),
        triton.Config({'BLOCK_NC': 8,   'BLOCK_HW': 128}, num_warps=4),
        triton.Config({'BLOCK_NC': 64,  'BLOCK_HW': 8},   num_warps=2),
        triton.Config({'BLOCK_NC': 32,  'BLOCK_HW': 16},  num_warps=2),
    ],
    key=['NC', 'H_OUT', 'W_OUT'],
)
@triton.jit
def relu_maxpool2x2_kernel(
    x_ptr, y_ptr,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_yn, stride_yc, stride_yh, stride_yw,
    C, NC, H_OUT, W_OUT,
    BLOCK_NC: tl.constexpr, BLOCK_HW: tl.constexpr,
):
    # 2D program id: (NC, HW_OUT)
    pid_nc = tl.program_id(0)
    pid_hw = tl.program_id(1)

    # Flattened (N*C) and (H_out*W_out) indices for this tile
    offs_nc = pid_nc * BLOCK_NC + tl.arange(0, BLOCK_NC)
    offs_hw = pid_hw * BLOCK_HW + tl.arange(0, BLOCK_HW)

    nc = offs_nc[:, None]  # (BLOCK_NC, 1)
    hw = offs_hw[None, :]  # (1, BLOCK_HW)

    HW_OUT = H_OUT * W_OUT

    # Valid output positions mask shared by all fused ops
    mask = (nc < NC) & (hw < HW_OUT)

    # Decode (n, c) from nc = n * C + c
    n = nc // C
    c = nc - n * C

    # Decode (h_out, w_out) from hw = h_out * W_OUT + w_out
    h_out = hw // W_OUT
    w_out = hw - h_out * W_OUT

    # Corresponding top-left input index for the 2x2 pooling window
    h_in = h_out * 2
    w_in = w_out * 2

    # Base pointer x[n, c, h_in, w_in]
    x_base = (
        x_ptr
        + n * stride_xn
        + c * stride_xc
        + h_in * stride_xh
        + w_in * stride_xw
    )

    # Load the 2x2 window (all loads use the same mask)
    x00 = tl.load(x_base, mask=mask, other=0.0)
    x01 = tl.load(x_base + stride_xw, mask=mask, other=0.0)
    x10 = tl.load(x_base + stride_xh, mask=mask, other=0.0)
    x11 = tl.load(x_base + stride_xh + stride_xw, mask=mask, other=0.0)

    # Max over the 2x2 window (no ReLU yet)
    m0 = tl.maximum(x00, x01)
    m1 = tl.maximum(x10, x11)
    pooled = tl.maximum(m0, m1)

    # Fused ReLU on the pooled result: ReLU(max) == max(ReLU(...))
    zero = pooled * 0.0
    y_val = tl.maximum(pooled, zero)

    # Output pointer y[n, c, h_out, w_out]
    y_ptrs = (
        y_ptr
        + n * stride_yn
        + c * stride_yc
        + h_out * stride_yh
        + w_out * stride_yw
    )

    # Store result (same mask as loads)
    tl.store(y_ptrs, y_val, mask=mask)


def fused_relu_maxpool2x2(x: torch.Tensor) -> torch.Tensor:
    """
    Fused ReLU + 2x2 MaxPool (stride=2) for NCHW tensors using Triton.
    Replaces:
        y = F.relu(x)
        y = F.max_pool2d(y, kernel_size=2, stride=2)
    """
    if not x.is_cuda:
        return torch.nn.functional.max_pool2d(
            torch.nn.functional.relu(x),
            kernel_size=2,
            stride=2,
        )

    # Ensure contiguous NCHW layout for fast strides
    x = x.contiguous()
    N, C, H, W = x.shape
    H_out = H // 2
    W_out = W // 2

    y = torch.empty((N, C, H_out, W_out), device=x.device, dtype=x.dtype)

    NC = N * C
    HW_OUT = H_out * W_out

    def grid(meta):
        return (
            triton.cdiv(NC, meta['BLOCK_NC']),
            triton.cdiv(HW_OUT, meta['BLOCK_HW']),
        )

    relu_maxpool2x2_kernel[grid](
        x, y,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3),
        C, NC, H_out, W_out,
    )
    return y


class StageNew(nn.Module):
    """
    One stage of the network:
        Conv2d -> BatchNorm2d -> ReLU ->
        Conv2d -> BatchNorm2d -> (fused ReLU + MaxPool2d)
    """
    def __init__(self, in_channels: int, out_channels: int):
        super(StageNew, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.conv1(x)
        x = self.bn1(x)
        x = torch.nn.functional.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = fused_relu_maxpool2x2(x)
        return x


class ModelNew(nn.Module):
    def __init__(self, input_channels, stages, block_widths, output_classes):
        """
        :param input_channels: int, Number of input channels for the first layer
        :param stages: int, Number of stages in the RegNet-like architecture
        :param block_widths: List[int], Width (number of channels) for each block in the stages
        :param output_classes: int, Number of output classes for classification
        """
        super(ModelNew, self).__init__()

        self.stages = stages
        self.block_widths = block_widths

        layers = []
        current_channels = input_channels

        # Construct the stages with their respective blocks
        for i in range(stages):
            layers.append(StageNew(current_channels, block_widths[i]))
            current_channels = block_widths[i]

        self.feature_extractor = nn.Sequential(*layers)

        # Final fully connected layer for classification
        self.fc = nn.Linear(block_widths[-1], output_classes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through the model.
        :param x: torch.Tensor of shape (batch_size, input_channels, height, width)
        :return: torch.Tensor of shape (batch_size, output_classes)
        """
        x = self.feature_extractor(x)
        # Global Average Pooling over spatial dimensions
        x = torch.mean(x, dim=[2, 3])
        x = self.fc(x)
        return x
```

[NCU PROFILING METRICS]
No NCU metrics available (skipped for network-level model)

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

TRITON API CONSTRAINTS (CRITICAL):
- Triton has NO: tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
