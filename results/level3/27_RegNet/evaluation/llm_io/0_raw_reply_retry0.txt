```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_NC': 32, 'BLOCK_HW': 32}, num_warps=4),
        triton.Config({'BLOCK_NC': 64, 'BLOCK_HW': 16}, num_warps=4),
    ],
    key=['NC', 'HW_OUT'],
)
@triton.jit
def relu_maxpool2x2_kernel(
    x_ptr, y_ptr,
    N, C, H, W,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_yn, stride_yc, stride_yh, stride_yw,
    NC, HW_OUT, H_OUT, W_OUT,
    BLOCK_NC: tl.constexpr, BLOCK_HW: tl.constexpr,
):
    """
    Fused ReLU + 2x2 MaxPool (stride=2) for NCHW tensors.

    Input:  x[N, C, H, W]
    Output: y[N, C, H_out, W_out] where H_out = H // 2, W_out = W // 2

    The kernel flattens (N,C) into NC and (H_out, W_out) into HW_OUT.
    Each program instance processes a BLOCK_NC x BLOCK_HW tile.
    """
    pid_nc = tl.program_id(0)
    pid_hw = tl.program_id(1)

    offs_nc = pid_nc * BLOCK_NC + tl.arange(0, BLOCK_NC)
    offs_hw = pid_hw * BLOCK_HW + tl.arange(0, BLOCK_HW)

    nc = offs_nc[:, None]  # (BLOCK_NC, 1)
    hw = offs_hw[None, :]  # (1, BLOCK_HW)

    # Mask for valid (n, c, h_out, w_out) positions
    mask = (nc < NC) & (hw < HW_OUT)

    # Decode flattened indices
    # nc = n * C + c
    n = nc // C
    c = nc - n * C

    # hw = h_out * W_out + w_out
    h_out = hw // W_OUT
    w_out = hw - h_out * W_OUT

    # Corresponding top-left input index for the 2x2 pooling window
    h_in = h_out * 2
    w_in = w_out * 2

    # Base pointer to x[n, c, h_in, w_in]
    x_base = (
        x_ptr
        + n * stride_xn
        + c * stride_xc
        + h_in * stride_xh
        + w_in * stride_xw
    )

    # Load the 2x2 window
    x00 = tl.load(x_base, mask=mask, other=0.0)
    x01 = tl.load(x_base + stride_xw, mask=mask, other=0.0)
    x10 = tl.load(x_base + stride_xh, mask=mask, other=0.0)
    x11 = tl.load(x_base + stride_xh + stride_xw, mask=mask, other=0.0)

    # ReLU on each element
    zero = 0.0
    x00 = tl.maximum(x00, zero)
    x01 = tl.maximum(x01, zero)
    x10 = tl.maximum(x10, zero)
    x11 = tl.maximum(x11, zero)

    # Max over the 2x2 window
    m0 = tl.maximum(x00, x01)
    m1 = tl.maximum(x10, x11)
    y_val = tl.maximum(m0, m1)

    # Output pointer y[n, c, h_out, w_out]
    y_ptrs = (
        y_ptr
        + n * stride_yn
        + c * stride_yc
        + h_out * stride_yh
        + w_out * stride_yw
    )

    tl.store(y_ptrs, y_val, mask=mask)


def fused_relu_maxpool2x2(x: torch.Tensor) -> torch.Tensor:
    """
    Fused ReLU + 2x2 MaxPool with stride=2 using Triton.

    This replaces:
        y = F.relu(x)
        y = F.max_pool2d(y, kernel_size=2, stride=2)

    For non-CUDA tensors, falls back to the PyTorch implementation.
    """
    if not x.is_cuda:
        return F.max_pool2d(F.relu(x), kernel_size=2, stride=2)

    x = x.contiguous()
    N, C, H, W = x.shape
    H_out = H // 2
    W_out = W // 2

    y = torch.empty((N, C, H_out, W_out), device=x.device, dtype=x.dtype)

    NC = N * C
    HW_OUT = H_out * W_out

    def grid(meta):
        return (
            triton.cdiv(NC, meta['BLOCK_NC']),
            triton.cdiv(HW_OUT, meta['BLOCK_HW']),
        )

    relu_maxpool2x2_kernel[grid](
        x, y,
        N, C, H, W,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3),
        NC, HW_OUT, H_out, W_out,
    )
    return y


class StageNew(nn.Module):
    """
    One stage of the network:
        Conv2d -> BatchNorm2d -> ReLU ->
        Conv2d -> BatchNorm2d -> (fused ReLU + MaxPool2d)
    """
    def __init__(self, in_channels: int, out_channels: int):
        super(StageNew, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = fused_relu_maxpool2x2(x)
        return x


class ModelNew(nn.Module):
    def __init__(self, input_channels, stages, block_widths, output_classes):
        """
        :param input_channels: int, Number of input channels for the first layer
        :param stages: int, Number of stages in the RegNet architecture
        :param block_widths: List[int], Width (number of channels) for each block in the stages
        :param output_classes: int, Number of output classes for classification
        """
        super(ModelNew, self).__init__()

        self.stages = stages
        self.block_widths = block_widths

        layers = []
        current_channels = input_channels

        # Construct the stages with their respective blocks
        for i in range(stages):
            layers.append(self._make_stage(current_channels, block_widths[i]))
            current_channels = block_widths[i]

        self.feature_extractor = nn.Sequential(*layers)

        # Final fully connected layer for classification
        self.fc = nn.Linear(block_widths[-1], output_classes)

    def _make_stage(self, in_channels, out_channels):
        """
        Creates a simple block for each stage, matching the original
        structure but fusing the last ReLU + MaxPool via Triton.
        """
        return StageNew(in_channels, out_channels)

    def forward(self, x):
        """
        Forward pass through the RegNet-like model.
        :param x: torch.Tensor of shape (batch_size, input_channels, height, width)
        :return: torch.Tensor of shape (batch_size, output_classes)
        """
        x = self.feature_extractor(x)
        # Global Average Pooling over spatial dimensions
        x = torch.mean(x, dim=[2, 3])
        x = self.fc(x)
        return x
```