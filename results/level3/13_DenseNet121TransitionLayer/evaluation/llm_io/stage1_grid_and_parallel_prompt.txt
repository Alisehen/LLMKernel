You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU: 4090

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Grid layout & indexing for FUSED operations.

Key Principle:
- All fused ops share the SAME grid AND the SAME (offsets, mask) tuple
- Grid covers OUTPUT tensor dimensions

Hard Rules:
- Every fused op MUST use identical offset calculation
- Every fused op MUST use identical boundary mask
- If broadcast needed: explicit `[None, :]` or `[:, None]`, NOT different offsets
- Element-wise: 1D grid, single `offs = pid * BLOCK + tl.arange(0, BLOCK)`
- Matmul fusion: 2D grid, `offs_m/offs_n` shared by bias add & activation

Verification:
- Check: all tl.load/tl.store use same `offsets` variable
- Check: all masks derived from same boundary condition
- If ANY op needs different indexing → do NOT fuse, split kernel



[CURRENT CODE]
```python
import math
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def fused_relu_conv1x1_kernel(
    x_ptr, w_ptr, y_ptr,
    M, N_OUT, K,
    N, H, W,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_wk, stride_wn,
    stride_yn, stride_yc, stride_yh, stride_yw,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    # program ids for M (flattened N*H*W) and output channels
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    mask_m = offs_m < M
    mask_n = offs_n < N_OUT

    # map flattened m -> (n_idx, h_idx, w_idx)
    HW = H * W
    n_idx = offs_m // HW
    rem = offs_m % HW
    h_idx = rem // W
    w_idx = rem % W

    # base pointer for each row in input (before channel offset)
    x_row_base = x_ptr + n_idx * stride_xn + h_idx * stride_xh + w_idx * stride_xw

    # accumulator in fp32
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # iterate over input channels K
    for k in range(0, K, BLOCK_K):
        offs_k = k + tl.arange(0, BLOCK_K)
        mask_k = offs_k < K

        # A: input activations, apply ReLU on the fly
        a_ptrs = x_row_base[:, None] + offs_k[None, :] * stride_xc
        a_mask = mask_m[:, None] & mask_k[None, :]
        a = tl.load(a_ptrs, mask=a_mask, other=0.0)
        a = tl.maximum(a, 0.0)

        # B: weight matrix (K, N_OUT)
        b_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn
        b_mask = mask_k[:, None] & mask_n[None, :]
        b = tl.load(b_ptrs, mask=b_mask, other=0.0)

        acc += tl.dot(a, b, allow_tf32=True)

    # write back: map m,n to (n_idx, h_idx, w_idx, c_out)
    y_row_base = y_ptr + n_idx * stride_yn + h_idx * stride_yh + w_idx * stride_yw
    y_ptrs = y_row_base[:, None] + offs_n[None, :] * stride_yc
    y_mask = mask_m[:, None] & mask_n[None, :]

    tl.store(y_ptrs, acc, mask=y_mask)


@triton.jit
def avg_pool2d_2x2_kernel(
    x_ptr, y_ptr,
    N, C, H, W, OH, OW,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_yn, stride_yc, stride_yh, stride_yw,
    BLOCK: tl.constexpr,
):
    pid_nc = tl.program_id(0)
    pid_hw = tl.program_id(1)

    offs_hw = pid_hw * BLOCK + tl.arange(0, BLOCK)
    total_hw = OH * OW
    mask_hw = offs_hw < total_hw

    # output spatial indices
    oh = offs_hw // OW
    ow = offs_hw % OW

    # decode n and c from pid_nc
    n = pid_nc // C
    c = pid_nc - n * C

    # top-left corner in input for each output position
    ih0 = oh * 2
    iw0 = ow * 2

    in_base = x_ptr + n * stride_xn + c * stride_xc + ih0 * stride_xh + iw0 * stride_xw

    v00 = tl.load(in_base, mask=mask_hw, other=0.0)
    v01 = tl.load(in_base + stride_xw, mask=mask_hw, other=0.0)
    v10 = tl.load(in_base + stride_xh, mask=mask_hw, other=0.0)
    v11 = tl.load(in_base + stride_xh + stride_xw, mask=mask_hw, other=0.0)

    out = (v00 + v01 + v10 + v11) * 0.25

    out_base = y_ptr + n * stride_yn + c * stride_yc + oh * stride_yh + ow * stride_yw
    tl.store(out_base, out, mask=mask_hw)


def fused_relu_conv1x1(x: torch.Tensor, weight: torch.Tensor) -> torch.Tensor:
    """
    x: (N, C_in, H, W)
    weight: (C_out, C_in, 1, 1)
    Applies ReLU to x, then 1x1 conv with 'weight'.
    """
    assert x.is_cuda and weight.is_cuda, "Triton kernels require CUDA tensors"

    N, C_in, H, W = x.shape
    C_out = weight.shape[0]
    M = N * H * W
    K = C_in
    N_out = C_out

    x = x.contiguous()
    weight = weight.contiguous()

    # weight as (K, N_out)
    w_2d = weight.view(C_out, C_in)
    w_t = w_2d.t().contiguous()  # (K, N_out)

    y = torch.empty((N, C_out, H, W), device=x.device, dtype=x.dtype)

    stride_xn, stride_xc, stride_xh, stride_xw = x.stride()
    stride_wk, stride_wn = w_t.stride()
    stride_yn, stride_yc, stride_yh, stride_yw = y.stride()

    def grid(meta):
        return (
            triton.cdiv(M, meta["BLOCK_M"]),
            triton.cdiv(N_out, meta["BLOCK_N"]),
        )

    fused_relu_conv1x1_kernel[grid](
        x, w_t, y,
        M, N_out, K,
        N, H, W,
        stride_xn, stride_xc, stride_xh, stride_xw,
        stride_wk, stride_wn,
        stride_yn, stride_yc, stride_yh, stride_yw,
        BLOCK_M=64, BLOCK_N=64, BLOCK_K=32,
    )
    return y


def avg_pool2d_2x2(x: torch.Tensor) -> torch.Tensor:
    """
    x: (N, C, H, W)
    returns: (N, C, H//2, W//2)
    """
    assert x.is_cuda, "Triton kernels require CUDA tensors"

    N, C, H, W = x.shape
    OH = H // 2
    OW = W // 2

    x = x.contiguous()
    y = torch.empty((N, C, OH, OW), device=x.device, dtype=x.dtype)

    stride_xn, stride_xc, stride_xh, stride_xw = x.stride()
    stride_yn, stride_yc, stride_yh, stride_yw = y.stride()

    def grid(meta):
        return (
            N * C,
            triton.cdiv(OH * OW, meta["BLOCK"]),
        )

    avg_pool2d_2x2_kernel[grid](
        x, y,
        N, C, H, W, OH, OW,
        stride_xn, stride_xc, stride_xh, stride_xw,
        stride_yn, stride_yc, stride_yh, stride_yw,
        BLOCK=128,
    )
    return y


class ModelNew(nn.Module):
    def __init__(self, num_input_features: int, num_output_features: int):
        """
        Optimized version:
        BatchNorm2d -> fused ReLU + 1x1 Conv2d (Triton) -> AvgPool2d 2x2 stride 2 (Triton)
        """
        super(ModelNew, self).__init__()
        self.bn = nn.BatchNorm2d(num_input_features)
        self.conv_weight = nn.Parameter(
            torch.empty(num_output_features, num_input_features, 1, 1)
        )
        nn.init.kaiming_uniform_(self.conv_weight, a=math.sqrt(5))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (N, C_in, H, W)
        x = self.bn(x)
        x = fused_relu_conv1x1(x, self.conv_weight)
        x = avg_pool2d_2x2(x)
        return x
```

[NCU PROFILING METRICS]
No NCU metrics available

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

## CRITICAL — Code MUST compile and run:
1. EVERY kernel function MUST have `@triton.jit` decorator
2. Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
3. BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
4. `tl.program_id(axis)` only supports axis = 0, 1, 2
5. No `continue`, `break`, `return` inside loops — use masking
6. No tensor indexing with loop vars: `x[:, i]` is INVALID
7. mask shape MUST match data shape in tl.load/tl.store

## Missing Triton Functions (implement manually):
- tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

## OUTPUT FORMAT (STRICT):
1. Imports: torch, torch.nn, triton, triton.language as tl
2. @triton.jit decorated kernel function(s)
3. Wrapper function(s) for grid calculation and kernel launch
4. class ModelNew(nn.Module) that calls your kernels

Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
