You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]f

## Current Optimization Stage

Focus: BLOCK_SIZE with register pressure awareness.

Key Principle:
- Fusion increases register usage (intermediates stay in registers)
- Spill to local memory kills fusion benefit

Register Pressure Signals (from NCU):
- launch__registers_per_thread > 128 → likely spilling
- launch__occupancy_limit_registers < other limits → register-bound

Rules:
- Start conservative: BLOCK_SIZE ∈ {256, 512} for element-wise
- For matmul fusion: BLOCK_M/N ∈ {32, 64}, BLOCK_K ∈ {32}
- If registers > 128: reduce BLOCK_* by half
- Trade-off: recompute cheap ops (e.g., x*0.5) vs store intermediate

When to Recompute vs Keep:
- Keep: expensive ops (exp, log, div, sqrt)
- Recompute: cheap ops (add, mul, max) if register pressure high
- Example: `y = relu(x); z = y * scale` → keep y
- Example: `y = x * 0.5; z = y + bias` → can recompute y if needed

Autotune:
- 2-3 BLOCK_SIZE configs, always include smaller fallback



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


# ----------------------------
# Triton kernels
# ----------------------------

@triton.autotune(
    configs=[
        triton.Config(
            {'BLOCK_HW': 64, 'BLOCK_CO': 32, 'BLOCK_CI': 32},
            num_warps=4,
        ),
        triton.Config(
            {'BLOCK_HW': 128, 'BLOCK_CO': 64, 'BLOCK_CI': 32},
            num_warps=8,
        ),
    ],
    key=['N', 'C_out', 'H', 'W'],
)
@triton.jit
def conv2d_nchw_kernel(
    x_ptr,         # float32*  [N, C_in, H, W]
    w_ptr,         # float32*  [C_out, C_in, K, K]
    b_ptr,         # float32*  [C_out]
    y_ptr,         # float32*  [N, C_out, H_out, W_out]

    N, C_in, H, W, C_out,
    stride_x_n, stride_x_c, stride_x_h, stride_x_w,
    stride_w_oc, stride_w_ic, stride_w_kh, stride_w_kw,
    stride_y_n, stride_y_c, stride_y_h, stride_y_w,

    K: tl.constexpr,
    PAD: tl.constexpr,
    BLOCK_HW: tl.constexpr,
    BLOCK_CO: tl.constexpr,
    BLOCK_CI: tl.constexpr,
):
    # Output spatial dimensions (stride=1, dilation=1)
    H_out = H + 2 * PAD - K + 1
    W_out = W + 2 * PAD - K + 1
    HW_out = H_out * W_out
    S = N * HW_out  # total number of output positions

    pid_hw = tl.program_id(0)  # over N*H_out*W_out
    pid_co = tl.program_id(1)  # over C_out

    offs_hw = pid_hw * BLOCK_HW + tl.arange(0, BLOCK_HW)
    offs_co = pid_co * BLOCK_CO + tl.arange(0, BLOCK_CO)

    hw_mask = offs_hw < S
    co_mask = offs_co < C_out

    # Decode (n, h_out, w_out) from a flat index
    n = offs_hw // HW_out
    hw_rem = offs_hw % HW_out
    h_out = hw_rem // W_out
    w_out = hw_rem % W_out

    # Accumulator for output tile
    acc = tl.zeros((BLOCK_HW, BLOCK_CO), dtype=tl.float32)

    # Loop over input channels in chunks
    for ci0 in range(0, C_in, BLOCK_CI):
        offs_ci = ci0 + tl.arange(0, BLOCK_CI)
        ci_mask = offs_ci < C_in

        # Iterate over kernel window
        for kh in range(0, K):
            for kw in range(0, K):
                # Map output position to input position with padding
                h_in = h_out + kh - PAD
                w_in = w_out + kw - PAD

                # Check bounds for input coordinates
                in_bounds = (
                    (h_in >= 0)
                    & (h_in < H)
                    & (w_in >= 0)
                    & (w_in < W)
                    & hw_mask
                )

                # Input pointers: [BLOCK_HW, BLOCK_CI]
                x_ptrs = (
                    x_ptr
                    + n[:, None] * stride_x_n
                    + offs_ci[None, :] * stride_x_c
                    + h_in[:, None] * stride_x_h
                    + w_in[:, None] * stride_x_w
                )
                mask_x = in_bounds[:, None] & ci_mask[None, :]

                x_vals = tl.load(x_ptrs, mask=mask_x, other=0.0)

                # Weight pointers: [BLOCK_CI, BLOCK_CO]
                w_ptrs = (
                    w_ptr
                    + offs_ci[:, None] * stride_w_ic
                    + offs_co[None, :] * stride_w_oc
                    + kh * stride_w_kh
                    + kw * stride_w_kw
                )
                mask_w = ci_mask[:, None] & co_mask[None, :]

                w_vals = tl.load(w_ptrs, mask=mask_w, other=0.0)

                # Multiply-accumulate: (BLOCK_HW, BLOCK_CI) x (BLOCK_CI, BLOCK_CO)
                acc += tl.dot(x_vals, w_vals)

    # Add bias
    bias = tl.load(b_ptr + offs_co, mask=co_mask, other=0.0)  # (BLOCK_CO,)
    acc += bias[None, :]

    # Store result
    y_ptrs = (
        y_ptr
        + n[:, None] * stride_y_n
        + offs_co[None, :] * stride_y_c
        + h_out[:, None] * stride_y_h
        + w_out[:, None] * stride_y_w
    )
    out_mask = hw_mask[:, None] & co_mask[None, :]
    tl.store(y_ptrs, acc, mask=out_mask)


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_HW': 64, 'BLOCK_C': 32}, num_warps=4),
        triton.Config({'BLOCK_HW': 128, 'BLOCK_C': 64}, num_warps=8),
    ],
    key=['N', 'C', 'H', 'W'],
)
@triton.jit
def maxpool2d_3x3_s1_p1_kernel(
    x_ptr,  # float32* [N, C, H, W]
    y_ptr,  # float32* [N, C, H, W]
    N, C, H, W,
    stride_x_n, stride_x_c, stride_x_h, stride_x_w,
    stride_y_n, stride_y_c, stride_y_h, stride_y_w,
    BLOCK_HW: tl.constexpr,
    BLOCK_C: tl.constexpr,
):
    # 3x3, stride=1, padding=1 -> H_out = H, W_out = W
    H_out = H
    W_out = W
    HW_out = H_out * W_out
    S = N * HW_out

    pid_hw = tl.program_id(0)
    pid_c = tl.program_id(1)

    offs_hw = pid_hw * BLOCK_HW + tl.arange(0, BLOCK_HW)
    offs_c = pid_c * BLOCK_C + tl.arange(0, BLOCK_C)

    hw_mask = offs_hw < S
    c_mask = offs_c < C

    # Decode (n, h, w) from flat index
    n = offs_hw // HW_out
    hw_rem = offs_hw % HW_out
    h_out = hw_rem // W_out
    w_out = hw_rem % W_out

    # Initialize with -inf
    val_max = tl.full((BLOCK_HW, BLOCK_C), -float('inf'), dtype=tl.float32)

    # Pool over 3x3 window
    for kh in range(0, 3):
        for kw in range(0, 3):
            h_in = h_out + kh - 1
            w_in = w_out + kw - 1

            in_bounds = (
                (h_in >= 0)
                & (h_in < H)
                & (w_in >= 0)
                & (w_in < W)
                & hw_mask
            )

            x_ptrs = (
                x_ptr
                + n[:, None] * stride_x_n
                + offs_c[None, :] * stride_x_c
                + h_in[:, None] * stride_x_h
                + w_in[:, None] * stride_x_w
            )
            mask_x = in_bounds[:, None] & c_mask[None, :]

            vals = tl.load(x_ptrs, mask=mask_x, other=-float('inf'))
            val_max = tl.maximum(val_max, vals)

    y_ptrs = (
        y_ptr
        + n[:, None] * stride_y_n
        + offs_c[None, :] * stride_y_c
        + h_out[:, None] * stride_y_h
        + w_out[:, None] * stride_y_w
    )
    out_mask = hw_mask[:, None] & c_mask[None, :]
    tl.store(y_ptrs, val_max, mask=out_mask)


# ----------------------------
# Wrapper functions
# ----------------------------

def triton_conv2d_nchw(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, padding: int):
    """
    NCHW convolution, stride=1, dilation=1, square kernel, symmetric padding.
    x:      [N, C_in, H, W]
    weight: [C_out, C_in, K, K]
    bias:   [C_out]
    """
    if not x.is_cuda:
        # Fallback for non-CUDA tensors
        return torch.nn.functional.conv2d(x, weight, bias, stride=1, padding=padding, dilation=1)

    x = x.contiguous()
    weight = weight.contiguous()
    bias = bias.contiguous()

    N, C_in, H, W = x.shape
    C_out, C_in_w, K, K_w = weight.shape
    assert C_in_w == C_in and K == K_w, "Incompatible weight shape"

    H_out = H + 2 * padding - K + 1
    W_out = W + 2 * padding - K + 1

    y = torch.empty((N, C_out, H_out, W_out), device=x.device, dtype=x.dtype)

    grid = lambda META: (
        triton.cdiv(N * H_out * W_out, META['BLOCK_HW']),
        triton.cdiv(C_out, META['BLOCK_CO']),
    )

    conv2d_nchw_kernel[grid](
        x, weight, bias, y,
        N, C_in, H, W, C_out,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        weight.stride(0), weight.stride(1), weight.stride(2), weight.stride(3),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3),
        K=K,
        PAD=padding,
    )

    return y


def triton_maxpool2d_3x3_s1_p1(x: torch.Tensor):
    """
    3x3 max pooling, stride=1, padding=1, NCHW layout.
    """
    if not x.is_cuda:
        return torch.nn.functional.max_pool2d(x, kernel_size=3, stride=1, padding=1)

    x = x.contiguous()
    N, C, H, W = x.shape
    y = torch.empty_like(x)

    grid = lambda META: (
        triton.cdiv(N * H * W, META['BLOCK_HW']),
        triton.cdiv(C, META['BLOCK_C']),
    )

    maxpool2d_3x3_s1_p1_kernel[grid](
        x, y,
        N, C, H, W,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3),
    )

    return y


# ----------------------------
# Model using Triton kernels
# ----------------------------

class ModelNew(nn.Module):
    def __init__(self, in_channels, out_1x1, reduce_3x3, out_3x3,
                 reduce_5x5, out_5x5, pool_proj):
        """
        Inception-style block with Triton-accelerated convolutions and pooling.

        Parameter structure matches the original Model so that state_dict
        is compatible.
        """
        super(ModelNew, self).__init__()

        # 1x1 convolution branch
        self.branch1x1 = nn.Conv2d(in_channels, out_1x1, kernel_size=1)

        # 3x3 convolution branch
        self.branch3x3 = nn.Sequential(
            nn.Conv2d(in_channels, reduce_3x3, kernel_size=1),
            nn.Conv2d(reduce_3x3, out_3x3, kernel_size=3, padding=1),
        )

        # 5x5 convolution branch
        self.branch5x5 = nn.Sequential(
            nn.Conv2d(in_channels, reduce_5x5, kernel_size=1),
            nn.Conv2d(reduce_5x5, out_5x5, kernel_size=5, padding=2),
        )

        # Max pooling branch
        self.branch_pool = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),
            nn.Conv2d(in_channels, pool_proj, kernel_size=1),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Ensure NCHW contiguous for Triton
        x = x.contiguous()

        # Branch 1: 1x1 conv
        w1 = self.branch1x1.weight
        b1 = self.branch1x1.bias
        branch1x1 = triton_conv2d_nchw(x, w1, b1, padding=0)

        # Branch 2: 1x1 reduction -> 3x3 conv
        w3r = self.branch3x3[0].weight
        b3r = self.branch3x3[0].bias
        x3 = triton_conv2d_nchw(x, w3r, b3r, padding=0)

        w3 = self.branch3x3[1].weight
        b3 = self.branch3x3[1].bias
        branch3x3 = triton_conv2d_nchw(x3, w3, b3, padding=1)

        # Branch 3: 1x1 reduction -> 5x5 conv
        w5r = self.branch5x5[0].weight
        b5r = self.branch5x5[0].bias
        x5 = triton_conv2d_nchw(x, w5r, b5r, padding=0)

        w5 = self.branch5x5[1].weight
        b5 = self.branch5x5[1].bias
        branch5x5 = triton_conv2d_nchw(x5, w5, b5, padding=2)

        # Branch 4: 3x3 max pool -> 1x1 conv
        pooled = triton_maxpool2d_3x3_s1_p1(x)

        wp = self.branch_pool[1].weight
        bp = self.branch_pool[1].bias
        branch_pool = triton_conv2d_nchw(pooled, wp, bp, padding=0)

        # Concatenate along channel dimension
        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]
        return torch.cat(outputs, dim=1)
```

[NCU PROFILING METRICS]
{
  "conv2d_nchw_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 17.99,
    "launch__grid_size": 3920,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 32.82,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 93.89,
    "lts__t_sector_hit_rate.pct": 38.8
  },
  "maxpool2d_3x3_s1_p1_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 26.31,
    "launch__grid_size": 117600,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 61.57,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 91.96,
    "lts__t_sector_hit_rate.pct": 84.54
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

TRITON API CONSTRAINTS (CRITICAL):
- Triton has NO: tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
