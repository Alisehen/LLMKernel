You are a Triton kernel debugging expert. Analyze the error and identify the root cause.

## ERROR LOG
```
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 659, in compare_and_bench
    ref_t  = _bench(ref_model,  inp, dev, warmup, repeat)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 148, in _bench
    model(*inp)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/KernelBench/p/31_VisionAttention.py", line 24, in forward
    attn_output, _ = self.attn(x, x, x)
                     ^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/functional.py", line 6450, in multi_head_attention_forward
    attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 23.52 GiB of which 6.83 GiB is free. Including non-PyTorch memory, this process has 16.62 GiB memory in use. Of the allocated memory 8.27 GiB is allocated by PyTorch, and 7.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
```

## Expected Behavior (PyTorch Reference)
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, embed_dim, num_heads):
        """
        Attention Block using Multihead Self-Attention.
        :param embed_dim: Embedding dimension (the number of channels)
        :param num_heads: Number of attention heads
        """
        super(Model, self).__init__()
        self.attn = nn.MultiheadAttention(embed_dim, num_heads)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        """
        Forward pass of the AttentionBlock.
        :param x: Input tensor of shape (B, C, H, W)
        :return: Output tensor of the same shape (B, C, H, W)
        """
        B, C, H, W = x.shape
        x = x.view(B, C, H * W).permute(2, 0, 1)  # (seq_len, batch_size, embed_dim)
        attn_output, _ = self.attn(x, x, x)
        x = self.norm(attn_output + x)  # (seq_len, batch_size, embed_dim)
        x = x.permute(1, 2, 0).view(B, C, H, W)
        return x

embed_dim = 128
num_heads = 4
batch_size = 2
num_channels = embed_dim
image_height = 128
image_width = 128

def get_inputs():
    return [torch.rand(batch_size, num_channels, image_height, image_width)]

def get_init_inputs():
    return [embed_dim, num_heads]
```

## Current Implementation (Broken Triton Kernel)
```python
# <corrected code>

import torch
import torch.nn as nn
import triton
import triton.language as tl
import math


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_D': 64}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_D': 64}, num_warps=8, num_stages=2),
    ],
    key=['S'],
)
@triton.jit
def mha_fwd_kernel(
    q_ptr, k_ptr, v_ptr, o_ptr,
    stride_qbh, stride_qm, stride_qd,
    stride_kbh, stride_km, stride_kd,
    stride_vbh, stride_vm, stride_vd,
    stride_obh, stride_om, stride_od,
    BH, S, D, scale,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr,
):
    """
    FlashAttention-style fused scaled dot-product attention.

    Q, K, V: [BH, S, D]
    O: [BH, S, D]
    Softmax is along S (keys) for each query position.
    """
    pid_bh = tl.program_id(0)  # batch * head index
    pid_m = tl.program_id(1)   # block index along sequence dimension (queries)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)  # [BLOCK_M]
    offs_d = tl.arange(0, BLOCK_D)                    # [BLOCK_D]
    offs_n = tl.arange(0, BLOCK_N)                    # [BLOCK_N]

    q_valid = offs_m < S
    d_valid = offs_d < D

    # Load a block of Q: [BLOCK_M, BLOCK_D]
    q_ptrs = (
        q_ptr
        + pid_bh * stride_qbh
        + offs_m[:, None] * stride_qm
        + offs_d[None, :] * stride_qd
    )
    q = tl.load(q_ptrs, mask=q_valid[:, None] & d_valid[None, :], other=0.0)
    q = tl.cast(q, tl.float32)

    # Streaming softmax state per query row
    m_i = tl.full((BLOCK_M,), -float("inf"), dtype=tl.float32)
    l_i = tl.zeros((BLOCK_M,), dtype=tl.float32)
    acc = tl.zeros((BLOCK_M, BLOCK_D), dtype=tl.float32)

    # Iterate over key/value blocks using a while loop (runtime S)
    n_start = 0
    while n_start < S:
        k_idx = n_start + offs_n
        k_valid = k_idx < S

        # Load K block: [BLOCK_N, BLOCK_D]
        k_ptrs = (
            k_ptr
            + pid_bh * stride_kbh
            + k_idx[:, None] * stride_km
            + offs_d[None, :] * stride_kd
        )
        k = tl.load(k_ptrs, mask=k_valid[:, None] & d_valid[None, :], other=0.0)
        k = tl.cast(k, tl.float32)

        # Load V block: [BLOCK_N, BLOCK_D]
        v_ptrs = (
            v_ptr
            + pid_bh * stride_vbh
            + k_idx[:, None] * stride_vm
            + offs_d[None, :] * stride_vd
        )
        v = tl.load(v_ptrs, mask=k_valid[:, None] & d_valid[None, :], other=0.0)
        v = tl.cast(v, tl.float32)

        # Attention logits: [BLOCK_M, BLOCK_N]
        scores = tl.dot(q, tl.trans(k), allow_tf32=True) * scale
        mask_qk = q_valid[:, None] & k_valid[None, :]
        scores = tl.where(mask_qk, scores, -float("inf"))

        # Block-wise softmax update
        scores_max = tl.max(scores, axis=1)                     # [BLOCK_M]
        m_i_new = tl.maximum(m_i, scores_max)                   # [BLOCK_M]
        exp_m_diff = tl.exp(m_i - m_i_new)                      # [BLOCK_M]

        scores_shifted = scores - m_i_new[:, None]
        exp_scores = tl.exp(scores_shifted)
        exp_scores = tl.where(mask_qk, exp_scores, 0.0)

        sum_exp = tl.sum(exp_scores, axis=1)                    # [BLOCK_M]
        l_i_new = l_i * exp_m_diff + sum_exp                    # [BLOCK_M]

        # Avoid degenerate scaling for out-of-range queries
        l_i_new = tl.where(q_valid, l_i_new, 1.0)

        # Compute scaling factor for previous accumulator
        alpha = tl.where(l_i_new > 0, (l_i * exp_m_diff) / l_i_new, 0.0)

        # Update accumulator with current block contribution
        block_acc = tl.dot(exp_scores, v, allow_tf32=True)      # [BLOCK_M, BLOCK_D]
        acc = acc * alpha[:, None] + block_acc / l_i_new[:, None]

        # Commit new softmax stats
        m_i = m_i_new
        l_i = l_i_new

        n_start += BLOCK_N

    # Write output O: [BH, S, D]
    o_ptrs = (
        o_ptr
        + pid_bh * stride_obh
        + offs_m[:, None] * stride_om
        + offs_d[None, :] * stride_od
    )
    tl.store(o_ptrs, acc, mask=q_valid[:, None] & d_valid[None, :])


def triton_mha(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, num_heads: int) -> torch.Tensor:
    """
    Fused scaled dot-product attention using the Triton kernel.

    q, k, v: [B, S, E]
    returns: [B, S, E]
    """
    assert q.is_cuda and k.is_cuda and v.is_cuda
    B, S, E = q.shape
    assert k.shape == (B, S, E)
    assert v.shape == (B, S, E)
    assert E % num_heads == 0

    head_dim = E // num_heads
    scale = 1.0 / math.sqrt(head_dim)

    # [B, S, E] -> [BH, S, D]
    q_heads = (
        q.view(B, S, num_heads, head_dim)
        .permute(0, 2, 1, 3)
        .contiguous()
        .view(B * num_heads, S, head_dim)
    )
    k_heads = (
        k.view(B, S, num_heads, head_dim)
        .permute(0, 2, 1, 3)
        .contiguous()
        .view(B * num_heads, S, head_dim)
    )
    v_heads = (
        v.view(B, S, num_heads, head_dim)
        .permute(0, 2, 1, 3)
        .contiguous()
        .view(B * num_heads, S, head_dim)
    )

    BH = B * num_heads
    o_heads = torch.empty_like(q_heads)

    def grid(meta):
        return (
            BH,                                 # program_id(0): batch * head
            triton.cdiv(S, meta['BLOCK_M']),    # program_id(1): sequence blocks
        )

    mha_fwd_kernel[grid](
        q_heads, k_heads, v_heads, o_heads,
        q_heads.stride(0), q_heads.stride(1), q_heads.stride(2),
        k_heads.stride(0), k_heads.stride(1), k_heads.stride(2),
        v_heads.stride(0), v_heads.stride(1), v_heads.stride(2),
        o_heads.stride(0), o_heads.stride(1), o_heads.stride(2),
        BH, S, head_dim, scale,
    )

    # [BH, S, D] -> [B, S, E]
    o = (
        o_heads.view(B, num_heads, S, head_dim)
        .permute(0, 2, 1, 3)
        .contiguous()
        .view(B, S, E)
    )
    return o


class ModelNew(nn.Module):
    def __init__(self, embed_dim, num_heads):
        """
        Vision attention block using a memory-efficient Triton FlashAttention kernel.
        Parameters are shared with nn.MultiheadAttention for fair comparison.
        """
        super(ModelNew, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        assert embed_dim % num_heads == 0

        # Parameter container matching nn.MultiheadAttention, but we won't use its forward
        self.mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        """
        x: (B, C, H, W)
        returns: (B, C, H, W)
        """
        B, C, H, W = x.shape
        S = H * W
        E = C
        assert E == self.embed_dim

        # (B, C, H, W) -> (B, S, E)
        x_seq = x.view(B, C, S).permute(0, 2, 1).contiguous()

        # In-projection to Q, K, V using MHA parameters (no dense SxS allocation)
        qkv = torch.nn.functional.linear(x_seq, self.mha.in_proj_weight, self.mha.in_proj_bias)
        q, k, v = qkv.chunk(3, dim=-1)  # each: (B, S, E)

        # Triton fused attention (FlashAttention-style, O(B * S * E) memory)
        attn_out = triton_mha(q, k, v, self.num_heads)  # (B, S, E)

        # Out projection with shared weights
        out = torch.nn.functional.linear(attn_out, self.mha.out_proj.weight, self.mha.out_proj.bias)

        # Residual + LayerNorm
        out = self.norm(out + x_seq)  # (B, S, E)

        # (B, S, E) -> (B, C, H, W)
        out = out.permute(0, 2, 1).contiguous().view(B, C, H, W)
        return out
```

---

## Your Task

Identify the **single most critical issue** that causes the error above.

### Analysis Guidelines

1. **Focus on root cause**, not symptoms
   - Bad: "Output is wrong"
   - Good: "BLOCK_K loop missing, only processes first 32 elements of K dimension"

2. **Be specific about WHAT and WHERE**
   - Bad: "Memory access issue"
   - Good: "Line 45: tl.atomic_add(c_block_ptr, acc) - atomic_add requires scalar pointer, not block_ptr"

3. **Prioritize by impact**
   - Correctness bugs > Performance issues > Style problems
   - Algorithm errors > Implementation details

### Output Format

**CRITICAL: You MUST output ONLY valid JSON. No other text allowed.**

```json
{
  "critical_issue": "<Concise description of THE root cause, max 30 words>",
  "why_it_matters": "<Why this causes the observed error, max 35 words>",
  "minimal_fix_hint": "<What needs to change (not how), max 30 words>"
}
```

**Remember**: Output ONLY the JSON block. No explanations, no commentary, no additional text.
