Fix the Triton kernel errors. Generate correct code.

## ERROR LOG
```
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 644, in compare_and_bench
    raise ValueError(
ValueError: Outputs are not close (atol=1, rtol=1). max_abs_err=2.140e+00, mean_abs_err=5.489e-01
```

## Broken Code
```python
import math
import torch
import torch.nn as nn
import triton
import triton.language as tl


# ==============================
#  Triton kernels
# ==============================

@triton.jit
def linear_bias_triton_kernel(
    x_ptr, w_ptr, b_ptr, y_ptr,
    M, N, K,
    stride_xm, stride_xk,
    stride_wk, stride_wn,
    stride_ym, stride_yn,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    """
    Matrix multiplication with bias:
      Y[M, N] = X[M, K] @ W[K, N] + b[N]
    X: (M, K)
    W: (K, N)
    b: (N,)
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk
    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    for k in range(0, K, BLOCK_K):
        k_remaining = K - k
        x_mask = (offs_m[:, None] < M) & (offs_k[None, :] < k_remaining)
        w_mask = (offs_k[:, None] < k_remaining) & (offs_n[None, :] < N)

        x_block = tl.load(x_ptrs, mask=x_mask, other=0.0)
        w_block = tl.load(w_ptrs, mask=w_mask, other=0.0)

        acc += tl.dot(x_block, w_block, allow_tf32=True)

        x_ptrs += BLOCK_K * stride_xk
        w_ptrs += BLOCK_K * stride_wk

    # Add bias
    b = tl.load(b_ptr + offs_n, mask=offs_n < N, other=0.0)
    acc += b[None, :]

    y_ptrs = y_ptr + offs_m[:, None] * stride_ym + offs_n[None, :] * stride_yn
    y_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(y_ptrs, acc, mask=y_mask)


@triton.jit
def sdpa_triton_kernel(
    q_ptr, k_ptr, v_ptr, o_ptr,
    seqlen, head_dim,
    stride_qbh, stride_qm, stride_qd,
    stride_kbh, stride_km, stride_kd,
    stride_vbh, stride_vm, stride_vd,
    stride_obh, stride_om, stride_od,
    sm_scale,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr,
):
    """
    Fused scaled dot-product attention for a single batch-head block:
      Q, K, V: [BH, S, D]
      O:      [BH, S, D]
    Computes:
      scores = softmax(Q K^T / sqrt(D))
      O = scores @ V
    Using flash-attention style online softmax.
    """
    pid_m = tl.program_id(0)  # tile of queries
    pid_bh = tl.program_id(1)  # batch*head index

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)  # query indices
    offs_d = tl.arange(0, BLOCK_D)                    # feature indices

    # Pointers for Q
    q_ptrs = (
        q_ptr
        + pid_bh * stride_qbh
        + offs_m[:, None] * stride_qm
        + offs_d[None, :] * stride_qd
    )

    q_mask = (offs_m[:, None] < seqlen) & (offs_d[None, :] < head_dim)
    q = tl.load(q_ptrs, mask=q_mask, other=0.0)

    # Online softmax state: m_i = max, l_i = sum exp
    m_i = tl.full((BLOCK_M,), -float("inf"), dtype=tl.float32)
    l_i = tl.zeros((BLOCK_M,), dtype=tl.float32)
    acc = tl.zeros((BLOCK_M, BLOCK_D), dtype=tl.float32)

    for start_n in range(0, seqlen, BLOCK_N):
        offs_n = start_n + tl.arange(0, BLOCK_N)

        # Load K^T: shape (BLOCK_D, BLOCK_N)
        k_ptrs = (
            k_ptr
            + pid_bh * stride_kbh
            + offs_n[None, :] * stride_km
            + offs_d[:, None] * stride_kd
        )
        k_mask = (offs_n[None, :] < seqlen) & (offs_d[:, None] < head_dim)
        k = tl.load(k_ptrs, mask=k_mask, other=0.0)

        # Load V: shape (BLOCK_N, BLOCK_D)
        v_ptrs = (
            v_ptr
            + pid_bh * stride_vbh
            + offs_n[:, None] * stride_vm
            + offs_d[None, :] * stride_vd
        )
        v_mask = (offs_n[:, None] < seqlen) & (offs_d[None, :] < head_dim)
        v = tl.load(v_ptrs, mask=v_mask, other=0.0)

        # Scores: (BLOCK_M, BLOCK_N)
        scores = tl.dot(q, k, allow_tf32=True) * sm_scale

        # Mask out invalid positions (beyond seqlen)
        mask_m = offs_m[:, None] < seqlen
        mask_n = offs_n[None, :] < seqlen
        valid = mask_m & mask_n
        scores = tl.where(valid, scores, -1e9)

        # Online softmax
        block_max = tl.max(scores, axis=1)
        m_i_new = tl.maximum(m_i, block_max)
        p = tl.exp(scores - m_i_new[:, None])

        l_i_new = l_i * tl.exp(m_i - m_i_new) + tl.sum(p, axis=1)

        # Normalize and update accumulator
        alpha = l_i * tl.exp(m_i - m_i_new) / l_i_new
        beta = 1.0 / l_i_new

        # (BLOCK_M, BLOCK_D) = (BLOCK_M, BLOCK_N) @ (BLOCK_N, BLOCK_D)
        acc = acc * alpha[:, None] + tl.dot(p, v, allow_tf32=True) * beta[:, None]

        m_i = m_i_new
        l_i = l_i_new

    # Store output
    o_ptrs = (
        o_ptr
        + pid_bh * stride_obh
        + offs_m[:, None] * stride_om
        + offs_d[None, :] * stride_od
    )
    o_mask = (offs_m[:, None] < seqlen) & (offs_d[None, :] < head_dim)
    tl.store(o_ptrs, acc, mask=o_mask)


@triton.jit
def layernorm_triton_kernel(
    x_ptr, gamma_ptr, beta_ptr, y_ptr,
    M, N, eps,
    stride_xm, stride_xn,
    stride_ym, stride_yn,
    BLOCK_N: tl.constexpr,
):
    """
    LayerNorm over last dimension N:
      y[m, n] = (x[m, n] - mean_m) / sqrt(var_m + eps) * gamma[n] + beta[n]
    Assumes N <= BLOCK_N.
    """
    pid = tl.program_id(0)
    offs_n = tl.arange(0, BLOCK_N)

    mask = offs_n < N

    x_ptrs = x_ptr + pid * stride_xm + offs_n * stride_xn
    x = tl.load(x_ptrs, mask=mask, other=0.0)

    mean = tl.sum(x, axis=0) / N
    diff = x - mean
    var = tl.sum(diff * diff, axis=0) / N
    rstd = 1.0 / tl.sqrt(var + eps)

    y = diff * rstd

    gamma = tl.load(gamma_ptr + offs_n, mask=mask, other=0.0)
    beta = tl.load(beta_ptr + offs_n, mask=mask, other=0.0)

    y = y * gamma + beta

    y_ptrs = y_ptr + pid * stride_ym + offs_n * stride_yn
    tl.store(y_ptrs, y, mask=mask)


# ==============================
#  Python wrappers
# ==============================

def triton_linear_bias(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:
    """
    x:      (M, K)
    weight: (N, K)  (PyTorch Linear layout)
    bias:   (N,)
    returns (M, N)
    """
    assert x.is_cuda and weight.is_cuda and bias.is_cuda
    M, K = x.shape
    N = weight.shape[0]
    y = torch.empty((M, N), device=x.device, dtype=x.dtype)

    w_t = weight.t().contiguous()  # (K, N)

    grid = lambda META: (
        triton.cdiv(M, META["BLOCK_M"]),
        triton.cdiv(N, META["BLOCK_N"]),
    )
    linear_bias_triton_kernel[grid](
        x, w_t, bias, y,
        M, N, K,
        x.stride(0), x.stride(1),
        w_t.stride(0), w_t.stride(1),
        y.stride(0), y.stride(1),
        BLOCK_M=64, BLOCK_N=64, BLOCK_K=64,
    )
    return y


def triton_sdpa(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:
    """
    q, k, v: (BH, S, D) contiguous
    returns: (BH, S, D)
    """
    assert q.is_cuda and k.is_cuda and v.is_cuda
    BH, S, D = q.shape
    o = torch.empty_like(q)

    sm_scale = 1.0 / math.sqrt(D)

    grid = lambda META: (
        triton.cdiv(S, META["BLOCK_M"]),
        BH,
    )
    sdpa_triton_kernel[grid](
        q, k, v, o,
        S, D,
        q.stride(0), q.stride(1), q.stride(2),
        k.stride(0), k.stride(1), k.stride(2),
        v.stride(0), v.stride(1), v.stride(2),
        o.stride(0), o.stride(1), o.stride(2),
        sm_scale,
        BLOCK_M=64, BLOCK_N=64, BLOCK_D=64,
    )
    return o


def triton_layernorm(x: torch.Tensor, gamma: torch.Tensor, beta: torch.Tensor, eps: float) -> torch.Tensor:
    """
    x:     (M, N)
    gamma: (N,)
    beta:  (N,)
    returns (M, N)
    """
    assert x.is_cuda and gamma.is_cuda and beta.is_cuda
    M, N = x.shape
    y = torch.empty_like(x)

    grid = lambda META: (M,)
    layernorm_triton_kernel[grid](
        x, gamma, beta, y,
        M, N, eps,
        x.stride(0), x.stride(1),
        y.stride(0), y.stride(1),
        BLOCK_N=1024,  # supports embed_dim up to 1024
    )
    return y


# ==============================
#  High-performance Model
# ==============================

class ModelNew(nn.Module):
    def __init__(self, embed_dim, num_heads):
        """
        Triton-optimized Attention Block with Multihead Self-Attention + LayerNorm.
        """
        super(ModelNew, self).__init__()
        assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # In-projection (Q, K, V) combined: (3*E, E)
        self.in_proj_weight = nn.Parameter(torch.empty(3 * embed_dim, embed_dim))
        self.in_proj_bias = nn.Parameter(torch.zeros(3 * embed_dim))

        # Out projection: (E, E)
        self.out_proj_weight = nn.Parameter(torch.empty(embed_dim, embed_dim))
        self.out_proj_bias = nn.Parameter(torch.zeros(embed_dim))

        # LayerNorm parameters
        self.ln_weight = nn.Parameter(torch.ones(embed_dim))
        self.ln_bias = nn.Parameter(torch.zeros(embed_dim))
        self.ln_eps = 1e-5

        self._reset_parameters()

    def _reset_parameters(self):
        nn.init.xavier_uniform_(self.in_proj_weight)
        nn.init.xavier_uniform_(self.out_proj_weight)
        nn.init.zeros_(self.in_proj_bias)
        nn.init.zeros_(self.out_proj_bias)

    def forward(self, x):
        """
        x: (B, C, H, W)
        returns: (B, C, H, W)
        """
        B, C, H, W = x.shape
        E = self.embed_dim
        assert C == E, "Input channels must equal embed_dim"
        L = H * W

        # (B, C, H, W) -> (L, B, E)
        x_seq = x.view(B, C, L).permute(2, 0, 1).contiguous()  # (L, B, E)
        L_seq, B_seq, E_seq = x_seq.shape

        # Prepare for linear: (L*B, E)
        x_lin = x_seq.view(L_seq * B_seq, E_seq).contiguous()

        # Fused QKV projection
        qkv = triton_linear_bias(x_lin, self.in_proj_weight, self.in_proj_bias)  # (L*B, 3E)
        qkv = qkv.view(L_seq, B_seq, 3, E_seq)

        q = qkv[:, :, 0, :]  # (L, B, E)
        k = qkv[:, :, 1, :]
        v = qkv[:, :, 2, :]

        # Split heads: (L, B, H, Dh) -> (B*H, L, Dh)
        Hh = self.num_heads
        Dh = self.head_dim

        q = q.view(L_seq, B_seq, Hh, Dh).permute(1, 2, 0, 3).contiguous()  # (B, H, L, Dh)
        k = k.view(L_seq, B_seq, Hh, Dh).permute(1, 2, 0, 3).contiguous()
        v = v.view(L_seq, B_seq, Hh, Dh).permute(1, 2, 0, 3).contiguous()

        BH = B_seq * Hh
        q = q.view(BH, L_seq, Dh).contiguous()
        k = k.view(BH, L_seq, Dh).contiguous()
        v = v.view(BH, L_seq, Dh).contiguous()

        # Scaled dot-product attention via Triton
        context = triton_sdpa(q, k, v)  # (BH, L, Dh)

        # Merge heads back: (BH, L, Dh) -> (L, B, E)
        context = context.view(B_seq, Hh, L_seq, Dh).permute(2, 0, 1, 3).contiguous()
        context = context.view(L_seq, B_seq, E_seq)  # (L, B, E)

        # Output projection
        context_lin = context.view(L_seq * B_seq, E_seq).contiguous()
        attn_output = triton_linear_bias(context_lin, self.out_proj_weight, self.out_proj_bias)
        attn_output = attn_output.view(L_seq, B_seq, E_seq)

        # Residual + LayerNorm
        residual = x_seq  # (L, B, E)
        y = attn_output + residual

        y_flat = y.view(L_seq * B_seq, E_seq).contiguous()
        y_norm = triton_layernorm(y_flat, self.ln_weight.cuda(), self.ln_bias.cuda(), self.ln_eps)
        y_norm = y_norm.view(L_seq, B_seq, E_seq)

        # Back to (B, C, H, W)
        y_out = y_norm.permute(1, 2, 0).contiguous().view(B, C, H, W)
        return y_out
```

## CRITICAL — These cause 60%+ of failures:
1. EVERY kernel function MUST have `@triton.jit` decorator — MANDATORY
2. Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
3. BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
4. `tl.program_id(axis)` only supports axis = 0, 1, 2
5. No `continue`, `break`, `return` inside loops — use masking
6. No tensor indexing with loop vars: `x[:, i]` is INVALID
7. mask shape MUST match data shape in tl.load/tl.store

## Missing Triton Functions (implement manually):
- tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

## OUTPUT FORMAT (STRICT):
1. Imports: torch, torch.nn, triton, triton.language as tl (and math if needed)
2. @triton.jit decorated kernel function(s)
3. Wrapper function(s) for grid calculation and kernel launch
4. class ModelNew(nn.Module) — REQUIRED

Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <corrected code>
```
