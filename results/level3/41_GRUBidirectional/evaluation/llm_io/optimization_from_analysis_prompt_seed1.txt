You are optimizing a Triton kernel based on algorithmic analysis.

# PyTorch Reference (Target Behavior)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):
        """
        :param input_size: The number of expected features in the input x
        :param hidden_size: The number of features in the hidden state h
        :param num_layers: Number of recurrent layers (default: 1)
        :param bias: If False, then the layer does not use bias weights b_ih and b_hh (default: True)
        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature) (default: False)
        """
        super(Model, self).__init__()
        
        self.gru = nn.GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout=0, bidirectional=True)
        self.h0 = torch.randn((num_layers * 2, batch_size, hidden_size))
    
    def forward(self, x,h0):
        """
        :param x: The input tensor, shape (seq_len, batch_size, input_size) if batch_first=False, otherwise (batch_size, seq_len, input_size)
        :param h_0: The initial hidden state for the input sequence, shape (num_layers * num_directions, batch_size, hidden_size) (default: None)
        :return: output, h_n
            - output: The output features (h_t) from the last layer of the GRU, for each t, shape (seq_len, batch_size, num_directions * hidden_size) if batch_first=False, otherwise (batch_size, seq_len, num_directions * hidden_size)
            - h_n: The hidden state for t = seq_len, shape (num_layers * num_directions, batch_size, hidden_size)
        """
        output, h_n = self.gru(x, h0)
        return output

# Test code
batch_size = 10
seq_len = 512
input_size = 128
hidden_size = 256
num_layers = 6

def get_inputs():
    return [torch.rand(seq_len, batch_size, input_size),torch.rand((num_layers*2, batch_size, hidden_size))]

def get_init_inputs():
    return [input_size, hidden_size, num_layers]
```

**CRITICAL**: Study the PyTorch code carefully to understand:
- What does `forward()` return? (full output sequence vs final hidden state only)
- What is the computational pattern?
- What are the input/output shapes?

Your optimized kernel MUST match this exact behavior.

---

# Analysis Results

**Bottleneck**: The time loop over seq_len (512) is in Python, so gru_step_triton is launched 512× per layer×direction, each doing a small GRU step. This per-timestep launch pattern dominates runtime and underutilizes the GPU compared to a persistent RNN kernel that iterates over time inside the device.

**Optimization Strategy**: Replace the per-timestep GRU step kernel with a persistent GRU kernel that keeps the time loop inside Triton: launch once per (layer, direction), and have the kernel iterate over all timesteps, using the precomputed input-side gates and recurrent weights to update h_t in registers/shared memory. This removes hundreds of launches and better amortizes memory traffic for recurrent weights and hidden state.

**Implementation Plan**: Design a new triton kernel, e.g. gru_layer_kernel, whose inputs are ig_r/ig_z/ig_n of shape (T, B, H), initial h0 (B, H), and w_hh/b_hh; inside the kernel, assign each program instance a tile over batch×hidden and run a for t in range(T) loop that computes the recurrent matmul (h @ w_hh), applies gate functions, and writes h_t to the appropriate output slice. Call this kernel once per (layer, direction), replacing the Python-level for t in range(seq_len) loop and the per-timestep gru_step_triton launches, while reusing the existing matmul_bias precomputation for input gates. Ensure that recurrent weights are loaded in a cache-friendly layout and reused across timesteps without re-reading them from global memory unnecessarily.

**Expected Speedup**: 7-10x vs the current Triton implementation (bringing it roughly to or better than the PyTorch baseline).

---

# Current Kernel (needs optimization)

```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


# ---- Elementwise activations implemented in Triton ----

@triton.jit
def triton_sigmoid(x):
    # Stable sigmoid: 1 / (1 + exp(-x))
    return 1.0 / (1.0 + tl.exp(-x))


@triton.jit
def triton_tanh(x):
    # Numerically stable tanh implementation
    # For x >= 0: tanh(x) = (1 - e^{-2x}) / (1 + e^{-2x})
    # For x < 0 : tanh(x) = (e^{2x} - 1) / (e^{2x} + 1)
    two = 2.0
    x2 = two * x

    e_neg = tl.exp(-x2)
    tanh_pos = (1.0 - e_neg) / (1.0 + e_neg)  # x >= 0

    e_pos = tl.exp(x2)
    tanh_neg = (e_pos - 1.0) / (e_pos + 1.0)  # x < 0

    mask_pos = x >= 0
    return tl.where(mask_pos, tanh_pos, tanh_neg)


# ---- Matmul + bias kernel ----

@triton.jit
def matmul_bias_kernel(
    a_ptr, b_ptr, bias_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    """
    Compute C = A @ B + bias
    A: (M, K)
    B: (K, N)
    bias: (N,)
    C: (M, N)
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    for k0 in range(0, K, BLOCK_K):
        k_ids = k0 + offs_k

        a_ptrs = a_ptr + offs_m[:, None] * stride_am + k_ids[None, :] * stride_ak
        b_ptrs = b_ptr + k_ids[:, None] * stride_bk + offs_n[None, :] * stride_bn

        a = tl.load(
            a_ptrs,
            mask=(offs_m[:, None] < M) & (k_ids[None, :] < K),
            other=0.0,
        )
        b = tl.load(
            b_ptrs,
            mask=(k_ids[:, None] < K) & (offs_n[None, :] < N),
            other=0.0,
        )

        a = a.to(tl.float32)
        b = b.to(tl.float32)
        acc += tl.dot(a, b, allow_tf32=True)

    bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)
    bias = bias.to(tl.float32)
    acc = acc + bias[None, :]

    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    tl.store(
        c_ptrs,
        acc,
        mask=(offs_m[:, None] < M) & (offs_n[None, :] < N),
    )


def matmul_bias(a: torch.Tensor, w: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:
    """
    Wrapper for matmul_bias_kernel.
    a: (M, K)
    w: (K, N)
    bias: (N,)
    returns c: (M, N)
    """
    assert a.is_cuda and w.is_cuda and bias.is_cuda, "Inputs must be CUDA tensors"
    assert a.dtype == torch.float32 and w.dtype == torch.float32 and bias.dtype == torch.float32

    a_contig = a.contiguous()
    w_contig = w.contiguous()
    bias_contig = bias.contiguous()

    M, K = a_contig.shape
    Kw, N = w_contig.shape
    assert Kw == K, "Incompatible matmul dimensions"
    assert bias_contig.numel() == N

    c = torch.empty((M, N), device=a_contig.device, dtype=torch.float32)

    grid = lambda META: (
        max(1, triton.cdiv(M, META["BLOCK_M"])),
        max(1, triton.cdiv(N, META["BLOCK_N"])),
    )

    matmul_bias_kernel[grid](
        a_contig, w_contig, bias_contig, c,
        M, N, K,
        a_contig.stride(0), a_contig.stride(1),
        w_contig.stride(0), w_contig.stride(1),
        c.stride(0), c.stride(1),
        BLOCK_M=64, BLOCK_N=64, BLOCK_K=32,
        num_warps=4,
    )
    return c


# ---- GRU step kernel ----

@triton.jit
def gru_step_kernel(
    h_prev_ptr,
    ig_r_ptr, ig_z_ptr, ig_n_ptr,
    w_r_ptr, w_z_ptr, w_n_ptr,
    b_r_ptr, b_z_ptr, b_n_ptr,
    h_next_ptr,
    M, N, K,
    stride_hm, stride_hk,
    stride_igm, stride_ign,
    stride_wk, stride_wn,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    """
    One GRU timestep for a single layer & direction.

    h_prev: (M, N)  -- batch_size x hidden_size
    ig_r, ig_z, ig_n: (M, N)  -- input pre-activations for gates (already with b_ih)
    w_r, w_z, w_n: (K, N)  -- recurrent weights for each gate
    b_r, b_z, b_n: (N,)    -- recurrent biases for each gate
    h_next: (M, N)
    M: batch_size, N: hidden_size, K: hidden_size
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    acc_r = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    acc_z = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    acc_n = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Recurrent matmuls: h_prev @ w_{r,z,n}
    for k0 in range(0, K, BLOCK_K):
        k_ids = k0 + offs_k

        a_ptrs = h_prev_ptr + offs_m[:, None] * stride_hm + k_ids[None, :] * stride_hk
        a = tl.load(
            a_ptrs,
            mask=(offs_m[:, None] < M) & (k_ids[None, :] < K),
            other=0.0,
        )
        a = a.to(tl.float32)

        w_r_ptrs = w_r_ptr + k_ids[:, None] * stride_wk + offs_n[None, :] * stride_wn
        w_z_ptrs = w_z_ptr + k_ids[:, None] * stride_wk + offs_n[None, :] * stride_wn
        w_n_ptrs = w_n_ptr + k_ids[:, None] * stride_wk + offs_n[None, :] * stride_wn

        mask_kw = (k_ids[:, None] < K) & (offs_n[None, :] < N)
        w_r = tl.load(w_r_ptrs, mask=mask_kw, other=0.0)
        w_z = tl.load(w_z_ptrs, mask=mask_kw, other=0.0)
        w_n = tl.load(w_n_ptrs, mask=mask_kw, other=0.0)

        w_r = w_r.to(tl.float32)
        w_z = w_z.to(tl.float32)
        w_n = w_n.to(tl.float32)

        acc_r += tl.dot(a, w_r, allow_tf32=True)
        acc_z += tl.dot(a, w_z, allow_tf32=True)
        acc_n += tl.dot(a, w_n, allow_tf32=True)

    mask_mn = (offs_m[:, None] < M) & (offs_n[None, :] < N)

    # Input gate pre-activations (already include input bias)
    ig_r = tl.load(
        ig_r_ptr + offs_m[:, None] * stride_igm + offs_n[None, :] * stride_ign,
        mask=mask_mn,
        other=0.0,
    )
    ig_z = tl.load(
        ig_z_ptr + offs_m[:, None] * stride_igm + offs_n[None, :] * stride_ign,
        mask=mask_mn,
        other=0.0,
    )
    ig_n = tl.load(
        ig_n_ptr + offs_m[:, None] * stride_igm + offs_n[None, :] * stride_ign,
        mask=mask_mn,
        other=0.0,
    )

    ig_r = ig_r.to(tl.float32)
    ig_z = ig_z.to(tl.float32)
    ig_n = ig_n.to(tl.float32)

    # Recurrent biases
    b_r = tl.load(b_r_ptr + offs_n, mask=offs_n < N, other=0.0).to(tl.float32)
    b_z = tl.load(b_z_ptr + offs_n, mask=offs_n < N, other=0.0).to(tl.float32)
    b_n = tl.load(b_n_ptr + offs_n, mask=offs_n < N, other=0.0).to(tl.float32)

    # Gate pre-activations
    pre_r = ig_r + acc_r + b_r[None, :]
    pre_z = ig_z + acc_z + b_z[None, :]

    # Sigmoid gates
    r = triton_sigmoid(pre_r)
    z = triton_sigmoid(pre_z)

    # Candidate state
    h_n_lin = acc_n + b_n[None, :]
    pre_n = ig_n + r * h_n_lin

    # Stable tanh for candidate
    n_tilde = triton_tanh(pre_n)

    # Previous hidden state tile
    h_prev_tile = tl.load(
        h_prev_ptr + offs_m[:, None] * stride_hm + offs_n[None, :] * stride_hk,
        mask=mask_mn,
        other=0.0,
    )
    h_prev_tile = h_prev_tile.to(tl.float32)

    # GRU update: h_t = (1 - z) * n_tilde + z * h_{t-1}
    h_new = (1.0 - z) * n_tilde + z * h_prev_tile

    tl.store(
        h_next_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn,
        h_new,
        mask=mask_mn,
    )


def gru_step_triton(
    h_prev: torch.Tensor,
    ig_r: torch.Tensor,
    ig_z: torch.Tensor,
    ig_n: torch.Tensor,
    w_hh: torch.Tensor,
    b_hh: torch.Tensor,
) -> torch.Tensor:
    """
    h_prev: (B, H)
    ig_r, ig_z, ig_n: (B, H)
    w_hh: (3, H, H)
    b_hh: (3, H)
    returns h_next: (B, H)
    """
    assert h_prev.is_cuda, "h_prev must be CUDA tensor"
    assert h_prev.dtype == torch.float32
    assert w_hh.is_cuda and b_hh.is_cuda

    B, H = h_prev.shape
    assert ig_r.shape == (B, H)
    assert ig_z.shape == (B, H)
    assert ig_n.shape == (B, H)
    assert w_hh.shape == (3, H, H)
    assert b_hh.shape == (3, H)

    h_prev_c = h_prev.contiguous()
    ig_r_c = ig_r.contiguous()
    ig_z_c = ig_z.contiguous()
    ig_n_c = ig_n.contiguous()

    w_r = w_hh[0].contiguous()
    w_z = w_hh[1].contiguous()
    w_n = w_hh[2].contiguous()

    b_r = b_hh[0].contiguous()
    b_z = b_hh[1].contiguous()
    b_n = b_hh[2].contiguous()

    h_next = torch.empty_like(h_prev_c, dtype=torch.float32)

    M = B
    N = H
    K = H

    grid = lambda META: (
        max(1, triton.cdiv(M, META["BLOCK_M"])),
        max(1, triton.cdiv(N, META["BLOCK_N"])),
    )

    gru_step_kernel[grid](
        h_prev_c,
        ig_r_c, ig_z_c, ig_n_c,
        w_r, w_z, w_n,
        b_r, b_z, b_n,
        h_next,
        M, N, K,
        h_prev_c.stride(0), h_prev_c.stride(1),
        ig_r_c.stride(0), ig_r_c.stride(1),
        w_r.stride(0), w_r.stride(1),
        h_next.stride(0), h_next.stride(1),
        BLOCK_M=16, BLOCK_N=64, BLOCK_K=32,
        num_warps=4,
    )

    return h_next


class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):
        super(ModelNew, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bias = bias
        self.batch_first = batch_first
        self.bidirectional = True
        self.num_directions = 2 if self.bidirectional else 1

        self.weight_ih = nn.ParameterList()
        self.weight_hh = nn.ParameterList()
        self.bias_ih = nn.ParameterList()
        self.bias_hh = nn.ParameterList()

        for layer in range(num_layers):
            layer_input_size = input_size if layer == 0 else hidden_size * self.num_directions
            for direction in range(self.num_directions):
                w_ih = nn.Parameter(torch.randn(3, layer_input_size, hidden_size))
                w_hh = nn.Parameter(torch.randn(3, hidden_size, hidden_size))
                self.weight_ih.append(w_ih)
                self.weight_hh.append(w_hh)
                if bias:
                    b_ih = nn.Parameter(torch.randn(3, hidden_size))
                    b_hh = nn.Parameter(torch.randn(3, hidden_size))
                else:
                    b_ih = nn.Parameter(torch.zeros(3, hidden_size), requires_grad=False)
                    b_hh = nn.Parameter(torch.zeros(3, hidden_size), requires_grad=False)
                self.bias_ih.append(b_ih)
                self.bias_hh.append(b_hh)

    def forward(self, x, h0=None):
        """
        x: (seq_len, batch, input_size) if batch_first=False
           (batch, seq_len, input_size) if batch_first=True
        h0: (num_layers * num_directions, batch, hidden_size)
        returns: output, h_n
          output: (seq_len, batch, num_directions * hidden_size) if batch_first=False
                  (batch, seq_len, num_directions * hidden_size) if batch_first=True
          h_n: (num_layers * num_directions, batch, hidden_size)
        """
        if self.batch_first:
            x = x.transpose(0, 1)  # (T, B, C)

        seq_len, batch_size, _ = x.shape

        if h0 is None:
            h0 = torch.zeros(
                self.num_layers * self.num_directions,
                batch_size,
                self.hidden_size,
                device=x.device,
                dtype=x.dtype,
            )

        x_seq = x
        final_h_list = []

        for layer in range(self.num_layers):
            layer_input_size = x_seq.shape[2]
            layer_out_fwd = torch.empty(
                seq_len, batch_size, self.hidden_size,
                device=x_seq.device, dtype=x_seq.dtype,
            )
            if self.num_directions == 2:
                layer_out_bwd = torch.empty_like(layer_out_fwd)

            for direction in range(self.num_directions):
                idx = layer * self.num_directions + direction

                w_ih = self.weight_ih[idx]
                w_hh = self.weight_hh[idx]
                b_ih = self.bias_ih[idx]
                b_hh = self.bias_hh[idx]

                if direction == 0:
                    seq = x_seq
                else:
                    seq = torch.flip(x_seq, dims=[0])

                seq_flat = seq.reshape(seq_len * batch_size, layer_input_size)

                ig_r_flat = matmul_bias(
                    seq_flat,
                    w_ih[0],
                    b_ih[0],
                )
                ig_z_flat = matmul_bias(
                    seq_flat,
                    w_ih[1],
                    b_ih[1],
                )
                ig_n_flat = matmul_bias(
                    seq_flat,
                    w_ih[2],
                    b_ih[2],
                )

                ig_r = ig_r_flat.view(seq_len, batch_size, self.hidden_size)
                ig_z = ig_z_flat.view(seq_len, batch_size, self.hidden_size)
                ig_n = ig_n_flat.view(seq_len, batch_size, self.hidden_size)

                h_prev = h0[idx]

                for t in range(seq_len):
                    ig_r_t = ig_r[t]
                    ig_z_t = ig_z[t]
                    ig_n_t = ig_n[t]

                    h_prev = gru_step_triton(
                        h_prev,
                        ig_r_t,
                        ig_z_t,
                        ig_n_t,
                        w_hh,
                        b_hh,
                    )

                    if direction == 0:
                        layer_out_fwd[t] = h_prev
                    else:
                        orig_t = seq_len - 1 - t
                        layer_out_bwd[orig_t] = h_prev

                final_h_list.append(h_prev)

            if self.num_directions == 1:
                x_seq = layer_out_fwd
            else:
                x_seq = torch.cat([layer_out_fwd, layer_out_bwd], dim=2)

        output = x_seq
        h_n = torch.stack(final_h_list, dim=0)

        if self.batch_first:
            output = output.transpose(0, 1)

        return output, h_n
```

---

# Your Task

Implement the optimization strategy above. Focus on the specific bottleneck identified.

## Key Requirements

1. **Preserve correctness**: Maintain the same input/output behavior
2. **Apply the optimization**: Follow the implementation plan exactly
3. **Use valid Triton syntax**:
   - Every kernel MUST have `@triton.jit` decorator
   - Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
   - BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
   - No `continue`, `break`, `return` inside kernels (use masking)
   - Prefer `tl.dot(a, b, allow_tf32=True)` for matmul operations

4. **CRITICAL for RNN/GRU/LSTM Persistent Kernels**:
   - Time loop MUST be inside @triton.jit kernel, NOT in Python forward()
   - **HYBRID computation strategy** (CRITICAL for performance):
     * Precompute input-side gates OUTSIDE kernel: `gates_x = (T*B, In) @ W_ih` (ONE large GEMM)
     * INSIDE kernel: only recurrent-side: `for t: gates_h = h @ W_hh` (T small GEMMs)
   - CORRECT (FAST - use this):
     ```python
     # Python forward():
     gates_x_all = x.reshape(T*B, In) @ W_ih + b_ih  # ONE large GEMM
     gates_x_all = gates_x_all.view(T, B, 3*H)
     gru_persistent_kernel[grid](gates_x_all, h0, W_hh, ...)  # Launch ONCE

     @triton.jit
     def gru_persistent_kernel(gates_x_ptr, h_ptr, W_hh_ptr, ...):
         for t in range(T):  # Inside kernel
             gates_x_t = tl.load(gates_x_ptr + t*...)  # Precomputed
             gates_h = h @ W_hh  # Only recurrent GEMM
             h = (1-z)*n + z*h   # Fuse and update
     ```

5. **Output format**:
   - Imports: `import torch, torch.nn as nn, triton, triton.language as tl`
   - `@triton.jit` kernel(s)
   - Wrapper function(s)
   - `class ModelNew(nn.Module)` — REQUIRED
   - NO testing code, NO `if __name__ == "__main__"`

---

Generate the optimized kernel now. Output ONLY the complete Python code.
