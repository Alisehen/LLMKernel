You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]f

## Current Optimization Stage

Focus: Grid layout & indexing for FUSED operations.

⚠️ FUSION EXCLUSIONS (do NOT apply fusion rules to these):
- Reduction ops (sum, mean, softmax along axis)
- Atomic operations
- Irregular/data-dependent access patterns
- Cross-block dependencies

Key Principle:
- All fused ops share the SAME grid AND the SAME (offsets, mask) tuple
- Grid covers OUTPUT tensor dimensions

Hard Rules:
- Every fused op MUST use identical offset calculation
- Every fused op MUST use identical boundary mask
- If broadcast needed: explicit `[None, :]` or `[:, None]`, NOT different offsets
- Element-wise: 1D grid, single `offs = pid * BLOCK + tl.arange(0, BLOCK)`
- Matmul fusion: 2D grid, `offs_m/offs_n` shared by bias add & activation

Verification:
- Check: all tl.load/tl.store use same `offsets` variable
- Check: all masks derived from same boundary condition
- If ANY op needs different indexing → do NOT fuse, split kernel



[CURRENT CODE]
```python
# <complete ModelNew code with optimized Triton kernels>
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def linear_bias_kernel(
    a_ptr,  # (M, K)
    b_ptr,  # (K, N)
    bias_ptr,  # (N,)
    c_ptr,  # (M, N)
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """
    Compute C = A @ B + bias
    A: (M, K)
    B: (K, N)
    bias: (N,)
    C: (M, N)
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    for k in range(0, K, BLOCK_K):
        k_mask = offs_k[None, :] < (K - k)
        a = tl.load(
            a_ptrs,
            mask=(offs_m[:, None] < M) & k_mask,
            other=0.0,
        )
        b = tl.load(
            b_ptrs,
            mask=k_mask.T & (offs_n[None, :] < N),
            other=0.0,
        )
        acc += tl.dot(a, b, allow_tf32=True)
        a_ptrs += BLOCK_K * stride_ak
        b_ptrs += BLOCK_K * stride_bk

    # Add bias
    bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0)
    acc += bias[None, :]

    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    tl.store(
        c_ptrs,
        acc,
        mask=(offs_m[:, None] < M) & (offs_n[None, :] < N),
    )


def linear_triton(x: torch.Tensor, weight_t: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:
    """
    Compute y = x @ weight + bias using Triton.
    x: (M, K)
    weight_t: (K, N)  -- transposed weight
    bias: (N,)
    Returns y: (M, N)
    """
    assert x.is_cuda and weight_t.is_cuda and bias.is_cuda, "Inputs must be CUDA tensors"
    assert x.dtype == weight_t.dtype == bias.dtype, "Dtypes must match"

    M, K = x.shape
    K_w, N = weight_t.shape
    assert K_w == K, "Incompatible shapes for matmul"

    y = torch.empty((M, N), device=x.device, dtype=x.dtype)

    stride_am, stride_ak = x.stride()
    stride_bk, stride_bn = weight_t.stride()
    stride_cm, stride_cn = y.stride()

    BLOCK_M = 64
    BLOCK_N = 64
    BLOCK_K = 32

    grid = (
        triton.cdiv(M, BLOCK_M),
        triton.cdiv(N, BLOCK_N),
    )

    linear_bias_kernel[grid](
        x, weight_t, bias, y,
        M, N, K,
        stride_am, stride_ak,
        stride_bk, stride_bn,
        stride_cm, stride_cn,
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
        BLOCK_K=BLOCK_K,
    )
    return y


def gru_layer_triton(
    x: torch.Tensor,          # (seq_len, batch, input_size_or_2H)
    h0: torch.Tensor,         # (batch, hidden_size)
    w_ih_t: torch.Tensor,     # (input_size_or_2H, 3*hidden_size)  transposed
    b_ih: torch.Tensor,       # (3*hidden_size,)
    w_hh_t: torch.Tensor,     # (hidden_size, 3*hidden_size)       transposed
    b_hh: torch.Tensor,       # (3*hidden_size,)
    reverse: bool,
) -> (torch.Tensor, torch.Tensor):
    """
    Single GRU layer for one direction, using Triton for linear ops.

    Returns:
        output: (seq_len, batch, hidden_size)
        h_n: (batch, hidden_size)  final hidden state
    """
    seq_len, batch_size, _ = x.shape
    hidden_size = h0.shape[1]
    device = x.device
    dtype = x.dtype

    output = torch.empty((seq_len, batch_size, hidden_size), device=device, dtype=dtype)
    h_t = h0

    if reverse:
        time_range = range(seq_len - 1, -1, -1)
    else:
        time_range = range(seq_len)

    for t in time_range:
        x_t = x[t]  # (batch, input_size_or_2H)

        gates_x = linear_triton(x_t, w_ih_t, b_ih)   # (batch, 3H)
        gates_h = linear_triton(h_t, w_hh_t, b_hh)   # (batch, 3H)

        i_r, i_z, i_n = gates_x.chunk(3, dim=1)
        h_r, h_z, h_n = gates_h.chunk(3, dim=1)

        r = torch.sigmoid(i_r + h_r)
        z = torch.sigmoid(i_z + h_z)
        n = torch.tanh(i_n + r * h_n)

        h_t = (1.0 - z) * n + z * h_t
        output[t] = h_t

    return output, h_t


def gru_triton_forward(
    x: torch.Tensor,
    h0: torch.Tensor,
    gru_module: nn.GRU,
) -> (torch.Tensor, torch.Tensor):
    """
    Multi-layer (possibly bidirectional) GRU forward using Triton for linear parts.
    x: (seq_len, batch, input_size) if not batch_first
       or (batch, seq_len, input_size) if batch_first
    h0: (num_layers * num_directions, batch, hidden_size)
    """
    batch_first = getattr(gru_module, "batch_first", False)
    if batch_first:
        # (batch, seq, feat) -> (seq, batch, feat)
        x = x.transpose(0, 1)

    seq_len, batch_size, _ = x.shape
    num_layers = gru_module.num_layers
    hidden_size = gru_module.hidden_size
    num_directions = 2 if gru_module.bidirectional else 1
    use_bias = gru_module.bias

    assert h0.shape[0] == num_layers * num_directions
    assert h0.shape[1] == batch_size
    assert h0.shape[2] == hidden_size

    current_input = x
    h_n_list = []

    for layer in range(num_layers):
        layer_outputs = []

        for direction in range(num_directions):
            suffix = "" if direction == 0 else "_reverse"
            reverse = direction == 1

            w_ih = getattr(gru_module, f"weight_ih_l{layer}{suffix}")
            w_hh = getattr(gru_module, f"weight_hh_l{layer}{suffix}")

            if use_bias:
                b_ih = getattr(gru_module, f"bias_ih_l{layer}{suffix}")
                b_hh = getattr(gru_module, f"bias_hh_l{layer}{suffix}")
            else:
                b_ih = torch.zeros(
                    3 * hidden_size, device=current_input.device, dtype=current_input.dtype
                )
                b_hh = torch.zeros(
                    3 * hidden_size, device=current_input.device, dtype=current_input.dtype
                )

            # Transpose weights once per forward pass for Triton kernel
            w_ih_t = w_ih.transpose(0, 1).contiguous()   # (input_size, 3H)
            w_hh_t = w_hh.transpose(0, 1).contiguous()   # (H, 3H)

            h0_ld = h0[layer * num_directions + direction]  # (batch, H)

            out_dir, h_n_dir = gru_layer_triton(
                current_input, h0_ld, w_ih_t, b_ih, w_hh_t, b_hh, reverse
            )
            layer_outputs.append(out_dir)
            h_n_list.append(h_n_dir.unsqueeze(0))

        if num_directions == 1:
            current_input = layer_outputs[0]
        else:
            # Concatenate forward and backward along feature dimension
            current_input = torch.cat(layer_outputs, dim=2)

    output = current_input
    h_n = torch.cat(h_n_list, dim=0)  # (num_layers * num_directions, batch, H)

    if batch_first:
        output = output.transpose(0, 1)  # back to (batch, seq, feat)

    return output, h_n


class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):
        """
        Matches the original Model interface but uses Triton-accelerated GRU computations.
        """
        super(ModelNew, self).__init__()
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            bias=bias,
            batch_first=batch_first,
            dropout=0.0,
            bidirectional=True,
        )

    def forward(self, x, h0):
        output, _ = gru_triton_forward(x, h0, self.gru)
        # Original Model returns only `output`
        return output
```

[NCU PROFILING METRICS]
No NCU metrics available (skipped for network-level model)

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

TRITON API CONSTRAINTS (CRITICAL):
- Triton has NO: tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
