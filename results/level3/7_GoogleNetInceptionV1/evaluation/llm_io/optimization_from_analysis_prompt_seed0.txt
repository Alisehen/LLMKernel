You are optimizing a Triton kernel based on algorithmic analysis.

# PyTorch Reference (Target Behavior)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class InceptionModule(nn.Module):
    def __init__(self, in_channels, out_1x1, reduce_3x3, out_3x3, reduce_5x5, out_5x5, pool_proj):
        """
        :param in_channels: Number of input channels
        :param out_1x1: Number of output channels for the 1x1 convolution
        :param reduce_3x3: Number of output channels for the 1x1 reduction before 3x3 convolution
        :param out_3x3: Number of output channels for the 3x3 convolution
        :param reduce_5x5: Number of output channels for the 1x1 reduction before 5x5 convolution
        :param out_5x5: Number of output channels for the 5x5 convolution
        :param pool_proj: Number of output channels for the pooling projection
        """
        super(InceptionModule, self).__init__()
        
        # 1x1 convolution branch
        self.branch1x1 = nn.Conv2d(in_channels, out_1x1, kernel_size=1)
        
        # 3x3 convolution branch
        self.branch3x3 = nn.Sequential(
            nn.Conv2d(in_channels, reduce_3x3, kernel_size=1),
            nn.Conv2d(reduce_3x3, out_3x3, kernel_size=3, padding=1)
        )
        
        # 5x5 convolution branch
        self.branch5x5 = nn.Sequential(
            nn.Conv2d(in_channels, reduce_5x5, kernel_size=1),
            nn.Conv2d(reduce_5x5, out_5x5, kernel_size=5, padding=2)
        )
        
        # Max pooling branch
        self.branch_pool = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),
            nn.Conv2d(in_channels, pool_proj, kernel_size=1)
        )
    
    def forward(self, x):
        """
        :param x: Input tensor, shape (batch_size, in_channels, height, width)
        :return: Output tensor, shape (batch_size, out_channels, height, width)
        """
        branch1x1 = self.branch1x1(x)
        branch3x3 = self.branch3x3(x)
        branch5x5 = self.branch5x5(x)
        branch_pool = self.branch_pool(x)
        
        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]
        return torch.cat(outputs, 1)

class Model(nn.Module):
    def __init__(self, num_classes=1000):
        """
        :param num_classes: Number of output classes
        """
        super(Model, self).__init__()
        
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
        self.maxpool1 = nn.MaxPool2d(3, stride=2, padding=1)
        self.conv2 = nn.Conv2d(64, 64, kernel_size=1)
        self.conv3 = nn.Conv2d(64, 192, kernel_size=3, padding=1)
        self.maxpool2 = nn.MaxPool2d(3, stride=2, padding=1)
        
        self.inception3a = InceptionModule(192, 64, 96, 128, 16, 32, 32)
        self.inception3b = InceptionModule(256, 128, 128, 192, 32, 96, 64)
        self.maxpool3 = nn.MaxPool2d(3, stride=2, padding=1)
        
        self.inception4a = InceptionModule(480, 192, 96, 208, 16, 48, 64)
        self.inception4b = InceptionModule(512, 160, 112, 224, 24, 64, 64)
        self.inception4c = InceptionModule(512, 128, 128, 256, 24, 64, 64)
        self.inception4d = InceptionModule(512, 112, 144, 288, 32, 64, 64)
        self.inception4e = InceptionModule(528, 256, 160, 320, 32, 128, 128)
        self.maxpool4 = nn.MaxPool2d(3, stride=2, padding=1)
        
        self.inception5a = InceptionModule(832, 256, 160, 320, 32, 128, 128)
        self.inception5b = InceptionModule(832, 384, 192, 384, 48, 128, 128)
        
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(0.0)
        self.fc = nn.Linear(1024, num_classes)
    
    def forward(self, x):
        """
        :param x: Input tensor, shape (batch_size, 3, height, width)
        :return: Output tensor, shape (batch_size, num_classes)
        """
        x = self.maxpool1(F.relu(self.conv1(x)))
        x = F.relu(self.conv2(x))
        x = self.maxpool2(F.relu(self.conv3(x)))
        
        x = self.inception3a(x)
        x = self.inception3b(x)
        x = self.maxpool3(x)
        
        x = self.inception4a(x)
        x = self.inception4b(x)
        x = self.inception4c(x)
        x = self.inception4d(x)
        x = self.inception4e(x)
        x = self.maxpool4(x)
        
        x = self.inception5a(x)
        x = self.inception5b(x)
        
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        x = self.fc(x)
        
        return x

# Test code
batch_size = 10
input_channels = 3
height = 224
width = 224
num_classes = 1000

def get_inputs():
    return [torch.rand(batch_size, input_channels, height, width)]

def get_init_inputs():
    return [num_classes]
```

**CRITICAL**: Study the PyTorch code carefully to understand:
- What does `forward()` return? (full output sequence vs final hidden state only)
- What is the computational pattern?
- What are the input/output shapes?

Your optimized kernel MUST match this exact behavior.

---

# Analysis Results

**Bottleneck**: The conv2d_nchw_kernel performs a fully nested loop over C_in, KH, KW with scalar loads and outer-product accumulation, which prevents tiling over the reduction dimension, yields poor memory reuse, and does not leverage GEMM-style compute or tensor cores.

**Optimization Strategy**: Replace the direct-loop convolution with a GEMM-based convolution (im2col or implicit-GEMM) so that each conv is expressed as a matrix multiplication over K = C_in * KH * KW, enabling a Triton matmul-style kernel (with BLOCK_K tiling and tl.dot) and fusion of bias/ReLU.

**Implementation Plan**: Reshape each convolution into y_mat = x_im2col @ w_mat, where x_im2col has shape [N * H_out * W_out, C_in * KH * KW] and w_mat has shape [C_in * KH * KW, C_out]; implement x_im2col either explicitly once per conv or implicitly inside a GEMM-like Triton kernel that tiles over M, N, and K with BLOCK_K chunks. Reuse or adapt the existing linear_triton matmul kernel to handle this GEMM, fusing bias and optional ReLU in the epilogue, and call this new GEMM-based path from conv2d_triton for all convolutions in ModelNew.

**Expected Speedup**: 5-10x (bringing Triton conv performance close to the PyTorch/cuDNN baseline)

---

# Current Kernel (needs optimization)

```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def conv2d_nchw_kernel(
    x_ptr, w_ptr, b_ptr, y_ptr,
    N, C_in, H_in, W_in,
    C_out, H_out, W_out,
    stride_h, stride_w,
    pad_h, pad_w,
    s_xn, s_xc, s_xh, s_xw,
    s_wn, s_wc, s_wh, s_ww,
    s_yn, s_yc, s_yh, s_yw,
    BLOCK_M: tl.constexpr,  # tile over N * H_out * W_out
    BLOCK_N: tl.constexpr,  # tile over C_out
    KH: tl.constexpr,
    KW: tl.constexpr,
    HAS_BIAS: tl.constexpr,
    ADD_RELU: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    # Tile indices
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    M = N * H_out * W_out
    mask_m = offs_m < M
    mask_n = offs_n < C_out

    # Decode (n, ho, wo) from flattened m index
    hw = offs_m % (H_out * W_out)
    ho = hw // W_out
    wo = hw % W_out
    n = offs_m // (H_out * W_out)

    # Accumulator in fp32
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Loop over input channels and kernel spatial dims
    for c in range(0, C_in):
        for kh in range(0, KH):
            for kw in range(0, KW):
                # Compute input coordinates
                h_in = ho * stride_h + kh - pad_h
                w_in = wo * stride_w + kw - pad_w

                valid_h = (h_in >= 0) & (h_in < H_in)
                valid_w = (w_in >= 0) & (w_in < W_in)
                mask_x = mask_m & valid_h & valid_w

                # Load input values x[n, c, h_in, w_in]
                x_offsets = (
                    n * s_xn
                    + c * s_xc
                    + h_in * s_xh
                    + w_in * s_xw
                )
                x_vals = tl.load(x_ptr + x_offsets, mask=mask_x, other=0.0).to(tl.float32)

                # Load weight values w[out_c, c, kh, kw]
                w_offsets = (
                    offs_n * s_wn
                    + c * s_wc
                    + kh * s_wh
                    + kw * s_ww
                )
                w_vals = tl.load(w_ptr + w_offsets, mask=mask_n, other=0.0).to(tl.float32)

                # Outer product accumulate
                acc += x_vals[:, None] * w_vals[None, :]

    # Optional bias add
    if HAS_BIAS:
        bias_vals = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0).to(tl.float32)
        acc += bias_vals[None, :]

    # Optional ReLU
    if ADD_RELU:
        acc = tl.maximum(acc, 0.0)

    # Store output y[n, out_c, ho, wo]
    y_offsets = (
        n[:, None] * s_yn
        + offs_n[None, :] * s_yc
        + ho[:, None] * s_yh
        + wo[:, None] * s_yw
    )
    mask_out = mask_m[:, None] & mask_n[None, :]
    tl.store(y_ptr + y_offsets, acc, mask=mask_out)


def conv2d_triton(x, weight, bias=None, stride=1, padding=0, activation=None):
    """
    NCHW conv2d using Triton.
    Supports:
      - groups=1, dilation=1
      - arbitrary batch size
    """
    assert x.is_cuda and weight.is_cuda, "Inputs must be CUDA tensors"
    x = x.contiguous()
    weight = weight.contiguous()
    if bias is not None:
        bias = bias.contiguous()

    N, C_in, H_in, W_in = x.shape
    C_out, C_in_w, KH, KW = weight.shape
    assert C_in == C_in_w, "Incompatible in_channels"

    if isinstance(stride, int):
        stride_h = stride_w = stride
    else:
        stride_h, stride_w = stride
    if isinstance(padding, int):
        pad_h = pad_w = padding
    else:
        pad_h, pad_w = padding

    # No dilation, groups
    H_out = (H_in + 2 * pad_h - KH) // stride_h + 1
    W_out = (W_in + 2 * pad_w - KW) // stride_w + 1

    y = torch.empty((N, C_out, H_out, W_out), device=x.device, dtype=torch.float32)

    M = N * H_out * W_out
    BLOCK_M = 64
    BLOCK_N = 64

    grid = lambda META: (
        triton.cdiv(M, META["BLOCK_M"]),
        triton.cdiv(C_out, META["BLOCK_N"]),
    )

    has_bias = 1 if bias is not None else 0
    add_relu = 1 if activation == "relu" else 0
    b_ptr = bias if bias is not None else x  # dummy if unused

    conv2d_nchw_kernel[grid](
        x, weight, b_ptr, y,
        N, C_in, H_in, W_in,
        C_out, H_out, W_out,
        stride_h, stride_w,
        pad_h, pad_w,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        weight.stride(0), weight.stride(1), weight.stride(2), weight.stride(3),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3),
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
        KH=KH,
        KW=KW,
        HAS_BIAS=has_bias,
        ADD_RELU=add_relu,
    )

    return y


@triton.jit
def linear_bias_kernel(
    a_ptr, b_ptr, bias_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
    ADD_BIAS: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    mask_m = offs_m < M
    mask_n = offs_n < N

    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    k = 0
    while k < K:
        k_remaining = K - k
        k_mask = offs_k < k_remaining

        a = tl.load(a_ptrs, mask=mask_m[:, None] & k_mask[None, :], other=0.0)
        b = tl.load(b_ptrs, mask=k_mask[:, None] & mask_n[None, :], other=0.0)
        acc += tl.dot(a, b, allow_tf32=True)

        a_ptrs += BLOCK_K * stride_ak
        b_ptrs += BLOCK_K * stride_bk
        k += BLOCK_K

    if ADD_BIAS:
        bias_vals = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0).to(tl.float32)
        acc += bias_vals[None, :]

    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    tl.store(c_ptrs, acc, mask=mask_m[:, None] & mask_n[None, :])


def linear_triton(x, weight, bias=None):
    """
    x: [M, K]
    weight: [N, K] (same as nn.Linear.weight)
    bias: [N] or None
    """
    assert x.is_cuda and weight.is_cuda, "Inputs must be CUDA tensors"
    x = x.contiguous()
    weight = weight.contiguous()
    if bias is not None:
        bias = bias.contiguous()

    M, K = x.shape
    N, K_w = weight.shape
    assert K == K_w

    # We compute x @ weight.T, but kernel expects B [K, N]
    b = weight.t().contiguous()  # [K, N]
    out = torch.empty((M, N), device=x.device, dtype=torch.float32)

    BLOCK_M = 64
    BLOCK_N = 64
    BLOCK_K = 32

    grid = lambda META: (
        triton.cdiv(M, META["BLOCK_M"]),
        triton.cdiv(N, META["BLOCK_N"]),
    )

    add_bias = 1 if bias is not None else 0
    bias_ptr = bias if bias is not None else x  # dummy

    linear_bias_kernel[grid](
        x, b, bias_ptr, out,
        M, N, K,
        x.stride(0), x.stride(1),
        b.stride(0), b.stride(1),
        out.stride(0), out.stride(1),
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
        BLOCK_K=BLOCK_K,
        ADD_BIAS=add_bias,
    )

    return out


class InceptionModuleNew(nn.Module):
    def __init__(self, in_channels, out_1x1, reduce_3x3, out_3x3, reduce_5x5, out_5x5, pool_proj):
        super(InceptionModuleNew, self).__init__()

        # 1x1 branch
        self.branch1x1_weight = nn.Parameter(torch.empty(out_1x1, in_channels, 1, 1))
        self.branch1x1_bias = nn.Parameter(torch.empty(out_1x1))

        # 3x3 branch
        self.branch3x3_reduce_weight = nn.Parameter(torch.empty(reduce_3x3, in_channels, 1, 1))
        self.branch3x3_reduce_bias = nn.Parameter(torch.empty(reduce_3x3))
        self.branch3x3_weight = nn.Parameter(torch.empty(out_3x3, reduce_3x3, 3, 3))
        self.branch3x3_bias = nn.Parameter(torch.empty(out_3x3))

        # 5x5 branch
        self.branch5x5_reduce_weight = nn.Parameter(torch.empty(reduce_5x5, in_channels, 1, 1))
        self.branch5x5_reduce_bias = nn.Parameter(torch.empty(reduce_5x5))
        self.branch5x5_weight = nn.Parameter(torch.empty(out_5x5, reduce_5x5, 5, 5))
        self.branch5x5_bias = nn.Parameter(torch.empty(out_5x5))

        # Pool branch
        self.branch_pool_pool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.branch_pool_proj_weight = nn.Parameter(torch.empty(pool_proj, in_channels, 1, 1))
        self.branch_pool_proj_bias = nn.Parameter(torch.empty(pool_proj))

        self._reset_parameters()

    def _reset_parameters(self):
        for w in [
            self.branch1x1_weight,
            self.branch3x3_reduce_weight,
            self.branch3x3_weight,
            self.branch5x5_reduce_weight,
            self.branch5x5_weight,
            self.branch_pool_proj_weight,
        ]:
            nn.init.kaiming_uniform_(w, a=math.sqrt(5))
        for b in [
            self.branch1x1_bias,
            self.branch3x3_reduce_bias,
            self.branch3x3_bias,
            self.branch5x5_reduce_bias,
            self.branch5x5_bias,
            self.branch_pool_proj_bias,
        ]:
            fan_in = b.numel()
            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
            nn.init.uniform_(b, -bound, bound)

    def forward(self, x):
        # 1x1 branch
        branch1x1 = conv2d_triton(x, self.branch1x1_weight, self.branch1x1_bias, stride=1, padding=0)

        # 3x3 branch: 1x1 reduce -> 3x3 conv
        branch3x3 = conv2d_triton(x, self.branch3x3_reduce_weight, self.branch3x3_reduce_bias, stride=1, padding=0)
        branch3x3 = conv2d_triton(branch3x3, self.branch3x3_weight, self.branch3x3_bias, stride=1, padding=1)

        # 5x5 branch: 1x1 reduce -> 5x5 conv
        branch5x5 = conv2d_triton(x, self.branch5x5_reduce_weight, self.branch5x5_reduce_bias, stride=1, padding=0)
        branch5x5 = conv2d_triton(branch5x5, self.branch5x5_weight, self.branch5x5_bias, stride=1, padding=2)

        # Pool branch: maxpool -> 1x1 conv
        branch_pool = self.branch_pool_pool(x)
        branch_pool = conv2d_triton(branch_pool, self.branch_pool_proj_weight, self.branch_pool_proj_bias, stride=1, padding=0)

        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]
        return torch.cat(outputs, 1)


import math


class ModelNew(nn.Module):
    def __init__(self, num_classes=1000):
        super(ModelNew, self).__init__()

        # conv1: 3 -> 64, 7x7, stride=2, padding=3
        self.conv1_weight = nn.Parameter(torch.empty(64, 3, 7, 7))
        self.conv1_bias = nn.Parameter(torch.empty(64))
        self.maxpool1 = nn.MaxPool2d(3, stride=2, padding=1)

        # conv2: 64 -> 64, 1x1
        self.conv2_weight = nn.Parameter(torch.empty(64, 64, 1, 1))
        self.conv2_bias = nn.Parameter(torch.empty(64))

        # conv3: 64 -> 192, 3x3, padding=1
        self.conv3_weight = nn.Parameter(torch.empty(192, 64, 3, 3))
        self.conv3_bias = nn.Parameter(torch.empty(192))
        self.maxpool2 = nn.MaxPool2d(3, stride=2, padding=1)

        # Inception modules
        self.inception3a = InceptionModuleNew(192, 64, 96, 128, 16, 32, 32)
        self.inception3b = InceptionModuleNew(256, 128, 128, 192, 32, 96, 64)
        self.maxpool3 = nn.MaxPool2d(3, stride=2, padding=1)

        self.inception4a = InceptionModuleNew(480, 192, 96, 208, 16, 48, 64)
        self.inception4b = InceptionModuleNew(512, 160, 112, 224, 24, 64, 64)
        self.inception4c = InceptionModuleNew(512, 128, 128, 256, 24, 64, 64)
        self.inception4d = InceptionModuleNew(512, 112, 144, 288, 32, 64, 64)
        self.inception4e = InceptionModuleNew(528, 256, 160, 320, 32, 128, 128)
        self.maxpool4 = nn.MaxPool2d(3, stride=2, padding=1)

        self.inception5a = InceptionModuleNew(832, 256, 160, 320, 32, 128, 128)
        self.inception5b = InceptionModuleNew(832, 384, 192, 384, 48, 128, 128)

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(0.0)

        # Fully connected: 1024 -> num_classes
        self.fc_weight = nn.Parameter(torch.empty(num_classes, 1024))
        self.fc_bias = nn.Parameter(torch.empty(num_classes))

        self._reset_parameters()

    def _reset_parameters(self):
        # Conv-like weights
        for w in [
            self.conv1_weight,
            self.conv2_weight,
            self.conv3_weight,
        ]:
            nn.init.kaiming_uniform_(w, a=math.sqrt(5))
        for b in [
            self.conv1_bias,
            self.conv2_bias,
            self.conv3_bias,
        ]:
            fan_in = b.numel()
            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
            nn.init.uniform_(b, -bound, bound)

        # FC
        nn.init.kaiming_uniform_(self.fc_weight, a=math.sqrt(5))
        fan_in = self.fc_weight.size(1)
        bound = 1 / math.sqrt(fan_in)
        nn.init.uniform_(self.fc_bias, -bound, bound)

    def forward(self, x):
        # x: [B, 3, H, W]

        # conv1 + ReLU + maxpool1
        x = conv2d_triton(x, self.conv1_weight, self.conv1_bias, stride=2, padding=3, activation="relu")
        x = self.maxpool1(x)

        # conv2 + ReLU
        x = conv2d_triton(x, self.conv2_weight, self.conv2_bias, stride=1, padding=0, activation="relu")

        # conv3 + ReLU + maxpool2
        x = conv2d_triton(x, self.conv3_weight, self.conv3_bias, stride=1, padding=1, activation="relu")
        x = self.maxpool2(x)

        # Inception 3a, 3b, maxpool3
        x = self.inception3a(x)
        x = self.inception3b(x)
        x = self.maxpool3(x)

        # Inception 4a-e, maxpool4
        x = self.inception4a(x)
        x = self.inception4b(x)
        x = self.inception4c(x)
        x = self.inception4d(x)
        x = self.inception4e(x)
        x = self.maxpool4(x)

        # Inception 5a, 5b
        x = self.inception5a(x)
        x = self.inception5b(x)

        # Global avgpool
        x = self.avgpool(x)  # [B, 1024, 1, 1]
        x = torch.flatten(x, 1)  # [B, 1024]
        x = self.dropout(x)

        # Linear
        x = linear_triton(x, self.fc_weight, self.fc_bias)

        return x
```

---

# Your Task

Implement the optimization strategy above. Focus on the specific bottleneck identified.

## Key Requirements

1. **Preserve correctness**: Maintain the same input/output behavior
2. **Apply the optimization**: Follow the implementation plan exactly
3. **Use valid Triton syntax**:
   - Every kernel MUST have `@triton.jit` decorator
   - Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
   - BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
   - No `continue`, `break`, `return` inside kernels (use masking)
   - Prefer `tl.dot(a, b, allow_tf32=True)` for matmul operations

4. **CRITICAL for RNN/GRU/LSTM Persistent Kernels**:
   - Time loop MUST be inside @triton.jit kernel, NOT in Python forward()
   - **HYBRID computation strategy** (CRITICAL for performance):
     * Precompute input-side gates OUTSIDE kernel: `gates_x = (T*B, In) @ W_ih` (ONE large GEMM)
     * INSIDE kernel: only recurrent-side: `for t: gates_h = h @ W_hh` (T small GEMMs)
   - CORRECT (FAST - use this):
     ```python
     # Python forward():
     gates_x_all = x.reshape(T*B, In) @ W_ih + b_ih  # ONE large GEMM
     gates_x_all = gates_x_all.view(T, B, 3*H)
     gru_persistent_kernel[grid](gates_x_all, h0, W_hh, ...)  # Launch ONCE

     @triton.jit
     def gru_persistent_kernel(gates_x_ptr, h_ptr, W_hh_ptr, ...):
         for t in range(T):  # Inside kernel
             gates_x_t = tl.load(gates_x_ptr + t*...)  # Precomputed
             gates_h = h @ W_hh  # Only recurrent GEMM
             h = (1-z)*n + z*h   # Fuse and update
     ```

5. **Output format**:
   - Imports: `import torch, torch.nn as nn, triton, triton.language as tl`
   - `@triton.jit` kernel(s)
   - Wrapper function(s)
   - `class ModelNew(nn.Module)` â€” REQUIRED
   - NO testing code, NO `if __name__ == "__main__"`

---

Generate the optimized kernel now. Output ONLY the complete Python code.
