```python
# complete ModelNew code with optimized Triton kernels
import torch
import torch.nn as nn
import triton
import triton.language as tl


# ---------------------------
# Triton kernels
# ---------------------------

@triton.jit
def grouped_pointwise_conv1x1_kernel(
    x_ptr, w_ptr, y_ptr,
    N, C_in, H, W,
    C_out, G,
    C_in_g, C_out_g,
    BLOCK_M: tl.constexpr,  # rows: NHW
    BLOCK_N: tl.constexpr,  # cols: C_out_per_group
    BLOCK_K: tl.constexpr,  # reduction: C_in_per_group
):
    # Program IDs
    pid_m = tl.program_id(0)  # over NHW
    pid_n = tl.program_id(1)  # over C_out_g
    pid_g = tl.program_id(2)  # over groups

    HW = H * W
    M = N * HW
    K = C_in_g

    # Row (NHW) and col (C_out_g) indices
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    mask_m = offs_m < M
    mask_n = offs_n < C_out_g

    # Map m -> (n, h, w)
    bn = offs_m // HW
    rem = offs_m % HW
    h = rem // W
    w = rem % W

    # Base channel index for this group
    c0 = pid_g * C_in_g
    # Base output channel index for this group
    oc = pid_g * C_out_g + offs_n  # (BLOCK_N,)

    # Precompute base offsets
    CinHW = C_in * HW
    CoutHW = C_out * HW

    base_x = bn * CinHW + h * W + w + c0 * HW        # (BLOCK_M,)
    base_y = bn * CoutHW + h * W + w                 # (BLOCK_M,)

    # Reduction indices
    offs_k = tl.arange(0, BLOCK_K)  # (BLOCK_K,)

    # Pointers for A: shape (BM, BK)
    a_ptrs = x_ptr + base_x[:, None] + offs_k[None, :] * HW
    # Pointers for B: shape (BK, BN)
    b_ptrs = w_ptr + oc[None, :] * C_in_g + offs_k[:, None]

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    for k in range(0, K, BLOCK_K):
        k_remaining = K - k
        k_mask = offs_k < k_remaining

        a_mask = mask_m[:, None] & k_mask[None, :]
        b_mask = k_mask[:, None] & mask_n[None, :]

        a = tl.load(a_ptrs, mask=a_mask, other=0.0)
        b = tl.load(b_ptrs, mask=b_mask, other=0.0)

        a = tl.cast(a, tl.float32)
        b = tl.cast(b, tl.float32)

        acc += tl.dot(a, b, allow_tf32=True)

        a_ptrs += BLOCK_K * HW
        b_ptrs += BLOCK_K

    # Store
    y_ptrs = y_ptr + base_y[:, None] + oc[None, :] * HW
    store_mask = mask_m[:, None] & mask_n[None, :]
    tl.store(y_ptrs, acc, mask=store_mask)


@triton.jit
def depthwise_conv3x3_kernel(
    x_ptr, w_ptr, y_ptr,
    N, C, H, W,
    BLOCK_P: tl.constexpr,
):
    # Each program handles one (n, c) pair over a tile of spatial positions
    pid_nc = tl.program_id(0)  # 0 .. N*C-1
    pid_p = tl.program_id(1)   # spatial tiles

    NC = N * C
    P = H * W

    # We set grid0 = NC, so pid_nc < NC always
    n = pid_nc // C
    c = pid_nc % C

    offs_p = pid_p * BLOCK_P + tl.arange(0, BLOCK_P)
    mask_p = offs_p < P

    # Spatial indices
    h = offs_p // W
    w = offs_p % W

    # Precompute base index for (n, c)
    base = (n * C + c) * P

    # Load 3x3 weights for this channel
    w_base = c * 9
    w00 = tl.load(w_ptr + w_base + 0)
    w01 = tl.load(w_ptr + w_base + 1)
    w02 = tl.load(w_ptr + w_base + 2)
    w10 = tl.load(w_ptr + w_base + 3)
    w11 = tl.load(w_ptr + w_base + 4)
    w12 = tl.load(w_ptr + w_base + 5)
    w20 = tl.load(w_ptr + w_base + 6)
    w21 = tl.load(w_ptr + w_base + 7)
    w22 = tl.load(w_ptr + w_base + 8)

    acc = tl.zeros((BLOCK_P,), dtype=tl.float32)

    # Common clamped coordinates
    h_m1 = h - 1
    h_p1 = h + 1
    w_m1 = w - 1
    w_p1 = w + 1

    h_m1_clamp = tl.maximum(h_m1, 0)
    h_p1_clamp = tl.minimum(h_p1, H - 1)
    w_m1_clamp = tl.maximum(w_m1, 0)
    w_p1_clamp = tl.minimum(w_p1, W - 1)

    # (h-1, w-1)
    mask00 = mask_p & (h > 0) & (w > 0)
    idx00 = base + h_m1_clamp * W + w_m1_clamp
    v00 = tl.load(x_ptr + idx00, mask=mask00, other=0.0)
    acc += tl.cast(v00, tl.float32) * tl.cast(w00, tl.float32)

    # (h-1, w)
    mask01 = mask_p & (h > 0)
    idx01 = base + h_m1_clamp * W + w
    v01 = tl.load(x_ptr + idx01, mask=mask01, other=0.0)
    acc += tl.cast(v01, tl.float32) * tl.cast(w01, tl.float32)

    # (h-1, w+1)
    mask02 = mask_p & (h > 0) & (w < W - 1)
    idx02 = base + h_m1_clamp * W + w_p1_clamp
    v02 = tl.load(x_ptr + idx02, mask=mask02, other=0.0)
    acc += tl.cast(v02, tl.float32) * tl.cast(w02, tl.float32)

    # (h, w-1)
    mask10 = mask_p & (w > 0)
    idx10 = base + h * W + w_m1_clamp
    v10 = tl.load(x_ptr + idx10, mask=mask10, other=0.0)
    acc += tl.cast(v10, tl.float32) * tl.cast(w10, tl.float32)

    # (h, w)
    mask11 = mask_p
    idx11 = base + h * W + w
    v11 = tl.load(x_ptr + idx11, mask=mask11, other=0.0)
    acc += tl.cast(v11, tl.float32) * tl.cast(w11, tl.float32)

    # (h, w+1)
    mask12 = mask_p & (w < W - 1)
    idx12 = base + h * W + w_p1_clamp
    v12 = tl.load(x_ptr + idx12, mask=mask12, other=0.0)
    acc += tl.cast(v12, tl.float32) * tl.cast(w12, tl.float32)

    # (h+1, w-1)
    mask20 = mask_p & (h < H - 1) & (w > 0)
    idx20 = base + h_p1_clamp * W + w_m1_clamp
    v20 = tl.load(x_ptr + idx20, mask=mask20, other=0.0)
    acc += tl.cast(v20, tl.float32) * tl.cast(w20, tl.float32)

    # (h+1, w)
    mask21 = mask_p & (h < H - 1)
    idx21 = base + h_p1_clamp * W + w
    v21 = tl.load(x_ptr + idx21, mask=mask21, other=0.0)
    acc += tl.cast(v21, tl.float32) * tl.cast(w21, tl.float32)

    # (h+1, w+1)
    mask22 = mask_p & (h < H - 1) & (w < W - 1)
    idx22 = base + h_p1_clamp * W + w_p1_clamp
    v22 = tl.load(x_ptr + idx22, mask=mask22, other=0.0)
    acc += tl.cast(v22, tl.float32) * tl.cast(w22, tl.float32)

    # Store result
    y_idx = base + offs_p
    tl.store(y_ptr + y_idx, acc, mask=mask_p)


@triton.jit
def channel_shuffle_kernel(
    x_ptr, y_ptr,
    B, C, H, W, G,
    BLOCK_M: tl.constexpr,  # over B*H*W
    BLOCK_N: tl.constexpr,  # over C
):
    P = H * W
    M = B * P

    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    mask_m = offs_m < M
    mask_n = offs_n < C

    # Map m -> (b, s) where s is flattened spatial index
    b = offs_m // P
    s = offs_m % P

    co = offs_n  # output channels

    Cpg = C // G

    # Map output channel to input channel: ci = (co % G) * Cpg + (co // G)
    ci = (co % G) * Cpg + (co // G)

    # 2D broadcasting
    bC = b * C  # (BM,)
    bC_2d = bC[:, None]
    s_2d = s[:, None]
    co_2d = co[None, :]
    ci_2d = ci[None, :]

    x_idx = (bC_2d + ci_2d) * P + s_2d
    y_idx = (bC_2d + co_2d) * P + s_2d

    mask = mask_m[:, None] & mask_n[None, :]

    vals = tl.load(x_ptr + x_idx, mask=mask, other=0.0)
    tl.store(y_ptr + y_idx, vals, mask=mask)


@triton.jit
def linear_gemm_kernel(
    a_ptr, b_ptr, bias_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    HAS_BIAS: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    mask_m = offs_m < M
    mask_n = offs_n < N

    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    for k in range(0, K, BLOCK_K):
        k_remaining = K - k
        k_mask = offs_k < k_remaining

        a_mask = mask_m[:, None] & k_mask[None, :]
        b_mask = k_mask[:, None] & mask_n[None, :]

        a = tl.load(a_ptrs, mask=a_mask, other=0.0)
        b = tl.load(b_ptrs, mask=b_mask, other=0.0)

        a = tl.cast(a, tl.float32)
        b = tl.cast(b, tl.float32)

        acc += tl.dot(a, b, allow_tf32=True)

        a_ptrs += BLOCK_K * stride_ak
        b_ptrs += BLOCK_K * stride_bk

    if HAS_BIAS:
        bias = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0)
        bias = tl.cast(bias, tl.float32)
        acc += bias[None, :]

    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    tl.store(c_ptrs, acc, mask=mask_m[:, None] & mask_n[None, :])


@triton.jit
def shufflenet_fused_kernel(
    x_ptr,                # [N, C_mid, H, W]
    w_dw_ptr,             # [C_mid * 9] depthwise 3x3 weights
    bn2_scale_ptr,        # [C_mid]
    bn2_bias_ptr,         # [C_mid]
    w_pw_ptr,             # [C_out, C_in_g]
    bn3_scale_ptr,        # [C_out]
    bn3_bias_ptr,         # [C_out]
    y_ptr,                # [N, C_out, H, W]
    N, C_mid, H, W,
    C_out, G,
    C_in_g, C_out_g,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """
    Fused kernel: depthwise 3x3 + BN2 + channel shuffle + 1x1 group conv (conv3) + BN3 + ReLU.

    Input:  x  = output of conv1+BN1+ReLU, shape [N, C_mid, H, W]
    Output: y  = output of ReLU(BN3(conv3(shuffle(BN2(depthwise3x3(x))))))
    """
    pid_m = tl.program_id(0)  # over NHW
    pid_n = tl.program_id(1)  # over C_out_g
    pid_g = tl.program_id(2)  # over groups

    HW = H * W
    M = N * HW

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    mask_m = offs_m < M
    mask_n = offs_n < C_out_g

    # Map m -> (n, h, w)
    bn = offs_m // HW
    rem = offs_m % HW
    h = rem // W
    w = rem % W

    # Global output channel indices
    oc = pid_g * C_out_g + offs_n  # (BLOCK_N,)

    # Prepare accumulation
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    P = HW
    Cin = C_mid
    CinP = Cin * P
    Cpg = C_in_g  # channels per group after shuffle

    for k0 in range(0, C_in_g, BLOCK_K):
        k_idx = k0 + offs_k  # per-group input channel indices 0..C_in_g-1
        k_mask = k_idx < C_in_g

        # Channel indices after shuffle for this group
        ci_shuffled = pid_g * C_in_g + k_idx  # [BK]

        # Map shuffled index -> original depthwise/Bn2 channel index
        ci_mod = ci_shuffled % G
        ci_div = ci_shuffled // G
        ci_orig = ci_mod * Cpg + ci_div  # [BK]

        # Load BN2 params and depthwise weights for these channels
        scale2 = tl.load(bn2_scale_ptr + ci_orig, mask=k_mask, other=0.0)
        bias2 = tl.load(bn2_bias_ptr + ci_orig, mask=k_mask, other=0.0)

        wbase = ci_orig * 9
        w00 = tl.load(w_dw_ptr + wbase + 0, mask=k_mask, other=0.0)
        w01 = tl.load(w_dw_ptr + wbase + 1, mask=k_mask, other=0.0)
        w02 = tl.load(w_dw_ptr + wbase + 2, mask=k_mask, other=0.0)
        w10 = tl.load(w_dw_ptr + wbase + 3, mask=k_mask, other=0.0)
        w11 = tl.load(w_dw_ptr + wbase + 4, mask=k_mask, other=0.0)
        w12 = tl.load(w_dw_ptr + wbase + 5, mask=k_mask, other=0.0)
        w20 = tl.load(w_dw_ptr + wbase + 6, mask=k_mask, other=0.0)
        w21 = tl.load(w_dw_ptr + wbase + 7, mask=k_mask, other=0.0)
        w22 = tl.load(w_dw_ptr + wbase + 8, mask=k_mask, other=0.0)

        scale2 = tl.cast(scale2, tl.float32)
        bias2 = tl.cast(bias2, tl.float32)
        w00 = tl.cast(w00, tl.float32)
        w01 = tl.cast(w01, tl.float32)
        w02 = tl.cast(w02, tl.float32)
        w10 = tl.cast(w10, tl.float32)
        w11 = tl.cast(w11, tl.float32)
        w12 = tl.cast(w12, tl.float32)
        w20 = tl.cast(w20, tl.float32)
        w21 = tl.cast(w21, tl.float32)
        w22 = tl.cast(w22, tl.float32)

        # Base index for (n, c_orig)
        base_nc = bn[:, None] * CinP + ci_orig[None, :] * P  # [BM, BK]

        # Precompute spatial offsets and masks
        h_m1 = h - 1
        h_p1 = h + 1
        w_m1 = w - 1
        w_p1 = w + 1

        h_m1_clamp = tl.maximum(h_m1, 0)
        h_p1_clamp = tl.minimum(h_p1, H - 1)
        w_m1_clamp = tl.maximum(w_m1, 0)
        w_p1_clamp = tl.minimum(w_p1, W - 1)

        conv = tl.zeros((BLOCK_M, BLOCK_K), dtype=tl.float32)

        # (h-1, w-1)
        mask00_m = mask_m & (h > 0) & (w > 0)
        sp00 = h_m1_clamp * W + w_m1_clamp
        idx00 = base_nc + sp00[:, None]
        mask00 = mask00_m[:, None] & k_mask[None, :]
        v00 = tl.load(x_ptr + idx00, mask=mask00, other=0.0)
        v00 = tl.cast(v00, tl.float32)
        conv += v00 * w00[None, :]

        # (h-1, w)
        mask01_m = mask_m & (h > 0)
        sp01 = h_m1_clamp * W + w
        idx01 = base_nc + sp01[:, None]
        mask01 = mask01_m[:, None] & k_mask[None, :]
        v01 = tl.load(x_ptr + idx01, mask=mask01, other=0.0)
        v01 = tl.cast(v01, tl.float32)
        conv += v01 * w01[None, :]

        # (h-1, w+1)
        mask02_m = mask_m & (h > 0) & (w < W - 1)
        sp02 = h_m1_clamp * W + w_p1_clamp
        idx02 = base_nc + sp02[:, None]
        mask02 = mask02_m[:, None] & k_mask[None, :]
        v02 = tl.load(x_ptr + idx02, mask=mask02, other=0.0)
        v02 = tl.cast(v02, tl.float32)
        conv += v02 * w02[None, :]

        # (h, w-1)
        mask10_m = mask_m & (w > 0)
        sp10 = h * W + w_m1_clamp
        idx10 = base_nc + sp10[:, None]
        mask10 = mask10_m[:, None] & k_mask[None, :]
        v10 = tl.load(x_ptr + idx10, mask=mask10, other=0.0)
        v10 = tl.cast(v10, tl.float32)
        conv += v10 * w10[None, :]

        # (h, w)
        mask11_m = mask_m
        sp11 = h * W + w
        idx11 = base_nc + sp11[:, None]
        mask11 = mask11_m[:, None] & k_mask[None, :]
        v11 = tl.load(x_ptr + idx11, mask=mask11, other=0.0)
        v11 = tl.cast(v11, tl.float32)
        conv += v11 * w11[None, :]

        # (h, w+1)
        mask12_m = mask_m & (w < W - 1)
        sp12 = h * W + w_p1_clamp
        idx12 = base_nc + sp12[:, None]
        mask12 = mask12_m[:, None] & k_mask[None, :]
        v12 = tl.load(x_ptr + idx12, mask=mask12, other=0.0)
        v12 = tl.cast(v12, tl.float32)
        conv += v12 * w12[None, :]

        # (h+1, w-1)
        mask20_m = mask_m & (h < H - 1) & (w > 0)
        sp20 = h_p1_clamp * W + w_m1_clamp
        idx20 = base_nc + sp20[:, None]
        mask20 = mask20_m[:, None] & k_mask[None, :]
        v20 = tl.load(x_ptr + idx20, mask=mask20, other=0.0)
        v20 = tl.cast(v20, tl.float32)
        conv += v20 * w20[None, :]

        # (h+1, w)
        mask21_m = mask_m & (h < H - 1)
        sp21 = h_p1_clamp * W + w
        idx21 = base_nc + sp21[:, None]
        mask21 = mask21_m[:, None] & k_mask[None, :]
        v21 = tl.load(x_ptr + idx21, mask=mask21, other=0.0)
        v21 = tl.cast(v21, tl.float32)
        conv += v21 * w21[None, :]

        # (h+1, w+1)
        mask22_m = mask_m & (h < H - 1) & (w < W - 1)
        sp22 = h_p1_clamp * W + w_p1_clamp
        idx22 = base_nc + sp22[:, None]
        mask22 = mask22_m[:, None] & k_mask[None, :]
        v22 = tl.load(x_ptr + idx22, mask=mask22, other=0.0)
        v22 = tl.cast(v22, tl.float32)
        conv += v22 * w22[None, :]

        # Fuse BN2: scale and bias
        conv = conv * scale2[None, :] + bias2[None, :]

        # Load 1x1 group conv weights for this K-chunk
        b_ptrs = w_pw_ptr + oc[None, :] * C_in_g + k_idx[:, None]
        b_mask = mask_n[None, :] & k_mask[:, None]
        B = tl.load(b_ptrs, mask=b_mask, other=0.0)
        B = tl.cast(B, tl.float32)

        # Accumulate GEMM
        acc += tl.dot(conv, B, allow_tf32=True)

    # Apply BN3 + ReLU
    scale3 = tl.load(bn3_scale_ptr + oc, mask=mask_n, other=0.0)
    bias3 = tl.load(bn3_bias_ptr + oc, mask=mask_n, other=0.0)
    scale3 = tl.cast(scale3, tl.float32)
    bias3 = tl.cast(bias3, tl.float32)

    acc = acc * scale3[None, :] + bias3[None, :]
    acc = tl.maximum(acc, 0.0)

    # Store result
    P = HW
    CoutP = C_out * P
    base_y = bn * CoutP + h * W + w  # [BM]
    y_ptrs = y_ptr + base_y[:, None] + oc[None, :] * P
    store_mask = mask_m[:, None] & mask_n[None, :]
    tl.store(y_ptrs, acc, mask=store_mask)


# ---------------------------
# Python wrappers
# ---------------------------

def grouped_pointwise_conv1x1_triton(x: torch.Tensor, weight: torch.Tensor, groups: int) -> torch.Tensor:
    """
    High-performance 1x1 grouped convolution using GEMM in Triton.
    x: [N, C_in, H, W]
    weight: [C_out, C_in/groups, 1, 1]
    """
    assert x.is_cuda and weight.is_cuda
    x = x.contiguous()
    weight = weight.contiguous()
    N, C_in, H, W = x.shape
    C_out, C_in_g, kH, kW = weight.shape
    assert kH == 1 and kW == 1
    assert C_in % groups == 0
    assert C_in_g == C_in // groups
    assert C_out % groups == 0
    C_out_g = C_out // groups

    y = torch.empty((N, C_out, H, W), device=x.device, dtype=x.dtype)
    w2 = weight.view(C_out, C_in_g).contiguous()

    BLOCK_M = 64
    BLOCK_N = 64
    BLOCK_K = 32

    M = N * H * W
    grid = (
        max(1, triton.cdiv(M, BLOCK_M)),
        max(1, triton.cdiv(C_out_g, BLOCK_N)),
        max(1, groups),
    )
    grouped_pointwise_conv1x1_kernel[grid](
        x, w2, y,
        N, C_in, H, W,
        C_out, groups,
        C_in_g, C_out_g,
        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,
    )
    return y


def depthwise_conv3x3_triton(x: torch.Tensor, weight: torch.Tensor) -> torch.Tensor:
    """
    Depthwise 3x3 convolution with stride=1, padding=1.
    x: [N, C, H, W]
    weight: [C, 1, 3, 3] or [C, 3, 3]
    """
    assert x.is_cuda and weight.is_cuda
    x = x.contiguous()
    N, C, H, W = x.shape

    if weight.dim() == 4:
        Cw, _, kH, kW = weight.shape
        assert Cw == C and kH == 3 and kW == 3
        w2 = weight.view(C, 3, 3).contiguous()
    else:
        Cw, kH, kW = weight.shape
        assert Cw == C and kH == 3 and kW == 3
        w2 = weight.contiguous()

    y = torch.empty_like(x)

    BLOCK_P = 256
    grid = (
        max(1, N * C),  # each program along axis-0 handles a single (n,c)
        max(1, triton.cdiv(H * W, BLOCK_P)),
    )
    depthwise_conv3x3_kernel[grid](
        x, w2.view(-1), y,
        N, C, H, W,
        BLOCK_P=BLOCK_P,
    )
    return y


def channel_shuffle_triton(x: torch.Tensor, groups: int) -> torch.Tensor:
    """
    Channel shuffle in Triton.
    x: [B, C, H, W]
    """
    assert x.is_cuda
    x = x.contiguous()
    B, C, H, W = x.shape
    assert C % groups == 0

    y = torch.empty_like(x)

    BLOCK_M = 64
    BLOCK_N = 64
    P = H * W
    M = B * P

    grid = (
        max(1, triton.cdiv(M, BLOCK_M)),
        max(1, triton.cdiv(C, BLOCK_N)),
    )
    channel_shuffle_kernel[grid](
        x, y,
        B, C, H, W, groups,
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
    )
    return y


def linear_triton(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:
    """
    Linear layer y = x @ weight.T + bias
    x: [M, K]
    weight: [N, K]
    bias: [N] or None
    """
    assert x.is_cuda and weight.is_cuda
    x = x.contiguous()
    weight = weight.contiguous()
    M, K = x.shape
    N = weight.shape[0]
    assert weight.shape[1] == K

    # B is [K, N]
    b_mat = weight.t().contiguous()

    y = torch.empty((M, N), device=x.device, dtype=x.dtype)

    BLOCK_M = 32
    BLOCK_N = 64
    BLOCK_K = 32

    grid = (
        max(1, triton.cdiv(M, BLOCK_M)),
        max(1, triton.cdiv(N, BLOCK_N)),
    )

    has_bias = bias is not None
    bias_ptr = bias if has_bias else x  # dummy if not used

    linear_gemm_kernel[grid](
        x, b_mat, bias_ptr, y,
        M, N, K,
        x.stride(0), x.stride(1),
        b_mat.stride(0), b_mat.stride(1),
        y.stride(0), y.stride(1),
        HAS_BIAS=has_bias,
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
        BLOCK_K=BLOCK_K,
    )
    return y


def shufflenet_fused_triton(
    x: torch.Tensor,
    conv2: nn.Conv2d,
    bn2: nn.BatchNorm2d,
    conv3: nn.Conv2d,
    bn3: nn.BatchNorm2d,
    groups: int,
) -> torch.Tensor:
    """
    Fused computation for ShuffleNet unit core:
    depthwise conv3x3 (conv2) + BN2 + channel shuffle + grouped 1x1 conv (conv3) + BN3 + ReLU.

    x: [N, C_mid, H, W]  (output of conv1+BN1+ReLU)
    Returns: tensor with shape [N, C_out, H, W]
    """
    assert x.is_cuda
    device = x.device
    dtype = x.dtype

    x = x.contiguous()
    N, C_mid, H, W = x.shape

    # conv2: depthwise 3x3
    assert isinstance(conv2, nn.Conv2d)
    assert conv2.in_channels == C_mid and conv2.out_channels == C_mid
    assert conv2.kernel_size == (3, 3)
    assert conv2.groups == C_mid
    w_dw = conv2.weight  # [C_mid, 1, 3, 3]
    if w_dw.dim() == 4:
        w_dw = w_dw.view(C_mid, 3, 3)
    w_dw = w_dw.contiguous().view(-1)  # [C_mid*9]

    # BN2 folding parameters (eval-mode statistics)
    running_mean2 = bn2.running_mean
    running_var2 = bn2.running_var
    weight2 = bn2.weight
    bias2 = bn2.bias
    eps2 = bn2.eps

    scale2 = weight2 / torch.sqrt(running_var2 + eps2)
    bias2_fused = bias2 - running_mean2 * scale2

    scale2 = scale2.to(device=device, dtype=dtype).contiguous()
    bias2_fused = bias2_fused.to(device=device, dtype=dtype).contiguous()

    # conv3: grouped 1x1
    assert isinstance(conv3, nn.Conv2d)
    C_out = conv3.out_channels
    assert conv3.in_channels == C_mid
    assert conv3.kernel_size == (1, 1)
    assert conv3.groups == groups
    w_pw = conv3.weight  # [C_out, C_mid/groups, 1, 1]
    C_out_w, C_in_g, _, _ = w_pw.shape
    assert C_out_w == C_out
    w_pw = w_pw.view(C_out, C_in_g).contiguous()

    # BN3 folding parameters
    running_mean3 = bn3.running_mean
    running_var3 = bn3.running_var
    weight3 = bn3.weight
    bias3 = bn3.bias
    eps3 = bn3.eps

    scale3 = weight3 / torch.sqrt(running_var3 + eps3)
    bias3_fused = bias3 - running_mean3 * scale3

    scale3 = scale3.to(device=device, dtype=dtype).contiguous()
    bias3_fused = bias3_fused.to(device=device, dtype=dtype).contiguous()

    assert C_mid % groups == 0
    assert C_out % groups == 0
    C_in_g_calc = C_mid // groups
    C_out_g = C_out // groups
    assert C_in_g_calc == C_in_g

    y = torch.empty((N, C_out, H, W), device=device, dtype=dtype)

    BLOCK_M = 32
    BLOCK_N = 32
    BLOCK_K = 16

    M = N * H * W
    grid = (
        max(1, triton.cdiv(M, BLOCK_M)),
        max(1, triton.cdiv(C_out_g, BLOCK_N)),
        max(1, groups),
    )

    shufflenet_fused_kernel[grid](
        x, w_dw, scale2, bias2_fused,
        w_pw, scale3, bias3_fused,
        y,
        N, C_mid, H, W,
        C_out, groups,
        C_in_g_calc, C_out_g,
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
        BLOCK_K=BLOCK_K,
    )
    return y


# ---------------------------
# Triton-based modules
# ---------------------------

class ChannelShuffleNew(nn.Module):
    def __init__(self, groups: int):
        super(ChannelShuffleNew, self).__init__()
        self.groups = groups

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if not x.is_cuda:
            # Fallback to PyTorch for CPU
            b, c, h, w = x.size()
            g = self.groups
            assert c % g == 0
            x = x.view(b, g, c // g, h, w).transpose(1, 2).contiguous()
            return x.view(b, c, h, w)
        return channel_shuffle_triton(x, self.groups)


class ShuffleNetUnitNew(nn.Module):
    def __init__(self, in_channels, out_channels, groups=3):
        super(ShuffleNetUnitNew, self).__init__()

        assert out_channels % 4 == 0
        mid_channels = out_channels // 4

        # Keep Conv/Bn modules for parameters; computations are overridden in forward on CUDA.
        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1,
                               padding=0, groups=groups, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_channels)

        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1,
                               padding=1, groups=mid_channels, bias=False)
        self.bn2 = nn.BatchNorm2d(mid_channels)

        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1,
                               padding=0, groups=groups, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels)

        self.shuffle = ChannelShuffleNew(groups)
        self.groups = groups

        if in_channels == out_channels:
            self.use_shortcut = False
            self.shortcut = nn.Sequential()
        else:
            self.use_shortcut = True
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1,
                          padding=0, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # CPU path: always use reference behavior (PyTorch ops)
        if not x.is_cuda:
            out = torch.relu(self.bn1(self.conv1(x)))
            out = self.bn2(self.conv2(out))
            out = self.shuffle(out)
            out = torch.relu(self.bn3(self.conv3(out)))
            residual = self.shortcut(x) if self.use_shortcut else x
            out = out + residual
            return out

        # CUDA path
        # First 1x1 grouped conv (conv1) via Triton
        out = grouped_pointwise_conv1x1_triton(x, self.conv1.weight, self.conv1.groups)
        out = self.bn1(out)
        out = torch.relu(out)

        # Residual branch
        if self.use_shortcut:
            residual = grouped_pointwise_conv1x1_triton(
                x, self.shortcut[0].weight, self.shortcut[0].groups
            )
            residual = self.shortcut[1](residual)
        else:
            residual = x

        # Fused core path (only in eval mode where BN uses running stats)
        if (not self.bn2.training) and (not self.bn3.training):
            out = shufflenet_fused_triton(
                out,
                self.conv2, self.bn2,
                self.conv3, self.bn3,
                self.groups,
            )
        else:
            # Training-mode or unusual BN config: fall back to unfused path
            out = depthwise_conv3x3_triton(out, self.conv2.weight)
            out = self.bn2(out)
            out = self.shuffle(out)
            out = grouped_pointwise_conv1x1_triton(out, self.conv3.weight, self.conv3.groups)
            out = self.bn3(out)
            out = torch.relu(out)

        out = out + residual
        return out


class ModelNew(nn.Module):
    def __init__(
        self,
        num_classes=1000,
        groups=3,
        stages_repeats=[3, 7, 3],
        stages_out_channels=[24, 240, 480, 960],
    ):
        super(ModelNew, self).__init__()

        # Stem conv (3x3, stride2) – kept as PyTorch Conv2d; single layer, relatively low cost
        self.conv1 = nn.Conv2d(3, stages_out_channels[0], kernel_size=3, stride=2,
                               padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.stage2 = self._make_stage(stages_out_channels[0], stages_out_channels[1],
                                       stages_repeats[0], groups)
        self.stage3 = self._make_stage(stages_out_channels[1], stages_out_channels[2],
                                       stages_repeats[1], groups)
        self.stage4 = self._make_stage(stages_out_channels[2], stages_out_channels[3],
                                       stages_repeats[2], groups)

        # Final 1x1 conv – will be run via Triton 1x1 kernel in forward
        self.conv5 = nn.Conv2d(stages_out_channels[3], 1024, kernel_size=1, stride=1,
                               padding=0, bias=False)
        self.bn5 = nn.BatchNorm2d(1024)

        self.fc = nn.Linear(1024, num_classes)

    def _make_stage(self, in_channels, out_channels, repeats, groups):
        layers = [ShuffleNetUnitNew(in_channels, out_channels, groups)]
        for _ in range(1, repeats):
            layers.append(ShuffleNetUnitNew(out_channels, out_channels, groups))
        return nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Stem
        x = self.conv1(x)
        x = self.bn1(x)
        x = torch.relu(x)
        x = self.maxpool(x)

        # Stages
        x = self.stage2(x)
        x = self.stage3(x)
        x = self.stage4(x)

        # Final 1x1 conv via Triton when on GPU
        if x.is_cuda:
            x = grouped_pointwise_conv1x1_triton(x, self.conv5.weight, groups=1)
        else:
            x = self.conv5(x)
        x = self.bn5(x)
        x = torch.relu(x)

        # Global average pool
        x = torch.nn.functional.adaptive_avg_pool2d(x, (1, 1))
        x = x.view(x.size(0), -1)

        # FC via Triton GEMM when on GPU
        if x.is_cuda:
            x = linear_triton(x, self.fc.weight, self.fc.bias)
        else:
            x = self.fc(x)
        return x
```