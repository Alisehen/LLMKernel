```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_HW": 64}, num_warps=2),
        triton.Config({"BLOCK_HW": 128}, num_warps=4),
        triton.Config({"BLOCK_HW": 256}, num_warps=4),
        triton.Config({"BLOCK_HW": 512}, num_warps=8),
    ],
    key=["HW", "C"],
)
@triton.jit
def channel_shuffle_kernel(
    x_ptr, y_ptr,
    C, HW,
    groups, channels_per_group,
    BLOCK_HW: tl.constexpr,
):
    # 3D launch:
    #   pid_n  : batch index  [0, N)
    #   pid_c  : output channel index [0, C)
    #   pid_hw : block id over spatial elements [0, ceil_div(H*W, BLOCK_HW))
    pid_n = tl.program_id(0)
    pid_c_out = tl.program_id(1)
    pid_hw = tl.program_id(2)

    # Local offsets over HW dimension
    offs_hw = pid_hw * BLOCK_HW + tl.arange(0, BLOCK_HW)
    mask = offs_hw < HW
    tl.multiple_of(offs_hw, BLOCK_HW)
    tl.max_contiguous(offs_hw, BLOCK_HW)

    # Compute input channel index for shuffle:
    # c_out = idx * groups + group
    # idx = c_out // groups
    # group = c_out % groups
    group = pid_c_out % groups
    idx = pid_c_out // groups
    c_in = group * channels_per_group + idx

    # Base offsets for this (n, c_out) / (n, c_in) pair
    base_out = (pid_n * C + pid_c_out) * HW
    base_in = (pid_n * C + c_in) * HW

    x_offsets = base_in + offs_hw
    y_offsets = base_out + offs_hw

    x_vals = tl.load(x_ptr + x_offsets, mask=mask, other=0)
    tl.store(y_ptr + y_offsets, x_vals, mask=mask)


def channel_shuffle_triton(x: torch.Tensor, groups: int) -> torch.Tensor:
    """
    High-performance Triton implementation of channel shuffle.
    Input/Output: (N, C, H, W), with C % groups == 0.
    """
    assert x.is_cuda, "Triton channel_shuffle requires CUDA tensor"
    x = x.contiguous()
    N, C, H, W = x.shape
    assert C % groups == 0
    channels_per_group = C // groups
    HW = H * W

    y = torch.empty_like(x)

    def grid(meta):
        return (N, C, triton.cdiv(HW, meta["BLOCK_HW"]))

    channel_shuffle_kernel[grid](
        x, y,
        C, HW,
        groups, channels_per_group,
    )
    return y


class ChannelShuffleTriton(nn.Module):
    def __init__(self, groups: int):
        super(ChannelShuffleTriton, self).__init__()
        self.groups = groups

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if (not x.is_cuda) or (not x.is_contiguous()):
            # Fallback to native implementation on CPU or non-contiguous tensors
            batch_size, channels, height, width = x.size()
            channels_per_group = channels // self.groups
            x = x.view(batch_size, self.groups, channels_per_group, height, width)
            x = x.transpose(1, 2).contiguous()
            x = x.view(batch_size, -1, height, width)
            return x
        return channel_shuffle_triton(x, self.groups)


class ShuffleNetUnitNew(nn.Module):
    def __init__(self, in_channels, out_channels, groups=3):
        """
        ShuffleNet unit with Triton-accelerated channel shuffle.
        """
        super(ShuffleNetUnitNew, self).__init__()

        assert out_channels % 4 == 0
        mid_channels = out_channels // 4

        # First 1x1 group convolution
        self.conv1 = nn.Conv2d(
            in_channels, mid_channels,
            kernel_size=1, stride=1, padding=0,
            groups=groups, bias=False
        )
        self.bn1 = nn.BatchNorm2d(mid_channels)

        # Depthwise 3x3 convolution
        self.conv2 = nn.Conv2d(
            mid_channels, mid_channels,
            kernel_size=3, stride=1, padding=1,
            groups=mid_channels, bias=False
        )
        self.bn2 = nn.BatchNorm2d(mid_channels)

        # Second 1x1 group convolution
        self.conv3 = nn.Conv2d(
            mid_channels, out_channels,
            kernel_size=1, stride=1, padding=0,
            groups=groups, bias=False
        )
        self.bn3 = nn.BatchNorm2d(out_channels)

        # Triton-based shuffle
        self.shuffle = ChannelShuffleTriton(groups)

        # Shortcut connection
        if in_channels == out_channels:
            self.shortcut = nn.Sequential()
        else:
            self.shortcut = nn.Sequential(
                nn.Conv2d(
                    in_channels, out_channels,
                    kernel_size=1, stride=1, padding=0, bias=False
                ),
                nn.BatchNorm2d(out_channels),
            )

    def forward(self, x):
        out = self.bn1(self.conv1(x))
        out = torch.relu(out)
        out = self.bn2(self.conv2(out))
        out = self.shuffle(out)
        out = self.bn3(self.conv3(out))
        out = torch.relu(out)
        out = out + self.shortcut(x)
        return out


class ModelNew(nn.Module):
    def __init__(
        self,
        num_classes=1000,
        groups=3,
        stages_repeats=[3, 7, 3],
        stages_out_channels=[24, 240, 480, 960],
    ):
        """
        ShuffleNet architecture with Triton-accelerated channel shuffle.
        """
        super(ModelNew, self).__init__()

        self.conv1 = nn.Conv2d(
            3, stages_out_channels[0],
            kernel_size=3, stride=2, padding=1, bias=False
        )
        self.bn1 = nn.BatchNorm2d(stages_out_channels[0])
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.stage2 = self._make_stage(
            stages_out_channels[0],
            stages_out_channels[1],
            stages_repeats[0],
            groups,
        )
        self.stage3 = self._make_stage(
            stages_out_channels[1],
            stages_out_channels[2],
            stages_repeats[1],
            groups,
        )
        self.stage4 = self._make_stage(
            stages_out_channels[2],
            stages_out_channels[3],
            stages_repeats[2],
            groups,
        )

        self.conv5 = nn.Conv2d(
            stages_out_channels[3],
            1024,
            kernel_size=1, stride=1, padding=0, bias=False
        )
        self.bn5 = nn.BatchNorm2d(1024)

        self.fc = nn.Linear(1024, num_classes)

    def _make_stage(self, in_channels, out_channels, repeats, groups):
        layers = []
        layers.append(ShuffleNetUnitNew(in_channels, out_channels, groups))
        for _ in range(1, repeats):
            layers.append(ShuffleNetUnitNew(out_channels, out_channels, groups))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.bn1(self.conv1(x))
        x = torch.relu(x)
        x = self.maxpool(x)

        x = self.stage2(x)
        x = self.stage3(x)
        x = self.stage4(x)

        x = self.bn5(self.conv5(x))
        x = torch.relu(x)
        x = torch.nn.functional.adaptive_avg_pool2d(x, (1, 1))
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x
```