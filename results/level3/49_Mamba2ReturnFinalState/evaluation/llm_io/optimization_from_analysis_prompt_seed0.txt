You are optimizing a Triton kernel based on algorithmic analysis.

# PyTorch Reference (Target Behavior)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

class Model(nn.Module):
    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):
        """
        Mamba Structured State Space model implementation for benchmarking.
        
        :param batch_size: Size of the batch
        :param seq_length: Length of the input sequence
        :param n_heads: Number of attention heads
        :param d_head: Dimension of each head
        :param d_state: Dimension of the state space
        :param block_len: Length of each block for chunked computation
        """
        super(Model, self).__init__()
        
        assert seq_length % block_len == 0, "Sequence length must be divisible by block length"
        
        self.batch_size = batch_size
        self.seq_length = seq_length
        self.n_heads = n_heads
        self.d_head = d_head
        self.d_state = d_state
        self.block_len = block_len
        
        # Initialize parameters
        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))
        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))
        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))
        
    def segsum(self, x):
        """Naive segment sum calculation."""
        T = x.size(-1)
        x_cumsum = torch.cumsum(x, dim=-1)
        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]
        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)
        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)
        return x_segsum
    
    def forward(self, X, initial_states=None):
        """
        Forward pass implementing the SSD operation.
        
        :param X: Input tensor of shape (batch, length, n_heads, d_head)
        :param initial_states: Optional initial states
        :return: Output tensor Y and final state
        """
        # Rearrange into blocks/chunks
        X_blocks, A_blocks, B_blocks, C_blocks = [
            rearrange(x, "b (c l) ... -> b c l ...", l=self.block_len)
            for x in (X, self.A, self.B, self.C)
        ]
        
        A_blocks = rearrange(A_blocks, "b c l h -> b h c l")
        A_cumsum = torch.cumsum(A_blocks, dim=-1)
        
        # 1. Compute diagonal block outputs
        L = torch.exp(self.segsum(A_blocks))
        Y_diag = torch.einsum("bclhn,bcshn,bhcls,bcshp->bclhp", 
                             C_blocks, B_blocks, L, X_blocks)
        
        # 2. Compute intra-chunk states
        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))
        states = torch.einsum("bclhn,bhcl,bclhp->bchpn", 
                            B_blocks, decay_states, X_blocks)
        
        # 3. Compute inter-chunk recurrence
        if initial_states is None:
            initial_states = torch.zeros_like(states[:, :1])
        states = torch.cat([initial_states, states], dim=1)
        
        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))
        new_states = torch.einsum("bhzc,bchpn->bzhpn", decay_chunk, states)
        return new_states[:, -1]

# Test parameters
batch_size = 2048
seq_length = 128
n_heads = 8
d_head = 64
d_state = 16
block_len = 64

def get_inputs():
    return [torch.rand(batch_size, seq_length, n_heads, d_head)]

def get_init_inputs():
    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]
```

**CRITICAL**: Study the PyTorch code carefully to understand:
- What does `forward()` return? (full output sequence vs final hidden state only)
- What is the computational pattern?
- What are the input/output shapes?

Your optimized kernel MUST match this exact behavior.

---

# Analysis Results

**Bottleneck**: The current y_diag_kernel loops over all source positions s and time positions t (quadratic in BLOCK_L) and over d_state, performing a dense causal matmul with implicit L[t,s]=u[t]v[s] plus a causal mask. This quadratic work and repeated global-memory traffic for B/C/X dominate the runtime and negate any benefit over the baseline einsum.

**Optimization Strategy**: Replace the quadratic exp(segsum(A))-based formulation of Y_diag with a linear-time online scan: maintain a per-(b,c,h) prefix state S_t over (d_state, d_head) updated as S_t = S_{t-1} + v_t * (B_t ⊗ X_t), and compute Y_t = u_t * (C_t · S_t), thus removing the inner loop over s and the causal mask.

**Implementation Plan**: Derive the recurrence: define S_t[n,p] = Σ_{s≤t} v_s * B[s,n] * X[s,p]; this satisfies S_t = S_{t-1} + v_t * (B_t ⊗ X_t). Then Y[t,p] = u_t * Σ_n C[t,n] * S_t[n,p], which implements the same operator as the original causal exp(segsum(A)) weighting but in O(L * d_state * P). Implement a new Triton kernel that, for each (b,c,h) group, loops only over t (time), keeps S_t in registers/shared or L2-backed tiles, updates S_t from the current B_t, v_t, X_t, immediately computes Y_t with a dot over n using C_t and u_t, and writes Y_t; remove the inner s-loop, causal_mask, and per-(t,s) recomputation. Replace the existing y_diag_kernel call in fused_y_diag with this scan-based kernel while keeping the high-level interface unchanged.

**Expected Speedup**: 30-40% end-to-end (2-3x speedup for the y_diag part, which is currently the dominant cost).

---

# Current Kernel (needs optimization)

```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def y_diag_kernel(
    B_ptr, C_ptr, X_ptr, u_ptr, v_ptr, Y_ptr,
    M, P,
    stride_bm, stride_bl, stride_bn,
    stride_cm, stride_cl, stride_cn,
    stride_xm, stride_xl, stride_xp,
    stride_um, stride_ul,
    stride_vm, stride_vl,
    stride_ym, stride_yl, stride_yp,
    BLOCK_L: tl.constexpr, BLOCK_P: tl.constexpr, D_STATE: tl.constexpr,
):
    """
    Compute Y_diag for one (b,c,h) group using implicit exp(segsum(A)) weights.

    Shapes for flattened inputs:
      B_flat: [M, BLOCK_L, D_STATE]
      C_flat: [M, BLOCK_L, D_STATE]
      X_flat: [M, BLOCK_L, P]
      u_flat: [M, BLOCK_L]        where u_t = exp(cumsum(A)_t)
      v_flat: [M, BLOCK_L]        where v_s = exp(-cumsum(A)_s)
      Y_flat: [M, BLOCK_L, P]

    For each group m in [0, M):
      Y[t, p] = sum_{s <= t, n} C[t, n] * B[s, n] * u[t] * v[s] * X[s, p]
    which is equivalent to
      L[t, s] = exp(segsum(A)[t, s]) = u[t] * v[s]  (if t >= s else 0)
      Y[t, p] = sum_{s, n} C[t, n] * B[s, n] * L[t, s] * X[s, p]
    """
    pid_m = tl.program_id(0)
    pid_p = tl.program_id(1)

    # Offsets along block dimensions
    offs_l = tl.arange(0, BLOCK_L)  # time index within block
    offs_p = pid_p * BLOCK_P + tl.arange(0, BLOCK_P)

    # Mask for head dimension
    p_mask = offs_p < P

    # Row pointers for this (b,c,h) group
    B_row = B_ptr + pid_m * stride_bm
    C_row = C_ptr + pid_m * stride_cm
    X_row = X_ptr + pid_m * stride_xm
    u_row = u_ptr + pid_m * stride_um
    v_row = v_ptr + pid_m * stride_vm
    Y_row = Y_ptr + pid_m * stride_ym

    # Load u[t] = exp(cumsum(A)_t)
    u_t = tl.load(u_row + offs_l * stride_ul)

    # Accumulator for Y: [BLOCK_L, BLOCK_P]
    acc = tl.zeros((BLOCK_L, BLOCK_P), dtype=tl.float32)

    # Loop over source positions s (within block)
    for s in range(0, BLOCK_L):
        # v_s = exp(-cumsum(A)_s), scalar
        v_s = tl.load(v_row + s * stride_vl)

        # Temporary accumulator w[t] for this s: w[t] = sum_n u[t]*C[t,n]*v_s*B[s,n]
        w = tl.zeros((BLOCK_L,), dtype=tl.float32)

        # Loop over state dimension n
        for n_idx in range(0, D_STATE):
            # C[m, t, n_idx] -> [BLOCK_L]
            c_col = tl.load(C_row + offs_l * stride_cl + n_idx * stride_cn)
            # B[m, s, n_idx] -> scalar
            b_sn = tl.load(B_row + s * stride_bl + n_idx * stride_bn)
            # Accumulate: w += u_t * c_col * v_s * b_sn
            w += u_t * c_col * v_s * b_sn

        # Causal mask: only positions t >= s contribute
        causal_mask = offs_l >= s
        w = tl.where(causal_mask, w, 0.0)

        # Load X[m, s, p] -> [BLOCK_P]
        x_vec = tl.load(
            X_row + s * stride_xl + offs_p * stride_xp,
            mask=p_mask,
            other=0.0,
        )

        # Outer product: acc[t, p] += w[t] * x_vec[p]
        acc += w[:, None] * x_vec[None, :]

    # Store Y[m, t, p]
    mask_y = tl.broadcast_to(p_mask[None, :], (BLOCK_L, BLOCK_P))
    tl.store(
        Y_row + offs_l[:, None] * stride_yl + offs_p[None, :] * stride_yp,
        acc,
        mask=mask_y,
    )


def fused_y_diag(C_blocks, B_blocks, X_blocks, A_cumsum):
    """
    Fused computation of Y_diag using Triton.

    Original PyTorch:
      L = torch.exp(segsum(A_blocks))   # A_blocks: (b, h, c, l)
      Y_diag = torch.einsum(
          "bclhn,bcshn,bhcls,bcshp->bclhp",
          C_blocks, B_blocks, L, X_blocks
      )

    Here we avoid materializing L by using:
      u = exp(cumsum(A))
      v = exp(-cumsum(A))
      L[t,s] = u[t] * v[s]  for t >= s else 0
    """
    # Shapes
    b, c, l, h, n = B_blocks.shape
    _, _, _, _, p = X_blocks.shape
    # A_cumsum: (b, h, c, l)

    # Precompute u_t = exp(cumsum(A)_t), v_s = exp(-cumsum(A)_s)
    # A_cumsum is already cumsum(A_blocks) along last dim.
    u = torch.exp(A_cumsum)      # (b, h, c, l)
    v = torch.exp(-A_cumsum)     # (b, h, c, l)

    # Reorder u, v to (b, c, h, l) so we can flatten (b, c, h)
    u = u.permute(0, 2, 1, 3).contiguous()  # (b, c, h, l)
    v = v.permute(0, 2, 1, 3).contiguous()  # (b, c, h, l)

    # Reorder B, C, X to (b, c, h, l, *)
    B_perm = B_blocks.permute(0, 1, 3, 2, 4).contiguous()  # (b, c, h, l, n)
    C_perm = C_blocks.permute(0, 1, 3, 2, 4).contiguous()  # (b, c, h, l, n)
    X_perm = X_blocks.permute(0, 1, 3, 2, 4).contiguous()  # (b, c, h, l, p)

    # Flatten outer dimensions (b, c, h) -> M
    M = b * c * h
    B_flat = B_perm.view(M, l, n)
    C_flat = C_perm.view(M, l, n)
    X_flat = X_perm.view(M, l, p)
    u_flat = u.view(M, l)
    v_flat = v.view(M, l)

    # Allocate output
    Y_flat = torch.empty((M, l, p), device=X_blocks.device, dtype=X_blocks.dtype)

    # Strides
    stride_bm, stride_bl, stride_bn = B_flat.stride()
    stride_cm, stride_cl, stride_cn = C_flat.stride()
    stride_xm, stride_xl, stride_xp = X_flat.stride()
    stride_um, stride_ul = u_flat.stride()
    stride_vm, stride_vl = v_flat.stride()
    stride_ym, stride_yl, stride_yp = Y_flat.stride()

    BLOCK_L = l  # block_len; assumed <= 256 and power-of-two in benchmark
    BLOCK_P = 64
    # Ensure BLOCK_P is power-of-two and reasonable
    if BLOCK_P > 256:
        BLOCK_P = 256
    # Grid
    grid = lambda META: (
        M,
        triton.cdiv(p, META['BLOCK_P']),
    )

    y_diag_kernel[grid](
        B_flat, C_flat, X_flat, u_flat, v_flat, Y_flat,
        M, p,
        stride_bm, stride_bl, stride_bn,
        stride_cm, stride_cl, stride_cn,
        stride_xm, stride_xl, stride_xp,
        stride_um, stride_ul,
        stride_vm, stride_vl,
        stride_ym, stride_yl, stride_yp,
        BLOCK_L=BLOCK_L, BLOCK_P=BLOCK_P, D_STATE=n,
    )

    # Reshape back to (b, c, l, h, p)
    Y_perm = Y_flat.view(b, c, h, l, p)
    Y_diag = Y_perm.permute(0, 1, 3, 2, 4).contiguous()  # (b, c, l, h, p)
    return Y_diag


class ModelNew(nn.Module):
    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):
        """
        Triton-optimized Mamba Structured State Space model.
        """
        super(ModelNew, self).__init__()

        assert seq_length % block_len == 0, "Sequence length must be divisible by block length"

        self.batch_size = batch_size
        self.seq_length = seq_length
        self.n_heads = n_heads
        self.d_head = d_head
        self.d_state = d_state
        self.block_len = block_len

        # Parameters: same shapes as original model
        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))
        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))
        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))

    def segsum(self, x):
        """Segment sum along the last dimension (naive implementation, as in original)."""
        T = x.size(-1)
        x_cumsum = torch.cumsum(x, dim=-1)
        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]
        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=0)
        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)
        return x_segsum

    def forward(self, X, initial_states=None):
        """
        Forward pass implementing the SSD operation, with the heavy
        diagonal-block computation accelerated by Triton.
        """
        b, seq_len, h, p = X.shape
        l = self.block_len
        c = seq_len // l

        # 0. Rearrange into blocks/chunks: "b (c l) ... -> b c l ..."
        X_blocks = X.view(b, c, l, h, p)
        A_blocks = self.A.view(b, c, l, h)
        B_blocks = self.B.view(b, c, l, h, self.d_state)
        C_blocks = self.C.view(b, c, l, h, self.d_state)

        # A_blocks: (b, c, l, h) -> (b, h, c, l)
        A_bhcl = A_blocks.permute(0, 3, 1, 2).contiguous()
        A_cumsum = torch.cumsum(A_bhcl, dim=-1)  # (b, h, c, l)

        # 1. Compute diagonal block outputs (Y_diag) with fused Triton kernel
        Y_diag = fused_y_diag(C_blocks, B_blocks, X_blocks, A_cumsum)

        # 2. Compute intra-chunk states (same as original, using A_cumsum)
        decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)  # (b, h, c, l)
        states = torch.einsum(
            "bclhn,bhcl,bclhp->bchpn",
            B_blocks, decay_states, X_blocks
        )  # (b, c, h, p, n_state)

        # 3. Compute inter-chunk recurrence
        if initial_states is None:
            initial_states = torch.zeros_like(states[:, :1])  # (b, 1, h, p, n)

        states = torch.cat([initial_states, states], dim=1)  # (b, c+1, h, p, n)

        # Prepare chunk-level A sums for recurrence
        A_end = A_cumsum[:, :, :, -1]  # (b, h, c)
        zero_pad = torch.zeros_like(A_end[..., :1])  # (b, h, 1)
        A_padded = torch.cat([zero_pad, A_end], dim=-1)  # (b, h, c+1)

        decay_chunk = torch.exp(self.segsum(A_padded))  # (b, h, c+1, c+1)
        new_states = torch.einsum(
            "bhzc,bchpn->bzhpn",
            decay_chunk, states
        )  # (b, c+1, h, p, n)

        # Return final state as in original
        return new_states[:, -1]
```

---

# Your Task

Implement the optimization strategy above. Focus on the specific bottleneck identified.

## Key Requirements

1. **Preserve correctness**: Maintain the same input/output behavior
2. **Apply the optimization**: Follow the implementation plan exactly
3. **Use valid Triton syntax**:
   - Every kernel MUST have `@triton.jit` decorator
   - Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
   - BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
   - No `continue`, `break`, `return` inside kernels (use masking)
   - Prefer `tl.dot(a, b, allow_tf32=True)` for matmul operations

4. **CRITICAL for RNN/GRU/LSTM Persistent Kernels**:
   - Time loop MUST be inside @triton.jit kernel, NOT in Python forward()
   - **HYBRID computation strategy** (CRITICAL for performance):
     * Precompute input-side gates OUTSIDE kernel: `gates_x = (T*B, In) @ W_ih` (ONE large GEMM)
     * INSIDE kernel: only recurrent-side: `for t: gates_h = h @ W_hh` (T small GEMMs)
   - CORRECT (FAST - use this):
     ```python
     # Python forward():
     gates_x_all = x.reshape(T*B, In) @ W_ih + b_ih  # ONE large GEMM
     gates_x_all = gates_x_all.view(T, B, 3*H)
     gru_persistent_kernel[grid](gates_x_all, h0, W_hh, ...)  # Launch ONCE

     @triton.jit
     def gru_persistent_kernel(gates_x_ptr, h_ptr, W_hh_ptr, ...):
         for t in range(T):  # Inside kernel
             gates_x_t = tl.load(gates_x_ptr + t*...)  # Precomputed
             gates_h = h @ W_hh  # Only recurrent GEMM
             h = (1-z)*n + z*h   # Fuse and update
     ```

5. **Output format**:
   - Imports: `import torch, torch.nn as nn, triton, triton.language as tl`
   - `@triton.jit` kernel(s)
   - Wrapper function(s)
   - `class ModelNew(nn.Module)` — REQUIRED
   - NO testing code, NO `if __name__ == "__main__"`

---

Generate the optimized kernel now. Output ONLY the complete Python code.
