You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]f

## Current Optimization Stage

Focus: Grid layout & indexing for FUSED operations.

⚠️ FUSION EXCLUSIONS (do NOT apply fusion rules to these):
- Reduction ops (sum, mean, softmax along axis)
- Atomic operations
- Irregular/data-dependent access patterns
- Cross-block dependencies

Key Principle:
- All fused ops share the SAME grid AND the SAME (offsets, mask) tuple
- Grid covers OUTPUT tensor dimensions

Hard Rules:
- Every fused op MUST use identical offset calculation
- Every fused op MUST use identical boundary mask
- If broadcast needed: explicit `[None, :]` or `[:, None]`, NOT different offsets
- Element-wise: 1D grid, single `offs = pid * BLOCK + tl.arange(0, BLOCK)`
- Matmul fusion: 2D grid, `offs_m/offs_n` shared by bias add & activation

Verification:
- Check: all tl.load/tl.store use same `offsets` variable
- Check: all masks derived from same boundary condition
- If ANY op needs different indexing → do NOT fuse, split kernel



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import triton
import triton.language as tl
from einops import rearrange


@triton.jit
def segsum_from_cumsum_kernel(
    cs_ptr, out_ptr,
    B, T,
    stride_cs_b, stride_cs_t,
    stride_out_b, stride_out_i, stride_out_j,
    BLOCK_I: tl.constexpr, BLOCK_J: tl.constexpr,
):
    """
    Compute segmented sums from a precomputed cumulative sum tensor.

    Given cs[b, t] = cumsum(x[b, t]) along t, produce out[b, i, j] = cs[b, i] - cs[b, j]
    for i >= j (lower-triangular including diagonal), and -inf otherwise.

    Shapes:
      cs:  (B, T)
      out: (B, T, T)
    """
    pid_b = tl.program_id(0)
    pid_i = tl.program_id(1)
    pid_j = tl.program_id(2)

    offs_i = pid_i * BLOCK_I + tl.arange(0, BLOCK_I)
    offs_j = pid_j * BLOCK_J + tl.arange(0, BLOCK_J)

    mask_i = offs_i < T
    mask_j = offs_j < T

    # Load cs[b, i] and cs[b, j] vectors
    cs_row_ptr = cs_ptr + pid_b * stride_cs_b
    cs_i = tl.load(cs_row_ptr + offs_i * stride_cs_t, mask=mask_i, other=0.0)
    cs_j = tl.load(cs_row_ptr + offs_j * stride_cs_t, mask=mask_j, other=0.0)

    # Pairwise differences: cs_i[:, None] - cs_j[None, :]
    diff = cs_i[:, None] - cs_j[None, :]

    # Lower-triangular mask including diagonal (i >= j), and valid indices
    tri_mask = (offs_i[:, None] >= offs_j[None, :]) & mask_i[:, None] & mask_j[None, :]

    neg_inf = -float("inf")
    diff = tl.where(tri_mask, diff, neg_inf)

    out_row_ptr = out_ptr + pid_b * stride_out_b
    out_ptrs = out_row_ptr + offs_i[:, None] * stride_out_i + offs_j[None, :] * stride_out_j

    mask_out = mask_i[:, None] & mask_j[None, :]
    tl.store(out_ptrs, diff, mask=mask_out)


def segsum_triton(x: torch.Tensor) -> torch.Tensor:
    """
    Triton implementation of the segsum function:

      x_cumsum = torch.cumsum(x, dim=-1)
      x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]
      mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)
      x_segsum = x_segsum.masked_fill(~mask, -inf)

    This function matches the semantics above but uses Triton to efficiently
    form the (T, T) segmented sums for each batch element.
    """
    assert x.is_cuda, "Input must be on CUDA device for Triton kernel"

    orig_shape = x.shape  # (..., T)
    T = orig_shape[-1]
    if T <= 0:
        raise ValueError("Last dimension T must be > 0")

    # Flatten all leading dimensions into a single batch dimension B
    x_reshaped = x.reshape(-1, T).contiguous()  # (B, T)
    B = x_reshaped.shape[0]

    # Compute cumsum along the last dimension using PyTorch (O(B*T))
    cs = torch.cumsum(x_reshaped, dim=-1)

    # Allocate output (B, T, T)
    out = torch.empty((B, T, T), device=x.device, dtype=x.dtype)

    BLOCK_I = 32
    BLOCK_J = 32

    grid = lambda META: (
        B,
        triton.cdiv(T, META["BLOCK_I"]),
        triton.cdiv(T, META["BLOCK_J"]),
    )
    segsum_from_cumsum_kernel[grid](
        cs, out,
        B, T,
        cs.stride(0), cs.stride(1),
        out.stride(0), out.stride(1), out.stride(2),
        BLOCK_I=BLOCK_I, BLOCK_J=BLOCK_J,
    )

    # Reshape back to original shape with extra T dimension
    return out.reshape(*orig_shape, T)


class ModelNew(nn.Module):
    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):
        """
        Mamba Structured State Space model implementation with Triton-optimized segsum.
        """
        super(ModelNew, self).__init__()

        assert seq_length % block_len == 0, "Sequence length must be divisible by block length"

        self.batch_size = batch_size
        self.seq_length = seq_length
        self.n_heads = n_heads
        self.d_head = d_head
        self.d_state = d_state
        self.block_len = block_len

        # Initialize parameters (same shapes as original model)
        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))
        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))
        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))

    def segsum(self, x: torch.Tensor) -> torch.Tensor:
        # Use Triton-accelerated segsum
        return segsum_triton(x)

    def forward(self, X, initial_states=None):
        """
        Forward pass implementing the SSD operation, identical to the original
        model's semantics but using Triton for the segsum computation.
        """
        # Rearrange into blocks/chunks
        X_blocks, A_blocks, B_blocks, C_blocks = [
            rearrange(x, "b (c l) ... -> b c l ...", l=self.block_len)
            for x in (X, self.A, self.B, self.C)
        ]

        A_blocks = rearrange(A_blocks, "b c l h -> b h c l")
        A_cumsum = torch.cumsum(A_blocks, dim=-1)

        # 1. Compute diagonal block outputs
        L = torch.exp(self.segsum(A_blocks))
        Y_diag = torch.einsum(
            "bclhn,bcshn,bhcls,bcshp->bclhp",
            C_blocks,
            B_blocks,
            L,
            X_blocks,
        )

        # 2. Compute intra-chunk states
        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))
        states = torch.einsum(
            "bclhn,bhcl,bclhp->bchpn",
            B_blocks,
            decay_states,
            X_blocks,
        )

        # 3. Compute inter-chunk recurrence
        if initial_states is None:
            initial_states = torch.zeros_like(states[:, :1])
        states = torch.cat([initial_states, states], dim=1)

        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))
        new_states = torch.einsum(
            "bhzc,bchpn->bzhpn",
            decay_chunk,
            states,
        )
        return new_states[:, -1]
```

[NCU PROFILING METRICS]
No NCU metrics available (skipped for network-level model)

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

TRITON API CONSTRAINTS (CRITICAL):
- Triton has NO: tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
