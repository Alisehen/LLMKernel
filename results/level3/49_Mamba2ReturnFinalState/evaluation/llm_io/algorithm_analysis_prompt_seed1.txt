You are a GPU kernel optimization architect. Analyze the kernel and identify **ONE high-level algorithmic optimization**.

# PyTorch Reference
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

class Model(nn.Module):
    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):
        """
        Mamba Structured State Space model implementation for benchmarking.
        
        :param batch_size: Size of the batch
        :param seq_length: Length of the input sequence
        :param n_heads: Number of attention heads
        :param d_head: Dimension of each head
        :param d_state: Dimension of the state space
        :param block_len: Length of each block for chunked computation
        """
        super(Model, self).__init__()
        
        assert seq_length % block_len == 0, "Sequence length must be divisible by block length"
        
        self.batch_size = batch_size
        self.seq_length = seq_length
        self.n_heads = n_heads
        self.d_head = d_head
        self.d_state = d_state
        self.block_len = block_len
        
        # Initialize parameters
        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))
        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))
        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))
        
    def segsum(self, x):
        """Naive segment sum calculation."""
        T = x.size(-1)
        x_cumsum = torch.cumsum(x, dim=-1)
        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]
        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)
        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)
        return x_segsum
    
    def forward(self, X, initial_states=None):
        """
        Forward pass implementing the SSD operation.
        
        :param X: Input tensor of shape (batch, length, n_heads, d_head)
        :param initial_states: Optional initial states
        :return: Output tensor Y and final state
        """
        # Rearrange into blocks/chunks
        X_blocks, A_blocks, B_blocks, C_blocks = [
            rearrange(x, "b (c l) ... -> b c l ...", l=self.block_len)
            for x in (X, self.A, self.B, self.C)
        ]
        
        A_blocks = rearrange(A_blocks, "b c l h -> b h c l")
        A_cumsum = torch.cumsum(A_blocks, dim=-1)
        
        # 1. Compute diagonal block outputs
        L = torch.exp(self.segsum(A_blocks))
        Y_diag = torch.einsum("bclhn,bcshn,bhcls,bcshp->bclhp", 
                             C_blocks, B_blocks, L, X_blocks)
        
        # 2. Compute intra-chunk states
        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))
        states = torch.einsum("bclhn,bhcl,bclhp->bchpn", 
                            B_blocks, decay_states, X_blocks)
        
        # 3. Compute inter-chunk recurrence
        if initial_states is None:
            initial_states = torch.zeros_like(states[:, :1])
        states = torch.cat([initial_states, states], dim=1)
        
        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))
        new_states = torch.einsum("bhzc,bchpn->bzhpn", decay_chunk, states)
        return new_states[:, -1]

# Test parameters
batch_size = 2048
seq_length = 128
n_heads = 8
d_head = 64
d_state = 16
block_len = 64

def get_inputs():
    return [torch.rand(batch_size, seq_length, n_heads, d_head)]

def get_init_inputs():
    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]
```

# Current Triton Kernel
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


# ---------------------------------------------------------------------------
# Triton kernels
# ---------------------------------------------------------------------------

@triton.jit
def segsum_exp_kernel(
    x_ptr,  # [B, T]
    y_ptr,  # [B, T, T]
    B, T,
    stride_xb, stride_xt,
    stride_yb, stride_yi, stride_yj,
    BLOCK_J: tl.constexpr,  # power-of-2, >= T
):
    """
    For each batch element b and start index i, compute:
        y[b, i, j] = exp(sum_{k=i..j} x[b, k]) for j >= i
    and 0 otherwise.

    This kernel launches one program per (b, i) pair and computes
    all j in a vectorized way along the BLOCK_J dimension.
    """
    pid_b = tl.program_id(0)
    pid_i = tl.program_id(1)

    b = pid_b
    i = pid_i

    # Masks for valid batch and row
    mask_b = b < B
    mask_i = i < T

    # Column indices j handled by this program
    offs_j = tl.arange(0, BLOCK_J)
    j = i + offs_j
    mask_j = j < T

    # Combined mask for [BLOCK_J] vectors
    mask = mask_b & mask_i & mask_j

    # Base pointers
    x_base = x_ptr + b * stride_xb
    y_base = y_ptr + b * stride_yb + i * stride_yi

    # Load x[b, j] for j = i + offs_j
    x_ptrs = x_base + j * stride_xt
    x = tl.load(x_ptrs, mask=mask, other=0.0)
    x = x.to(tl.float32)

    # Cumulative sum along j dimension => segment sums starting at i
    segsum = tl.cumsum(x, axis=0)  # [BLOCK_J]

    out = tl.exp(segsum)

    # Store to y[b, i, j]
    y_ptrs = y_base + j * stride_yj
    tl.store(y_ptrs, out, mask=mask)


@triton.jit
def intra_states_kernel(
    B_ptr,        # [G, L, N]
    decay_ptr,    # [G, L]
    X_ptr,        # [G, L, P]
    states_ptr,   # [G, P, N]
    G, N, P,
    stride_bg, stride_bl, stride_bn,
    stride_dg, stride_dl,
    stride_xg, stride_xl, stride_xp,
    stride_sg, stride_sp, stride_sn,
    L: tl.constexpr,
    BLOCK_P: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    """
    Compute, for each group g in [0, G):

        states[g, p, n] = sum_{l=0..L-1} decay[g, l] * X[g, l, p] * B[g, l, n]

    where:
        B_ptr:     [G, L, N]
        decay_ptr: [G, L]
        X_ptr:     [G, L, P]
        states:    [G, P, N]

    The kernel is tiled over P and N.
    """
    gid = tl.program_id(0)  # group index
    pid_p = tl.program_id(1)
    pid_n = tl.program_id(2)

    mask_g = gid < G

    offs_p = pid_p * BLOCK_P + tl.arange(0, BLOCK_P)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    mask_p = offs_p < P
    mask_n = offs_n < N

    acc = tl.zeros((BLOCK_P, BLOCK_N), dtype=tl.float32)

    # Loop over sequence dimension L (compile-time constant)
    for l in range(0, L):
        # decay[g, l] as scalar
        decay = tl.load(
            decay_ptr + gid * stride_dg + l * stride_dl,
            mask=mask_g,
            other=0.0,
        )
        decay = decay.to(tl.float32)

        # B[g, l, n]
        b_ptrs = (
            B_ptr
            + gid * stride_bg
            + l * stride_bl
            + offs_n * stride_bn
        )
        B_l_n = tl.load(b_ptrs, mask=mask_g & mask_n, other=0.0)
        B_l_n = B_l_n.to(tl.float32)

        # X[g, l, p]
        x_ptrs = (
            X_ptr
            + gid * stride_xg
            + l * stride_xl
            + offs_p * stride_xp
        )
        X_l_p = tl.load(x_ptrs, mask=mask_g & mask_p, other=0.0)
        X_l_p = X_l_p.to(tl.float32)

        Bl = B_l_n[None, :]      # [1, BLOCK_N]
        Xl = X_l_p[:, None]      # [BLOCK_P, 1]

        acc += (Xl * Bl) * decay

    # Store result states[g, p, n]
    out_ptrs = (
        states_ptr
        + gid * stride_sg
        + offs_p[:, None] * stride_sp
        + offs_n[None, :] * stride_sn
    )
    tl.store(out_ptrs, acc, mask=mask_g & mask_p[:, None] & mask_n[None, :])


# ---------------------------------------------------------------------------
# Python wrappers for Triton kernels
# ---------------------------------------------------------------------------

def segsum_exp_triton(x: torch.Tensor) -> torch.Tensor:
    """
    Fused segsum + exp along the last dimension using Triton.

    Input:  x [..., T]
    Output: y [..., T, T] where
        y[..., i, j] = exp(sum_{k=i..j} x[..., k]) for j >= i
                       0 otherwise
    """
    assert x.is_cuda, "segsum_exp_triton requires CUDA tensors"
    orig_shape = x.shape
    T = orig_shape[-1]
    x_flat = x.contiguous().view(-1, T)
    B = x_flat.shape[0]

    y_flat = torch.empty(
        (B, T, T),
        device=x.device,
        dtype=x.dtype,
    )
    # Ensure lower triangle is zero for j < i
    y_flat.zero_()

    # Choose BLOCK_J as the smallest allowed power-of-two >= T
    if T <= 16:
        BLOCK_J = 16
    elif T <= 32:
        BLOCK_J = 32
    elif T <= 64:
        BLOCK_J = 64
    elif T <= 128:
        BLOCK_J = 128
    else:
        BLOCK_J = 256  # assumes T <= 256 in this workload

    grid = lambda META: (B, T)

    segsum_exp_kernel[grid](
        x_flat,
        y_flat,
        B,
        T,
        x_flat.stride(0),
        x_flat.stride(1),
        y_flat.stride(0),
        y_flat.stride(1),
        y_flat.stride(2),
        BLOCK_J=BLOCK_J,
    )
    return y_flat.view(*orig_shape[:-1], T, T)


def intra_states_triton(
    B_blocks: torch.Tensor,      # [b, c, l, h, d_state]
    decay_states: torch.Tensor,  # [b, h, c, l]
    X_blocks: torch.Tensor,      # [b, c, l, h, d_head]
) -> torch.Tensor:
    """
    Compute:
        states = einsum("bclhn,bhcl,bclhp->bchpn",
                        B_blocks, decay_states, X_blocks)

    using Triton.

    Output: [b, c, h, d_head, d_state]
    """
    assert B_blocks.is_cuda, "intra_states_triton requires CUDA tensors"
    b, c, l, h, d_state = B_blocks.shape
    _, _, _, _, d_head = X_blocks.shape  # 5D shape unpack

    # Reorder to group (b, c, h) together
    B_perm = B_blocks.permute(0, 1, 3, 2, 4).contiguous()       # [b, c, h, l, n]
    X_perm = X_blocks.permute(0, 1, 3, 2, 4).contiguous()       # [b, c, h, l, p]
    decay_perm = decay_states.permute(0, 2, 1, 3).contiguous()  # [b, c, h, l]

    G = b * c * h
    B_flat = B_perm.view(G, l, d_state)      # [G, L, N]
    X_flat = X_perm.view(G, l, d_head)       # [G, L, P]
    decay_flat = decay_perm.view(G, l)       # [G, L]

    states_flat = torch.empty(
        (G, d_head, d_state),
        device=B_blocks.device,
        dtype=B_blocks.dtype,
    )

    grid = lambda META: (
        G,
        triton.cdiv(d_head, META["BLOCK_P"]),
        triton.cdiv(d_state, META["BLOCK_N"]),
    )

    intra_states_kernel[grid](
        B_flat,
        decay_flat,
        X_flat,
        states_flat,
        G,
        d_state,
        d_head,
        B_flat.stride(0),
        B_flat.stride(1),
        B_flat.stride(2),
        decay_flat.stride(0),
        decay_flat.stride(1),
        X_flat.stride(0),
        X_flat.stride(1),
        X_flat.stride(2),
        states_flat.stride(0),
        states_flat.stride(1),
        states_flat.stride(2),
        L=l,
        BLOCK_P=64,
        BLOCK_N=32,
    )

    states = states_flat.view(b, c, h, d_head, d_state)
    return states


# ---------------------------------------------------------------------------
# Optimized Model
# ---------------------------------------------------------------------------

class ModelNew(nn.Module):
    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):
        super(ModelNew, self).__init__()

        assert seq_length % block_len == 0, "Sequence length must be divisible by block length"

        self.batch_size = batch_size
        self.seq_length = seq_length
        self.n_heads = n_heads
        self.d_head = d_head
        self.d_state = d_state
        self.block_len = block_len

        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))
        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))
        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))

    def forward(self, X, initial_states=None):
        """
        Forward pass implementing the SSD operation, partially optimized with Triton.

        X: (batch, length, n_heads, d_head)
        Returns: final state (batch, n_heads, d_head, d_state)
        """
        b, T, h, d_head = X.shape
        assert b == self.batch_size
        assert T == self.seq_length
        assert h == self.n_heads
        assert d_head == self.d_head

        c = T // self.block_len
        l = self.block_len

        # Reshape into blocks
        X_blocks = X.view(b, c, l, h, d_head)  # [b, c, l, h, p]
        A_blocks_bc = self.A.view(b, c, l, h)  # [b, c, l, h]
        B_blocks = self.B.view(b, c, l, h, self.d_state)  # [b, c, l, h, n]

        # A_blocks: [b, h, c, l]
        A_blocks = A_blocks_bc.permute(0, 3, 1, 2).contiguous()
        A_cumsum = torch.cumsum(A_blocks, dim=-1)  # [b, h, c, l]

        # 2. Intra-chunk states (using Triton kernel)
        decay_states = torch.exp(A_cumsum[:, :, :, -1:].clone() - A_cumsum)  # [b, h, c, l]
        states = intra_states_triton(B_blocks, decay_states, X_blocks)       # [b, c, h, p, n]

        # 3. Inter-chunk recurrence (sequential over chunks, numerically stable)
        if initial_states is None:
            current_state = torch.zeros(
                (b, h, d_head, self.d_state),
                device=X.device,
                dtype=X.dtype,
            )
        else:
            assert initial_states.shape == (b, h, d_head, self.d_state)
            current_state = initial_states

        # Last cumulative A per block: [b, h, c]
        A_last = A_cumsum[:, :, :, -1]  # [b, h, c]

        # Recurrence over chunks:
        # s_{k+1} = exp(A_last[..., k]) * s_k + states[:, k]
        for k in range(c):
            a_block = A_last[:, :, k]                       # [b, h]
            a_block = torch.exp(a_block).unsqueeze(-1).unsqueeze(-1)  # [b, h, 1, 1]
            current_state = a_block * current_state + states[:, k]    # [b, h, p, n]

        return current_state
```

# Performance
- **PyTorch baseline**: 17.67 ms
- **Current Triton**: 4.48 ms
- **Current speedup**: 3.94x (+74.7% vs baseline)


---

## Analysis Steps

1. **Code Analysis**: Count kernels, identify operations, check for inefficiencies
2. **Performance Diagnosis**: Use metrics/latency to identify bottleneck type
3. **Root Cause**: Combine code + performance to find the core issue

## Optimization Categories (pick ONE if worth optimizing):

### 1. Operator Fusion
Fuse consecutive ops into fewer kernels to reduce memory traffic and launch overhead.

### 2. Algorithm Replacement
Replace naive algorithm with optimized variant.
- For Attention: Flash Attention, online softmax
- For Convolution: Winograd, im2col
- **For RNN/GRU/LSTM**: Persistent kernel with HYBRID computation
  - **CRITICAL**: Use hybrid approach for best performance:
    * Precompute input-side gates ONCE (outside kernel): `gates_x = (T*B, In) @ W_ih`
    * Persistent kernel (inside): only recurrent-side: `for t: gates_h = h @ W_hh`
  - Time loop `for t in range(T)` must be inside kernel, NOT in Python
  - Launch kernel once per layer, not once per timestep
  - Expected speedup: 10-100x (vs per-timestep launches)

### 3. Kernel Launch Reduction
Combine multiple small kernels to reduce overhead.
- **For RNN/GRU/LSTM**: See "Algorithm Replacement" above for persistent kernel approach

### 4. Memory Layout Optimization
Use in-place operations, buffer reuse, or better layouts.

## Should We Optimize?

Before proposing optimization, determine if it's worthwhile:
- **Not worth optimizing** if:
  - Code is already near-optimal (expected speedup < 10%)
  - Bottleneck cannot be addressed (hardware limited, already optimal algorithm)
  - Optimization would add significant complexity with minimal gain

- **Worth optimizing** if:
  - Clear algorithmic inefficiency exists (multiple kernels, suboptimal algorithm)
  - Expected speedup >= 20%
  - Concrete optimization path available

## Output (JSON)

```json
{
  "worth_optimizing": "yes/no",
  "reason": "<Why worth or not worth optimizing, 1 sentence>",
  "bottleneck": "<Root cause in 1-2 sentences, empty if not worth optimizing>",
  "optimisation method": "<Specific optimization in 1-2 sentences, empty if not worth optimizing>",
  "modification plan": "<Implementation steps in 2-3 sentences, empty if not worth optimizing>",
  "expected_speedup": "<e.g., '30-40%', empty if not worth optimizing>"
}
```

Return JSON only.
