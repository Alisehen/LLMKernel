You are optimizing a Triton kernel based on algorithmic analysis.

# PyTorch Reference (Target Behavior)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

class Model(nn.Module):
    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):
        """
        Mamba Structured State Space model implementation for benchmarking.
        
        :param batch_size: Size of the batch
        :param seq_length: Length of the input sequence
        :param n_heads: Number of attention heads
        :param d_head: Dimension of each head
        :param d_state: Dimension of the state space
        :param block_len: Length of each block for chunked computation
        """
        super(Model, self).__init__()
        
        assert seq_length % block_len == 0, "Sequence length must be divisible by block length"
        
        self.batch_size = batch_size
        self.seq_length = seq_length
        self.n_heads = n_heads
        self.d_head = d_head
        self.d_state = d_state
        self.block_len = block_len
        
        # Initialize parameters
        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))
        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))
        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))
        
    def segsum(self, x):
        """Naive segment sum calculation."""
        T = x.size(-1)
        x_cumsum = torch.cumsum(x, dim=-1)
        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]
        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)
        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)
        return x_segsum
    
    def forward(self, X, initial_states=None):
        """
        Forward pass implementing the SSD operation.
        
        :param X: Input tensor of shape (batch, length, n_heads, d_head)
        :param initial_states: Optional initial states
        :return: Output tensor Y and final state
        """
        # Rearrange into blocks/chunks
        X_blocks, A_blocks, B_blocks, C_blocks = [
            rearrange(x, "b (c l) ... -> b c l ...", l=self.block_len)
            for x in (X, self.A, self.B, self.C)
        ]
        
        A_blocks = rearrange(A_blocks, "b c l h -> b h c l")
        A_cumsum = torch.cumsum(A_blocks, dim=-1)
        
        # 1. Compute diagonal block outputs
        L = torch.exp(self.segsum(A_blocks))
        Y_diag = torch.einsum("bclhn,bcshn,bhcls,bcshp->bclhp", 
                             C_blocks, B_blocks, L, X_blocks)
        
        # 2. Compute intra-chunk states
        decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))
        states = torch.einsum("bclhn,bhcl,bclhp->bchpn", 
                            B_blocks, decay_states, X_blocks)
        
        # 3. Compute inter-chunk recurrence
        if initial_states is None:
            initial_states = torch.zeros_like(states[:, :1])
        states = torch.cat([initial_states, states], dim=1)
        
        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))
        new_states = torch.einsum("bhzc,bchpn->bzhpn", decay_chunk, states)
        return new_states[:, -1]

# Test parameters
batch_size = 2048
seq_length = 128
n_heads = 8
d_head = 64
d_state = 16
block_len = 64

def get_inputs():
    return [torch.rand(batch_size, seq_length, n_heads, d_head)]

def get_init_inputs():
    return [batch_size, seq_length, n_heads, d_head, d_state, block_len]
```

**CRITICAL**: Study the PyTorch code carefully to understand:
- What does `forward()` return? (full output sequence vs final hidden state only)
- What is the computational pattern?
- What are the input/output shapes?

Your optimized kernel MUST match this exact behavior.

---

# Analysis Results

**Bottleneck**: In `ModelNew.forward`, the recurrence `for k in range(c): current_state = exp(A_last[..., k]) * current_state + states[:, k]` is a sequential host-side loop over time (chunks), performing many small pointwise ops and preventing the GPU from handling the time dimension efficiently in a single pass.

**Optimization Strategy**: Algorithm Replacement: move the inter‑chunk recurrence loop inside a single persistent Triton kernel that scans over chunks on the GPU (similar to the persistent RNN/GRU/LSTM pattern), so the time loop is on‑device instead of in Python.

**Implementation Plan**: Write a Triton kernel that takes `states` [b, c, h, p, n] and `A_last` [b, h, c] (plus optional `initial_states`) as input, flattens (b, h) into a group dimension, and tiles over (p, n). Inside the kernel, keep `current_state` in registers/shared memory and iterate over chunk index `k` from 0..c−1, loading `A_last[..., k]`, computing its exp once, and updating `current_state = exp(A_last[..., k]) * current_state + states[:, k]`. At the end of the loop, store `current_state` as the final state; this removes the Python loop and fuses the exp+mul+add recurrence into one GPU kernel.

**Expected Speedup**: 20-30% for typical workloads with more than a few chunks (larger seq_length/block_len), with higher gains as the number of chunks grows.

---

# Current Kernel (needs optimization)

```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


# ---------------------------------------------------------------------------
# Triton kernels
# ---------------------------------------------------------------------------

@triton.jit
def segsum_exp_kernel(
    x_ptr,  # [B, T]
    y_ptr,  # [B, T, T]
    B, T,
    stride_xb, stride_xt,
    stride_yb, stride_yi, stride_yj,
    BLOCK_J: tl.constexpr,  # power-of-2, >= T
):
    """
    For each batch element b and start index i, compute:
        y[b, i, j] = exp(sum_{k=i..j} x[b, k]) for j >= i
    and 0 otherwise.

    This kernel launches one program per (b, i) pair and computes
    all j in a vectorized way along the BLOCK_J dimension.
    """
    pid_b = tl.program_id(0)
    pid_i = tl.program_id(1)

    b = pid_b
    i = pid_i

    # Masks for valid batch and row
    mask_b = b < B
    mask_i = i < T

    # Column indices j handled by this program
    offs_j = tl.arange(0, BLOCK_J)
    j = i + offs_j
    mask_j = j < T

    # Combined mask for [BLOCK_J] vectors
    mask = mask_b & mask_i & mask_j

    # Base pointers
    x_base = x_ptr + b * stride_xb
    y_base = y_ptr + b * stride_yb + i * stride_yi

    # Load x[b, j] for j = i + offs_j
    x_ptrs = x_base + j * stride_xt
    x = tl.load(x_ptrs, mask=mask, other=0.0)
    x = x.to(tl.float32)

    # Cumulative sum along j dimension => segment sums starting at i
    segsum = tl.cumsum(x, axis=0)  # [BLOCK_J]

    out = tl.exp(segsum)

    # Store to y[b, i, j]
    y_ptrs = y_base + j * stride_yj
    tl.store(y_ptrs, out, mask=mask)


@triton.jit
def intra_states_kernel(
    B_ptr,        # [G, L, N]
    decay_ptr,    # [G, L]
    X_ptr,        # [G, L, P]
    states_ptr,   # [G, P, N]
    G, N, P,
    stride_bg, stride_bl, stride_bn,
    stride_dg, stride_dl,
    stride_xg, stride_xl, stride_xp,
    stride_sg, stride_sp, stride_sn,
    L: tl.constexpr,
    BLOCK_P: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    """
    Compute, for each group g in [0, G):

        states[g, p, n] = sum_{l=0..L-1} decay[g, l] * X[g, l, p] * B[g, l, n]

    where:
        B_ptr:     [G, L, N]
        decay_ptr: [G, L]
        X_ptr:     [G, L, P]
        states:    [G, P, N]

    The kernel is tiled over P and N.
    """
    gid = tl.program_id(0)  # group index
    pid_p = tl.program_id(1)
    pid_n = tl.program_id(2)

    mask_g = gid < G

    offs_p = pid_p * BLOCK_P + tl.arange(0, BLOCK_P)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    mask_p = offs_p < P
    mask_n = offs_n < N

    acc = tl.zeros((BLOCK_P, BLOCK_N), dtype=tl.float32)

    # Loop over sequence dimension L (compile-time constant)
    for l in range(0, L):
        # decay[g, l] as scalar
        decay = tl.load(
            decay_ptr + gid * stride_dg + l * stride_dl,
            mask=mask_g,
            other=0.0,
        )
        decay = decay.to(tl.float32)

        # B[g, l, n]
        b_ptrs = (
            B_ptr
            + gid * stride_bg
            + l * stride_bl
            + offs_n * stride_bn
        )
        B_l_n = tl.load(b_ptrs, mask=mask_g & mask_n, other=0.0)
        B_l_n = B_l_n.to(tl.float32)

        # X[g, l, p]
        x_ptrs = (
            X_ptr
            + gid * stride_xg
            + l * stride_xl
            + offs_p * stride_xp
        )
        X_l_p = tl.load(x_ptrs, mask=mask_g & mask_p, other=0.0)
        X_l_p = X_l_p.to(tl.float32)

        Bl = B_l_n[None, :]      # [1, BLOCK_N]
        Xl = X_l_p[:, None]      # [BLOCK_P, 1]

        acc += (Xl * Bl) * decay

    # Store result states[g, p, n]
    out_ptrs = (
        states_ptr
        + gid * stride_sg
        + offs_p[:, None] * stride_sp
        + offs_n[None, :] * stride_sn
    )
    tl.store(out_ptrs, acc, mask=mask_g & mask_p[:, None] & mask_n[None, :])


# ---------------------------------------------------------------------------
# Python wrappers for Triton kernels
# ---------------------------------------------------------------------------

def segsum_exp_triton(x: torch.Tensor) -> torch.Tensor:
    """
    Fused segsum + exp along the last dimension using Triton.

    Input:  x [..., T]
    Output: y [..., T, T] where
        y[..., i, j] = exp(sum_{k=i..j} x[..., k]) for j >= i
                       0 otherwise
    """
    assert x.is_cuda, "segsum_exp_triton requires CUDA tensors"
    orig_shape = x.shape
    T = orig_shape[-1]
    x_flat = x.contiguous().view(-1, T)
    B = x_flat.shape[0]

    y_flat = torch.empty(
        (B, T, T),
        device=x.device,
        dtype=x.dtype,
    )
    # Ensure lower triangle is zero for j < i
    y_flat.zero_()

    # Choose BLOCK_J as the smallest allowed power-of-two >= T
    if T <= 16:
        BLOCK_J = 16
    elif T <= 32:
        BLOCK_J = 32
    elif T <= 64:
        BLOCK_J = 64
    elif T <= 128:
        BLOCK_J = 128
    else:
        BLOCK_J = 256  # assumes T <= 256 in this workload

    grid = lambda META: (B, T)

    segsum_exp_kernel[grid](
        x_flat,
        y_flat,
        B,
        T,
        x_flat.stride(0),
        x_flat.stride(1),
        y_flat.stride(0),
        y_flat.stride(1),
        y_flat.stride(2),
        BLOCK_J=BLOCK_J,
    )
    return y_flat.view(*orig_shape[:-1], T, T)


def intra_states_triton(
    B_blocks: torch.Tensor,      # [b, c, l, h, d_state]
    decay_states: torch.Tensor,  # [b, h, c, l]
    X_blocks: torch.Tensor,      # [b, c, l, h, d_head]
) -> torch.Tensor:
    """
    Compute:
        states = einsum("bclhn,bhcl,bclhp->bchpn",
                        B_blocks, decay_states, X_blocks)

    using Triton.

    Output: [b, c, h, d_head, d_state]
    """
    assert B_blocks.is_cuda, "intra_states_triton requires CUDA tensors"
    b, c, l, h, d_state = B_blocks.shape
    _, _, _, _, d_head = X_blocks.shape  # 5D shape unpack

    # Reorder to group (b, c, h) together
    B_perm = B_blocks.permute(0, 1, 3, 2, 4).contiguous()       # [b, c, h, l, n]
    X_perm = X_blocks.permute(0, 1, 3, 2, 4).contiguous()       # [b, c, h, l, p]
    decay_perm = decay_states.permute(0, 2, 1, 3).contiguous()  # [b, c, h, l]

    G = b * c * h
    B_flat = B_perm.view(G, l, d_state)      # [G, L, N]
    X_flat = X_perm.view(G, l, d_head)       # [G, L, P]
    decay_flat = decay_perm.view(G, l)       # [G, L]

    states_flat = torch.empty(
        (G, d_head, d_state),
        device=B_blocks.device,
        dtype=B_blocks.dtype,
    )

    grid = lambda META: (
        G,
        triton.cdiv(d_head, META["BLOCK_P"]),
        triton.cdiv(d_state, META["BLOCK_N"]),
    )

    intra_states_kernel[grid](
        B_flat,
        decay_flat,
        X_flat,
        states_flat,
        G,
        d_state,
        d_head,
        B_flat.stride(0),
        B_flat.stride(1),
        B_flat.stride(2),
        decay_flat.stride(0),
        decay_flat.stride(1),
        X_flat.stride(0),
        X_flat.stride(1),
        X_flat.stride(2),
        states_flat.stride(0),
        states_flat.stride(1),
        states_flat.stride(2),
        L=l,
        BLOCK_P=64,
        BLOCK_N=32,
    )

    states = states_flat.view(b, c, h, d_head, d_state)
    return states


# ---------------------------------------------------------------------------
# Optimized Model
# ---------------------------------------------------------------------------

class ModelNew(nn.Module):
    def __init__(self, batch_size, seq_length, n_heads, d_head, d_state, block_len=64):
        super(ModelNew, self).__init__()

        assert seq_length % block_len == 0, "Sequence length must be divisible by block length"

        self.batch_size = batch_size
        self.seq_length = seq_length
        self.n_heads = n_heads
        self.d_head = d_head
        self.d_state = d_state
        self.block_len = block_len

        self.A = nn.Parameter(torch.randn(batch_size, seq_length, n_heads))
        self.B = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))
        self.C = nn.Parameter(torch.randn(batch_size, seq_length, n_heads, d_state))

    def forward(self, X, initial_states=None):
        """
        Forward pass implementing the SSD operation, partially optimized with Triton.

        X: (batch, length, n_heads, d_head)
        Returns: final state (batch, n_heads, d_head, d_state)
        """
        b, T, h, d_head = X.shape
        assert b == self.batch_size
        assert T == self.seq_length
        assert h == self.n_heads
        assert d_head == self.d_head

        c = T // self.block_len
        l = self.block_len

        # Reshape into blocks
        X_blocks = X.view(b, c, l, h, d_head)  # [b, c, l, h, p]
        A_blocks_bc = self.A.view(b, c, l, h)  # [b, c, l, h]
        B_blocks = self.B.view(b, c, l, h, self.d_state)  # [b, c, l, h, n]

        # A_blocks: [b, h, c, l]
        A_blocks = A_blocks_bc.permute(0, 3, 1, 2).contiguous()
        A_cumsum = torch.cumsum(A_blocks, dim=-1)  # [b, h, c, l]

        # 2. Intra-chunk states (using Triton kernel)
        decay_states = torch.exp(A_cumsum[:, :, :, -1:].clone() - A_cumsum)  # [b, h, c, l]
        states = intra_states_triton(B_blocks, decay_states, X_blocks)       # [b, c, h, p, n]

        # 3. Inter-chunk recurrence (sequential over chunks, numerically stable)
        if initial_states is None:
            current_state = torch.zeros(
                (b, h, d_head, self.d_state),
                device=X.device,
                dtype=X.dtype,
            )
        else:
            assert initial_states.shape == (b, h, d_head, self.d_state)
            current_state = initial_states

        # Last cumulative A per block: [b, h, c]
        A_last = A_cumsum[:, :, :, -1]  # [b, h, c]

        # Recurrence over chunks:
        # s_{k+1} = exp(A_last[..., k]) * s_k + states[:, k]
        for k in range(c):
            a_block = A_last[:, :, k]                       # [b, h]
            a_block = torch.exp(a_block).unsqueeze(-1).unsqueeze(-1)  # [b, h, 1, 1]
            current_state = a_block * current_state + states[:, k]    # [b, h, p, n]

        return current_state
```

---

# Your Task

Implement the optimization strategy above. Focus on the specific bottleneck identified.

## Key Requirements

1. **Preserve correctness**: Maintain the same input/output behavior
2. **Apply the optimization**: Follow the implementation plan exactly
3. **Use valid Triton syntax**:
   - Every kernel MUST have `@triton.jit` decorator
   - Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
   - BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
   - No `continue`, `break`, `return` inside kernels (use masking)
   - Prefer `tl.dot(a, b, allow_tf32=True)` for matmul operations

4. **CRITICAL for RNN/GRU/LSTM Persistent Kernels**:
   - Time loop MUST be inside @triton.jit kernel, NOT in Python forward()
   - **HYBRID computation strategy** (CRITICAL for performance):
     * Precompute input-side gates OUTSIDE kernel: `gates_x = (T*B, In) @ W_ih` (ONE large GEMM)
     * INSIDE kernel: only recurrent-side: `for t: gates_h = h @ W_hh` (T small GEMMs)
   - CORRECT (FAST - use this):
     ```python
     # Python forward():
     gates_x_all = x.reshape(T*B, In) @ W_ih + b_ih  # ONE large GEMM
     gates_x_all = gates_x_all.view(T, B, 3*H)
     gru_persistent_kernel[grid](gates_x_all, h0, W_hh, ...)  # Launch ONCE

     @triton.jit
     def gru_persistent_kernel(gates_x_ptr, h_ptr, W_hh_ptr, ...):
         for t in range(T):  # Inside kernel
             gates_x_t = tl.load(gates_x_ptr + t*...)  # Precomputed
             gates_h = h @ W_hh  # Only recurrent GEMM
             h = (1-z)*n + z*h   # Fuse and update
     ```

5. **Output format**:
   - Imports: `import torch, torch.nn as nn, triton, triton.language as tl`
   - `@triton.jit` kernel(s)
   - Wrapper function(s)
   - `class ModelNew(nn.Module)` — REQUIRED
   - NO testing code, NO `if __name__ == "__main__"`

---

Generate the optimized kernel now. Output ONLY the complete Python code.
