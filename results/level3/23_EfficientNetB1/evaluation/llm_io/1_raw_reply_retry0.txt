```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import triton
import triton.language as tl
import math


@triton.jit
def fused_pointwise_conv_bn_relu6_kernel(
    x_ptr,
    w_ptr,
    out_ptr,
    bn_weight_ptr,
    bn_bias_ptr,
    bn_mean_ptr,
    bn_var_ptr,
    N, C_in, C_out, H, W,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_wn, stride_wc,
    stride_on, stride_oc, stride_oh, stride_ow,
    eps: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_C: tl.constexpr,
    BLOCK_SIZE_HW: tl.constexpr,
):
    """Fused pointwise convolution + batch norm + ReLU6 kernel.
    
    Optimized for Ada Lovelace with better occupancy and memory access patterns.
    """
    # Combined spatial dimension for better grid utilization
    pid_hw = tl.program_id(0)
    pid_oc = tl.program_id(1)
    pid_n = tl.program_id(2)
    
    # Create offsets with proper broadcasting dimensions
    hw_offs = pid_hw * BLOCK_SIZE_HW + tl.arange(0, BLOCK_SIZE_HW)
    oc_offs = pid_oc * BLOCK_SIZE_C + tl.arange(0, BLOCK_SIZE_C)
    n_offs = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    
    # Compute spatial indices with masking
    h_idx = hw_offs // W
    w_idx = hw_offs % W
    hw_mask = hw_offs < H * W
    h_mask = h_idx < H
    w_mask = w_idx < W
    spatial_mask = h_mask & w_mask & hw_mask
    
    # Create masks for all dimensions
    n_mask = n_offs < N
    oc_mask = oc_offs < C_out
    
    # Reshape offsets for proper broadcasting
    n_offs_3d = n_offs[:, None, None]  # (BLOCK_SIZE_N, 1, 1)
    oc_offs_3d = oc_offs[None, :, None]  # (1, BLOCK_SIZE_C, 1)
    h_idx_3d = h_idx[None, None, :]  # (1, 1, BLOCK_SIZE_HW)
    w_idx_3d = w_idx[None, None, :]  # (1, 1, BLOCK_SIZE_HW)
    
    # Initialize accumulator with reduced precision
    acc = tl.zeros((BLOCK_SIZE_N, BLOCK_SIZE_C, BLOCK_SIZE_HW), dtype=tl.float32)
    
    # Tiled loop over input channels with better memory coalescing
    for c_block in range(0, C_in, BLOCK_SIZE_C):
        c_offs = c_block + tl.arange(0, BLOCK_SIZE_C)
        c_mask = c_offs < C_in
        
        # Load input block - optimized memory access pattern
        x_offsets = (n_offs_3d * stride_xn + 
                    c_offs[None, :, None] * stride_xc +
                    h_idx_3d * stride_xh +
                    w_idx_3d * stride_xw)
        
        x = tl.load(x_ptr + x_offsets,
                   mask=(n_mask[:, None, None] & 
                         c_mask[None, :, None] & 
                         spatial_mask[None, None, :]),
                   other=0.0,
                   eviction_policy="evict_first")
        
        # Load weight block with proper broadcasting
        w_offsets = (oc_offs_3d * stride_wn + 
                    c_offs[None, :, None] * stride_wc)
        
        w = tl.load(w_ptr + w_offsets,
                   mask=(oc_mask[None, :, None] & c_mask[None, :, None]),
                   other=0.0,
                   eviction_policy="evict_first")
        
        # Optimized matrix multiplication using tensor cores when possible
        # Reshape for proper broadcasting and use fp16 for tensor core acceleration
        x_fp16 = x.to(tl.float16)
        w_fp16 = w.to(tl.float16)
        
        # Manual tiled multiplication for better register usage
        # Unroll the computation for better performance
        for i in range(BLOCK_SIZE_N):
            for j in range(BLOCK_SIZE_C):
                for k in range(BLOCK_SIZE_HW):
                    if (i < BLOCK_SIZE_N and j < BLOCK_SIZE_C and k < BLOCK_SIZE_HW and
                        n_mask[i] and oc_mask[j] and spatial_mask[0, 0, k] and c_mask[0]):
                        acc = tl.where(
                            tl.arange(0, BLOCK_SIZE_N)[:, None, None] == i,
                            tl.where(
                                tl.arange(0, BLOCK_SIZE_C)[None, :, None] == j,
                                tl.where(
                                    tl.arange(0, BLOCK_SIZE_HW)[None, None, :] == k,
                                    acc + x_fp16[i, 0, k] * w_fp16[0, j, 0],
                                    acc
                                ),
                                acc
                            ),
                            acc
                        )
    
    # Load batch norm parameters with vectorized access
    bn_weight = tl.load(bn_weight_ptr + oc_offs[None, :, None],
                       mask=oc_mask[None, :, None], other=1.0)
    bn_bias = tl.load(bn_bias_ptr + oc_offs[None, :, None],
                     mask=oc_mask[None, :, None], other=0.0)
    bn_mean = tl.load(bn_mean_ptr + oc_offs[None, :, None],
                     mask=oc_mask[None, :, None], other=0.0)
    bn_var = tl.load(bn_var_ptr + oc_offs[None, :, None],
                    mask=oc_mask[None, :, None], other=1.0)
    
    # Fused batch norm computation with fast math
    inv_std = bn_weight * tl.rsqrt(bn_var + eps)
    normalized = acc * inv_std + (bn_bias - bn_mean * inv_std)
    
    # Fused ReLU6 with optimized min/max
    activated = tl.minimum(tl.maximum(normalized, 0.0), 6.0)
    
    # Store output with coalesced memory access
    out_offsets = (n_offs_3d * stride_on +
                  oc_offs_3d * stride_oc +
                  h_idx_3d * stride_oh +
                  w_idx_3d * stride_ow)
    
    tl.store(out_ptr + out_offsets, activated,
            mask=(n_mask[:, None, None] & 
                  oc_mask[None, :, None] & 
                  spatial_mask[None, None, :]))


def fused_pointwise_conv_bn_relu6(x, weight, bn_weight, bn_bias, bn_mean, bn_var, eps=1e-5):
    """Optimized fused pointwise convolution, batch norm, and ReLU6."""
    N, C_in, H, W = x.shape
    C_out = weight.shape[0]
    
    # Ensure inputs are contiguous
    x = x.contiguous()
    weight = weight.contiguous()
    
    # Create output tensor
    out = torch.empty((N, C_out, H, W), device=x.device, dtype=x.dtype)
    
    # Optimized block sizes for Ada Lovelace
    # Increased occupancy with more blocks
    BLOCK_SIZE_N = min(2, N)  # Reduced from 4 to 2 for more blocks
    BLOCK_SIZE_C = 32
    BLOCK_SIZE_HW = 32  # Reduced from 64 for better cache utilization
    
    # Reordered grid for better cache locality
    grid = (
        triton.cdiv(H * W, BLOCK_SIZE_HW),  # Spatial first for better cache locality
        triton.cdiv(C_out, BLOCK_SIZE_C),
        triton.cdiv(N, BLOCK_SIZE_N),  # Batch last
    )
    
    # Launch kernel
    fused_pointwise_conv_bn_relu6_kernel[grid](
        x, weight, out,
        bn_weight, bn_bias, bn_mean, bn_var,
        N, C_in, C_out, H, W,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        weight.stride(0), weight.stride(1),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3),
        eps,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_C=BLOCK_SIZE_C,
        BLOCK_SIZE_HW=BLOCK_SIZE_HW,
    )
    
    return out


@triton.jit
def fused_linear_bias_kernel(
    x_ptr,
    w_ptr,
    b_ptr,
    out_ptr,
    M, N, K,
    stride_xm, stride_xk,
    stride_wk, stride_wn,
    stride_bn,
    stride_om, stride_on,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """Optimized fused linear layer with bias addition.
    
    Uses tensor cores and better memory access patterns.
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    # Vectorized offsets for better memory access
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)
    
    # Create masks
    m_mask = offs_m < M
    n_mask = offs_n < N
    
    # Initialize accumulator
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    # Prefetch first blocks
    x_ptr_base = x_ptr + offs_m[:, None] * stride_xm
    w_ptr_base = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn
    
    # Accumulate over K dimension with tiling
    for k in range(0, K, BLOCK_K):
        # Create masks for current K block
        k_mask = offs_k < (K - k)
        mask_x = m_mask[:, None] & k_mask[None, :]
        mask_w = k_mask[:, None] & n_mask[None, :]
        
        # Load blocks
        x = tl.load(x_ptr_base, mask=mask_x, other=0.0)
        w = tl.load(w_ptr_base, mask=mask_w, other=0.0)
        
        # Use tensor cores for matrix multiplication
        acc += tl.dot(x.to(tl.float16), w.to(tl.float16), allow_tf32=True)
        
        # Update pointers
        x_ptr_base += BLOCK_K * stride_xk
        w_ptr_base += BLOCK_K * stride_wk
    
    # Add bias with vectorized load
    bias = tl.load(b_ptr + offs_n, mask=n_mask, other=0.0)
    acc += bias[None, :]
    
    # Store result with coalesced access
    out_ptrs = out_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on
    mask_out = m_mask[:, None] & n_mask[None, :]
    tl.store(out_ptrs, acc, mask=mask_out)


def fused_linear_bias(x, weight, bias):
    """Optimized fused linear layer with bias."""
    M, K = x.shape
    N = weight.shape[0]
    
    # Ensure inputs are contiguous and properly formatted for tensor cores
    x = x.contiguous()
    weight = weight.contiguous().t()
    bias = bias.contiguous()
    
    # Create output tensor
    out = torch.empty((M, N), device=x.device, dtype=x.dtype)
    
    # Optimized block sizes for Ada Lovelace tensor cores
    BLOCK_M = 128  # Increased for better tensor core utilization
    BLOCK_N = 64   # Balanced for tensor core efficiency
    BLOCK_K = 32   # Optimal for tensor core tile size
    
    # Grid configuration
    grid = (
        triton.cdiv(M, BLOCK_M),
        triton.cdiv(N, BLOCK_N),
    )
    
    # Launch kernel
    fused_linear_bias_kernel[grid](
        x, weight, bias, out,
        M, N, K,
        x.stride(0), x.stride(1),
        weight.stride(0), weight.stride(1),
        bias.stride(0),
        out.stride(0), out.stride(1),
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
        BLOCK_K=BLOCK_K,
    )
    
    return out


class FusedMBConvBlock(nn.Module):
    """Fused MBConv block with Triton optimizations."""
    def __init__(self, in_channels, out_channels, stride, expand_ratio):
        super(FusedMBConvBlock, self).__init__()
        self.stride = stride
        self.expand_ratio = expand_ratio
        hidden_dim = round(in_channels * expand_ratio)
        
        # Expansion phase (fused: conv + bn + relu6)
        self.expand_conv = nn.Conv2d(in_channels, hidden_dim, 
                                    kernel_size=1, stride=1, padding=0, bias=False)
        self.expand_bn = nn.BatchNorm2d(hidden_dim)
        
        # Depthwise convolution
        self.depthwise_conv = nn.Conv2d(hidden_dim, hidden_dim,
                                       kernel_size=3, stride=stride,
                                       padding=1, groups=hidden_dim, bias=False)
        self.depthwise_bn = nn.BatchNorm2d(hidden_dim)
        
        # Projection phase
        self.project_conv = nn.Conv2d(hidden_dim, out_channels,
                                     kernel_size=1, stride=1, padding=0, bias=False)
        self.project_bn = nn.BatchNorm2d(out_channels)
    
    def forward(self, x):
        # Fused expansion phase using Triton
        if self.expand_ratio != 1:
            x = fused_pointwise_conv_bn_relu6(
                x,
                self.expand_conv.weight,
                self.expand_bn.weight,
                self.expand_bn.bias,
                self.expand_bn.running_mean,
                self.expand_bn.running_var,
                self.expand_bn.eps
            )
        
        # Depthwise convolution (keep as PyTorch for now)
        x = self.depthwise_conv(x)
        x = self.depthwise_bn(x)
        x = F.relu6(x, inplace=True)
        
        # Projection phase
        x = self.project_conv(x)
        x = self.project_bn(x)
        
        return x


class ModelNew(nn.Module):
    """EfficientNetB1 with optimized Triton kernels."""
    def __init__(self, num_classes=1000):
        super(ModelNew, self).__init__()
        
        # Initial convolutional layer
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(32)
        
        # MBConv blocks with Triton fusion
        self.mbconv1 = FusedMBConvBlock(32, 16, 1, 1)
        self.mbconv2 = FusedMBConvBlock(16, 24, 2, 6)
        self.mbconv3 = FusedMBConvBlock(24, 40, 2, 6)
        self.mbconv4 = FusedMBConvBlock(40, 80, 2, 6)
        self.mbconv5 = FusedMBConvBlock(80, 112, 1, 6)
        self.mbconv6 = FusedMBConvBlock(112, 192, 2, 6)
        self.mbconv7 = FusedMBConvBlock(192, 320, 1, 6)
        
        # Final convolutional layer
        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(1280)
        
        # Fully connected layer with optimized Triton kernel
        self.fc_weight = nn.Parameter(torch.randn(num_classes, 1280))
        self.fc_bias = nn.Parameter(torch.randn(num_classes))
        
    def forward(self, x):
        # Initial conv + bn + relu
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x, inplace=True)
        
        # MBConv blocks
        x = self.mbconv1(x)
        x = self.mbconv2(x)
        x = self.mbconv3(x)
        x = self.mbconv4(x)
        x = self.mbconv5(x)
        x = self.mbconv6(x)
        x = self.mbconv7(x)
        
        # Final conv + bn + relu
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x, inplace=True)
        
        # Global pooling and flatten
        x = F.adaptive_avg_pool2d(x, (1, 1))
        x = torch.flatten(x, 1)
        
        # Final classification layer with optimized fused kernel
        x = fused_linear_bias(x, self.fc_weight, self.fc_bias)
        
        return x
```