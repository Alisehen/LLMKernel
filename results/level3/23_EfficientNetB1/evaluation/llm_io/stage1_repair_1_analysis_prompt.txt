You are a Triton kernel debugging expert. Analyze the error and identify the root cause.

## ERROR LOG
```
Traceback (most recent call last):
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 43, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 1094, in __getitem__
    for dim, sl in enumerate(slices):
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 41, in wrapper
    raise ValueError("Did you forget to add @triton.jit ? "
ValueError: Did you forget to add @triton.jit ? (`_semantic` argument must be provided outside of JIT functions.)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 538, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251215_151615_batch_range17to33_deepseek_deepseek/23_EfficientNetB1/code/kernel_20251215_191912.py", line 375, in forward
    x = self.mbconv2(x)
        ^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251215_151615_batch_range17to33_deepseek_deepseek/23_EfficientNetB1/code/kernel_20251215_191912.py", line 319, in forward
    x = fused_pointwise_conv_bn_relu6(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251215_151615_batch_range17to33_deepseek_deepseek/23_EfficientNetB1/code/kernel_20251215_191912.py", line 170, in fused_pointwise_conv_bn_relu6
    fused_pointwise_conv_bn_relu6_kernel[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 419, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 733, in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 861, in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 300, in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 80, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 91:24:
        # Optimized matrix multiplication using tensor cores when possible
        # Reshape for proper broadcasting and use fp16 for tensor core acceleration
        x_fp16 = x.to(tl.float16)
        w_fp16 = w.to(tl.float16)

        # Manual tiled multiplication for better register usage
        # Unroll the computation for better performance
        for i in range(BLOCK_SIZE_N):
            for j in range(BLOCK_SIZE_C):
                for k in range(BLOCK_SIZE_HW):
                    if (i < BLOCK_SIZE_N and j < BLOCK_SIZE_C and k < BLOCK_SIZE_HW and
                        n_mask[i] and oc_mask[j] and spatial_mask[0, 0, k] and c_mask[0]):
                        ^
Did you forget to add @triton.jit ? (`_semantic` argument must be provided outside of JIT functions.)
```

## Expected Behavior (PyTorch Reference)
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, num_classes=1000):
        """
        EfficientNetB1 architecture implementation.

        :param num_classes: The number of output classes (default is 1000 for ImageNet).
        """
        super(Model, self).__init__()
        
        # Initial convolutional layer
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(32)
        
        # MBConv blocks
        self.mbconv1 = self._make_mbconv_block(32, 16, 1, 1)
        self.mbconv2 = self._make_mbconv_block(16, 24, 2, 6)
        self.mbconv3 = self._make_mbconv_block(24, 40, 2, 6)
        self.mbconv4 = self._make_mbconv_block(40, 80, 2, 6)
        self.mbconv5 = self._make_mbconv_block(80, 112, 1, 6)
        self.mbconv6 = self._make_mbconv_block(112, 192, 2, 6)
        self.mbconv7 = self._make_mbconv_block(192, 320, 1, 6)
        
        # Final convolutional layer
        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(1280)
        
        # Fully connected layer
        self.fc = nn.Linear(1280, num_classes)
    
    def _make_mbconv_block(self, in_channels, out_channels, stride, expand_ratio):
        """
        Creates a MBConv block.

        :param in_channels: Number of input channels.
        :param out_channels: Number of output channels.
        :param stride: Stride of the depthwise convolution.
        :param expand_ratio: Expansion ratio for the hidden layer.
        :return: A sequential MBConv block.
        """
        hidden_dim = round(in_channels * expand_ratio)
        return nn.Sequential(
            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU6(inplace=True),
            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU6(inplace=True),
            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(out_channels),
        )
    
    def forward(self, x):
        """
        Forward pass of the EfficientNetB1 model.

        :param x: Input tensor, shape (batch_size, 3, 240, 240)
        :return: Output tensor, shape (batch_size, num_classes)
        """
        x = F.relu(self.bn1(self.conv1(x)))
        
        x = self.mbconv1(x)
        x = self.mbconv2(x)
        x = self.mbconv3(x)
        x = self.mbconv4(x)
        x = self.mbconv5(x)
        x = self.mbconv6(x)
        x = self.mbconv7(x)
        
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.adaptive_avg_pool2d(x, (1, 1))
        x = torch.flatten(x, 1)
        x = self.fc(x)
        
        return x

# Test code
batch_size = 10
input_shape = (3, 240, 240)
num_classes = 1000

def get_inputs():
    return [torch.rand(batch_size, *input_shape)]

def get_init_inputs():
    return [num_classes]
```

## Current Implementation (Broken Triton Kernel)
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import triton
import triton.language as tl
import math


@triton.jit
def fused_pointwise_conv_bn_relu6_kernel(
    x_ptr,
    w_ptr,
    out_ptr,
    bn_weight_ptr,
    bn_bias_ptr,
    bn_mean_ptr,
    bn_var_ptr,
    N, C_in, C_out, H, W,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_wn, stride_wc,
    stride_on, stride_oc, stride_oh, stride_ow,
    eps: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_C: tl.constexpr,
    BLOCK_SIZE_HW: tl.constexpr,
):
    """Fused pointwise convolution + batch norm + ReLU6 kernel.
    
    Optimized for Ada Lovelace with better occupancy and memory access patterns.
    """
    # Combined spatial dimension for better grid utilization
    pid_hw = tl.program_id(0)
    pid_oc = tl.program_id(1)
    pid_n = tl.program_id(2)
    
    # Create offsets with proper broadcasting dimensions
    hw_offs = pid_hw * BLOCK_SIZE_HW + tl.arange(0, BLOCK_SIZE_HW)
    oc_offs = pid_oc * BLOCK_SIZE_C + tl.arange(0, BLOCK_SIZE_C)
    n_offs = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    
    # Compute spatial indices with masking
    h_idx = hw_offs // W
    w_idx = hw_offs % W
    hw_mask = hw_offs < H * W
    h_mask = h_idx < H
    w_mask = w_idx < W
    spatial_mask = h_mask & w_mask & hw_mask
    
    # Create masks for all dimensions
    n_mask = n_offs < N
    oc_mask = oc_offs < C_out
    
    # Reshape offsets for proper broadcasting
    n_offs_3d = n_offs[:, None, None]  # (BLOCK_SIZE_N, 1, 1)
    oc_offs_3d = oc_offs[None, :, None]  # (1, BLOCK_SIZE_C, 1)
    h_idx_3d = h_idx[None, None, :]  # (1, 1, BLOCK_SIZE_HW)
    w_idx_3d = w_idx[None, None, :]  # (1, 1, BLOCK_SIZE_HW)
    
    # Initialize accumulator with reduced precision
    acc = tl.zeros((BLOCK_SIZE_N, BLOCK_SIZE_C, BLOCK_SIZE_HW), dtype=tl.float32)
    
    # Tiled loop over input channels with better memory coalescing
    for c_block in range(0, C_in, BLOCK_SIZE_C):
        c_offs = c_block + tl.arange(0, BLOCK_SIZE_C)
        c_mask = c_offs < C_in
        
        # Load input block - optimized memory access pattern
        x_offsets = (n_offs_3d * stride_xn + 
                    c_offs[None, :, None] * stride_xc +
                    h_idx_3d * stride_xh +
                    w_idx_3d * stride_xw)
        
        x = tl.load(x_ptr + x_offsets,
                   mask=(n_mask[:, None, None] & 
                         c_mask[None, :, None] & 
                         spatial_mask[None, None, :]),
                   other=0.0,
                   eviction_policy="evict_first")
        
        # Load weight block with proper broadcasting
        w_offsets = (oc_offs_3d * stride_wn + 
                    c_offs[None, :, None] * stride_wc)
        
        w = tl.load(w_ptr + w_offsets,
                   mask=(oc_mask[None, :, None] & c_mask[None, :, None]),
                   other=0.0,
                   eviction_policy="evict_first")
        
        # Optimized matrix multiplication using tensor cores when possible
        # Reshape for proper broadcasting and use fp16 for tensor core acceleration
        x_fp16 = x.to(tl.float16)
        w_fp16 = w.to(tl.float16)
        
        # Manual tiled multiplication for better register usage
        # Unroll the computation for better performance
        for i in range(BLOCK_SIZE_N):
            for j in range(BLOCK_SIZE_C):
                for k in range(BLOCK_SIZE_HW):
                    if (i < BLOCK_SIZE_N and j < BLOCK_SIZE_C and k < BLOCK_SIZE_HW and
                        n_mask[i] and oc_mask[j] and spatial_mask[0, 0, k] and c_mask[0]):
                        acc = tl.where(
                            tl.arange(0, BLOCK_SIZE_N)[:, None, None] == i,
                            tl.where(
                                tl.arange(0, BLOCK_SIZE_C)[None, :, None] == j,
                                tl.where(
                                    tl.arange(0, BLOCK_SIZE_HW)[None, None, :] == k,
                                    acc + x_fp16[i, 0, k] * w_fp16[0, j, 0],
                                    acc
                                ),
                                acc
                            ),
                            acc
                        )
    
    # Load batch norm parameters with vectorized access
    bn_weight = tl.load(bn_weight_ptr + oc_offs[None, :, None],
                       mask=oc_mask[None, :, None], other=1.0)
    bn_bias = tl.load(bn_bias_ptr + oc_offs[None, :, None],
                     mask=oc_mask[None, :, None], other=0.0)
    bn_mean = tl.load(bn_mean_ptr + oc_offs[None, :, None],
                     mask=oc_mask[None, :, None], other=0.0)
    bn_var = tl.load(bn_var_ptr + oc_offs[None, :, None],
                    mask=oc_mask[None, :, None], other=1.0)
    
    # Fused batch norm computation with fast math
    inv_std = bn_weight * tl.rsqrt(bn_var + eps)
    normalized = acc * inv_std + (bn_bias - bn_mean * inv_std)
    
    # Fused ReLU6 with optimized min/max
    activated = tl.minimum(tl.maximum(normalized, 0.0), 6.0)
    
    # Store output with coalesced memory access
    out_offsets = (n_offs_3d * stride_on +
                  oc_offs_3d * stride_oc +
                  h_idx_3d * stride_oh +
                  w_idx_3d * stride_ow)
    
    tl.store(out_ptr + out_offsets, activated,
            mask=(n_mask[:, None, None] & 
                  oc_mask[None, :, None] & 
                  spatial_mask[None, None, :]))


def fused_pointwise_conv_bn_relu6(x, weight, bn_weight, bn_bias, bn_mean, bn_var, eps=1e-5):
    """Optimized fused pointwise convolution, batch norm, and ReLU6."""
    N, C_in, H, W = x.shape
    C_out = weight.shape[0]
    
    # Ensure inputs are contiguous
    x = x.contiguous()
    weight = weight.contiguous()
    
    # Create output tensor
    out = torch.empty((N, C_out, H, W), device=x.device, dtype=x.dtype)
    
    # Optimized block sizes for Ada Lovelace
    # Increased occupancy with more blocks
    BLOCK_SIZE_N = min(2, N)  # Reduced from 4 to 2 for more blocks
    BLOCK_SIZE_C = 32
    BLOCK_SIZE_HW = 32  # Reduced from 64 for better cache utilization
    
    # Reordered grid for better cache locality
    grid = (
        triton.cdiv(H * W, BLOCK_SIZE_HW),  # Spatial first for better cache locality
        triton.cdiv(C_out, BLOCK_SIZE_C),
        triton.cdiv(N, BLOCK_SIZE_N),  # Batch last
    )
    
    # Launch kernel
    fused_pointwise_conv_bn_relu6_kernel[grid](
        x, weight, out,
        bn_weight, bn_bias, bn_mean, bn_var,
        N, C_in, C_out, H, W,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        weight.stride(0), weight.stride(1),
        out.stride(0), out.stride(1), out.stride(2), out.stride(3),
        eps,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_C=BLOCK_SIZE_C,
        BLOCK_SIZE_HW=BLOCK_SIZE_HW,
    )
    
    return out


@triton.jit
def fused_linear_bias_kernel(
    x_ptr,
    w_ptr,
    b_ptr,
    out_ptr,
    M, N, K,
    stride_xm, stride_xk,
    stride_wk, stride_wn,
    stride_bn,
    stride_om, stride_on,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """Optimized fused linear layer with bias addition.
    
    Uses tensor cores and better memory access patterns.
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    # Vectorized offsets for better memory access
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)
    
    # Create masks
    m_mask = offs_m < M
    n_mask = offs_n < N
    
    # Initialize accumulator
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    # Prefetch first blocks
    x_ptr_base = x_ptr + offs_m[:, None] * stride_xm
    w_ptr_base = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn
    
    # Accumulate over K dimension with tiling
    for k in range(0, K, BLOCK_K):
        # Create masks for current K block
        k_mask = offs_k < (K - k)
        mask_x = m_mask[:, None] & k_mask[None, :]
        mask_w = k_mask[:, None] & n_mask[None, :]
        
        # Load blocks
        x = tl.load(x_ptr_base, mask=mask_x, other=0.0)
        w = tl.load(w_ptr_base, mask=mask_w, other=0.0)
        
        # Use tensor cores for matrix multiplication
        acc += tl.dot(x.to(tl.float16), w.to(tl.float16), allow_tf32=True)
        
        # Update pointers
        x_ptr_base += BLOCK_K * stride_xk
        w_ptr_base += BLOCK_K * stride_wk
    
    # Add bias with vectorized load
    bias = tl.load(b_ptr + offs_n, mask=n_mask, other=0.0)
    acc += bias[None, :]
    
    # Store result with coalesced access
    out_ptrs = out_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on
    mask_out = m_mask[:, None] & n_mask[None, :]
    tl.store(out_ptrs, acc, mask=mask_out)


def fused_linear_bias(x, weight, bias):
    """Optimized fused linear layer with bias."""
    M, K = x.shape
    N = weight.shape[0]
    
    # Ensure inputs are contiguous and properly formatted for tensor cores
    x = x.contiguous()
    weight = weight.contiguous().t()
    bias = bias.contiguous()
    
    # Create output tensor
    out = torch.empty((M, N), device=x.device, dtype=x.dtype)
    
    # Optimized block sizes for Ada Lovelace tensor cores
    BLOCK_M = 128  # Increased for better tensor core utilization
    BLOCK_N = 64   # Balanced for tensor core efficiency
    BLOCK_K = 32   # Optimal for tensor core tile size
    
    # Grid configuration
    grid = (
        triton.cdiv(M, BLOCK_M),
        triton.cdiv(N, BLOCK_N),
    )
    
    # Launch kernel
    fused_linear_bias_kernel[grid](
        x, weight, bias, out,
        M, N, K,
        x.stride(0), x.stride(1),
        weight.stride(0), weight.stride(1),
        bias.stride(0),
        out.stride(0), out.stride(1),
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
        BLOCK_K=BLOCK_K,
    )
    
    return out


class FusedMBConvBlock(nn.Module):
    """Fused MBConv block with Triton optimizations."""
    def __init__(self, in_channels, out_channels, stride, expand_ratio):
        super(FusedMBConvBlock, self).__init__()
        self.stride = stride
        self.expand_ratio = expand_ratio
        hidden_dim = round(in_channels * expand_ratio)
        
        # Expansion phase (fused: conv + bn + relu6)
        self.expand_conv = nn.Conv2d(in_channels, hidden_dim, 
                                    kernel_size=1, stride=1, padding=0, bias=False)
        self.expand_bn = nn.BatchNorm2d(hidden_dim)
        
        # Depthwise convolution
        self.depthwise_conv = nn.Conv2d(hidden_dim, hidden_dim,
                                       kernel_size=3, stride=stride,
                                       padding=1, groups=hidden_dim, bias=False)
        self.depthwise_bn = nn.BatchNorm2d(hidden_dim)
        
        # Projection phase
        self.project_conv = nn.Conv2d(hidden_dim, out_channels,
                                     kernel_size=1, stride=1, padding=0, bias=False)
        self.project_bn = nn.BatchNorm2d(out_channels)
    
    def forward(self, x):
        # Fused expansion phase using Triton
        if self.expand_ratio != 1:
            x = fused_pointwise_conv_bn_relu6(
                x,
                self.expand_conv.weight,
                self.expand_bn.weight,
                self.expand_bn.bias,
                self.expand_bn.running_mean,
                self.expand_bn.running_var,
                self.expand_bn.eps
            )
        
        # Depthwise convolution (keep as PyTorch for now)
        x = self.depthwise_conv(x)
        x = self.depthwise_bn(x)
        x = F.relu6(x, inplace=True)
        
        # Projection phase
        x = self.project_conv(x)
        x = self.project_bn(x)
        
        return x


class ModelNew(nn.Module):
    """EfficientNetB1 with optimized Triton kernels."""
    def __init__(self, num_classes=1000):
        super(ModelNew, self).__init__()
        
        # Initial convolutional layer
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(32)
        
        # MBConv blocks with Triton fusion
        self.mbconv1 = FusedMBConvBlock(32, 16, 1, 1)
        self.mbconv2 = FusedMBConvBlock(16, 24, 2, 6)
        self.mbconv3 = FusedMBConvBlock(24, 40, 2, 6)
        self.mbconv4 = FusedMBConvBlock(40, 80, 2, 6)
        self.mbconv5 = FusedMBConvBlock(80, 112, 1, 6)
        self.mbconv6 = FusedMBConvBlock(112, 192, 2, 6)
        self.mbconv7 = FusedMBConvBlock(192, 320, 1, 6)
        
        # Final convolutional layer
        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(1280)
        
        # Fully connected layer with optimized Triton kernel
        self.fc_weight = nn.Parameter(torch.randn(num_classes, 1280))
        self.fc_bias = nn.Parameter(torch.randn(num_classes))
        
    def forward(self, x):
        # Initial conv + bn + relu
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x, inplace=True)
        
        # MBConv blocks
        x = self.mbconv1(x)
        x = self.mbconv2(x)
        x = self.mbconv3(x)
        x = self.mbconv4(x)
        x = self.mbconv5(x)
        x = self.mbconv6(x)
        x = self.mbconv7(x)
        
        # Final conv + bn + relu
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x, inplace=True)
        
        # Global pooling and flatten
        x = F.adaptive_avg_pool2d(x, (1, 1))
        x = torch.flatten(x, 1)
        
        # Final classification layer with optimized fused kernel
        x = fused_linear_bias(x, self.fc_weight, self.fc_bias)
        
        return x
```

---

## Your Task

Identify the **single most critical issue** that causes the error above.

### Analysis Guidelines

1. **Focus on root cause**, not symptoms
   - Bad: "Output is wrong"
   - Good: "BLOCK_K loop missing, only processes first 32 elements of K dimension"

2. **Be specific about WHAT and WHERE**
   - Bad: "Memory access issue"
   - Good: "Line 45: tl.atomic_add(c_block_ptr, acc) - atomic_add requires scalar pointer, not block_ptr"

3. **Prioritize by impact**
   - Correctness bugs > Performance issues > Style problems
   - Algorithm errors > Implementation details

### Output Format

**CRITICAL: You MUST output ONLY valid JSON. No other text allowed.**

```json
{
  "critical_issue": "<Concise description of THE root cause, max 30 words>",
  "why_it_matters": "<Why this causes the observed error, max 35 words>",
  "minimal_fix_hint": "<What needs to change (not how), max 30 words>"
}
```

**Remember**: Output ONLY the JSON block. No explanations, no commentary, no additional text.
