```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import triton
import triton.language as tl
import math


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_stages=2, num_warps=4),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_stages=3, num_warps=8),
    ],
    key=['M', 'N', 'K'],
)
@triton.jit
def fused_pointwise_conv_bn_relu6_mm_kernel(
    x_ptr,
    w_ptr,
    out_ptr,
    bn_weight_ptr,
    bn_bias_ptr,
    bn_mean_ptr,
    bn_var_ptr,
    M, N, K,
    stride_xm, stride_xk,
    stride_wk, stride_wn,
    stride_om, stride_on,
    eps: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)
    
    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk
    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn
    
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    for k in range(0, K, BLOCK_K):
        mask_x = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)
        mask_w = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)
        
        x = tl.load(x_ptrs, mask=mask_x, other=0.0)
        w = tl.load(w_ptrs, mask=mask_w, other=0.0)
        
        acc += tl.dot(x, w, allow_tf32=True)
        
        x_ptrs += BLOCK_K * stride_xk
        w_ptrs += BLOCK_K * stride_wk
    
    bn_weight = tl.load(bn_weight_ptr + offs_n, mask=offs_n < N, other=1.0)
    bn_bias = tl.load(bn_bias_ptr + offs_n, mask=offs_n < N, other=0.0)
    bn_mean = tl.load(bn_mean_ptr + offs_n, mask=offs_n < N, other=0.0)
    bn_var = tl.load(bn_var_ptr + offs_n, mask=offs_n < N, other=1.0)
    
    inv_std = bn_weight * tl.rsqrt(bn_var + eps)
    normalized = (acc - bn_mean) * inv_std + bn_bias
    
    activated = tl.minimum(tl.maximum(normalized, 0.0), 6.0)
    
    out_ptrs = out_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on
    mask_out = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(out_ptrs, activated, mask=mask_out)


def fused_pointwise_conv_bn_relu6(x, weight, bn_weight, bn_bias, bn_mean, bn_var, eps=1e-5):
    N, C_in, H, W = x.shape
    C_out = weight.shape[0]
    M = N * H * W
    
    x_2d = x.view(-1, C_in).contiguous()
    weight_2d = weight.view(C_out, C_in).t().contiguous()
    out_2d = torch.empty((M, C_out), device=x.device, dtype=x.dtype)
    
    grid = (
        triton.cdiv(M, 128),
        triton.cdiv(C_out, 128),
    )
    
    fused_pointwise_conv_bn_relu6_mm_kernel[grid](
        x_2d, weight_2d, out_2d,
        bn_weight, bn_bias, bn_mean, bn_var,
        M, C_out, C_in,
        x_2d.stride(0), x_2d.stride(1),
        weight_2d.stride(0), weight_2d.stride(1),
        out_2d.stride(0), out_2d.stride(1),
        eps,
    )
    
    out = out_2d.view(N, H, W, C_out).permute(0, 3, 1, 2).contiguous()
    return out


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_stages=2, num_warps=4),
        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_stages=3, num_warps=8),
    ],
    key=['M', 'N', 'K'],
)
@triton.jit
def fused_linear_bias_optimized_kernel(
    x_ptr,
    w_ptr,
    b_ptr,
    out_ptr,
    M, N, K,
    stride_xm, stride_xk,
    stride_wk, stride_wn,
    stride_bn,
    stride_om, stride_on,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)
    
    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk
    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn
    
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    for k in range(0, K, BLOCK_K):
        mask_x = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)
        mask_w = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)
        
        x = tl.load(x_ptrs, mask=mask_x, other=0.0)
        w = tl.load(w_ptrs, mask=mask_w, other=0.0)
        
        acc += tl.dot(x, w, allow_tf32=True)
        
        x_ptrs += BLOCK_K * stride_xk
        w_ptrs += BLOCK_K * stride_wk
    
    bias = tl.load(b_ptr + offs_n, mask=offs_n < N, other=0.0)
    acc += bias[None, :]
    
    out_ptrs = out_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on
    mask_out = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(out_ptrs, acc, mask=mask_out)


def fused_linear_bias(x, weight, bias):
    M, K = x.shape
    N = weight.shape[0]
    
    x = x.contiguous()
    weight = weight.contiguous().t()
    bias = bias.contiguous()
    
    out = torch.empty((M, N), device=x.device, dtype=x.dtype)
    
    grid = (
        triton.cdiv(M, 16),
        triton.cdiv(N, 128),
    )
    
    fused_linear_bias_optimized_kernel[grid](
        x, weight, bias, out,
        M, N, K,
        x.stride(0), x.stride(1),
        weight.stride(0), weight.stride(1),
        bias.stride(0),
        out.stride(0), out.stride(1),
    )
    
    return out


class FusedMBConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride, expand_ratio):
        super(FusedMBConvBlock, self).__init__()
        self.stride = stride
        self.expand_ratio = expand_ratio
        hidden_dim = round(in_channels * expand_ratio)
        
        self.expand_conv = nn.Conv2d(in_channels, hidden_dim, 
                                    kernel_size=1, stride=1, padding=0, bias=False)
        self.expand_bn = nn.BatchNorm2d(hidden_dim)
        
        self.depthwise_conv = nn.Conv2d(hidden_dim, hidden_dim,
                                       kernel_size=3, stride=stride,
                                       padding=1, groups=hidden_dim, bias=False)
        self.depthwise_bn = nn.BatchNorm2d(hidden_dim)
        
        self.project_conv = nn.Conv2d(hidden_dim, out_channels,
                                     kernel_size=1, stride=1, padding=0, bias=False)
        self.project_bn = nn.BatchNorm2d(out_channels)
    
    def forward(self, x):
        if self.expand_ratio != 1:
            x = fused_pointwise_conv_bn_relu6(
                x,
                self.expand_conv.weight,
                self.expand_bn.weight,
                self.expand_bn.bias,
                self.expand_bn.running_mean,
                self.expand_bn.running_var,
                self.expand_bn.eps
            )
        
        x = self.depthwise_conv(x)
        x = self.depthwise_bn(x)
        x = F.relu6(x, inplace=True)
        
        x = self.project_conv(x)
        x = self.project_bn(x)
        
        return x


class ModelNew(nn.Module):
    def __init__(self, num_classes=1000):
        super(ModelNew, self).__init__()
        
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(32)
        
        self.mbconv1 = FusedMBConvBlock(32, 16, 1, 1)
        self.mbconv2 = FusedMBConvBlock(16, 24, 2, 6)
        self.mbconv3 = FusedMBConvBlock(24, 40, 2, 6)
        self.mbconv4 = FusedMBConvBlock(40, 80, 2, 6)
        self.mbconv5 = FusedMBConvBlock(80, 112, 1, 6)
        self.mbconv6 = FusedMBConvBlock(112, 192, 2, 6)
        self.mbconv7 = FusedMBConvBlock(192, 320, 1, 6)
        
        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(1280)
        
        self.fc_weight = nn.Parameter(torch.randn(num_classes, 1280))
        self.fc_bias = nn.Parameter(torch.randn(num_classes))
        
    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x, inplace=True)
        
        x = self.mbconv1(x)
        x = self.mbconv2(x)
        x = self.mbconv3(x)
        x = self.mbconv4(x)
        x = self.mbconv5(x)
        x = self.mbconv6(x)
        x = self.mbconv7(x)
        
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x, inplace=True)
        
        x = F.adaptive_avg_pool2d(x, (1, 1))
        x = torch.flatten(x, 1)
        
        x = fused_linear_bias(x, self.fc_weight, self.fc_bias)
        
        return x
```