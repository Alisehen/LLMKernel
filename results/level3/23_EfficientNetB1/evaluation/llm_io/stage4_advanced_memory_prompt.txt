You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]f

## Current Optimization Stage

Focus: Fine-tuning fused kernel parameters.

Params:
- num_warps ∈ {4, 8}
- num_stages ∈ {2, 3}

Conditional Rules (NOT one-size-fits-all):

IF register pressure LOW (regs < 96, no spill):
  - Try num_warps=8 for compute-bound fusion
  - num_stages=3 may help hide latency

IF register pressure HIGH (regs > 128 or occupancy_limit_registers):
  - Use num_warps=4 (fewer warps = more registers per warp)
  - Keep num_stages=2 (higher stages need more registers)

IF multi-input fusion (3+ distinct loads):
  - num_stages=2 preferred (each stage buffers all inputs)
  - num_warps=4 often better than 8

Autotune:
- Max 2-3 configs to reduce compilation time
- Always include conservative baseline (num_warps=4, num_stages=2)
- Test before/after: revert if gain < 2%



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import triton
import triton.language as tl
import math


@triton.jit
def fused_pointwise_conv_bn_relu6_mm_kernel(
    x_ptr,
    w_ptr,
    out_ptr,
    bn_weight_ptr,
    bn_bias_ptr,
    bn_mean_ptr,
    bn_var_ptr,
    M, N, K,  # M = N*H*W, K = C_in, N = C_out
    stride_xm, stride_xk,
    stride_wk, stride_wn,
    stride_om, stride_on,
    eps: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """Fused pointwise convolution as matrix multiplication + batch norm + ReLU6.
    
    Input x: (M, K) where M = N*H*W, K = C_in
    Weight w: (K, N) - already transposed
    Output: (M, N) -> reshape to (N, C_out, H, W)
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    # Offsets for blocks
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)
    
    # Pointers to input and weight blocks
    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk
    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn
    
    # Initialize accumulator
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    # Matrix multiplication with tensor cores
    for k in range(0, K, BLOCK_K):
        # Load input and weight blocks with masks
        mask_x = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)
        mask_w = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)
        
        x = tl.load(x_ptrs, mask=mask_x, other=0.0)
        w = tl.load(w_ptrs, mask=mask_w, other=0.0)
        
        # Matrix multiplication using tensor cores
        acc += tl.dot(x, w, allow_tf32=True)
        
        # Update pointers
        x_ptrs += BLOCK_K * stride_xk
        w_ptrs += BLOCK_K * stride_wk
    
    # Load batch norm parameters
    bn_weight = tl.load(bn_weight_ptr + offs_n, mask=offs_n < N, other=1.0)
    bn_bias = tl.load(bn_bias_ptr + offs_n, mask=offs_n < N, other=0.0)
    bn_mean = tl.load(bn_mean_ptr + offs_n, mask=offs_n < N, other=0.0)
    bn_var = tl.load(bn_var_ptr + offs_n, mask=offs_n < N, other=1.0)
    
    # Apply batch norm: (x - mean) * (weight / sqrt(var + eps)) + bias
    inv_std = bn_weight * tl.rsqrt(bn_var + eps)
    normalized = (acc - bn_mean) * inv_std + bn_bias
    
    # Apply ReLU6: min(max(x, 0), 6)
    activated = tl.minimum(tl.maximum(normalized, 0.0), 6.0)
    
    # Store output
    out_ptrs = out_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on
    mask_out = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(out_ptrs, activated, mask=mask_out)


def fused_pointwise_conv_bn_relu6(x, weight, bn_weight, bn_bias, bn_mean, bn_var, eps=1e-5):
    """Fused pointwise convolution, batch norm, and ReLU6 using matrix multiplication."""
    N, C_in, H, W = x.shape
    C_out = weight.shape[0]
    M = N * H * W
    
    # Reshape input to 2D and transpose weight for matrix multiplication
    x_2d = x.view(-1, C_in).contiguous()
    weight_2d = weight.view(C_out, C_in).t().contiguous()
    
    # Create output tensor (2D)
    out_2d = torch.empty((M, C_out), device=x.device, dtype=x.dtype)
    
    # Grid configuration optimized for Ada Lovelace
    BLOCK_M = 128
    BLOCK_N = 128
    BLOCK_K = 32
    
    grid = (
        triton.cdiv(M, BLOCK_M),
        triton.cdiv(C_out, BLOCK_N),
    )
    
    # Launch kernel
    fused_pointwise_conv_bn_relu6_mm_kernel[grid](
        x_2d, weight_2d, out_2d,
        bn_weight, bn_bias, bn_mean, bn_var,
        M, C_out, C_in,
        x_2d.stride(0), x_2d.stride(1),
        weight_2d.stride(0), weight_2d.stride(1),
        out_2d.stride(0), out_2d.stride(1),
        eps,
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
        BLOCK_K=BLOCK_K,
        num_stages=2,
        num_warps=8,
    )
    
    # Reshape output back to 4D
    out = out_2d.view(N, H, W, C_out).permute(0, 3, 1, 2).contiguous()
    return out


@triton.jit
def fused_linear_bias_optimized_kernel(
    x_ptr,
    w_ptr,
    b_ptr,
    out_ptr,
    M, N, K,
    stride_xm, stride_xk,
    stride_wk, stride_wn,
    stride_bn,
    stride_om, stride_on,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """Optimized fused linear layer with bias addition."""
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)
    
    # Pointers to input and weight
    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk
    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn
    
    # Initialize accumulator
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    # Accumulate over K dimension
    for k in range(0, K, BLOCK_K):
        # Load input and weight blocks
        mask_x = (offs_m[:, None] < M) & (offs_k[None, :] < K - k)
        mask_w = (offs_k[:, None] < K - k) & (offs_n[None, :] < N)
        
        x = tl.load(x_ptrs, mask=mask_x, other=0.0)
        w = tl.load(w_ptrs, mask=mask_w, other=0.0)
        
        # Matrix multiplication with tensor cores
        acc += tl.dot(x, w, allow_tf32=True)
        
        # Update pointers
        x_ptrs += BLOCK_K * stride_xk
        w_ptrs += BLOCK_K * stride_wk
    
    # Add bias
    bias = tl.load(b_ptr + offs_n, mask=offs_n < N, other=0.0)
    acc += bias[None, :]
    
    # Store result
    out_ptrs = out_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on
    mask_out = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(out_ptrs, acc, mask=mask_out)


def fused_linear_bias(x, weight, bias):
    """Fused linear layer with bias - optimized for Ada Lovelace."""
    M, K = x.shape
    N = weight.shape[0]
    
    # Ensure inputs are contiguous
    x = x.contiguous()
    weight = weight.contiguous().t()
    bias = bias.contiguous()
    
    # Create output tensor
    out = torch.empty((M, N), device=x.device, dtype=x.dtype)
    
    # Grid configuration optimized for small batch sizes
    BLOCK_M = 32  # Reduced for better occupancy
    BLOCK_N = 64
    BLOCK_K = 32
    
    grid = (
        triton.cdiv(M, BLOCK_M),
        triton.cdiv(N, BLOCK_N),
    )
    
    # Launch kernel
    fused_linear_bias_optimized_kernel[grid](
        x, weight, bias, out,
        M, N, K,
        x.stride(0), x.stride(1),
        weight.stride(0), weight.stride(1),
        bias.stride(0),
        out.stride(0), out.stride(1),
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
        BLOCK_K=BLOCK_K,
        num_stages=2,
        num_warps=4,
    )
    
    return out


class FusedMBConvBlock(nn.Module):
    """Fused MBConv block with Triton optimizations."""
    def __init__(self, in_channels, out_channels, stride, expand_ratio):
        super(FusedMBConvBlock, self).__init__()
        self.stride = stride
        self.expand_ratio = expand_ratio
        hidden_dim = round(in_channels * expand_ratio)
        
        # Expansion phase (fused: conv + bn + relu6)
        self.expand_conv = nn.Conv2d(in_channels, hidden_dim, 
                                    kernel_size=1, stride=1, padding=0, bias=False)
        self.expand_bn = nn.BatchNorm2d(hidden_dim)
        
        # Depthwise convolution
        self.depthwise_conv = nn.Conv2d(hidden_dim, hidden_dim,
                                       kernel_size=3, stride=stride,
                                       padding=1, groups=hidden_dim, bias=False)
        self.depthwise_bn = nn.BatchNorm2d(hidden_dim)
        
        # Projection phase
        self.project_conv = nn.Conv2d(hidden_dim, out_channels,
                                     kernel_size=1, stride=1, padding=0, bias=False)
        self.project_bn = nn.BatchNorm2d(out_channels)
    
    def forward(self, x):
        # Fused expansion phase using optimized Triton kernel
        if self.expand_ratio != 1:
            x = fused_pointwise_conv_bn_relu6(
                x,
                self.expand_conv.weight,
                self.expand_bn.weight,
                self.expand_bn.bias,
                self.expand_bn.running_mean,
                self.expand_bn.running_var,
                self.expand_bn.eps
            )
        
        # Depthwise convolution (keep as PyTorch for now)
        x = self.depthwise_conv(x)
        x = self.depthwise_bn(x)
        x = F.relu6(x, inplace=True)
        
        # Projection phase
        x = self.project_conv(x)
        x = self.project_bn(x)
        
        return x


class ModelNew(nn.Module):
    """EfficientNetB1 with optimized Triton kernels."""
    def __init__(self, num_classes=1000):
        super(ModelNew, self).__init__()
        
        # Initial convolutional layer
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(32)
        
        # MBConv blocks with Triton fusion
        self.mbconv1 = FusedMBConvBlock(32, 16, 1, 1)
        self.mbconv2 = FusedMBConvBlock(16, 24, 2, 6)
        self.mbconv3 = FusedMBConvBlock(24, 40, 2, 6)
        self.mbconv4 = FusedMBConvBlock(40, 80, 2, 6)
        self.mbconv5 = FusedMBConvBlock(80, 112, 1, 6)
        self.mbconv6 = FusedMBConvBlock(112, 192, 2, 6)
        self.mbconv7 = FusedMBConvBlock(192, 320, 1, 6)
        
        # Final convolutional layer
        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(1280)
        
        # Fully connected layer (uses optimized Triton kernel)
        self.fc_weight = nn.Parameter(torch.randn(num_classes, 1280))
        self.fc_bias = nn.Parameter(torch.randn(num_classes))
        
    def forward(self, x):
        # Initial conv + bn + relu
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x, inplace=True)
        
        # MBConv blocks
        x = self.mbconv1(x)
        x = self.mbconv2(x)
        x = self.mbconv3(x)
        x = self.mbconv4(x)
        x = self.mbconv5(x)
        x = self.mbconv6(x)
        x = self.mbconv7(x)
        
        # Final conv + bn + relu
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x, inplace=True)
        
        # Global pooling and flatten
        x = F.adaptive_avg_pool2d(x, (1, 1))
        x = torch.flatten(x, 1)
        
        # Final classification layer (optimized fused linear + bias)
        x = fused_linear_bias(x, self.fc_weight, self.fc_bias)
        
        return x
```

[NCU PROFILING METRICS]
{
  "fused_pointwise_conv_bn_relu6_mm_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 9.89,
    "launch__grid_size": 45,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 16.55,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 7.29,
    "lts__t_sector_hit_rate.pct": 88.43
  },
  "fused_linear_bias_optimized_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 1.91,
    "launch__grid_size": 16,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 8.33,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 17.42,
    "lts__t_sector_hit_rate.pct": 16.11
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

TRITON API CONSTRAINTS (CRITICAL):
- Triton has NO: tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
