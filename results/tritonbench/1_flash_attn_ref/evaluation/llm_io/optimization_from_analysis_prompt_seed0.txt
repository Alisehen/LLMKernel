You are optimizing a Triton kernel based on algorithmic analysis.

# PyTorch Reference (Target Behavior)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class Model(nn.Module):
    """
    Flash Attention - Memory-efficient attention mechanism
    Computes scaled dot-product attention: softmax(Q @ K^T / sqrt(d)) @ V
    """
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:
        """
        Performs scaled dot-product attention.

        Args:
            Q (torch.Tensor): Query tensor of shape (batch, n_heads, seq_len, head_dim)
            K (torch.Tensor): Key tensor of shape (batch, n_heads, seq_len, head_dim)
            V (torch.Tensor): Value tensor of shape (batch, n_heads, seq_len, head_dim)

        Returns:
            torch.Tensor: Output tensor of shape (batch, n_heads, seq_len, head_dim)
        """
        batch, n_heads, seq_len, head_dim = Q.shape
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(head_dim)
        # Softmax
        attn_weights = F.softmax(scores, dim=-1)
        # Multiply by V
        output = torch.matmul(attn_weights, V)

        return output

BATCH = 2
N_HEADS = 2
SEQ_LEN = 128
HEAD_DIM = 64

def get_inputs():
    Q = torch.randn(BATCH, N_HEADS, SEQ_LEN, HEAD_DIM)
    K = torch.randn(BATCH, N_HEADS, SEQ_LEN, HEAD_DIM)
    V = torch.randn(BATCH, N_HEADS, SEQ_LEN, HEAD_DIM)
    return [Q, K, V]

def get_init_inputs():
    return []
```

**CRITICAL**: Study the PyTorch code carefully to understand:
- What does `forward()` return? (full output sequence vs final hidden state only)
- What is the computational pattern?
- What are the input/output shapes?

Your optimized kernel MUST match this exact behavior.

---

# Analysis Results

**Bottleneck**: The main bottleneck is quadratic global memory traffic to read/write the scores tensor and the separation of QK^T, softmax, and PV into distinct kernels. This wastes bandwidth and launch overhead compared to an on-chip, streaming attention algorithm.

**Optimization Strategy**: Replace the three-kernel pipeline (qk_matmul_scaled → softmax_inplace_last_dim → pv_matmul) with a single fused FlashAttention-style kernel that performs tiled QK^T, online softmax, and PV in one pass without ever materializing the scores matrix in global memory.

**Implementation Plan**: Implement a Triton kernel that, for each block of queries, iterates over K/V in tiles: (1) compute partial QK^T for the tile, (2) update per-row running max and normalization factor using online softmax formulas, and (3) update the output accumulation P@V in registers/SMEM. At the end of the loop over K/V tiles, write only the final output [B, H, S, D] to global memory, eliminating the scores buffer and the separate softmax and PV matmul kernels.

**Expected Speedup**: 30-40%

---

# Current Kernel (needs optimization)

```python
import torch, torch.nn as nn, triton, triton.language as tl
import math


@triton.jit
def qk_matmul_scaled_kernel(
    q_ptr, k_ptr, scores_ptr,
    BH, M, N, D, sm_scale,
    stride_qb, stride_qm, stride_qk,
    stride_kb, stride_kn, stride_kk,
    stride_sb, stride_sm, stride_sn,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    """
    Compute scaled Q @ K^T for batched heads:
    Q: [BH, M, D]
    K: [BH, N, D]
    scores: [BH, M, N]
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    pid_b = tl.program_id(2)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    # Pointers to the first K-tile in K dimension
    q_ptrs = q_ptr + pid_b * stride_qb \
             + offs_m[:, None] * stride_qm \
             + offs_k[None, :] * stride_qk  # (BLOCK_M, BLOCK_K)

    # We load K as shape (BLOCK_K, BLOCK_N) to do Q @ K^T
    k_ptrs = k_ptr + pid_b * stride_kb \
             + offs_n[None, :] * stride_kn \
             + offs_k[:, None] * stride_kk  # (BLOCK_K, BLOCK_N)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    for k0 in range(0, D, BLOCK_K):
        k_offsets = k0 + offs_k
        k_mask = k_offsets < D

        q_mask = (offs_m[:, None] < M) & (k_mask[None, :])
        k_mask_full = (k_mask[:, None]) & (offs_n[None, :] < N)

        q = tl.load(q_ptrs, mask=q_mask, other=0.0)
        k_block = tl.load(k_ptrs, mask=k_mask_full, other=0.0)

        acc += tl.dot(q, k_block, allow_tf32=True)

        q_ptrs += BLOCK_K * stride_qk
        k_ptrs += BLOCK_K * stride_kk

    acc = acc * sm_scale

    scores_ptrs = scores_ptr + pid_b * stride_sb \
                  + offs_m[:, None] * stride_sm \
                  + offs_n[None, :] * stride_sn

    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(scores_ptrs, acc, mask=out_mask)


@triton.jit
def softmax_kernel(
    x_ptr,
    BH, M, N,
    stride_xb, stride_xm, stride_xn,
    BLOCK_N: tl.constexpr,
):
    """
    In-place softmax over last dimension N of x: [BH, M, N].
    One program handles one row (fixed [b, m, :]).
    """
    row_id = tl.program_id(0)
    offs = tl.arange(0, BLOCK_N)

    # Map row_id -> (bh, m)
    bh = row_id // M
    m = row_id % M

    row_ptr = x_ptr + bh * stride_xb + m * stride_xm

    # Pass 1: compute max
    m_i = -float('inf')
    for n0 in range(0, N, BLOCK_N):
        offs_n = n0 + offs
        mask = offs_n < N
        x = tl.load(row_ptr + offs_n * stride_xn, mask=mask, other=-float('inf'))
        curr_max = tl.max(x, axis=0)
        m_i = tl.maximum(m_i, curr_max)

    # Pass 2: compute sum of exp(x - m_i), store exp(x - m_i) to x_ptr
    l_i = 0.0
    for n0 in range(0, N, BLOCK_N):
        offs_n = n0 + offs
        mask = offs_n < N
        x = tl.load(row_ptr + offs_n * stride_xn, mask=mask, other=-float('inf'))
        x = x - m_i
        p = tl.exp(x)
        tl.store(row_ptr + offs_n * stride_xn, p, mask=mask)
        l_i += tl.sum(p, axis=0)

    # Pass 3: normalize
    inv_l_i = 1.0 / l_i
    for n0 in range(0, N, BLOCK_N):
        offs_n = n0 + offs
        mask = offs_n < N
        p = tl.load(row_ptr + offs_n * stride_xn, mask=mask, other=0.0)
        p = p * inv_l_i
        tl.store(row_ptr + offs_n * stride_xn, p, mask=mask)


@triton.jit
def pv_matmul_kernel(
    p_ptr, v_ptr, out_ptr,
    BH, M, N_CTX, D,
    stride_pb, stride_pm, stride_pk,
    stride_vb, stride_vk, stride_vn,
    stride_ob, stride_om, stride_on,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    """
    Compute out = P @ V
    P: [BH, M, N_CTX]
    V: [BH, N_CTX, D]
    out: [BH, M, D]
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    pid_b = tl.program_id(2)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    p_ptrs = p_ptr + pid_b * stride_pb \
             + offs_m[:, None] * stride_pm \
             + offs_k[None, :] * stride_pk  # (BLOCK_M, BLOCK_K)

    v_ptrs = v_ptr + pid_b * stride_vb \
             + offs_k[:, None] * stride_vk \
             + offs_n[None, :] * stride_vn  # (BLOCK_K, BLOCK_N)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    for k0 in range(0, N_CTX, BLOCK_K):
        k_offsets = k0 + offs_k
        k_mask = k_offsets < N_CTX

        p_mask = (offs_m[:, None] < M) & (k_mask[None, :])
        v_mask = (k_mask[:, None]) & (offs_n[None, :] < D)

        p_block = tl.load(p_ptrs, mask=p_mask, other=0.0)
        v_block = tl.load(v_ptrs, mask=v_mask, other=0.0)

        acc += tl.dot(p_block, v_block, allow_tf32=True)

        p_ptrs += BLOCK_K * stride_pk
        v_ptrs += BLOCK_K * stride_vk

    out_ptrs = out_ptr + pid_b * stride_ob \
               + offs_m[:, None] * stride_om \
               + offs_n[None, :] * stride_on

    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < D)
    tl.store(out_ptrs, acc, mask=out_mask)


def qk_matmul_scaled(Q, K):
    """
    Q, K: [B, H, S, D]
    returns scores: [B, H, S, S]
    """
    B, H, S, D = Q.shape
    BH = B * H
    device = Q.device

    q = Q.contiguous().view(BH, S, D)
    k = K.contiguous().view(BH, S, D)
    scores = torch.empty((BH, S, S), device=device, dtype=Q.dtype)

    stride_qb, stride_qm, stride_qk = q.stride()
    stride_kb, stride_kn, stride_kk = k.stride()
    stride_sb, stride_sm, stride_sn = scores.stride()

    sm_scale = 1.0 / math.sqrt(D)

    BLOCK_M = 64
    BLOCK_N = 64
    BLOCK_K = 64

    grid = lambda META: (
        triton.cdiv(S, META['BLOCK_M']),
        triton.cdiv(S, META['BLOCK_N']),
        BH,
    )

    qk_matmul_scaled_kernel[grid](
        q, k, scores,
        BH, S, S, D, sm_scale,
        stride_qb, stride_qm, stride_qk,
        stride_kb, stride_kn, stride_kk,
        stride_sb, stride_sm, stride_sn,
        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,
    )

    return scores.view(B, H, S, S)


def softmax_inplace_last_dim(x):
    """
    In-place softmax over last dim of x: [B, H, S, S]
    """
    B, H, M, N = x.shape
    BH = B * H
    x_ = x.view(BH, M, N)

    stride_xb, stride_xm, stride_xn = x_.stride()
    BLOCK_N = 128

    num_rows = BH * M
    grid = lambda META: (triton.cdiv(num_rows, 1),)

    softmax_kernel[grid](
        x_,
        BH, M, N,
        stride_xb, stride_xm, stride_xn,
        BLOCK_N=BLOCK_N,
    )


def pv_matmul(P, V):
    """
    P: [B, H, S, S] (softmaxed attention)
    V: [B, H, S, D]
    returns: [B, H, S, D]
    """
    B, H, M, N_CTX = P.shape
    _, _, _, D = V.shape
    BH = B * H
    device = P.device

    p = P.contiguous().view(BH, M, N_CTX)
    v = V.contiguous().view(BH, N_CTX, D)
    out = torch.empty((BH, M, D), device=device, dtype=P.dtype)

    stride_pb, stride_pm, stride_pk = p.stride()
    stride_vb, stride_vk, stride_vn = v.stride()
    stride_ob, stride_om, stride_on = out.stride()

    BLOCK_M = 64
    BLOCK_N = 64
    BLOCK_K = 64

    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_M']),
        triton.cdiv(D, META['BLOCK_N']),
        BH,
    )

    pv_matmul_kernel[grid](
        p, v, out,
        BH, M, N_CTX, D,
        stride_pb, stride_pm, stride_pk,
        stride_vb, stride_vk, stride_vn,
        stride_ob, stride_om, stride_on,
        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,
    )

    return out.view(B, H, M, D)


class ModelNew(nn.Module):
    """
    Triton-optimized scaled dot-product attention:
    softmax(Q @ K^T / sqrt(d)) @ V
    """
    def __init__(self):
        super(ModelNew, self).__init__()

    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:
        scores = qk_matmul_scaled(Q, K)
        softmax_inplace_last_dim(scores)
        out = pv_matmul(scores, V)
        return out
```

---

# Your Task

Implement the optimization strategy above. Focus on the specific bottleneck identified.

## Key Requirements

1. **Preserve correctness**: Maintain the same input/output behavior
2. **Apply the optimization**: Follow the implementation plan exactly
3. **Use valid Triton syntax**:
   - Every kernel MUST have `@triton.jit` decorator
   - Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
   - BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
   - No `continue`, `break`, `return` inside kernels (use masking)
   - Prefer `tl.dot(a, b, allow_tf32=True)` for matmul operations

4. **CRITICAL for RNN/GRU/LSTM Persistent Kernels**:
   - Time loop MUST be inside @triton.jit kernel, NOT in Python forward()
   - **HYBRID computation strategy** (CRITICAL for performance):
     * Precompute input-side gates OUTSIDE kernel: `gates_x = (T*B, In) @ W_ih` (ONE large GEMM)
     * INSIDE kernel: only recurrent-side: `for t: gates_h = h @ W_hh` (T small GEMMs)
   - CORRECT (FAST - use this):
     ```python
     # Python forward():
     gates_x_all = x.reshape(T*B, In) @ W_ih + b_ih  # ONE large GEMM
     gates_x_all = gates_x_all.view(T, B, 3*H)
     gru_persistent_kernel[grid](gates_x_all, h0, W_hh, ...)  # Launch ONCE

     @triton.jit
     def gru_persistent_kernel(gates_x_ptr, h_ptr, W_hh_ptr, ...):
         for t in range(T):  # Inside kernel
             gates_x_t = tl.load(gates_x_ptr + t*...)  # Precomputed
             gates_h = h @ W_hh  # Only recurrent GEMM
             h = (1-z)*n + z*h   # Fuse and update
     ```

5. **Output format**:
   - Imports: `import torch, torch.nn as nn, triton, triton.language as tl`
   - `@triton.jit` kernel(s)
   - Wrapper function(s)
   - `class ModelNew(nn.Module)` — REQUIRED
   - NO testing code, NO `if __name__ == "__main__"`

---

Generate the optimized kernel now. Output ONLY the complete Python code.
