You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB



[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: BLOCK_M/N/K selection.

Metrics:
- sm__warps_active.avg.pct_of_peak_sustained_active (>50%)

Rules:
- BLOCK_* must be powers of 2
- Tensor Core: BLOCK_M/N multiple of 16, BLOCK_K multiple of 8 (preference)
- FP32: M/N ∈ {32,64,128,256}, K ∈ {16,32,64}
- Avoid oversized tiles (mask waste)
- Keep baseline tile if unsure

Autotune:
- 2–4 configs max
- Autotune ONLY on @triton.jit kernel



[CURRENT CODE]
```python
# <optimized Triton code>
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({}, num_warps=2, num_stages=1),
        triton.Config({}, num_warps=4, num_stages=2),
        triton.Config({}, num_warps=8, num_stages=2),
    ],
    key=["N", "C", "H", "W", "OH", "OW"],
)
@triton.jit
def avgpool2d_kernel(
    x_ptr,  # *f32, shape [N, C, H, W]
    y_ptr,  # *f32, shape [N, C, OH, OW]
    N, C, H, W,
    OH, OW,
    STRIDE_H, STRIDE_W,
    PAD_H, PAD_W,
    BLOCK_W: tl.constexpr,
    KERNEL_H: tl.constexpr,
    KERNEL_W: tl.constexpr,
):
    # program ids
    pid_row = tl.program_id(axis=0)       # over N*C*OH
    pid_w_blk = tl.program_id(axis=1)     # over OW blocks

    # decode pid_row -> (n, c, oh)
    OH_i = OH
    C_i = C

    nco = pid_row // OH_i
    oh = pid_row % OH_i
    n = nco // C_i
    c = nco % C_i

    # compute output w indices for this block
    offs_w = pid_w_blk * BLOCK_W + tl.arange(0, BLOCK_W)
    mask_ow = offs_w < OW
    tl.multiple_of(offs_w, BLOCK_W)
    tl.max_contiguous(offs_w, BLOCK_W)

    # accumulator
    acc = tl.zeros((BLOCK_W,), dtype=tl.float32)

    # precompute some scalars
    stride_h = STRIDE_H
    stride_w = STRIDE_W
    pad_h = PAD_H
    pad_w = PAD_W

    # cast dimensions to int64 for safe indexing
    n64 = tl.cast(n, tl.int64)
    c64 = tl.cast(c, tl.int64)
    C64 = tl.cast(C, tl.int64)
    H64 = tl.cast(H, tl.int64)
    W64 = tl.cast(W, tl.int64)

    base_nc = (n64 * C64 + c64) * H64  # (n*C + c) * H

    # loop over kernel window (unrolled by Triton since KERNEL_H/W are constexpr)
    for kh in range(KERNEL_H):
        ih = oh * stride_h + kh - pad_h   # scalar
        valid_h = (ih >= 0) & (ih < H)

        ih64 = tl.cast(ih, tl.int64)
        row_base = (base_nc + ih64) * W64  # ((n*C + c)*H + ih) * W

        # combine masks that are independent of kw
        mask_h = mask_ow & valid_h

        for kw in range(KERNEL_W):
            iw = offs_w * stride_w + kw - pad_w          # [BLOCK_W]
            mask_w = (iw >= 0) & (iw < W)
            mask = mask_h & mask_w

            iw64 = tl.cast(iw, tl.int64)
            ptr = x_ptr + row_base + iw64
            vals = tl.load(ptr, mask=mask, other=0.0)
            acc += vals

    # multiply by reciprocal instead of divide for better throughput
    inv_denom = 1.0 / float(KERNEL_H * KERNEL_W)
    acc = acc * inv_denom

    # write output
    # out index: (((n*C + c) * OH + oh) * OW + ow)
    OH64 = tl.cast(OH, tl.int64)
    OW64 = tl.cast(OW, tl.int64)
    base_out_nc = (n64 * C64 + c64) * OH64
    base_out_row = (base_out_nc + tl.cast(oh, tl.int64)) * OW64

    out_ptrs = y_ptr + base_out_row + tl.cast(offs_w, tl.int64)
    tl.store(out_ptrs, acc, mask=mask_ow)


def triton_avg_pool2d(
    x: torch.Tensor,
    kernel_size: int,
    stride: int = None,
    padding: int = 0,
) -> torch.Tensor:
    """
    Average Pooling 2D using Triton (NCHW, count_include_pad=True, ceil_mode=False).
    """
    assert x.is_cuda, "Input must be on CUDA device"
    assert x.dtype == torch.float32, "This implementation currently supports float32 only"
    assert x.dim() == 4, "Input must be 4D NCHW"

    if stride is None:
        stride = kernel_size

    N, C, H, W = x.shape
    kh = kw = int(kernel_size)
    sh = sw = int(stride)
    ph = pw = int(padding)

    # PyTorch AvgPool2d output size formula with ceil_mode=False
    OH = (H + 2 * ph - kh) // sh + 1
    OW = (W + 2 * pw - kw) // sw + 1

    y = torch.empty((N, C, OH, OW), device=x.device, dtype=x.dtype)

    BLOCK_W = 128  # power-of-two, tuned with num_warps/num_stages autotune

    grid = lambda META: (
        N * C * OH,                         # pid_row
        triton.cdiv(OW, BLOCK_W),          # pid_w_blk
    )

    avgpool2d_kernel[grid](
        x,
        y,
        N, C, H, W,
        OH, OW,
        sh, sw,
        ph, pw,
        BLOCK_W=BLOCK_W,
        KERNEL_H=kh,
        KERNEL_W=kw,
    )

    return y


class ModelNew(nn.Module):
    """
    Triton-backed 2D Average Pooling (NCHW) matching nn.AvgPool2d
    for kernel_size, stride, padding, count_include_pad=True, ceil_mode=False.
    """

    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0) -> None:
        super().__init__()
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_avg_pool2d(x, self.kernel_size, self.stride, self.padding)
```

[NCU PROFILING METRICS]
{
  "avgpool2d_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 7.47,
    "launch__grid_size": 380928,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 52.42,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 89.22,
    "lts__t_sector_hit_rate.pct": 8.73
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
