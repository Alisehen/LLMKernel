```python
# <complete ModelNew code with optimized Triton kernels>
# <corrected code>
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_W": 64}, num_warps=2, num_stages=1),
        triton.Config({"BLOCK_W": 128}, num_warps=4, num_stages=1),
        triton.Config({"BLOCK_W": 128}, num_warps=8, num_stages=2),
        triton.Config({"BLOCK_W": 256}, num_warps=8, num_stages=1),
    ],
    key=["N", "C", "H", "W", "OH", "OW"],
)
@triton.jit
def avgpool2d_kernel(
    x_ptr,  # *f32, shape [N, C, H, W]
    y_ptr,  # *f32, shape [N, C, OH, OW]
    N, C, H, W,
    OH, OW,
    STRIDE_H, STRIDE_W,
    PAD_H, PAD_W,
    KERNEL_H: tl.constexpr,
    KERNEL_W: tl.constexpr,
    BLOCK_W: tl.constexpr,
):
    # program ids
    pid_row = tl.program_id(axis=0)    # over N * C * OH
    pid_w_blk = tl.program_id(axis=1)  # over OW tiles

    # decode pid_row -> (n, c, oh)
    nco = pid_row // OH
    oh = pid_row - nco * OH
    n = nco // C
    c = nco - n * C

    # offsets along OW for this block
    offs_w = pid_w_blk * BLOCK_W + tl.arange(0, BLOCK_W)
    mask_ow = offs_w < OW
    tl.multiple_of(offs_w, BLOCK_W)
    tl.max_contiguous(offs_w, BLOCK_W)

    # accumulator
    acc = tl.zeros((BLOCK_W,), dtype=tl.float32)

    # scalar parameters
    stride_h = STRIDE_H
    stride_w = STRIDE_W
    pad_h = PAD_H
    pad_w = PAD_W
    H_i = H
    W_i = W
    C_i = C

    # base index for this (n, c)
    nc_base = (n * C_i + c) * H_i  # (n*C + c) * H

    # precompute base for h and w indices
    oh_h_base = oh * stride_h - pad_h          # scalar
    w_start = offs_w * stride_w - pad_w        # [BLOCK_W]

    # main pooling loop (fully unrolled by Triton)
    for kh in tl.static_range(KERNEL_H):
        ih = oh_h_base + kh
        valid_h = (ih >= 0) & (ih < H_i)

        # Clamp ih to valid range to keep pointers in-bounds even when masked
        ih_clamp = tl.maximum(tl.minimum(ih, H_i - 1), 0)
        row_off = (nc_base + ih_clamp) * W_i  # scalar

        mask_h = mask_ow & valid_h

        for kw in tl.static_range(KERNEL_W):
            iw = w_start + kw                 # [BLOCK_W]
            mask_w = (iw >= 0) & (iw < W_i)
            mask = mask_h & mask_w

            # Clamp iw to valid range so that all pointers are in-bounds
            iw_clamp = tl.maximum(tl.minimum(iw, W_i - 1), 0)

            ptr = x_ptr + row_off + iw_clamp
            vals = tl.load(ptr, mask=mask, other=0.0)
            acc += vals

    # average over kernel area (count_include_pad=True)
    inv_denom = 1.0 / float(KERNEL_H * KERNEL_W)
    acc *= inv_denom

    # write output
    OH_i = OH
    OW_i = OW
    base_out = ((n * C_i + c) * OH_i + oh) * OW_i
    out_ptrs = y_ptr + base_out + offs_w
    tl.store(out_ptrs, acc, mask=mask_ow)


def triton_avg_pool2d(
    x: torch.Tensor,
    kernel_size: int,
    stride: int = None,
    padding: int = 0,
) -> torch.Tensor:
    """
    Average Pooling 2D using Triton (NCHW, count_include_pad=True, ceil_mode=False).
    """
    assert x.is_cuda, "Input must be on CUDA device"
    assert x.dtype == torch.float32, "This implementation currently supports float32 only"
    assert x.dim() == 4, "Input must be 4D NCHW"

    if stride is None:
        stride = kernel_size

    N, C, H, W = x.shape
    kh = kw = int(kernel_size)
    sh = sw = int(stride)
    ph = pw = int(padding)

    # PyTorch AvgPool2d output size formula with ceil_mode=False
    OH = (H + 2 * ph - kh) // sh + 1
    OW = (W + 2 * pw - kw) // sw + 1

    y = torch.empty((N, C, OH, OW), device=x.device, dtype=x.dtype)

    def grid(meta):
        return (
            N * C * OH,                        # pid_row over (N, C, OH)
            triton.cdiv(OW, meta["BLOCK_W"]),  # pid_w_blk over OW tiles
        )

    avgpool2d_kernel[grid](
        x,
        y,
        N, C, H, W,
        OH, OW,
        sh, sw,
        ph, pw,
        KERNEL_H=kh,
        KERNEL_W=kw,
    )

    return y


class ModelNew(nn.Module):
    """
    Triton-backed 2D Average Pooling (NCHW) matching nn.AvgPool2d
    for kernel_size, stride, padding, count_include_pad=True, ceil_mode=False.
    """

    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0) -> None:
        super().__init__()
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_avg_pool2d(x, self.kernel_size, self.stride, self.padding)
```