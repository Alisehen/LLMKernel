```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def instance_norm_2d_kernel(
    x_ptr,         # *fp32
    y_ptr,         # *fp32
    inv_hw,        # scalar float32 = 1.0 / (H * W)
    eps,           # scalar float32
    HW: tl.constexpr,          # H * W, compile-time for specialization
    BLOCK_SIZE: tl.constexpr,  # number of elements processed per step
):
    pid = tl.program_id(axis=0)
    base = pid * HW

    # First pass: compute sum and sum of squares over spatial dimensions
    sum_vec = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)
    sum_sq_vec = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)

    for off in range(0, HW, BLOCK_SIZE):
        idx = off + tl.arange(0, BLOCK_SIZE)
        mask = idx < HW

        x = tl.load(x_ptr + base + idx, mask=mask, other=0.0)
        x_f32 = x.to(tl.float32)

        sum_vec += x_f32
        sum_sq_vec += x_f32 * x_f32

    total_sum = tl.sum(sum_vec, axis=0)
    total_sq = tl.sum(sum_sq_vec, axis=0)

    mean = total_sum * inv_hw
    var = total_sq * inv_hw - mean * mean
    rstd = 1.0 / tl.sqrt(var + eps)

    # Second pass: normalize and store
    for off in range(0, HW, BLOCK_SIZE):
        idx = off + tl.arange(0, BLOCK_SIZE)
        mask = idx < HW

        x = tl.load(x_ptr + base + idx, mask=mask, other=0.0)
        x_f32 = x.to(tl.float32)

        y = (x_f32 - mean) * rstd
        tl.store(y_ptr + base + idx, y, mask=mask)


def triton_instance_norm_2d(x: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:
    assert x.is_cuda, "Triton kernel requires CUDA tensor"
    x = x.contiguous()
    N, C, H, W = x.shape
    HW = H * W
    NC = N * C

    y = torch.empty_like(x)
    inv_hw = 1.0 / float(HW)

    BLOCK_SIZE = 256

    grid = lambda meta: (max(1, NC),)

    instance_norm_2d_kernel[grid](
        x, y,
        inv_hw,
        eps,
        HW=HW,
        BLOCK_SIZE=BLOCK_SIZE,
        num_warps=8,
    )
    return y


class ModelNew(nn.Module):
    """
    InstanceNorm2d implemented with a high-performance Triton kernel on CUDA.
    Falls back to PyTorch's InstanceNorm2d for non-CUDA or unsupported dtypes.
    """
    def __init__(self, num_features: int):
        super().__init__()
        # PyTorch InstanceNorm2d for fallback (CPU / non-fp32 / non-contiguous)
        self.inorm = nn.InstanceNorm2d(num_features=num_features)
        self.eps = self.inorm.eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Fast Triton path: CUDA, float32, contiguous NCHW
        if x.is_cuda and x.dtype == torch.float32 and x.is_contiguous():
            return triton_instance_norm_2d(x, eps=self.eps)
        # Fallback to PyTorch implementation otherwise
        return self.inorm(x)
```