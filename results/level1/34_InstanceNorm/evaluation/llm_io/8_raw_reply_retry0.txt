```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def fused_instance_norm_kernel(
    x_ptr,
    output_ptr,
    spatial_size,
    total_slices,
    eps: tl.constexpr,
    BLOCK_SIZE: tl.constexpr,
    VECTOR_SIZE: tl.constexpr,
    num_stages: tl.constexpr,
):
    """Fused kernel that computes mean, variance, and normalization in a single pass."""
    pid = tl.program_id(0)  # which (n,c) slice
    
    if pid >= total_slices:
        return
    
    base_offset = pid * spatial_size
    
    # Initialize accumulators - use consistent types
    sum_x = tl.zeros((VECTOR_SIZE,), dtype=tl.float32)
    sum_x2 = tl.zeros((VECTOR_SIZE,), dtype=tl.float32)
    count_vec = tl.zeros((VECTOR_SIZE,), dtype=tl.uint32)  # Vector of counts
    
    # Process in vectorized blocks
    for off in range(0, spatial_size, BLOCK_SIZE * VECTOR_SIZE):
        # Create vectorized indices
        row_idx = tl.arange(0, BLOCK_SIZE)[:, None]
        col_idx = tl.arange(0, VECTOR_SIZE)[None, :]
        idx = off + row_idx * VECTOR_SIZE + col_idx
        mask = idx < spatial_size
        
        # Vectorized load
        x_vec = tl.load(
            x_ptr + base_offset + idx,
            mask=mask,
            other=0.0,
            cache_modifier=".cg",
        )
        
        # Accumulate with masking - use consistent vector operations
        sum_x += tl.sum(x_vec, axis=0)
        sum_x2 += tl.sum(x_vec * x_vec, axis=0)
        count_vec += tl.sum(mask, axis=0)
    
    # Final reduction across vector lanes
    final_sum_x = tl.sum(sum_x)
    final_sum_x2 = tl.sum(sum_x2)
    final_count = tl.sum(tl.cast(count_vec, tl.float32))
    
    # Compute mean and variance
    mean_val = final_sum_x / final_count
    variance = tl.maximum(final_sum_x2 / final_count - mean_val * mean_val, 0.0)
    inv_std = tl.math.rsqrt(variance + eps)
    
    # Normalization pass
    for off in range(0, spatial_size, BLOCK_SIZE * VECTOR_SIZE):
        # Create vectorized indices
        row_idx = tl.arange(0, BLOCK_SIZE)[:, None]
        col_idx = tl.arange(0, VECTOR_SIZE)[None, :]
        idx = off + row_idx * VECTOR_SIZE + col_idx
        mask = idx < spatial_size
        
        # Load again
        x_vec = tl.load(
            x_ptr + base_offset + idx,
            mask=mask,
            other=0.0,
            cache_modifier=".ca",
        )
        
        # Normalize
        normalized = (x_vec - mean_val) * inv_std
        
        # Store
        tl.store(
            output_ptr + base_offset + idx,
            normalized,
            mask=mask,
        )


def optimized_triton_instance_norm(x: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:
    N, C, H, W = x.shape
    spatial_size = H * W
    total_slices = N * C
    
    # Ensure contiguous memory layout
    x_contig = x.contiguous()
    output = torch.empty_like(x_contig)
    
    # Autotune configurations
    configs = [
        triton.Config({'BLOCK_SIZE': 128, 'VECTOR_SIZE': 8, 'num_stages': 2}, num_warps=8),
        triton.Config({'BLOCK_SIZE': 128, 'VECTOR_SIZE': 8, 'num_stages': 3}, num_warps=8),
        triton.Config({'BLOCK_SIZE': 256, 'VECTOR_SIZE': 4, 'num_stages': 2}, num_warps=8),
        triton.Config({'BLOCK_SIZE': 256, 'VECTOR_SIZE': 4, 'num_stages': 3}, num_warps=8),
        triton.Config({'BLOCK_SIZE': 512, 'VECTOR_SIZE': 2, 'num_stages': 2}, num_warps=8),
        triton.Config({'BLOCK_SIZE': 512, 'VECTOR_SIZE': 2, 'num_stages': 3}, num_warps=8),
        triton.Config({'BLOCK_SIZE': 1024, 'VECTOR_SIZE': 1, 'num_stages': 2}, num_warps=8),
        triton.Config({'BLOCK_SIZE': 1024, 'VECTOR_SIZE': 1, 'num_stages': 3}, num_warps=8),
    ]
    
    @triton.autotune(configs=configs, key=['spatial_size', 'total_slices'])
    @triton.jit
    def tuned_kernel(
        x_ptr, output_ptr, spatial_size, total_slices, eps,
        BLOCK_SIZE: tl.constexpr, VECTOR_SIZE: tl.constexpr, num_stages: tl.constexpr
    ):
        fused_instance_norm_kernel(
            x_ptr, output_ptr, spatial_size, total_slices, eps,
            BLOCK_SIZE, VECTOR_SIZE, num_stages
        )
    
    # Launch optimized kernel
    grid = (total_slices,)
    tuned_kernel[grid](
        x_contig, output, spatial_size, total_slices, eps
    )
    
    return output


class ModelNew(nn.Module):
    def __init__(self, num_features: int):
        super().__init__()
        self.num_features = num_features

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return optimized_triton_instance_norm(x)
```