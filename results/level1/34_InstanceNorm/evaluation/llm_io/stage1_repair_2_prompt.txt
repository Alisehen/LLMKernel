Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/language/core.py", line 43, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/language/core.py", line 1094, in __getitem__
    for dim, sl in enumerate(slices):
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/language/core.py", line 41, in wrapper
    raise ValueError("Did you forget to add @triton.jit ? "
ValueError: Did you forget to add @triton.jit ? (`_semantic` argument must be provided outside of JIT functions.)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 533, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 131, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_085222_batch_range20to30_deepseek_deepseek/34_InstanceNorm/code/kernel_20251212_104544.py", line 204, in forward
    return triton_instance_norm(x)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_085222_batch_range20to30_deepseek_deepseek/34_InstanceNorm/code/kernel_20251212_104544.py", line 187, in triton_instance_norm
    normalize_kernel[grid2](
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/runtime/jit.py", line 419, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/runtime/jit.py", line 733, in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/runtime/jit.py", line 861, in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/compiler/compiler.py", line 300, in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/compiler/compiler.py", line 80, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 48:34:
        # Process each (n,c) slice
        for nc_idx in range(BLOCK_NC):
            nc_idx_val = pid_nc + nc_idx
            nc_valid = nc_idx_val < stride_nc

            if nc_valid and tl.sum(spatial_chunk_mask) > 0:
                # Load input
                x_ptrs = x_ptr + nc_idx_val * spatial_size + spatial_idx
                x = tl.load(x_ptrs, mask=spatial_chunk_mask, other=0.0)

                # Normalize
                normalized = (x - mean_val[nc_idx]) * inv_std[nc_idx]
                                  ^
Did you forget to add @triton.jit ? (`_semantic` argument must be provided outside of JIT functions.)

History Error:
Previous Repair Attempts (avoid repeating these errors):
Attempt 1:
        spatial_chunk_mask = spatial_mask & (spatial_idx < spatial_size)

        if tl.any(spatial_chunk_mask):
           ^
AttributeError("module 'triton.language' has no attribute 'any'")



PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Simple model that performs Instance Normalization.
    """
    def __init__(self, num_features: int):
        """
        Initializes the InstanceNorm layer.

        Args:
            num_features (int): Number of features in the input tensor.
        """
        super(Model, self).__init__()
        self.inorm = nn.InstanceNorm2d(num_features=num_features)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Applies Instance Normalization to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, num_features, height, width).

        Returns:
            torch.Tensor: Output tensor with Instance Normalization applied, same shape as input.
        """
        return self.inorm(x)

batch_size = 112  # heavier workload
features = 64
dim1 = 512
dim2 = 512

def get_inputs():
    x = torch.rand(batch_size, features, dim1, dim2)
    return [x]

def get_init_inputs():
    return [features]
```

Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def compute_moments_kernel(
    x_ptr,
    mean_ptr,
    var_ptr,
    spatial_size,
    stride_nc,
    eps: tl.constexpr,
    BLOCK_SPATIAL: tl.constexpr,
    BLOCK_NC: tl.constexpr,
):
    """Compute mean and variance for multiple (n,c) slices in parallel"""
    pid_spatial = tl.program_id(axis=0)
    pid_nc = tl.program_id(axis=1) * BLOCK_NC
    
    # Create ranges
    nc_offsets = pid_nc + tl.arange(0, BLOCK_NC)
    spatial_offsets = pid_spatial * BLOCK_SPATIAL + tl.arange(0, BLOCK_SPATIAL)
    
    # Masks
    nc_mask = nc_offsets < stride_nc
    spatial_mask = spatial_offsets < spatial_size
    
    # Initialize accumulators
    sum_x = tl.zeros((BLOCK_NC,), dtype=tl.float32)
    sum_x2 = tl.zeros((BLOCK_NC,), dtype=tl.float32)
    
    # Process spatial elements for multiple (n,c) slices
    for s in range(0, BLOCK_SPATIAL, 128):
        spatial_idx = spatial_offsets + s
        spatial_chunk_mask = spatial_mask & (spatial_idx < spatial_size)
        
        # Check if any valid elements in this chunk
        has_valid = tl.sum(spatial_chunk_mask) > 0
        
        if has_valid:
            # Vectorized loading for better memory throughput
            for nc_idx in range(0, BLOCK_NC):
                if nc_idx < BLOCK_NC:
                    nc_idx_val = pid_nc + nc_idx
                    nc_valid = nc_idx_val < stride_nc
                    
                    if nc_valid:
                        x_ptrs = x_ptr + nc_idx_val * spatial_size + spatial_idx
                        x_chunk = tl.load(x_ptrs, mask=spatial_chunk_mask, other=0.0)
                        
                        # Update sums using masked operations
                        valid_count = tl.sum(spatial_chunk_mask)
                        if valid_count > 0:
                            sum_x = tl.where(
                                tl.arange(0, BLOCK_NC) == nc_idx,
                                sum_x + tl.sum(x_chunk),
                                sum_x
                            )
                            sum_x2 = tl.where(
                                tl.arange(0, BLOCK_NC) == nc_idx,
                                sum_x2 + tl.sum(x_chunk * x_chunk),
                                sum_x2
                            )
    
    # Store partial results with proper masking
    if pid_spatial == 0:  # Only first spatial block stores
        tl.store(mean_ptr + nc_offsets, sum_x, mask=nc_mask)
        tl.store(var_ptr + nc_offsets, sum_x2, mask=nc_mask)


@triton.jit
def normalize_kernel(
    x_ptr,
    output_ptr,
    mean_ptr,
    var_ptr,
    spatial_size,
    stride_nc,
    eps: tl.constexpr,
    BLOCK_SPATIAL: tl.constexpr,
    BLOCK_NC: tl.constexpr,
):
    """Normalize multiple (n,c) slices in parallel"""
    pid_spatial = tl.program_id(axis=0)
    pid_nc = tl.program_id(axis=1) * BLOCK_NC
    
    # Load mean and variance for this block of (n,c) slices
    nc_offsets = pid_nc + tl.arange(0, BLOCK_NC)
    nc_mask = nc_offsets < stride_nc
    
    mean = tl.load(mean_ptr + nc_offsets, mask=nc_mask, other=0.0)
    var = tl.load(var_ptr + nc_offsets, mask=nc_mask, other=0.0)
    
    # Compute mean and variance from sums
    mean_val = mean / spatial_size
    var_val = tl.maximum(var / spatial_size - mean_val * mean_val, 0.0)
    inv_std = tl.math.rsqrt(var_val + eps)
    
    # Process spatial elements
    spatial_offsets = pid_spatial * BLOCK_SPATIAL + tl.arange(0, BLOCK_SPATIAL)
    spatial_mask = spatial_offsets < spatial_size
    
    # Process in vectorized chunks
    for s in range(0, BLOCK_SPATIAL, 128):
        spatial_idx = spatial_offsets + s
        spatial_chunk_mask = spatial_mask & (spatial_idx < spatial_size)
        
        # Process each (n,c) slice
        for nc_idx in range(BLOCK_NC):
            nc_idx_val = pid_nc + nc_idx
            nc_valid = nc_idx_val < stride_nc
            
            if nc_valid and tl.sum(spatial_chunk_mask) > 0:
                # Load input
                x_ptrs = x_ptr + nc_idx_val * spatial_size + spatial_idx
                x = tl.load(x_ptrs, mask=spatial_chunk_mask, other=0.0)
                
                # Normalize
                normalized = (x - mean_val[nc_idx]) * inv_std[nc_idx]
                
                # Store output
                out_ptrs = output_ptr + nc_idx_val * spatial_size + spatial_idx
                tl.store(out_ptrs, normalized, mask=spatial_chunk_mask)


@triton.jit
def combine_moments_kernel(
    mean_ptr,
    var_ptr,
    spatial_size,
    stride_nc,
    eps: tl.constexpr,
    BLOCK_NC: tl.constexpr,
):
    """Combine partial moments and compute final mean/variance"""
    pid = tl.program_id(axis=0)
    
    nc_offsets = pid * BLOCK_NC + tl.arange(0, BLOCK_NC)
    mask = nc_offsets < stride_nc
    
    mean = tl.load(mean_ptr + nc_offsets, mask=mask, other=0.0)
    var = tl.load(var_ptr + nc_offsets, mask=mask, other=0.0)
    
    # Convert sums to actual mean and variance
    mean = mean / spatial_size
    var = tl.maximum(var / spatial_size - mean * mean, 0.0)
    
    tl.store(mean_ptr + nc_offsets, mean, mask=mask)
    tl.store(var_ptr + nc_offsets, var, mask=mask)


def triton_instance_norm(x: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:
    N, C, H, W = x.shape
    spatial_size = H * W
    total_slices = N * C
    
    # Ensure contiguous memory layout
    x_contig = x.contiguous()
    output = torch.empty_like(x_contig)
    
    # Optimized block sizes for Ada Lovelace
    BLOCK_SPATIAL = 256
    BLOCK_NC = 8
    
    # Allocate buffers for moments
    mean = torch.zeros(total_slices, device=x.device, dtype=torch.float32)
    var = torch.zeros(total_slices, device=x.device, dtype=torch.float32)
    
    # Grid configuration for better SM utilization
    num_spatial_blocks = triton.cdiv(spatial_size, BLOCK_SPATIAL)
    num_nc_blocks = triton.cdiv(total_slices, BLOCK_NC)
    
    # Phase 1: Compute moments with optimized grid
    grid1 = (num_spatial_blocks, num_nc_blocks)
    compute_moments_kernel[grid1](
        x_contig, mean, var,
        spatial_size, total_slices,
        eps,
        BLOCK_SPATIAL=BLOCK_SPATIAL,
        BLOCK_NC=BLOCK_NC
    )
    
    # Phase 2: Normalize with same optimized grid
    grid2 = (num_spatial_blocks, num_nc_blocks)
    normalize_kernel[grid2](
        x_contig, output, mean, var,
        spatial_size, total_slices,
        eps,
        BLOCK_SPATIAL=BLOCK_SPATIAL,
        BLOCK_NC=BLOCK_NC
    )
    
    return output


class ModelNew(nn.Module):
    def __init__(self, num_features: int):
        super().__init__()
        self.num_features = num_features

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_instance_norm(x)
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes

```python
# <corrected code>
```
