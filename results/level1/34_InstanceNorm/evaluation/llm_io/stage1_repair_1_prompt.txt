Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 533, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 131, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_085222_batch_range20to30_deepseek_deepseek/34_InstanceNorm/code/kernel_20251212_104208.py", line 156, in forward
    return triton_instance_norm(x)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_085222_batch_range20to30_deepseek_deepseek/34_InstanceNorm/code/kernel_20251212_104208.py", line 129, in triton_instance_norm
    compute_moments_kernel[grid1](
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/runtime/jit.py", line 419, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/runtime/jit.py", line 733, in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/runtime/jit.py", line 861, in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/compiler/compiler.py", line 300, in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/compiler/compiler.py", line 80, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 32:11:
    spatial_mask = spatial_offsets < spatial_size

    # Initialize accumulators
    sum_x = tl.zeros((BLOCK_NC,), dtype=tl.float32)
    sum_x2 = tl.zeros((BLOCK_NC,), dtype=tl.float32)

    # Process spatial elements for multiple (n,c) slices
    for s in range(0, BLOCK_SPATIAL, 128):  # Process in chunks for better cache
        spatial_idx = spatial_offsets + s
        spatial_chunk_mask = spatial_mask & (spatial_idx < spatial_size)

        if tl.any(spatial_chunk_mask):
           ^
AttributeError("module 'triton.language' has no attribute 'any'")

History Error:
None


PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Simple model that performs Instance Normalization.
    """
    def __init__(self, num_features: int):
        """
        Initializes the InstanceNorm layer.

        Args:
            num_features (int): Number of features in the input tensor.
        """
        super(Model, self).__init__()
        self.inorm = nn.InstanceNorm2d(num_features=num_features)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Applies Instance Normalization to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, num_features, height, width).

        Returns:
            torch.Tensor: Output tensor with Instance Normalization applied, same shape as input.
        """
        return self.inorm(x)

batch_size = 112  # heavier workload
features = 64
dim1 = 512
dim2 = 512

def get_inputs():
    x = torch.rand(batch_size, features, dim1, dim2)
    return [x]

def get_init_inputs():
    return [features]
```

Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def compute_moments_kernel(
    x_ptr,
    mean_ptr,
    var_ptr,
    spatial_size,
    stride_nc,
    eps: tl.constexpr,
    BLOCK_SPATIAL: tl.constexpr,
    BLOCK_NC: tl.constexpr,
):
    """Compute mean and variance for multiple (n,c) slices in parallel"""
    pid_spatial = tl.program_id(axis=0)
    pid_nc = tl.program_id(axis=1) * BLOCK_NC
    
    # Create ranges
    nc_offsets = pid_nc + tl.arange(0, BLOCK_NC)
    spatial_offsets = pid_spatial * BLOCK_SPATIAL + tl.arange(0, BLOCK_SPATIAL)
    
    # Masks
    nc_mask = nc_offsets < stride_nc
    spatial_mask = spatial_offsets < spatial_size
    
    # Initialize accumulators
    sum_x = tl.zeros((BLOCK_NC,), dtype=tl.float32)
    sum_x2 = tl.zeros((BLOCK_NC,), dtype=tl.float32)
    
    # Process spatial elements for multiple (n,c) slices
    for s in range(0, BLOCK_SPATIAL, 128):  # Process in chunks for better cache
        spatial_idx = spatial_offsets + s
        spatial_chunk_mask = spatial_mask & (spatial_idx < spatial_size)
        
        if tl.any(spatial_chunk_mask):
            # Vectorized loading for better memory throughput
            for nc_idx in range(0, BLOCK_NC, 4):  # Process 4 (n,c) slices at once
                if nc_idx < BLOCK_NC and (pid_nc + nc_idx) < stride_nc:
                    x_ptrs = x_ptr + (pid_nc + nc_idx) * spatial_size + spatial_idx
                    x_chunk = tl.load(x_ptrs, mask=spatial_chunk_mask, other=0.0)
                    
                    # Update sums
                    sum_x = tl.sum(x_chunk, axis=0) + tl.view(sum_x, tl.float32)
                    sum_x2 = tl.sum(x_chunk * x_chunk, axis=0) + tl.view(sum_x2, tl.float32)
    
    # Store partial results
    nc_write_mask = nc_mask & (pid_spatial == 0)  # Only first spatial block stores
    if tl.any(nc_write_mask):
        tl.store(mean_ptr + nc_offsets, sum_x, mask=nc_write_mask)
        tl.store(var_ptr + nc_offsets, sum_x2, mask=nc_write_mask)


@triton.jit
def normalize_kernel(
    x_ptr,
    output_ptr,
    mean_ptr,
    var_ptr,
    spatial_size,
    stride_nc,
    eps: tl.constexpr,
    BLOCK_SPATIAL: tl.constexpr,
    BLOCK_NC: tl.constexpr,
):
    """Normalize multiple (n,c) slices in parallel"""
    pid_spatial = tl.program_id(axis=0)
    pid_nc = tl.program_id(axis=1) * BLOCK_NC
    
    # Load mean and variance for this block of (n,c) slices
    nc_offsets = pid_nc + tl.arange(0, BLOCK_NC)
    nc_mask = nc_offsets < stride_nc
    
    mean = tl.load(mean_ptr + nc_offsets, mask=nc_mask, other=0.0)
    var = tl.load(var_ptr + nc_offsets, mask=nc_mask, other=0.0)
    inv_std = tl.math.rsqrt(var + eps)
    
    # Process spatial elements
    spatial_offsets = pid_spatial * BLOCK_SPATIAL + tl.arange(0, BLOCK_SPATIAL)
    spatial_mask = spatial_offsets < spatial_size
    
    # Process in vectorized chunks
    for s in range(0, BLOCK_SPATIAL, 128):
        spatial_idx = spatial_offsets + s
        spatial_chunk_mask = spatial_mask & (spatial_idx < spatial_size)
        
        if tl.any(spatial_chunk_mask):
            # Process 4 (n,c) slices at once for better memory coalescing
            for nc_idx in range(0, BLOCK_NC, 4):
                if nc_idx < BLOCK_NC and (pid_nc + nc_idx) < stride_nc:
                    # Load input
                    x_ptrs = x_ptr + (pid_nc + nc_idx) * spatial_size + spatial_idx
                    x = tl.load(x_ptrs, mask=spatial_chunk_mask)
                    
                    # Normalize
                    normalized = (x - tl.view(mean[nc_idx], tl.float32)) * tl.view(inv_std[nc_idx], tl.float32)
                    
                    # Store output
                    out_ptrs = output_ptr + (pid_nc + nc_idx) * spatial_size + spatial_idx
                    tl.store(out_ptrs, normalized, mask=spatial_chunk_mask)


def triton_instance_norm(x: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:
    N, C, H, W = x.shape
    spatial_size = H * W
    total_slices = N * C
    
    # Ensure contiguous memory layout
    x_contig = x.contiguous()
    output = torch.empty_like(x_contig)
    
    # Optimized block sizes for Ada Lovelace
    BLOCK_SPATIAL = 256  # Reduced from 1024 to increase blocks per SM
    BLOCK_NC = 8  # Process 8 (n,c) slices per block
    
    # Allocate buffers for moments
    mean = torch.zeros(total_slices, device=x.device, dtype=torch.float32)
    var = torch.zeros(total_slices, device=x.device, dtype=torch.float32)
    
    # Grid configuration for better SM utilization
    num_spatial_blocks = triton.cdiv(spatial_size, BLOCK_SPATIAL)
    num_nc_blocks = triton.cdiv(total_slices, BLOCK_NC)
    
    # Phase 1: Compute moments with optimized grid
    grid1 = lambda meta: (num_spatial_blocks, num_nc_blocks)
    compute_moments_kernel[grid1](
        x_contig, mean, var,
        spatial_size, total_slices,
        eps, 
        BLOCK_SPATIAL=BLOCK_SPATIAL,
        BLOCK_NC=BLOCK_NC
    )
    
    # Phase 2: Normalize with same optimized grid
    grid2 = lambda meta: (num_spatial_blocks, num_nc_blocks)
    normalize_kernel[grid2](
        x_contig, output, mean, var,
        spatial_size, total_slices,
        eps,
        BLOCK_SPATIAL=BLOCK_SPATIAL,
        BLOCK_NC=BLOCK_NC
    )
    
    return output


class ModelNew(nn.Module):
    def __init__(self, num_features: int):
        super().__init__()
        self.num_features = num_features

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_instance_norm(x)
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes

```python
# <corrected code>
```
