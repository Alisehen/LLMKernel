You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

**Focus**: Optimize grid layout and parallel work distribution.

**NCU Metrics**:
• `sm__throughput.avg.pct_of_peak_sustained_elapsed` (target >60%)
• `launch__grid_size` (primarily determined by grid mapping)

**Guidelines**:
- 1D → `(cdiv(N, BLOCK))`
- 2D → `(cdiv(M, BLOCK_M), cdiv(N, BLOCK_N))`
- 3D (batch×M×N) → `(batch, cdiv(M, BLOCK_M), cdiv(N, BLOCK_N))`
- 4D+ → flatten **independent parallel dims** to 3D
- Prefer batch / head / expert parallelism when available, before shrinking BLOCK sizes
- Change grid only if SM utilization is clearly low
- Ensure `grid = (...)` matches `tl.program_id(axis)` logic

**Safety Rules**:
- Max 3 grid dimensions; grid rank must be static
- If unsure about correctness, do not modify grid

**Autotune (safe)**:
- Autotune **either** BLOCK_* **or** (`num_warps`, `num_stages`)
- If BLOCK_* are autotuned, use `grid = lambda META: (...)` with `META["BLOCK_*"]`
- Never redefine BLOCK_* in both configs and launch



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def bmm_kernel(
    # Pointers to matrices
    A_ptr, B_ptr, C_ptr,
    # Matrix dimensions
    M, N, K,
    # Strides for A
    stride_Ab, stride_Am, stride_Ak,
    # Strides for B
    stride_Bb, stride_Bk, stride_Bn,
    # Strides for C
    stride_Cb, stride_Cm, stride_Cn,
    # Meta-parameters
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    """
    Optimized batched matrix multiplication kernel.
    
    Each program computes a tile of C = A @ B.
    Using double-buffering, software pipelining, and warp specialization.
    """
    # -----------------------------------------------------------
    # Map program ids to tile coordinates
    # -----------------------------------------------------------
    pid = tl.program_id(axis=0)
    pid_batch = tl.program_id(axis=1)
    
    # Tile coordination for M and N dimensions
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    pid_m = first_pid_m + (pid % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m
    
    # -----------------------------------------------------------
    # Create pointers for the first blocks of A and B
    # -----------------------------------------------------------
    # A block pointer
    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_ak = tl.arange(0, BLOCK_SIZE_K)
    A_ptrs = (
        A_ptr + 
        pid_batch * stride_Ab + 
        (offs_am[:, None] * stride_Am + offs_ak[None, :] * stride_Ak)
    )
    
    # B block pointer
    offs_bk = tl.arange(0, BLOCK_SIZE_K)
    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    B_ptrs = (
        B_ptr + 
        pid_batch * stride_Bb + 
        (offs_bk[:, None] * stride_Bk + offs_bn[None, :] * stride_Bn)
    )
    
    # -----------------------------------------------------------
    # Iterate to compute a C tile
    # -----------------------------------------------------------
    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        # Load A tile
        a_mask = (
            (offs_am[:, None] < M) & 
            (k * BLOCK_SIZE_K + offs_ak[None, :] < K)
        )
        a = tl.load(A_ptrs, mask=a_mask, other=0.0)
        
        # Load B tile
        b_mask = (
            (k * BLOCK_SIZE_K + offs_bk[:, None] < K) & 
            (offs_bn[None, :] < N)
        )
        b = tl.load(B_ptrs, mask=b_mask, other=0.0)
        
        # Update pointers for next iteration
        A_ptrs += BLOCK_SIZE_K * stride_Ak
        B_ptrs += BLOCK_SIZE_K * stride_Bk
        
        # Compute matrix multiplication
        accumulator += tl.dot(a, b)
    
    # -----------------------------------------------------------
    # Write back the C tile
    # -----------------------------------------------------------
    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    C_ptrs = (
        C_ptr + 
        pid_batch * stride_Cb + 
        (offs_cm[:, None] * stride_Cm + offs_cn[None, :] * stride_Cn)
    )
    
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    tl.store(C_ptrs, accumulator, mask=c_mask)

@triton.jit
def bmm_kernel_tensor_core(
    # Pointers to matrices
    A_ptr, B_ptr, C_ptr,
    # Matrix dimensions
    M, N, K,
    # Strides
    stride_Ab, stride_Am, stride_Ak,
    stride_Bb, stride_Bk, stride_Bn,
    stride_Cb, stride_Cm, stride_Cn,
    # Meta-parameters
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    """
    Tensor Core optimized version using tf32 precision.
    """
    pid = tl.program_id(axis=0)
    pid_batch = tl.program_id(axis=1)
    
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    pid_m = first_pid_m + (pid % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m
    
    # Tensor Core requires specific block sizes
    # Using 128x128x32 tile for optimal tensor core utilization
    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_ak = tl.arange(0, BLOCK_SIZE_K)
    offs_bk = tl.arange(0, BLOCK_SIZE_K)
    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    
    A_ptrs = (
        A_ptr + 
        pid_batch * stride_Ab + 
        (offs_am[:, None] * stride_Am + offs_ak[None, :] * stride_Ak)
    )
    B_ptrs = (
        B_ptr + 
        pid_batch * stride_Bb + 
        (offs_bk[:, None] * stride_Bk + offs_bn[None, :] * stride_Bn)
    )
    
    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a_mask = (
            (offs_am[:, None] < M) & 
            (k * BLOCK_SIZE_K + offs_ak[None, :] < K)
        )
        b_mask = (
            (k * BLOCK_SIZE_K + offs_bk[:, None] < K) & 
            (offs_bn[None, :] < N)
        )
        
        a = tl.load(A_ptrs, mask=a_mask, other=0.0)
        b = tl.load(B_ptrs, mask=b_mask, other=0.0)
        
        A_ptrs += BLOCK_SIZE_K * stride_Ak
        B_ptrs += BLOCK_SIZE_K * stride_Bk
        
        # Tensor core operation with tf32 precision
        accumulator += tl.dot(a, b, allow_tf32=True)
    
    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    C_ptrs = (
        C_ptr + 
        pid_batch * stride_Cb + 
        (offs_cm[:, None] * stride_Cm + offs_cn[None, :] * stride_Cn)
    )
    
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    tl.store(C_ptrs, accumulator, mask=c_mask)

def triton_bmm(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
    """
    Batched matrix multiplication using Triton.
    
    Args:
        A: Tensor of shape (batch, M, K)
        B: Tensor of shape (batch, K, N)
    
    Returns:
        C: Tensor of shape (batch, M, N)
    """
    assert A.dim() == 3 and B.dim() == 3, "Inputs must be 3D tensors"
    assert A.shape[0] == B.shape[0], "Batch dimensions must match"
    assert A.shape[2] == B.shape[1], "Inner dimensions must match"
    
    batch_size, M, K = A.shape
    _, _, N = B.shape
    
    # Allocate output tensor
    C = torch.empty((batch_size, M, N), device=A.device, dtype=A.dtype)
    
    # Choose kernel based on tensor sizes and hardware
    # For large matrices, use tensor core optimized version
    use_tensor_cores = M >= 128 and N >= 128 and K >= 32
    
    if use_tensor_cores:
        # Tensor core optimized parameters
        BLOCK_SIZE_M = 128
        BLOCK_SIZE_N = 128
        BLOCK_SIZE_K = 32
        GROUP_SIZE_M = 8
        kernel = bmm_kernel_tensor_core
    else:
        # General parameters
        BLOCK_SIZE_M = 64
        BLOCK_SIZE_N = 64
        BLOCK_SIZE_K = 32
        GROUP_SIZE_M = 8
        kernel = bmm_kernel
    
    # Compute grid
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),
        batch_size,
    )
    
    # Launch kernel
    kernel[grid](
        A, B, C,
        M, N, K,
        A.stride(0), A.stride(1), A.stride(2),
        B.stride(0), B.stride(1), B.stride(2),
        C.stride(0), C.stride(1), C.stride(2),
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=GROUP_SIZE_M,
    )
    
    return C

class ModelNew(nn.Module):
    """
    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.
    """
    def __init__(self):
        super(ModelNew, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        """
        Performs batched matrix multiplication.

        Args:
            A: Input tensor of shape (batch_size, m, k).
            B: Input tensor of shape (batch_size, k, n).

        Returns:
            C: Output tensor of shape (batch_size, m, n).
        """
        return triton_bmm(A, B)
```

[NCU PROFILING METRICS]
{
  "bmm_kernel_tensor_core": [
    {
      "sm__throughput.avg.pct_of_peak_sustained_elapsed": 44.26,
      "launch__grid_size": 8192,
      "sm__warps_active.avg.pct_of_peak_sustained_active": 8.33,
      "dram__throughput.avg.pct_of_peak_sustained_elapsed": 42.72,
      "lts__t_sector_hit_rate.pct": 85.3
    },
    {
      "sm__throughput.avg.pct_of_peak_sustained_elapsed": 44.26,
      "launch__grid_size": 8192,
      "sm__warps_active.avg.pct_of_peak_sustained_active": 8.33,
      "dram__throughput.avg.pct_of_peak_sustained_elapsed": 42.72,
      "lts__t_sector_hit_rate.pct": 85.3
    }
  ]
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
