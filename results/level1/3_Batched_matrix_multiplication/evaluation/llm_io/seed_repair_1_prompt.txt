Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 533, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 131, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_131222_batch_range1to20_deepseek_deepseek/3_Batched_matrix_multiplication/code/kernel_20251212_131822.py", line 270, in forward
    return triton_bmm(A, B)
           ^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_131222_batch_range1to20_deepseek_deepseek/3_Batched_matrix_multiplication/code/kernel_20251212_131822.py", line 237, in triton_bmm
    kernel[grid](
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/runtime/jit.py", line 419, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/runtime/jit.py", line 730, in run
    options, signature, constexprs, attrs = self._pack_args(backend, kwargs, bound_args, specialization,
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/runtime/jit.py", line 698, in _pack_args
    raise KeyError("Keyword argument %s was specified but unrecognised" % k)
KeyError: 'Keyword argument USE_TMA was specified but unrecognised'

History Error:
None


PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.
    """
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        """
        Performs batched matrix multiplication.

        Args:
            A: Input tensor of shape (batch_size, m, k).
            B: Input tensor of shape (batch_size, k, n).

        Returns:
            C: Output tensor of shape (batch_size, m, n).
        """
        return torch.bmm(A, B)

batch_size = 128
m = 128 * 4
k = 256 * 4
n = 512 * 4

def get_inputs():
    A = torch.rand(batch_size, m, k)
    B = torch.rand(batch_size, k, n)
    return [A, B]

def get_init_inputs():
    return []  # No special initialization inputs needed
```

Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def bmm_kernel(
    # Pointers to matrices
    A_ptr, B_ptr, C_ptr,
    # Matrix dimensions
    M, N, K,
    # Strides for A
    stride_Ab, stride_Am, stride_Ak,
    # Strides for B
    stride_Bb, stride_Bk, stride_Bn,
    # Strides for C
    stride_Cb, stride_Cm, stride_Cn,
    # Meta-parameters
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    USE_TMA: tl.constexpr,
):
    """
    Optimized batched matrix multiplication kernel.
    
    Each program computes a tile of C = A @ B.
    Using double-buffering, software pipelining, and warp specialization.
    """
    # -----------------------------------------------------------
    # Map program ids to tile coordinates
    # -----------------------------------------------------------
    pid = tl.program_id(axis=0)
    pid_batch = tl.program_id(axis=1)
    
    # Tile coordination for M and N dimensions
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    pid_m = first_pid_m + (pid % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m
    
    # -----------------------------------------------------------
    # Create pointers for the first blocks of A and B
    # -----------------------------------------------------------
    # A block pointer
    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_ak = tl.arange(0, BLOCK_SIZE_K)
    A_ptrs = (
        A_ptr + 
        pid_batch * stride_Ab + 
        (offs_am[:, None] * stride_Am + offs_ak[None, :] * stride_Ak)
    )
    
    # B block pointer
    offs_bk = tl.arange(0, BLOCK_SIZE_K)
    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    B_ptrs = (
        B_ptr + 
        pid_batch * stride_Bb + 
        (offs_bk[:, None] * stride_Bk + offs_bn[None, :] * stride_Bn)
    )
    
    # -----------------------------------------------------------
    # Iterate to compute a C tile
    # -----------------------------------------------------------
    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        # Load A tile
        a_mask = (
            (offs_am[:, None] < M) & 
            (k * BLOCK_SIZE_K + offs_ak[None, :] < K)
        )
        a = tl.load(A_ptrs, mask=a_mask, other=0.0)
        
        # Load B tile
        b_mask = (
            (k * BLOCK_SIZE_K + offs_bk[:, None] < K) & 
            (offs_bn[None, :] < N)
        )
        b = tl.load(B_ptrs, mask=b_mask, other=0.0)
        
        # Update pointers for next iteration
        A_ptrs += BLOCK_SIZE_K * stride_Ak
        B_ptrs += BLOCK_SIZE_K * stride_Bk
        
        # Compute matrix multiplication
        accumulator += tl.dot(a, b)
    
    # -----------------------------------------------------------
    # Write back the C tile
    # -----------------------------------------------------------
    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    C_ptrs = (
        C_ptr + 
        pid_batch * stride_Cb + 
        (offs_cm[:, None] * stride_Cm + offs_cn[None, :] * stride_Cn)
    )
    
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    tl.store(C_ptrs, accumulator, mask=c_mask)

@triton.jit
def bmm_kernel_tensor_core(
    # Pointers to matrices
    A_ptr, B_ptr, C_ptr,
    # Matrix dimensions
    M, N, K,
    # Strides
    stride_Ab, stride_Am, stride_Ak,
    stride_Bb, stride_Bk, stride_Bn,
    stride_Cb, stride_Cm, stride_Cn,
    # Meta-parameters
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    """
    Tensor Core optimized version using tf32 precision.
    """
    pid = tl.program_id(axis=0)
    pid_batch = tl.program_id(axis=1)
    
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    pid_m = first_pid_m + (pid % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m
    
    # Tensor Core requires specific block sizes
    # Using 128x128x32 tile for optimal tensor core utilization
    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_ak = tl.arange(0, BLOCK_SIZE_K)
    offs_bk = tl.arange(0, BLOCK_SIZE_K)
    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    
    A_ptrs = (
        A_ptr + 
        pid_batch * stride_Ab + 
        (offs_am[:, None] * stride_Am + offs_ak[None, :] * stride_Ak)
    )
    B_ptrs = (
        B_ptr + 
        pid_batch * stride_Bb + 
        (offs_bk[:, None] * stride_Bk + offs_bn[None, :] * stride_Bn)
    )
    
    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a_mask = (
            (offs_am[:, None] < M) & 
            (k * BLOCK_SIZE_K + offs_ak[None, :] < K)
        )
        b_mask = (
            (k * BLOCK_SIZE_K + offs_bk[:, None] < K) & 
            (offs_bn[None, :] < N)
        )
        
        a = tl.load(A_ptrs, mask=a_mask, other=0.0)
        b = tl.load(B_ptrs, mask=b_mask, other=0.0)
        
        A_ptrs += BLOCK_SIZE_K * stride_Ak
        B_ptrs += BLOCK_SIZE_K * stride_Bk
        
        # Tensor core operation with tf32 precision
        accumulator += tl.dot(a, b, allow_tf32=True)
    
    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    C_ptrs = (
        C_ptr + 
        pid_batch * stride_Cb + 
        (offs_cm[:, None] * stride_Cm + offs_cn[None, :] * stride_Cn)
    )
    
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    tl.store(C_ptrs, accumulator, mask=c_mask)

def triton_bmm(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
    """
    Batched matrix multiplication using Triton.
    
    Args:
        A: Tensor of shape (batch, M, K)
        B: Tensor of shape (batch, K, N)
    
    Returns:
        C: Tensor of shape (batch, M, N)
    """
    assert A.dim() == 3 and B.dim() == 3, "Inputs must be 3D tensors"
    assert A.shape[0] == B.shape[0], "Batch dimensions must match"
    assert A.shape[2] == B.shape[1], "Inner dimensions must match"
    
    batch_size, M, K = A.shape
    _, _, N = B.shape
    
    # Allocate output tensor
    C = torch.empty((batch_size, M, N), device=A.device, dtype=A.dtype)
    
    # Choose kernel based on tensor sizes and hardware
    # For large matrices, use tensor core optimized version
    use_tensor_cores = M >= 128 and N >= 128 and K >= 32
    
    if use_tensor_cores:
        # Tensor core optimized parameters
        BLOCK_SIZE_M = 128
        BLOCK_SIZE_N = 128
        BLOCK_SIZE_K = 32
        GROUP_SIZE_M = 8
        kernel = bmm_kernel_tensor_core
    else:
        # General parameters
        BLOCK_SIZE_M = 64
        BLOCK_SIZE_N = 64
        BLOCK_SIZE_K = 32
        GROUP_SIZE_M = 8
        kernel = bmm_kernel
    
    # Compute grid
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),
        batch_size,
    )
    
    # Launch kernel
    kernel[grid](
        A, B, C,
        M, N, K,
        A.stride(0), A.stride(1), A.stride(2),
        B.stride(0), B.stride(1), B.stride(2),
        C.stride(0), C.stride(1), C.stride(2),
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=GROUP_SIZE_M,
        USE_TMA=False,
    )
    
    return C

class ModelNew(nn.Module):
    """
    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.
    """
    def __init__(self):
        super(ModelNew, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        """
        Performs batched matrix multiplication.

        Args:
            A: Input tensor of shape (batch_size, m, k).
            B: Input tensor of shape (batch_size, k, n).

        Returns:
            C: Output tensor of shape (batch_size, m, n).
        """
        return triton_bmm(A, B)
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes

```python
# <corrected code>
```
