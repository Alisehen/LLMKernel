You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Final micro-tuning.

Params:
- num_warps ∈ {2,4,8,16}
- num_stages ∈ {2,3,4}

Rules:
- Change num_warps only if occupancy suggests it
- Change num_stages by ±1 only
- Do NOT modify grid or BLOCK sizes

Autotune:
- 3–6 nearby configs
- Always include original config
- Revert if gain <1–2% or unstable



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def avg_pool3d_kernel(
    x_ptr,
    y_ptr,
    n_elements,
    N,
    C,
    D,
    H,
    W,
    OD,
    OH,
    OW,
    stride_d,
    stride_h,
    stride_w,
    pad_d,
    pad_h,
    pad_w,
    OUT_DTYPE: tl.constexpr,
    KD: tl.constexpr,
    KH: tl.constexpr,
    KW: tl.constexpr,
    BLOCK: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK
    offsets = block_start + tl.arange(0, BLOCK)
    mask = offsets < n_elements

    ow = offsets % OW
    tmp = offsets // OW
    oh = tmp % OH
    tmp = tmp // OH
    od = tmp % OD
    tmp = tmp // OD
    c = tmp % C
    n = tmp // C

    stride_c = D * H * W
    stride_n = stride_c * C
    stride_d_in = H * W
    stride_h_in = W

    in_d_start = od * stride_d - pad_d
    in_h_start = oh * stride_h - pad_h
    in_w_start = ow * stride_w - pad_w

    base_nc = n.to(tl.int64) * stride_n + c.to(tl.int64) * stride_c

    sum_val = tl.zeros([BLOCK], dtype=tl.float32)

    inv_window = 1.0 / (KD * KH * KW)

    for kd in tl.static_range(0, KD):
        cur_d = in_d_start + kd
        valid_d = (cur_d >= 0) & (cur_d < D)
        cur_d = tl.where(valid_d, cur_d, 0)
        d_offset = cur_d.to(tl.int64) * stride_d_in

        for kh in tl.static_range(0, KH):
            cur_h = in_h_start + kh
            valid_dh = valid_d & (cur_h >= 0) & (cur_h < H)
            cur_h = tl.where(valid_dh, cur_h, 0)
            h_offset = cur_h.to(tl.int64) * stride_h_in

            for kw in tl.static_range(0, KW):
                cur_w = in_w_start + kw
                valid = mask & valid_dh & (cur_w >= 0) & (cur_w < W)
                cur_w = tl.where(valid, cur_w, 0)
                w_offset = cur_w.to(tl.int64)

                input_offset = base_nc + d_offset + h_offset + w_offset
                vals = tl.load(x_ptr + input_offset, mask=valid, other=0.0)
                sum_val += vals.to(tl.float32)

    avg = sum_val * inv_window

    if OUT_DTYPE == 0:
        avg_out = avg.to(tl.float16)
    elif OUT_DTYPE == 1:
        avg_out = avg
    elif OUT_DTYPE == 2:
        avg_out = avg.to(tl.bfloat16)
    else:
        avg_out = avg

    tl.store(y_ptr + offsets, avg_out, mask=mask)


def triton_avgpool3d(x, kernel_size, stride=None, padding=0):
    stride_for_torch = kernel_size if stride is None else stride
    if (not x.is_cuda) or x.dtype not in {torch.float16, torch.float32, torch.bfloat16}:
        return torch.nn.functional.avg_pool3d(x, kernel_size, stride_for_torch, padding)

    kd, kh, kw = (kernel_size, kernel_size, kernel_size) if isinstance(kernel_size, int) else tuple(kernel_size)
    sd, sh, sw = (stride_for_torch, stride_for_torch, stride_for_torch) if isinstance(stride_for_torch, int) else tuple(stride_for_torch)
    pd, ph, pw = (padding, padding, padding) if isinstance(padding, int) else tuple(padding)

    x_contig = x.contiguous()
    N, C, D, H, W = x_contig.shape

    OD = (D + 2 * pd - kd) // sd + 1
    OH = (H + 2 * ph - kh) // sh + 1
    OW = (W + 2 * pw - kw) // sw + 1

    out = torch.empty((N, C, OD, OH, OW), device=x.device, dtype=x.dtype)
    n_elements = out.numel()

    dtype_map = {torch.float16: 0, torch.float32: 1, torch.bfloat16: 2}
    out_dtype = dtype_map[out.dtype]

    BLOCK = 256
    grid = lambda meta: (triton.cdiv(n_elements, meta["BLOCK"]),)

    avg_pool3d_kernel[grid](
        x_contig,
        out,
        n_elements,
        N,
        C,
        D,
        H,
        W,
        OD,
        OH,
        OW,
        sd,
        sh,
        sw,
        pd,
        ph,
        pw,
        OUT_DTYPE=out_dtype,
        KD=kd,
        KH=kh,
        KW=kw,
        BLOCK=BLOCK,
    )

    return out


class ModelNew(nn.Module):
    def __init__(self, kernel_size: int, stride: int = None, padding: int = 0) -> None:
        super().__init__()
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_avgpool3d(x, self.kernel_size, self.stride, self.padding)
```

[NCU PROFILING METRICS]
{
  "avg_pool3d_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 35.37,
    "launch__grid_size": 1048576,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 92.49,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 94.75,
    "lts__t_sector_hit_rate.pct": 49.75
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
