Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 535, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_170043_batch_range5to40_deepseek_deepseek/23_Softmax/code/kernel_20251212_222601.py", line 177, in forward
    return triton_softmax(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_170043_batch_range5to40_deepseek_deepseek/23_Softmax/code/kernel_20251212_222601.py", line 134, in triton_softmax
    softmax_max_kernel[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 230, in compile
    key = f"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}"
                            ^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 77, in hash
    key = f"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}"
             ^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 665, in cache_key
    dependencies_finder.visit(self.parse())
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 149, in visit_FunctionDef
    self.generic_visit(node)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 203, in visit_Assign
    self.generic_visit(node)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 428, in generic_visit
    self.visit(value)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 428, in generic_visit
    self.visit(value)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 142, in visit_Attribute
    ret = getattr(lhs, node.attr)
          ^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'triton.language' has no attribute 'inf'

History Error:
Previous Repair Attempts (avoid repeating these errors):
Attempt 1:
           ^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_170043_batch_range5to40_deepseek_deepseek/23_Softmax/code/kernel_20251212_222325.py", line 161, in triton_softmax
    exp_vals = torch.empty_like(x)
               ^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 23.52 GiB of which 5.01 GiB is free. Including non-PyTorch memory, this process has 18.44 GiB memory in use. Of the allocated memory 18.00 GiB is allocated by PyTorch, and 1.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)



PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Simple model that performs a Softmax activation.
    """
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Applies Softmax activation to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, num_features).

        Returns:
            torch.Tensor: Output tensor with Softmax applied, same shape as input.
        """
        return torch.softmax(x, dim=1)

batch_size = 4096
dim = 393216

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed
```

Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def softmax_max_kernel(
    x_ptr,
    max_ptr,
    stride_x0,
    stride_x1,
    n_rows,
    n_cols,
    BLOCK_SIZE: tl.constexpr,
):
    """Compute max along dim=1 for 2D tensor."""
    row_idx = tl.program_id(0)
    
    if row_idx >= n_rows:
        return
    
    row_start_ptr = x_ptr + row_idx * stride_x0
    row_max = -tl.inf
    
    col_offsets = tl.arange(0, BLOCK_SIZE)
    for block_start in range(0, n_cols, BLOCK_SIZE):
        col_indices = block_start + col_offsets
        mask = col_indices < n_cols
        
        ptrs = row_start_ptr + col_indices * stride_x1
        x_block = tl.load(ptrs, mask=mask, other=-tl.inf)
        row_max = tl.maximum(row_max, tl.max(x_block, 0))
    
    tl.store(max_ptr + row_idx, row_max)

@triton.jit
def softmax_exp_sum_kernel(
    x_ptr,
    max_ptr,
    sum_ptr,
    stride_x0,
    stride_x1,
    n_rows,
    n_cols,
    BLOCK_SIZE: tl.constexpr,
):
    """Compute sum of exp(x - max) along dim=1."""
    row_idx = tl.program_id(0)
    
    if row_idx >= n_rows:
        return
    
    row_max = tl.load(max_ptr + row_idx)
    row_start_ptr = x_ptr + row_idx * stride_x0
    
    row_sum = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
    
    col_offsets = tl.arange(0, BLOCK_SIZE)
    for block_start in range(0, n_cols, BLOCK_SIZE):
        col_indices = block_start + col_offsets
        mask = col_indices < n_cols
        
        ptrs = row_start_ptr + col_indices * stride_x1
        x_block = tl.load(ptrs, mask=mask, other=0.0)
        shifted = x_block - row_max
        exp_block = tl.exp(shifted)
        
        row_sum += tl.sum(exp_block, 0)
    
    total_sum = tl.sum(row_sum, 0)
    tl.store(sum_ptr + row_idx, total_sum)

@triton.jit
def softmax_final_kernel(
    x_ptr,
    max_ptr,
    sum_ptr,
    output_ptr,
    stride_x0,
    stride_x1,
    stride_out0,
    stride_out1,
    n_rows,
    n_cols,
    BLOCK_SIZE: tl.constexpr,
):
    """Compute final softmax: exp(x - max) / sum."""
    row_idx = tl.program_id(0)
    
    if row_idx >= n_rows:
        return
    
    row_max = tl.load(max_ptr + row_idx)
    row_sum = tl.load(sum_ptr + row_idx)
    
    row_start_x = x_ptr + row_idx * stride_x0
    row_start_out = output_ptr + row_idx * stride_out0
    
    inv_sum = 1.0 / (row_sum + 1e-8)
    
    col_offsets = tl.arange(0, BLOCK_SIZE)
    for block_start in range(0, n_cols, BLOCK_SIZE):
        col_indices = block_start + col_offsets
        mask = col_indices < n_cols
        
        x_ptrs = row_start_x + col_indices * stride_x1
        x_block = tl.load(x_ptrs, mask=mask, other=0.0)
        
        shifted = x_block - row_max
        exp_block = tl.exp(shifted)
        output_block = exp_block * inv_sum
        
        out_ptrs = row_start_out + col_indices * stride_out1
        tl.store(out_ptrs, output_block, mask=mask)

def triton_softmax(x: torch.Tensor) -> torch.Tensor:
    """
    Softmax along dim=1 using Triton kernels.
    Memory-efficient three-stage approach.
    """
    output = torch.empty_like(x)
    batch_size, n_cols = x.shape
    
    # Small intermediate tensors only
    max_vals = torch.empty(batch_size, dtype=x.dtype, device=x.device)
    row_sums = torch.empty(batch_size, dtype=x.dtype, device=x.device)
    
    # Choose block size based on column dimension
    BLOCK_SIZE = 1024 if n_cols % 1024 == 0 else 512
    
    grid = (batch_size,)
    
    # 1. Compute max per row
    softmax_max_kernel[grid](
        x, max_vals,
        x.stride(0), x.stride(1),
        batch_size, n_cols,
        BLOCK_SIZE=BLOCK_SIZE
    )
    
    # 2. Compute sum of exp per row
    softmax_exp_sum_kernel[grid](
        x, max_vals, row_sums,
        x.stride(0), x.stride(1),
        batch_size, n_cols,
        BLOCK_SIZE=BLOCK_SIZE
    )
    
    # 3. Compute final softmax
    softmax_final_kernel[grid](
        x, max_vals, row_sums, output,
        x.stride(0), x.stride(1),
        output.stride(0), output.stride(1),
        batch_size, n_cols,
        BLOCK_SIZE=BLOCK_SIZE
    )
    
    return output

class ModelNew(nn.Module):
    """
    Simple model that performs a Softmax activation using optimized Triton kernels.
    """
    def __init__(self):
        super().__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Applies Softmax activation to the input tensor along dim=1.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, num_features).

        Returns:
            torch.Tensor: Output tensor with Softmax applied, same shape as input.
        """
        return triton_softmax(x)
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes

```python
# <corrected code>
```
