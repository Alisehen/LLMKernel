```python
# import section
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_K": 256, "BLOCK_R": 1}, num_warps=4, num_stages=1),
        triton.Config({"BLOCK_K": 512, "BLOCK_R": 1}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_K": 1024, "BLOCK_R": 2}, num_warps=8, num_stages=2),
        triton.Config({"BLOCK_K": 2048, "BLOCK_R": 2}, num_warps=8, num_stages=3),
        triton.Config({"BLOCK_K": 4096, "BLOCK_R": 4}, num_warps=8, num_stages=3),
    ],
    key=["reduce_size"],
)
@triton.jit
def max_reduce_lastdim_kernel(
    x_ptr,
    out_ptr,
    n_rows,
    reduce_size,
    stride_row,
    BLOCK_K: tl.constexpr,
    BLOCK_R: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    row_offsets = pid * BLOCK_R + tl.arange(0, BLOCK_R)
    row_mask = row_offsets < n_rows

    acc = tl.full((BLOCK_R,), float("-inf"), dtype=tl.float32)

    k_offsets = tl.arange(0, BLOCK_K)
    for k in range(0, reduce_size, BLOCK_K):
        col_offsets = k + k_offsets
        load_mask = row_mask[:, None] & (col_offsets[None, :] < reduce_size)

        ptrs = (
            x_ptr
            + row_offsets[:, None] * stride_row
            + col_offsets[None, :]
        )
        vals = tl.load(ptrs, mask=load_mask, other=float("-inf"))
        block_max = tl.max(vals, axis=1)
        acc = tl.maximum(acc, block_max)

    tl.store(out_ptr + row_offsets, acc, mask=row_mask)


def triton_max_reduce(x: torch.Tensor, dim: int) -> torch.Tensor:
    ndim = x.ndim
    if dim < 0:
        dim += ndim
    assert 0 <= dim < ndim, "Invalid reduction dimension"

    if dim == ndim - 1:
        x_perm = x.contiguous()
    else:
        perm = list(range(ndim))
        perm.append(perm.pop(dim))
        x_perm = x.permute(perm).contiguous()

    reduce_size = x_perm.shape[-1]
    outer = x_perm.numel() // reduce_size
    out = torch.empty(outer, device=x.device, dtype=x.dtype)

    grid = lambda META: (
        triton.cdiv(outer, META["BLOCK_R"]),
    )
    max_reduce_lastdim_kernel[grid](
        x_perm,
        out,
        outer,
        reduce_size,
        reduce_size,
    )

    out = out.view(*x_perm.shape[:-1])
    if dim == ndim - 1:
        return out

    perm = list(range(ndim))
    perm.append(perm.pop(dim))
    perm_without_last = perm[:-1]
    inv_perm = sorted(range(len(perm_without_last)), key=lambda i: perm_without_last[i])
    return out.permute(inv_perm).contiguous()


class ModelNew(nn.Module):
    """
    High-performance max reduction over a specified dimension using an optimized Triton kernel.
    """
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_max_reduce(x, self.dim)
```