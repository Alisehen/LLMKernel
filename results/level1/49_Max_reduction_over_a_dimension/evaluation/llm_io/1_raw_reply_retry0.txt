```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def max_reduce_strided_phase1_kernel(
    x_ptr,                  # *dtype
    partial_ptr,            # *dtype, [NUM_R_BLOCKS, out_numel]
    shape_outer_ptr,        # *int32, length = MAX_OUT_DIMS
    stride_outer_ptr,       # *int32, length = MAX_OUT_DIMS
    stride_r,               # int32, stride of reduction dim (last dim of x_perm)
    R,                      # int32, length of reduction dim
    out_numel,              # int32, number of output elements (product of outer dims)
    NUM_R_BLOCKS,           # int32, number of R tiles (unused in body but kept for symmetry)
    BLOCK_M: tl.constexpr,  # number of output elements per program
    BLOCK_R: tl.constexpr,  # reduction tile size per program (phase1 handles a single tile)
    MAX_OUT_DIMS: tl.constexpr,
):
    """
    Phase 1: compute partial maxima over a single BLOCK_R tile of the reduction
    dimension R for BLOCK_M output elements.

    Grid:
      pid_m = program_id(0) in [0, ceil_div(out_numel, BLOCK_M))
      pid_rb = program_id(1) in [0, NUM_R_BLOCKS)

    Each program computes:
      partial[pid_rb, m] = max_{r in tile(pid_rb)} x[m, r]
    """
    pid_m = tl.program_id(axis=0)
    pid_rb = tl.program_id(axis=1)

    # Offsets over output elements (outer dims collapsed)
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    mask_m = offs_m < out_numel  # [BLOCK_M]

    # Decode linear output index -> base offset in x (excluding reduction dim)
    linear = offs_m
    base_offsets = tl.zeros([BLOCK_M], dtype=tl.int32)

    # Mixed-radix decomposition over outer dims, in reverse order
    for rev in tl.static_range(0, MAX_OUT_DIMS):
        dim = MAX_OUT_DIMS - 1 - rev
        size_i = tl.load(shape_outer_ptr + dim)   # scalar int32
        stride_i = tl.load(stride_outer_ptr + dim)
        idx_i = linear % size_i
        linear = linear // size_i
        base_offsets += idx_i * stride_i

    # Compute the tile of the reduction dimension handled by this program
    r_start = pid_rb * BLOCK_R
    r_idx = r_start + tl.arange(0, BLOCK_R)       # [BLOCK_R]
    mask_r = r_idx < R                            # [BLOCK_R]

    # Compute pointers for a [BLOCK_M, BLOCK_R] tile
    base = base_offsets[:, None]                  # [BLOCK_M, 1]
    r_offsets = r_idx[None, :] * stride_r         # [1, BLOCK_R]
    ptrs = x_ptr + base + r_offsets               # [BLOCK_M, BLOCK_R]

    mask = mask_m[:, None] & mask_r[None, :]      # [BLOCK_M, BLOCK_R]
    vals = tl.load(ptrs, mask=mask, other=-float("inf"))
    vals_f32 = vals.to(tl.float32)

    # Reduce over this R tile
    tile_max = tl.max(vals_f32, axis=1)           # [BLOCK_M]

    # Store partial maxima:
    # partial layout is [NUM_R_BLOCKS, out_numel], row-major w.r.t. NUM_R_BLOCKS
    partial_base = pid_rb * out_numel + offs_m
    tl.store(partial_ptr + partial_base, tile_max, mask=mask_m)


@triton.jit
def max_reduce_strided_phase2_kernel(
    partial_ptr,            # *dtype, [NUM_R_BLOCKS, out_numel]
    out_ptr,                # *dtype, [out_numel]
    out_numel,              # int32
    NUM_R_BLOCKS: tl.constexpr,  # number of tiles along R
    BLOCK_M: tl.constexpr,       # number of output elements per program
):
    """
    Phase 2: reduce partial maxima over NUM_R_BLOCKS for each output element.

    For each output index m:
      out[m] = max_{rb in 0..NUM_R_BLOCKS-1} partial[rb, m]
    """
    pid = tl.program_id(axis=0)
    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)
    mask_m = offs_m < out_numel                     # [BLOCK_M]

    running_max = tl.full([BLOCK_M], -float("inf"), dtype=tl.float32)

    # Iterate over all R tiles and accumulate the max
    for rb in tl.static_range(0, NUM_R_BLOCKS):
        ptrs = partial_ptr + rb * out_numel + offs_m
        vals = tl.load(ptrs, mask=mask_m, other=-float("inf"))
        vals_f32 = vals.to(tl.float32)
        running_max = tl.maximum(running_max, vals_f32)

    tl.store(out_ptr + offs_m, running_max, mask=mask_m)


def triton_max_reduce(x: torch.Tensor, dim: int) -> torch.Tensor:
    """
    High-performance two-pass max reduction along an arbitrary dimension
    using Triton. Works over the original tensor layout (permute only, no copy).

    Equivalent to: torch.max(x, dim=dim)[0]
    """
    assert x.is_cuda, "Input tensor must be on CUDA device for Triton kernel"

    ndim = x.dim()
    if ndim == 0:
        # Scalar: same behavior as PyTorch (no dim argument)
        return x

    # Normalize dimension
    dim = dim if dim >= 0 else dim + ndim
    if not (0 <= dim < ndim):
        raise ValueError(f"Invalid dim={dim} for tensor of dim {ndim}")

    # Handle empty tensors or empty reduction dims via PyTorch for correct errors
    if x.numel() == 0 or x.shape[dim] == 0:
        return torch.max(x, dim=dim)[0]

    # Restrict to floating-point dtypes. Fallback otherwise.
    if x.dtype not in (torch.float16, torch.bfloat16, torch.float32):
        return torch.max(x, dim=dim)[0]

    # Move reduction dimension to the last axis using a view (no data copy).
    perm = [i for i in range(ndim) if i != dim] + [dim]
    x_perm = x.permute(perm)

    # Logical shape [*outer_shape, R] where we reduce over R (last dim)
    shape_perm = list(x_perm.shape)
    R = int(shape_perm[-1])
    outer_shape = shape_perm[:-1]

    # Output tensor has the same outer shape (original tensor with `dim` removed)
    out = torch.empty(outer_shape, device=x.device, dtype=x.dtype)
    out_numel = out.numel()

    # Support up to MAX_OUT_DIMS outer dimensions
    MAX_OUT_DIMS = 8
    outer_ndim = len(outer_shape)
    if outer_ndim > MAX_OUT_DIMS:
        # Fallback for very high-rank tensors
        return torch.max(x, dim=dim)[0]

    # Build shape/stride arrays for outer dimensions (padded to MAX_OUT_DIMS)
    shape_outer = torch.ones(MAX_OUT_DIMS, dtype=torch.int32, device=x.device)
    stride_outer = torch.zeros(MAX_OUT_DIMS, dtype=torch.int32, device=x.device)
    for i, size in enumerate(outer_shape):
        shape_outer[i] = size
        stride_outer[i] = x_perm.stride(i)

    stride_r = int(x_perm.stride(-1))

    # Tiling parameters (powers of two as required)
    BLOCK_M = 128
    BLOCK_R = 128

    # Number of tiles along R
    NUM_R_BLOCKS = triton.cdiv(R, BLOCK_R)

    # Allocate partial buffer: [NUM_R_BLOCKS, out_numel]
    # We keep the same dtype as input and accumulate in fp32 inside kernels.
    partial = torch.empty(
        (NUM_R_BLOCKS, out_numel), device=x.device, dtype=x.dtype
    )

    # ---- Phase 1: parallel over outer elements (M) and R-tiles (NUM_R_BLOCKS) ----
    def grid_phase1(meta):
        # 2D grid: (M-tiles, R-tiles)
        return (triton.cdiv(out_numel, BLOCK_M), NUM_R_BLOCKS)

    max_reduce_strided_phase1_kernel[grid_phase1](
        x_perm,
        partial,
        shape_outer,
        stride_outer,
        stride_r,
        R,
        out_numel,
        NUM_R_BLOCKS,
        BLOCK_M=BLOCK_M,
        BLOCK_R=BLOCK_R,
        MAX_OUT_DIMS=MAX_OUT_DIMS,
        num_warps=4,
        num_stages=2,
    )

    # ---- Phase 2: reduce partials over NUM_R_BLOCKS for each output element ----
    def grid_phase2(meta):
        return (triton.cdiv(out_numel, BLOCK_M),)

    max_reduce_strided_phase2_kernel[grid_phase2](
        partial,
        out,
        out_numel,
        NUM_R_BLOCKS=NUM_R_BLOCKS,
        BLOCK_M=BLOCK_M,
        num_warps=4,
        num_stages=2,
    )

    return out


class ModelNew(nn.Module):
    """
    Triton-optimized model that performs max reduction over a specific dimension.
    """
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_max_reduce(x, self.dim)
```