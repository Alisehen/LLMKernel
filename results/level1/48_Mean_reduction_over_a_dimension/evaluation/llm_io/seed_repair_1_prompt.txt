Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 535, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_170112_batch_range40to70_deepseek_deepseek/48_Mean_reduction_over_a_dimension/code/kernel_20251212_193949.py", line 244, in forward
    return triton_mean(x, self.dim)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_170112_batch_range40to70_deepseek_deepseek/48_Mean_reduction_over_a_dimension/code/kernel_20251212_193949.py", line 204, in triton_mean
    mean_kernel_3d[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 278, in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 81, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 47:11:
        ptrs = x_ptr + base_offset + inner_offsets * stride_inner
        block = tl.load(ptrs, mask=inner_mask, other=0.0)

        # Accumulate
        sum_vals += block

    # Compute mean
    mean_vals = sum_vals / dim_size

    # Store results
    for i in range(INNER_BLOCK_SIZE):
        if inner_mask[i]:
           ^
ValueError('Did you forget to add @triton.jit ? (`_builder` argument must be provided outside of JIT functions.)')

History Error:
None


PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Simple model that performs mean reduction over a specific dimension.
    """
    def __init__(self, dim: int):
        """
        Initializes the model with the dimension to reduce over.

        Args:
            dim (int): The dimension to reduce over.
        """
        super(Model, self).__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Reduces the input tensor along the specified dimension by taking the mean.

        Args:
            x (torch.Tensor): Input tensor of arbitrary shape.

        Returns:
            torch.Tensor: Output tensor with reduced dimension. The shape of the output is the same as the input except for the reduced dimension which is removed.
        """
        return torch.mean(x, dim=self.dim)

batch_size = 128
dim1 = 4096
dim2 = 4095

def get_inputs():
    x = torch.rand(batch_size, dim1, dim2)
    return [x]

def get_init_inputs():
    return [1]
```

Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def mean_kernel_2d(
    x_ptr,
    output_ptr,
    outer_dim,
    inner_dim,
    dim_size,
    stride_outer,
    stride_inner,
    output_stride_outer,
    output_stride_inner,
    BLOCK_SIZE: tl.constexpr,
):
    """Kernel for 2D mean reduction with optimized memory access patterns."""
    pid_outer = tl.program_id(0)
    pid_inner = tl.program_id(1)
    
    if pid_outer >= outer_dim or pid_inner >= inner_dim:
        return
    
    # Base pointer for this output position
    output_offset = pid_outer * output_stride_outer + pid_inner * output_stride_inner
    
    # Accumulate sum using block-wise reduction
    sum_val = 0.0
    block_start = 0
    
    # Process reduction dimension in blocks
    while block_start < dim_size:
        offsets = block_start + tl.arange(0, BLOCK_SIZE)
        mask = offsets < dim_size
        
        # Compute input pointer for this block
        x_offset = pid_outer * stride_outer + pid_inner * stride_inner
        ptr = x_ptr + x_offset + offsets * stride_inner * inner_dim
        
        # Load block
        block = tl.load(ptr, mask=mask, other=0.0)
        
        # Accumulate sum
        block_sum = tl.sum(block, axis=0)
        sum_val += block_sum
        
        block_start += BLOCK_SIZE
    
    # Compute mean
    mean_val = sum_val / dim_size
    tl.store(output_ptr + output_offset, mean_val)

@triton.jit
def mean_kernel_3d(
    x_ptr,
    output_ptr,
    outer_dim,
    inner_dim,
    dim_size,
    stride_outer,
    stride_inner,
    stride_reduce,
    output_stride_outer,
    output_stride_inner,
    BLOCK_SIZE: tl.constexpr,
    INNER_BLOCK_SIZE: tl.constexpr,
):
    """Kernel for 3D mean reduction with optimized memory access and vectorization."""
    pid_outer = tl.program_id(0)
    pid_inner_block = tl.program_id(1)
    
    if pid_outer >= outer_dim:
        return
    
    # Inner dimension block processing
    inner_start = pid_inner_block * INNER_BLOCK_SIZE
    inner_offsets = inner_start + tl.arange(0, INNER_BLOCK_SIZE)
    inner_mask = inner_offsets < inner_dim
    
    # Initialize accumulation register
    sum_vals = tl.zeros((INNER_BLOCK_SIZE,), dtype=tl.float32)
    
    # Reduce over the specified dimension
    for k in range(dim_size):
        # Compute base pointer for this reduction step
        base_offset = pid_outer * stride_outer + k * stride_reduce
        
        # Load block with vectorization
        ptrs = x_ptr + base_offset + inner_offsets * stride_inner
        block = tl.load(ptrs, mask=inner_mask, other=0.0)
        
        # Accumulate
        sum_vals += block
    
    # Compute mean
    mean_vals = sum_vals / dim_size
    
    # Store results
    for i in range(INNER_BLOCK_SIZE):
        if inner_mask[i]:
            output_offset = pid_outer * output_stride_outer + (inner_start + i) * output_stride_inner
            tl.store(output_ptr + output_offset, mean_vals[i])

def triton_mean(x: torch.Tensor, dim: int) -> torch.Tensor:
    """Optimized Triton implementation of mean reduction."""
    if x.dim() == 0:
        return x.clone()
    
    # Handle negative dimensions
    if dim < 0:
        dim = x.dim() + dim
    
    # Get input shape and strides
    input_shape = x.shape
    dim_size = input_shape[dim]
    
    # Handle trivial cases
    if dim_size == 1:
        return x.squeeze(dim)
    
    # Compute output shape
    output_shape = list(input_shape)
    output_shape.pop(dim)
    output_shape = tuple(output_shape)
    
    # Create output tensor
    output = torch.empty(output_shape, device=x.device, dtype=x.dtype)
    
    # For 1D tensors, use simple reduction
    if x.dim() == 1:
        # Handle 1D case with custom kernel or fallback to PyTorch
        # For simplicity, use PyTorch for 1D
        return torch.mean(x, dim=dim)
    
    # For 2D tensors (batch_size × features)
    if x.dim() == 2:
        if dim == 0:
            # Reduce over batch dimension
            outer_dim = 1
            inner_dim = input_shape[1]
            stride_outer = input_shape[1] * dim_size
            stride_inner = 1
        else:  # dim == 1
            # Reduce over feature dimension
            outer_dim = input_shape[0]
            inner_dim = 1
            stride_outer = dim_size
            stride_inner = 1
        
        output_stride_outer = inner_dim
        output_stride_inner = 1
        
        # Choose block size based on reduction dimension
        BLOCK_SIZE = 1024 if dim_size >= 1024 else triton.next_power_of_2(dim_size)
        if BLOCK_SIZE < 16:
            BLOCK_SIZE = 16
        
        grid = lambda meta: (outer_dim, inner_dim)
        mean_kernel_2d[grid](
            x, output,
            outer_dim, inner_dim, dim_size,
            stride_outer, stride_inner,
            output_stride_outer, output_stride_inner,
            BLOCK_SIZE=BLOCK_SIZE
        )
        
        return output
    
    # For 3D tensors (batch_size × dim1 × dim2)
    if x.dim() == 3:
        # Reshape to handle arbitrary reduction dimension
        if dim == 0:
            # Reduce over batch
            outer_dim = 1
            inner_dim = input_shape[1] * input_shape[2]
            stride_outer = input_shape[1] * input_shape[2] * dim_size
            stride_inner = 1
            stride_reduce = input_shape[1] * input_shape[2]
        elif dim == 1:
            # Reduce over first feature dimension
            outer_dim = input_shape[0]
            inner_dim = input_shape[2]
            stride_outer = input_shape[1] * input_shape[2]
            stride_inner = 1
            stride_reduce = input_shape[2]
        else:  # dim == 2
            # Reduce over second feature dimension
            outer_dim = input_shape[0] * input_shape[1]
            inner_dim = 1
            stride_outer = input_shape[2] * dim_size
            stride_inner = 1
            stride_reduce = 1
        
        output_stride_outer = inner_dim
        output_stride_inner = 1
        
        # Optimize block sizes for A100
        BLOCK_SIZE = 128  # Good for L1 cache
        INNER_BLOCK_SIZE = 256  # Process multiple inner dimensions together
        
        grid = lambda meta: (outer_dim, triton.cdiv(inner_dim, meta['INNER_BLOCK_SIZE']))
        mean_kernel_3d[grid](
            x, output,
            outer_dim, inner_dim, dim_size,
            stride_outer, stride_inner, stride_reduce,
            output_stride_outer, output_stride_inner,
            BLOCK_SIZE=BLOCK_SIZE,
            INNER_BLOCK_SIZE=INNER_BLOCK_SIZE
        )
        
        return output.reshape(output_shape)
    
    # For higher dimensions, reshape to 3D and recursively call
    # This is efficient for the given example (3D tensor)
    return torch.mean(x, dim=dim)

class ModelNew(nn.Module):
    """
    Optimized model that performs mean reduction over a specific dimension using Triton.
    """
    def __init__(self, dim: int):
        """
        Initializes the model with the dimension to reduce over.

        Args:
            dim (int): The dimension to reduce over.
        """
        super(ModelNew, self).__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Reduces the input tensor along the specified dimension by taking the mean.
        Uses optimized Triton kernels for 2D and 3D tensors.

        Args:
            x (torch.Tensor): Input tensor of arbitrary shape.

        Returns:
            torch.Tensor: Output tensor with reduced dimension.
        """
        return triton_mean(x, self.dim)
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes

```python
# <corrected code>
```
