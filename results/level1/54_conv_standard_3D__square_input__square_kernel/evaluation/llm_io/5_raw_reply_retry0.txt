```python
# <complete ModelNew code with optimized Triton kernels>
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64, 'num_warps': 8, 'num_stages': 4}),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64, 'num_warps': 8, 'num_stages': 4}),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'num_warps': 8, 'num_stages': 4}),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64, 'num_warps': 8, 'num_stages': 4}),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32, 'num_warps': 4, 'num_stages': 3}),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'num_warps': 4, 'num_stages': 3}),
    ],
    key=['M_SIZE', 'N_SIZE', 'K_SIZE'],
)
@triton.jit
def conv3d_im2col_kernel(
    input_ptr,
    weight_ptr,
    bias_ptr,
    output_ptr,
    N_batches,
    Cin,
    Cout,
    D,
    H,
    W,
    KD,
    KH,
    KW,
    stride_d,
    stride_h,
    stride_w,
    pad_d,
    pad_h,
    pad_w,
    dil_d,
    dil_h,
    dil_w,
    D_out,
    H_out,
    W_out,
    M_SIZE,
    N_SIZE,
    K_SIZE,
    HAS_BIAS: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    pid_m = tl.program_id(axis=0)
    pid_n = tl.program_id(axis=1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    mask_m = offs_m < M_SIZE
    mask_n = offs_n < N_SIZE

    # decode m -> (n_batch, od, oh, ow)
    ow = offs_m % W_out
    tmp = offs_m // W_out
    oh = tmp % H_out
    tmp = tmp // H_out
    od = tmp % D_out
    n_batch = tmp // D_out

    n_b = n_batch[:, None]
    od_tile = od[:, None]
    oh_tile = oh[:, None]
    ow_tile = ow[:, None]

    oc = offs_n
    oc_tile = oc[None, :]

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    if HAS_BIAS:
        bias_vals = tl.load(bias_ptr + offs_n, mask=mask_n, other=0.0).to(tl.float32)
        acc += bias_vals[None, :]

    CinKD = Cin * KD
    CinKDKH = CinKD * KH
    K_total = CinKDKH * KW  # equals K_SIZE

    for k_start in range(0, K_SIZE, BLOCK_K):
        k_idx = k_start + tl.arange(0, BLOCK_K)
        k_mask = k_idx < K_SIZE
        k_idx_safe = tl.where(k_mask, k_idx, 0)

        kw = k_idx_safe % KW
        tmp_k = k_idx_safe // KW
        kh = tmp_k % KH
        tmp_k = tmp_k // KH
        kd = tmp_k % KD
        ic = tmp_k // KD

        kw_tile = kw[None, :]
        kh_tile = kh[None, :]
        kd_tile = kd[None, :]
        ic_tile = ic[None, :]

        in_w = ow_tile * stride_w + kw_tile * dil_w - pad_w
        in_h = oh_tile * stride_h + kh_tile * dil_h - pad_h
        in_d = od_tile * stride_d + kd_tile * dil_d - pad_d

        valid_in = (
            mask_m[:, None]
            & k_mask[None, :]
            & (in_w >= 0) & (in_w < W)
            & (in_h >= 0) & (in_h < H)
            & (in_d >= 0) & (in_d < D)
        )

        n_b64 = n_b.to(tl.int64)
        ic64 = ic_tile.to(tl.int64)
        in_d64 = in_d.to(tl.int64)
        in_h64 = in_h.to(tl.int64)
        in_w64 = in_w.to(tl.int64)

        input_ptrs = ((((n_b64 * Cin + ic64) * D + in_d64) * H + in_h64) * W + in_w64)
        inp = tl.load(input_ptr + input_ptrs, mask=valid_in, other=0.0)

        k_linear64 = k_idx_safe[:, None].to(tl.int64)
        oc64 = oc_tile.to(tl.int64)
        weight_ptrs = oc64 * K_total + k_linear64
        w_mask = k_mask[:, None] & mask_n[None, :]
        w = tl.load(weight_ptr + weight_ptrs, mask=w_mask, other=0.0)

        acc += tl.dot(inp, w)

    od64 = od_tile.to(tl.int64)
    oh64 = oh_tile.to(tl.int64)
    ow64 = ow_tile.to(tl.int64)
    n_b64 = n_b.to(tl.int64)
    oc64 = oc_tile.to(tl.int64)

    out_ptrs = (((((n_b64 * Cout + oc64) * D_out + od64) * H_out + oh64) * W_out + ow64))
    out_mask = mask_m[:, None] & mask_n[None, :]
    tl.store(output_ptr + out_ptrs, acc, mask=out_mask)


def _triple(x):
    if isinstance(x, tuple):
        assert len(x) == 3
        return x
    return (x, x, x)


def triton_conv3d(x, weight, bias, stride, padding, dilation):
    if x.dtype != torch.float32 or weight.dtype != torch.float32:
        raise NotImplementedError("Only float32 tensors are supported.")
    stride_d, stride_h, stride_w = _triple(stride)
    pad_d, pad_h, pad_w = _triple(padding)
    dil_d, dil_h, dil_w = _triple(dilation)

    N, Cin, D, H, W = x.shape
    Cout, Cin_w, KD, KH, KW = weight.shape
    if Cin != Cin_w:
        raise NotImplementedError("Grouped convolution is not supported.")

    D_out = (D + 2 * pad_d - dil_d * (KD - 1) - 1) // stride_d + 1
    H_out = (H + 2 * pad_h - dil_h * (KH - 1) - 1) // stride_h + 1
    W_out = (W + 2 * pad_w - dil_w * (KW - 1) - 1) // stride_w + 1

    x_contig = x.contiguous()
    w_contig = weight.contiguous()
    out = torch.empty((N, Cout, D_out, H_out, W_out), device=x.device, dtype=x.dtype)

    M_SIZE = N * D_out * H_out * W_out
    N_SIZE = Cout
    K_SIZE = Cin * KD * KH * KW

    grid = lambda META: (triton.cdiv(M_SIZE, META['BLOCK_M']), triton.cdiv(N_SIZE, META['BLOCK_N']))
    bias_ptr = bias.contiguous() if bias is not None else out.new_empty(1)

    conv3d_im2col_kernel[grid](
        x_contig,
        w_contig,
        bias_ptr,
        out,
        N,
        Cin,
        Cout,
        D,
        H,
        W,
        KD,
        KH,
        KW,
        stride_d,
        stride_h,
        stride_w,
        pad_d,
        pad_h,
        pad_w,
        dil_d,
        dil_h,
        dil_w,
        D_out,
        H_out,
        W_out,
        M_SIZE,
        N_SIZE,
        K_SIZE,
        HAS_BIAS=bias is not None,
    )

    return out


class ModelNew(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: int = 0,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = False,
    ):
        super().__init__()
        if groups != 1:
            raise NotImplementedError("Grouped convolution is not supported.")
        kernel_size = _triple(kernel_size) if isinstance(kernel_size, tuple) else (kernel_size, kernel_size, kernel_size)
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.groups = groups

        self.weight = nn.Parameter(torch.empty(out_channels, in_channels, *kernel_size))
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter("bias", None)
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.weight, a=5**0.5)
        if self.bias is not None:
            fan_in = self.in_channels * self.kernel_size[0] * self.kernel_size[1] * self.kernel_size[2]
            bound = 1 / fan_in**0.5
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_conv3d(x, self.weight, self.bias, self.stride, self.padding, self.dilation)
```