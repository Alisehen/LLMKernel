```python
import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def max_pool2d_kernel(
    x_ptr,
    output_ptr,
    # Tensor dimensions
    batch_stride_x, channel_stride_x, height_stride_x, width_stride_x,
    batch_stride_out, channel_stride_out, height_stride_out, width_stride_out,
    batch_size, channels, height, width,
    pooled_height, pooled_width,
    # Pooling parameters
    kernel_h, kernel_w,
    stride_h, stride_w,
    pad_h, pad_w,
    dilation_h, dilation_w,
    # Block parameters
    BLOCK_SIZE_H: tl.constexpr,
    BLOCK_SIZE_W: tl.constexpr,
    BLOCK_SIZE_C: tl.constexpr,
):
    # Parallelize across output positions and channels
    pid_batch = tl.program_id(axis=0)
    pid_channel = tl.program_id(axis=1)
    pid_ph = tl.program_id(axis=2)
    
    # Create block of output positions in H dimension
    ph_block_start = pid_ph * BLOCK_SIZE_H
    ph_offsets = ph_block_start + tl.arange(0, BLOCK_SIZE_H)
    ph_mask = ph_offsets < pooled_height
    
    # Process multiple channels per block
    channel_block_start = pid_channel * BLOCK_SIZE_C
    channel_offsets = channel_block_start + tl.arange(0, BLOCK_SIZE_C)
    channel_mask = channel_offsets < channels
    
    # Create block for width dimension
    pw = tl.arange(0, BLOCK_SIZE_W)
    pw_mask = pw < pooled_width
    
    # Combine masks for efficient loading
    hw_mask = tl.broadcast_to(ph_mask[:, None], (BLOCK_SIZE_H, BLOCK_SIZE_W)) & tl.broadcast_to(pw_mask[None, :], (BLOCK_SIZE_H, BLOCK_SIZE_W))
    final_mask = tl.broadcast_to(hw_mask[:, :, None], (BLOCK_SIZE_H, BLOCK_SIZE_W, BLOCK_SIZE_C)) & tl.broadcast_to(channel_mask[None, None, :], (BLOCK_SIZE_H, BLOCK_SIZE_W, BLOCK_SIZE_C))
    
    # Initialize output with -inf
    output_block = tl.full((BLOCK_SIZE_H, BLOCK_SIZE_W, BLOCK_SIZE_C), float('-inf'), dtype=tl.float32)
    
    # For each position in the pooling window
    for kh in range(kernel_h):
        for kw in range(kernel_w):
            # Compute input positions with dilation
            h_idx = ph_offsets[:, None, None] * stride_h - pad_h + kh * dilation_h
            w_idx = pw[None, :, None] * stride_w - pad_w + kw * dilation_w
            
            # Create mask for valid input positions
            h_valid = (h_idx >= 0) & (h_idx < height)
            w_valid = (w_idx >= 0) & (w_idx < width)
            valid_mask = h_valid & w_valid
            
            if tl.sum(valid_mask) > 0:
                # Compute input pointers
                x_ptrs = (
                    x_ptr +
                    pid_batch * batch_stride_x +
                    channel_offsets[None, None, :] * channel_stride_x +
                    h_idx[:, None, :] * height_stride_x +
                    w_idx[:, :, None] * width_stride_x
                )
                
                # Load input values with boundary checking
                x_vals = tl.load(x_ptrs, mask=valid_mask[:, :, None] & channel_mask[None, None, :], other=float('-inf'))
                
                # Update max
                output_block = tl.maximum(output_block, x_vals)
    
    # Store results
    output_ptrs = (
        output_ptr +
        pid_batch * batch_stride_out +
        channel_offsets[None, None, :] * channel_stride_out +
        ph_offsets[:, None, None] * height_stride_out +
        pw[None, :, None] * width_stride_out
    )
    
    tl.store(output_ptrs, output_block, mask=final_mask)


def triton_max_pool2d(
    x: torch.Tensor,
    kernel_size: int,
    stride: int = None,
    padding: int = 0,
    dilation: int = 1,
) -> torch.Tensor:
    """Triton implementation of 2D max pooling."""
    if stride is None:
        stride = kernel_size
    
    # Ensure kernel_size is tuple
    if isinstance(kernel_size, int):
        kernel_h = kernel_w = kernel_size
    else:
        kernel_h, kernel_w = kernel_size
    
    # Ensure other parameters are tuples
    if isinstance(stride, int):
        stride_h = stride_w = stride
    else:
        stride_h, stride_w = stride
    
    if isinstance(padding, int):
        pad_h = pad_w = padding
    else:
        pad_h, pad_w = padding
    
    if isinstance(dilation, int):
        dilation_h = dilation_w = dilation
    else:
        dilation_h, dilation_w = dilation
    
    # Compute output dimensions
    batch_size, channels, height, width = x.shape
    pooled_height = (height + 2 * pad_h - dilation_h * (kernel_h - 1) - 1) // stride_h + 1
    pooled_width = (width + 2 * pad_w - dilation_w * (kernel_w - 1) - 1) // stride_w + 1
    
    # Allocate output tensor
    output = torch.empty(
        (batch_size, channels, pooled_height, pooled_width),
        device=x.device,
        dtype=x.dtype
    )
    
    # Strides for tensor access
    batch_stride_x = x.stride(0)
    channel_stride_x = x.stride(1)
    height_stride_x = x.stride(2)
    width_stride_x = x.stride(3)
    
    batch_stride_out = output.stride(0)
    channel_stride_out = output.stride(1)
    height_stride_out = output.stride(2)
    width_stride_out = output.stride(3)
    
    # Configure kernel launch
    BLOCK_SIZE_H = 16
    BLOCK_SIZE_W = 16
    BLOCK_SIZE_C = 4
    
    grid = lambda meta: (
        batch_size,
        triton.cdiv(channels, meta['BLOCK_SIZE_C']),
        triton.cdiv(pooled_height, meta['BLOCK_SIZE_H']),
    )
    
    # Launch kernel
    max_pool2d_kernel[grid](
        x,
        output,
        batch_stride_x, channel_stride_x, height_stride_x, width_stride_x,
        batch_stride_out, channel_stride_out, height_stride_out, width_stride_out,
        batch_size, channels, height, width,
        pooled_height, pooled_width,
        kernel_h, kernel_w,
        stride_h, stride_w,
        pad_h, pad_w,
        dilation_h, dilation_w,
        BLOCK_SIZE_H=BLOCK_SIZE_H,
        BLOCK_SIZE_W=BLOCK_SIZE_W,
        BLOCK_SIZE_C=BLOCK_SIZE_C,
    )
    
    return output


class ModelNew(nn.Module):
    """
    Optimized model that performs Max Pooling 2D using Triton kernels.
    """
    def __init__(self, kernel_size: int, stride: int, padding: int, dilation: int):
        """
        Initializes the Max Pooling 2D layer.

        Args:
            kernel_size (int): Size of the pooling window.
            stride (int): Stride of the pooling window.
            padding (int): Padding to be applied before pooling.
            dilation (int): Spacing between kernel elements.
        """
        super(ModelNew, self).__init__()
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dilation = dilation

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Applies optimized Max Pooling 2D to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).

        Returns:
            torch.Tensor: Output tensor after Max Pooling 2D.
        """
        return triton_max_pool2d(
            x,
            kernel_size=self.kernel_size,
            stride=self.stride,
            padding=self.padding,
            dilation=self.dilation
        )
```