Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 535, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251213_091928_batch_range68to99_openai_deepseek/84_conv_depthwise_2D_asymmetric_input_square_kernel/code/kernel_20251213_111519.py", line 151, in forward
    return triton_depthwise_conv2d(x, self.weight, self.bias, self.stride, self.padding)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251213_091928_batch_range68to99_openai_deepseek/84_conv_depthwise_2D_asymmetric_input_square_kernel/code/kernel_20251213_111519.py", line 127, in triton_depthwise_conv2d
    OUTPUT_DTYPE=_to_triton_dtype(x.dtype),
                 ^^^^^^^^^^^^^^^^
NameError: name '_to_triton_dtype' is not defined

History Error:
None


PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Performs a depthwise 2D convolution with asymmetric input and square kernel.

    Args:
        in_channels (int): Number of channels in the input tensor.
        out_channels (int): Number of channels produced by the convolution.
        kernel_size (int): Size of the square convolution kernel.
        stride (int, optional): Stride of the convolution. Defaults to 1.
        padding (int, optional): Padding applied to the input. Defaults to 0.
        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = False):
        super(Model, self).__init__()
        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, kernel_size), stride=stride, padding=padding, groups=in_channels, bias=bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Performs the depthwise 2D convolution.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).
        """
        return self.conv2d(x)

# Test code
batch_size = 64
in_channels = 128
out_channels = 128
kernel_size = 3
width_in = 512
height_in = 256
stride = 1
padding = 0

def get_inputs():
    x = torch.rand(batch_size, in_channels, height_in, width_in)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding]
```

Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def depthwise_conv2d_kernel(
    x_ptr,
    w_ptr,
    b_ptr,
    out_ptr,
    B,
    C_in,
    H_in,
    W_in,
    C_out,
    OH,
    OW,
    stride_h,
    stride_w,
    pad_h,
    pad_w,
    channels_per_group,
    total_spatial,
    HAS_BIAS: tl.constexpr,
    KH: tl.constexpr,
    KW: tl.constexpr,
    BLOCK_SP: tl.constexpr,
    OUTPUT_DTYPE: tl.constexpr,
):
    pid_b = tl.program_id(0)
    pid_c = tl.program_id(1)
    pid_sp = tl.program_id(2)

    sp_offsets = pid_sp * BLOCK_SP + tl.arange(0, BLOCK_SP)
    valid_sp = sp_offsets < total_spatial

    oh = sp_offsets // OW
    ow = sp_offsets % OW

    oh_in = oh * stride_h - pad_h
    ow_in = ow * stride_w - pad_w

    c_in = pid_c // channels_per_group

    base_input = ((pid_b * C_in + c_in) * H_in) * W_in
    base_output = ((pid_b * C_out + pid_c) * OH) * OW

    acc = tl.zeros((BLOCK_SP,), dtype=tl.float32)

    for kh in range(KH):
        in_y = oh_in + kh
        mask_y = (in_y >= 0) & (in_y < H_in)
        y_offset = (in_y * W_in) + base_input

        for kw in range(KW):
            in_x = ow_in + kw
            mask_x = (in_x >= 0) & (in_x < W_in)
            mask = valid_sp & mask_y & mask_x

            ptrs = y_offset + in_x
            vals = tl.load(x_ptr + ptrs, mask=mask, other=0.0)

            w_offset = ((pid_c * KH) + kh) * KW + kw
            weight = tl.load(w_ptr + w_offset)

            acc += vals.to(tl.float32) * weight.to(tl.float32)

    if HAS_BIAS:
        bias = tl.load(b_ptr + pid_c)
        acc += bias.to(tl.float32)

    out_ptrs = base_output + oh * OW + ow
    tl.store(out_ptr + out_ptrs, acc.to(OUTPUT_DTYPE), mask=valid_sp)


def triton_depthwise_conv2d(x, weight, bias, stride, padding):
    assert x.is_cuda and weight.is_cuda, "Inputs must be on CUDA"
    assert x.dtype == weight.dtype, "Input and weight dtypes must match"

    stride_h = stride_w = stride
    pad_h = pad_w = padding

    B, C_in, H_in, W_in = x.shape
    C_out, _, KH, KW = weight.shape
    channels_per_group = C_out // C_in

    OH = (H_in + 2 * pad_h - KH) // stride_h + 1
    OW = (W_in + 2 * pad_w - KW) // stride_w + 1
    total_spatial = OH * OW

    output = torch.empty((B, C_out, OH, OW), device=x.device, dtype=x.dtype)

    bias_ptr = bias if bias is not None else output
    has_bias = bias is not None

    BLOCK_SP = 128
    grid = (
        B,
        C_out,
        triton.cdiv(total_spatial, BLOCK_SP),
    )

    depthwise_conv2d_kernel[grid](
        x,
        weight,
        bias_ptr,
        output,
        B,
        C_in,
        H_in,
        W_in,
        C_out,
        OH,
        OW,
        stride_h,
        stride_w,
        pad_h,
        pad_w,
        channels_per_group,
        total_spatial,
        HAS_BIAS=has_bias,
        KH=KH,
        KW=KW,
        BLOCK_SP=BLOCK_SP,
        OUTPUT_DTYPE=_to_triton_dtype(x.dtype),
        num_warps=4,
        num_stages=2,
    )
    return output


class ModelNew(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = False):
        super().__init__()
        self.weight = nn.Parameter(torch.empty(out_channels, 1, kernel_size, kernel_size))
        self.bias = nn.Parameter(torch.empty(out_channels)) if bias else None
        nn.init.kaiming_uniform_(self.weight, a=5 ** 0.5)
        if bias:
            fan_in = in_channels * kernel_size * kernel_size
            bound = 1 / fan_in ** 0.5
            nn.init.uniform_(self.bias, -bound, bound)
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_depthwise_conv2d(x, self.weight, self.bias, self.stride, self.padding)
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes

```python
# <corrected code>
```
