You are optimizing a Triton kernel based on algorithmic analysis.

# PyTorch Reference (Target Behavior)

```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    A model that performs a masked cumulative sum, only summing elements that satisfy a condition.

    Parameters:
        dim (int): The dimension along which to perform the masked cumulative sum.
    """

    def __init__(self, dim):
        super(Model, self).__init__()
        self.dim = dim

    def forward(self, x, mask):
        """
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, *input_shape).
            mask (torch.Tensor): Boolean mask of the same shape as x.

        Returns:
            torch.Tensor: Cumulative sum of elements where mask is True.
        """
        return torch.cumsum(x * mask, dim=self.dim)

batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    x = torch.rand(batch_size, *input_shape)
    mask = torch.randint(0, 2, x.shape).bool()  # Random boolean mask
    return [x, mask]

def get_init_inputs():
    return [dim]
```

**CRITICAL**: Study the PyTorch code carefully to understand:
- What does `forward()` return? (full output sequence vs final hidden state only)
- What is the computational pattern?
- What are the input/output shapes?

Your optimized kernel MUST match this exact behavior.

---

# Analysis Results

**Bottleneck**: The code first computes `x_masked = x * mask.to(x.dtype)` as a separate CUDA kernel and writes it to global memory, then reads `x_masked` again inside the Triton scan. This doubles global memory traffic for the input and adds an extra launch, which is costly for large tensors and bandwidth-bound scans.

**Optimization Strategy**: Fuse the mask application into the Triton cumsum kernel: pass `mask_ptr` into `_cumsum_tile_kernel` and zero out elements based on the boolean mask at load time, eliminating the separate `x * mask` kernel and the intermediate `x_masked` tensor.

**Implementation Plan**: Remove the precomputation of `x_masked` in Python and keep `x` and `mask` in their original form. Extend `_cumsum_tile_kernel` (and the surrounding 2D reshape/transposition logic) to accept a `mask_ptr` with the same shape/strides as `x`, load both `x` and `mask` for each element, and compute `vals = tl.where(mask_vals, x_vals, 0)` before the local `tl.cumsum`. This keeps the rest of the multi-pass block-sum logic unchanged but reduces memory traffic and kernel launches.

**Expected Speedup**: 20-30% for large, memory-bound tensors where the cost of the extra read/write of the full input dominates.

---

# Current Kernel (needs optimization)

```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def _cumsum_tile_kernel(
    x_ptr,               # *x_dtype, [M, N]
    y_ptr,               # *x_dtype, [M, N]
    M,                   # number of rows
    N,                   # number of columns (cumsum dim)
    stride_m,            # row stride in elements
    stride_n,            # col stride in elements (usually 1)
    BLOCK_SIZE: tl.constexpr,
):
    pid_m = tl.program_id(axis=0)  # row index
    pid_b = tl.program_id(axis=1)  # block index within the row

    # starting column for this tile
    col_start = pid_b * BLOCK_SIZE

    # local indices [0, BLOCK_SIZE)
    idx = tl.arange(0, BLOCK_SIZE)
    offs = col_start + idx

    # mask for elements within the row length N
    mask = offs < N

    # base pointers for this row
    x_row_ptr = x_ptr + pid_m * stride_m
    y_row_ptr = y_ptr + pid_m * stride_m

    # load this tile, zero-padding out-of-range columns
    vals = tl.load(x_row_ptr + offs * stride_n, mask=mask, other=0.0)

    # perform an inclusive scan within the tile in registers
    # vals is 1D, so axis=0
    vals = tl.cumsum(vals, axis=0)

    # store back
    tl.store(y_row_ptr + offs * stride_n, vals, mask=mask)


@triton.jit
def _add_block_offsets_kernel(
    y_ptr,                # *x_dtype, [M, N]
    block_offsets_ptr,    # *x_dtype, [M, num_blocks]
    M,
    N,
    num_blocks,
    stride_m,
    stride_n,
    BLOCK_SIZE: tl.constexpr,
):
    pid_m = tl.program_id(axis=0)  # row index
    pid_b = tl.program_id(axis=1)  # block index within row

    col_start = pid_b * BLOCK_SIZE
    idx = tl.arange(0, BLOCK_SIZE)
    offs = col_start + idx
    mask = offs < N

    # base pointer for this row
    y_row_ptr = y_ptr + pid_m * stride_m

    # scalar block offset for this tile
    bs_offset = pid_m * num_blocks + pid_b
    block_off = tl.load(block_offsets_ptr + bs_offset)

    # load current prefix sums, add block offset, and store back
    vals = tl.load(y_row_ptr + offs * stride_n, mask=mask, other=0.0)
    vals = vals + block_off
    tl.store(y_row_ptr + offs * stride_n, vals, mask=mask)


def triton_masked_cumsum(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:
    """
    Masked cumulative sum using Triton, equivalent to torch.cumsum(x * mask, dim=dim).
    Assumes x and mask are on CUDA device.
    """
    assert x.device.type == "cuda", "Input tensor must be on CUDA for Triton kernels."
    assert mask.device == x.device, "Mask must be on the same device as input."
    assert x.shape == mask.shape, "x and mask must have the same shape."

    # Apply mask (boolean -> same dtype as x)
    x_masked = x * mask.to(x.dtype)

    if x_masked.numel() == 0:
        return x_masked

    dim = dim if dim >= 0 else x_masked.dim() + dim
    assert 0 <= dim < x_masked.dim()

    # Move cumsum dimension to the last axis and make contiguous
    if dim != x_masked.dim() - 1:
        x_perm = x_masked.transpose(dim, -1).contiguous()
        transposed = True
    else:
        x_perm = x_masked.contiguous()
        transposed = False

    M = x_perm.numel() // x_perm.size(-1)
    N = x_perm.size(-1)

    if N == 0:
        # Nothing to scan along cumsum dimension
        out_perm = x_perm
    else:
        x2d = x_perm.view(M, N)
        y2d = torch.empty_like(x2d)

        BLOCK_SIZE = 256  # power-of-two, as required
        num_blocks = triton.cdiv(N, BLOCK_SIZE)

        stride_m = x2d.stride(0)
        stride_n = x2d.stride(1)

        grid = (M, num_blocks)

        # First pass: per-tile prefix sums (local scan, no inter-tile carry)
        _cumsum_tile_kernel[grid](
            x2d,
            y2d,
            M,
            N,
            stride_m,
            stride_n,
            BLOCK_SIZE=BLOCK_SIZE,
            num_warps=4,
        )

        # Compute tile sums on PyTorch: sum of original masked x within each tile
        # Shape [M, num_blocks]
        if num_blocks == 1:
            # Single tile per row: sum across the row (not actually needed for offsets)
            block_sums = x2d.sum(dim=1, keepdim=True)
        else:
            total_elems = num_blocks * BLOCK_SIZE
            pad = total_elems - N
            if pad > 0:
                pad_tensor = torch.zeros(
                    (M, pad), device=x2d.device, dtype=x2d.dtype
                )
                x_padded = torch.cat([x2d, pad_tensor], dim=1)
            else:
                x_padded = x2d
            x_blocks = x_padded.view(M, num_blocks, BLOCK_SIZE)
            block_sums = x_blocks.sum(dim=2)

        # Second pass: prefix sum over tiles per row on PyTorch (small, fast)
        # exclusive prefix: offset[b] = sum_{k < b} tile_sum[k]
        if num_blocks == 1:
            block_offsets = torch.zeros_like(block_sums)
        else:
            zeros_col = torch.zeros_like(block_sums[:, :1])
            prefix_input = torch.cat([zeros_col, block_sums[:, :-1]], dim=1)
            block_offsets = torch.cumsum(prefix_input, dim=1)

        # Third pass: add tile offsets to each element
        _add_block_offsets_kernel[grid](
            y2d,
            block_offsets,
            M,
            N,
            num_blocks,
            stride_m,
            stride_n,
            BLOCK_SIZE=BLOCK_SIZE,
            num_warps=4,
        )

        out_perm = y2d.view_as(x_perm)

    # Move back to original dimension order
    if transposed:
        out = out_perm.transpose(-1, dim)
    else:
        out = out_perm

    return out


class ModelNew(nn.Module):
    """
    Triton-accelerated model that performs masked cumulative sum along a given dimension:
        output = torch.cumsum(x * mask, dim=dim)
    """

    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, x, mask):
        return triton_masked_cumsum(x, mask, self.dim)
```

---

# Your Task

Implement the optimization strategy above. Focus on the specific bottleneck identified.

## Key Requirements

1. **Preserve correctness**: Maintain the same input/output behavior
2. **Apply the optimization**: Follow the implementation plan exactly
3. **Use valid Triton syntax**:
   - Every kernel MUST have `@triton.jit` decorator
   - Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
   - BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
   - No `continue`, `break`, `return` inside kernels (use masking)
   - Prefer `tl.dot(a, b, allow_tf32=True)` for matmul operations

4. **CRITICAL for RNN/GRU/LSTM Persistent Kernels**:
   - Time loop MUST be inside @triton.jit kernel, NOT in Python forward()
   - **HYBRID computation strategy** (CRITICAL for performance):
     * Precompute input-side gates OUTSIDE kernel: `gates_x = (T*B, In) @ W_ih` (ONE large GEMM)
     * INSIDE kernel: only recurrent-side: `for t: gates_h = h @ W_hh` (T small GEMMs)
   - CORRECT (FAST - use this):
     ```python
     # Python forward():
     gates_x_all = x.reshape(T*B, In) @ W_ih + b_ih  # ONE large GEMM
     gates_x_all = gates_x_all.view(T, B, 3*H)
     gru_persistent_kernel[grid](gates_x_all, h0, W_hh, ...)  # Launch ONCE

     @triton.jit
     def gru_persistent_kernel(gates_x_ptr, h_ptr, W_hh_ptr, ...):
         for t in range(T):  # Inside kernel
             gates_x_t = tl.load(gates_x_ptr + t*...)  # Precomputed
             gates_h = h @ W_hh  # Only recurrent GEMM
             h = (1-z)*n + z*h   # Fuse and update
     ```

5. **Output format**:
   - Imports: `import torch, torch.nn as nn, triton, triton.language as tl`
   - `@triton.jit` kernel(s)
   - Wrapper function(s)
   - `class ModelNew(nn.Module)` â€” REQUIRED
   - NO testing code, NO `if __name__ == "__main__"`

---

Generate the optimized kernel now. Output ONLY the complete Python code.
