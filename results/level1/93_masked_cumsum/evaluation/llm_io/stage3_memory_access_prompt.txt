You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Memory efficiency & latency hiding.

Metrics:
- dram__throughput.avg.pct_of_peak_sustained_elapsed
- lts__t_sector_hit_rate.pct
- smsp__warp_issue_stalled_memory_dependency_per_warp_active.pct (<20%)

Rules:
- Increase num_stages only if memory stalls are high
- Do not rewrite access patterns without metric evidence
- Larger BLOCK_K improves reuse but increases register pressure

Autotune:
- If unsure, try num_stages ∈ {1,2,3} on kernel



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def masked_chunk_scan_kernel(
    x_ptr,
    mask_ptr,
    partial_ptr,
    chunk_sums_ptr,
    rows,
    cols,
    n_chunks,
    stride_row,
    BLOCK_COL: tl.constexpr,
):
    row = tl.program_id(axis=0)
    chunk = tl.program_id(axis=1)
    if (row >= rows) or (chunk >= n_chunks):
        return

    chunk_start = chunk * BLOCK_COL
    offs = chunk_start + tl.arange(0, BLOCK_COL)
    base_offset = row * stride_row

    running = tl.zeros((), dtype=tl.float32)
    chunk_has_work = chunk_start < cols

    for i in tl.static_range(0, BLOCK_COL):
        col_idx = chunk_start + i
        element_active = (col_idx < cols) & chunk_has_work

        x_val = tl.load(x_ptr + base_offset + col_idx, mask=element_active, other=0.0).to(tl.float32)
        m_val = tl.load(mask_ptr + base_offset + col_idx, mask=element_active, other=0).to(tl.float32)
        running += x_val * m_val
        tl.store(partial_ptr + base_offset + col_idx, running, mask=element_active)

    chunk_ptr = chunk_sums_ptr + row * n_chunks + chunk
    tl.store(chunk_ptr, running, mask=chunk_has_work)


@triton.jit
def chunk_prefix_kernel(
    chunk_sums_ptr,
    chunk_offsets_ptr,
    rows,
    n_chunks,
    MAX_CHUNKS: tl.constexpr,
):
    row = tl.program_id(axis=0)
    if row >= rows:
        return

    base = row * n_chunks
    running = tl.zeros((), dtype=tl.float32)

    for idx in tl.static_range(0, MAX_CHUNKS):
        valid = idx < n_chunks
        ptr = chunk_sums_ptr + base + idx
        val = tl.load(ptr, mask=valid, other=0.0).to(tl.float32)
        tl.store(chunk_offsets_ptr + base + idx, running, mask=valid)
        running += val


@triton.jit
def add_chunk_offsets_kernel(
    partial_ptr,
    chunk_offsets_ptr,
    out_ptr,
    rows,
    cols,
    n_chunks,
    stride_row,
    BLOCK_COL: tl.constexpr,
):
    row = tl.program_id(axis=0)
    chunk = tl.program_id(axis=1)
    if (row >= rows) or (chunk >= n_chunks):
        return

    chunk_start = chunk * BLOCK_COL
    offs = chunk_start + tl.arange(0, BLOCK_COL)
    mask = offs < cols
    chunk_active = chunk_start < cols
    base_offset = row * stride_row

    vals = tl.load(partial_ptr + base_offset + offs, mask=mask, other=0.0)
    offset = tl.load(chunk_offsets_ptr + row * n_chunks + chunk, mask=chunk_active, other=0.0)
    vals = vals + offset
    tl.store(out_ptr + base_offset + offs, vals, mask=mask)


def _move_dim_last(tensor: torch.Tensor, dim: int):
    dim = dim % tensor.ndim
    permute_order = [i for i in range(tensor.ndim) if i != dim] + [dim]
    inv_permute = [0] * tensor.ndim
    for i, p in enumerate(permute_order):
        inv_permute[p] = i
    tensor_perm = tensor.permute(permute_order).contiguous()
    shape_perm = tensor_perm.shape
    cols = shape_perm[-1]
    rows = tensor_perm.numel() // cols
    tensor_2d = tensor_perm.view(rows, cols)
    return tensor_2d, shape_perm, inv_permute


def masked_cumsum_triton(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:
    assert x.shape == mask.shape, "Input and mask must have the same shape"
    x = x.contiguous()
    mask = mask.contiguous()

    x_2d, perm_shape, inv_perm = _move_dim_last(x, dim)
    mask_2d, _, _ = _move_dim_last(mask, dim)

    rows, cols = x_2d.shape
    out_2d = torch.empty_like(x_2d)
    partial = torch.empty_like(x_2d)

    BLOCK_COL = 128
    MAX_CHUNKS = 1024
    n_chunks = triton.cdiv(cols, BLOCK_COL)
    if n_chunks > MAX_CHUNKS:
        raise ValueError(f"Number of chunks {n_chunks} exceeds MAX_CHUNKS={MAX_CHUNKS}")

    chunk_sums = torch.zeros((rows, n_chunks), dtype=x.dtype, device=x.device)
    chunk_offsets = torch.zeros_like(chunk_sums)

    stride_row = x_2d.stride(0)

    grid_blocks = (rows, n_chunks)
    masked_chunk_scan_kernel[grid_blocks](
        x_2d, mask_2d, partial, chunk_sums,
        rows, cols, n_chunks, stride_row,
        BLOCK_COL=BLOCK_COL,
    )

    grid_rows = (rows,)
    chunk_prefix_kernel[grid_rows](
        chunk_sums, chunk_offsets,
        rows, n_chunks,
        MAX_CHUNKS=MAX_CHUNKS,
    )

    add_chunk_offsets_kernel[grid_blocks](
        partial, chunk_offsets, out_2d,
        rows, cols, n_chunks, stride_row,
        BLOCK_COL=BLOCK_COL,
    )

    out_perm = out_2d.view(perm_shape)
    out = out_perm.permute(inv_perm).contiguous()
    return out


class ModelNew(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, x, mask):
        return masked_cumsum_triton(x, mask, self.dim)
```

[NCU PROFILING METRICS]
{
  "masked_chunk_scan_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 99.5,
    "launch__grid_size": 8388608,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 92.39,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 10.86,
    "lts__t_sector_hit_rate.pct": 86.58
  },
  "chunk_prefix_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 99.74,
    "launch__grid_size": 32768,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 97.2,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 2.03,
    "lts__t_sector_hit_rate.pct": 88.83
  },
  "add_chunk_offsets_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 9.0,
    "launch__grid_size": 8388608,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 84.1,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 83.91,
    "lts__t_sector_hit_rate.pct": 51.05
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
