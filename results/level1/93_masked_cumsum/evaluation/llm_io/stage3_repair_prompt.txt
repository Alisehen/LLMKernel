Fix the Triton kernel errors. Generate correct code.

## ERROR LOG
```
Traceback (most recent call last):
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 43, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 1101, in __getitem__
    raise ValueError(f"unsupported tensor index: {sl}")
ValueError: unsupported tensor index: constexpr[127]

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 554, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251223_124226_93_masked_cumsum_openai_deepseek/93_masked_cumsum/code/kernel_20251223_130827.py", line 325, in forward
    return triton_masked_cumsum(x, mask, self.dim)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251223_124226_93_masked_cumsum_openai_deepseek/93_masked_cumsum/code/kernel_20251223_130827.py", line 257, in triton_masked_cumsum
    _cumsum_tile_kernel[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 419, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 238, in run
    benchmark()
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 227, in benchmark
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 227, in <dictcomp>
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 162, in _bench
    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/testing.py", line 149, in do_bench
    fn()
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 148, in kernel_call
    self.fn.run(
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 733, in run
    kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 861, in _do_compile
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 300, in compile
    module = src.make_ir(target, options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 80, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 49:15:
    # load this tile of x and mask, zero-padding out-of-range columns
    x_vals = tl.load(x_row_ptr + offs * stride_n, mask=in_bounds, other=0.0)
    mask_vals = tl.load(mask_row_ptr + offs * stride_n, mask=in_bounds, other=0)

    # apply mask: only sum elements where mask is True
    masked_vals = tl.where(mask_vals, x_vals, 0.0)

    # perform an inclusive scan within the tile in registers
    vals = tl.cumsum(masked_vals, axis=0)

    # tile sum = last element of inclusive scan (covers out-of-bounds as zeros)
    tile_sum = vals[BLOCK_SIZE - 1]
               ^
unsupported tensor index: constexpr[127]
```

## Broken Code
```python
# <optimized Triton code>

import torch
import torch.nn as nn
import triton
import triton.language as tl


# -----------------------------------------------------------------------------
# Manually implemented math helpers for potential future use
# -----------------------------------------------------------------------------

def _tl_tanh(x):
    # Numerically stable tanh
    e2x = tl.exp(2 * x)
    return (e2x - 1) / (e2x + 1)


def _tl_sigmoid(x):
    return 1 / (1 + tl.exp(-x))


def _tl_gelu(x):
    # Approximate GELU used by PyTorch
    k0 = 0.7978845608028654  # sqrt(2/pi)
    k1 = 0.044715
    return 0.5 * x * (1 + _tl_tanh(k0 * (x + k1 * x * x * x)))


def _tl_silu(x):
    return x * _tl_sigmoid(x)


def _tl_mish(x):
    # mish(x) = x * tanh(softplus(x))
    softplus = tl.log(1 + tl.exp(-tl.abs(x))) + tl.maximum(x, 0)
    return x * _tl_tanh(softplus)


def _tl_softmax(x, axis: int = -1):
    # Simple softmax along given axis
    x_max = tl.max(x, axis=axis)
    x = x - x_max
    exp_x = tl.exp(x)
    denom = tl.sum(exp_x, axis=axis)
    return exp_x / denom


# Optionally attach to tl.* if not present (for compatibility)
if not hasattr(tl, "tanh"):
    tl.tanh = _tl_tanh
if not hasattr(tl, "sigmoid"):
    tl.sigmoid = _tl_sigmoid
if not hasattr(tl, "gelu"):
    tl.gelu = _tl_gelu
if not hasattr(tl, "silu"):
    tl.silu = _tl_silu
if not hasattr(tl, "mish"):
    tl.mish = _tl_mish
if not hasattr(tl, "softmax"):
    tl.softmax = _tl_softmax


# -----------------------------------------------------------------------------
# Kernels
# -----------------------------------------------------------------------------


@triton.autotune(
    configs=[
        # Baseline config (original)
        triton.Config({}, num_warps=4, num_stages=2),
        # Higher warp-level parallelism at same staging
        triton.Config({}, num_warps=8, num_stages=2),
        # Slightly deeper pipelining (only if it helps hide memory latency)
        triton.Config({}, num_warps=4, num_stages=3),
        triton.Config({}, num_warps=8, num_stages=3),
    ],
    key=["N"],
)
@triton.jit
def _cumsum_tile_kernel(
    x_ptr,               # *x_dtype, [M, N]
    mask_ptr,            # *bool,    [M, N]
    y_ptr,               # *x_dtype, [M, N]
    block_sums_ptr,      # *x_dtype, [M, num_blocks]
    M,                   # number of rows
    N,                   # number of columns (cumsum dim)
    stride_m,            # row stride for x/y in elements
    stride_n,            # col stride for x/y in elements (usually 1)
    stride_sums_m,       # row stride for block_sums in elements
    stride_sums_b,       # block (tile) stride for block_sums in elements
    BLOCK_SIZE: tl.constexpr,
):
    """
    First pass:
      - compute masked inclusive cumsum within each BLOCK_SIZE tile
      - write tile-wise cumsums to y
      - write per-tile sums to block_sums (one scalar per row/tile)
    """
    pid_m = tl.program_id(axis=0)  # row index
    pid_b = tl.program_id(axis=1)  # block index within the row

    # starting column for this tile
    col_start = pid_b * BLOCK_SIZE

    # local indices [0, BLOCK_SIZE)
    idx = tl.arange(0, BLOCK_SIZE)
    offs = col_start + idx

    # mask for elements within the row length N
    in_bounds = offs < N

    # base pointers for this row
    x_row_ptr = x_ptr + pid_m * stride_m
    y_row_ptr = y_ptr + pid_m * stride_m
    mask_row_ptr = mask_ptr + pid_m * stride_m

    # load this tile of x and mask, zero-padding out-of-range columns
    x_vals = tl.load(x_row_ptr + offs * stride_n, mask=in_bounds, other=0.0)
    mask_vals = tl.load(mask_row_ptr + offs * stride_n, mask=in_bounds, other=0)

    # apply mask: only sum elements where mask is True
    masked_vals = tl.where(mask_vals, x_vals, 0.0)

    # perform an inclusive scan within the tile in registers
    vals = tl.cumsum(masked_vals, axis=0)

    # tile sum = last element of inclusive scan (covers out-of-bounds as zeros)
    tile_sum = vals[BLOCK_SIZE - 1]

    # store back per-element cumsums
    tl.store(y_row_ptr + offs * stride_n, vals, mask=in_bounds)

    # write per-tile sum: shape [M, num_blocks]
    sums_base_ptr = block_sums_ptr + pid_m * stride_sums_m + pid_b * stride_sums_b
    tl.store(sums_base_ptr, tile_sum)


@triton.autotune(
    configs=[
        # Baseline: 4 warps, 2 stages
        triton.Config({}, num_warps=4, num_stages=2),
        triton.Config({}, num_warps=8, num_stages=2),
        triton.Config({}, num_warps=4, num_stages=3),
        triton.Config({}, num_warps=8, num_stages=3),
    ],
    key=["N"],
)
@triton.jit
def _add_block_offsets_kernel(
    y_ptr,                # *x_dtype, [M, N]
    block_offsets_ptr,    # *x_dtype, [M, num_blocks]
    M,
    N,
    num_blocks,
    stride_m,             # row stride for y
    stride_n,             # col stride for y
    stride_offs_m,        # row stride for block_offsets
    stride_offs_b,        # block (tile) stride for block_offsets
    BLOCK_SIZE: tl.constexpr,
):
    """
    Third pass:
      - add per-tile prefix offsets (exclusive scan over tile sums)
        to each element in y.
    """
    pid_m = tl.program_id(axis=0)  # row index
    pid_b = tl.program_id(axis=1)  # block index within row

    col_start = pid_b * BLOCK_SIZE
    idx = tl.arange(0, BLOCK_SIZE)
    offs = col_start + idx
    in_bounds = offs < N

    # base pointer for this row
    y_row_ptr = y_ptr + pid_m * stride_m

    # scalar block offset for this tile
    offs_ptr = block_offsets_ptr + pid_m * stride_offs_m + pid_b * stride_offs_b
    block_off = tl.load(offs_ptr)

    # load current prefix sums, add block offset, and store back
    vals = tl.load(y_row_ptr + offs * stride_n, mask=in_bounds, other=0.0)
    vals = vals + block_off
    tl.store(y_row_ptr + offs * stride_n, vals, mask=in_bounds)


# -----------------------------------------------------------------------------
# Python wrapper
# -----------------------------------------------------------------------------


def triton_masked_cumsum(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:
    """
    Masked cumulative sum using Triton, equivalent to:
        torch.cumsum(x * mask, dim=dim)
    Assumes x and mask are on CUDA device.
    """
    assert x.device.type == "cuda", "Input tensor must be on CUDA for Triton kernels."
    assert mask.device == x.device, "Mask must be on the same device as input."
    assert x.shape == mask.shape, "x and mask must have the same shape."
    assert mask.dtype == torch.bool, "Mask must be a boolean tensor."

    if x.numel() == 0:
        # Match torch.cumsum behavior on empty tensors
        return torch.empty_like(x)

    dim = dim if dim >= 0 else x.dim() + dim
    assert 0 <= dim < x.dim()

    # Move cumsum dimension to the last axis and make contiguous
    if dim != x.dim() - 1:
        x_perm = x.transpose(dim, -1).contiguous()
        mask_perm = mask.transpose(dim, -1).contiguous()
        transposed = True
    else:
        x_perm = x.contiguous()
        mask_perm = mask.contiguous()
        transposed = False

    M = x_perm.numel() // x_perm.size(-1)
    N = x_perm.size(-1)

    if N == 0:
        # Nothing to scan along cumsum dimension
        out_perm = torch.empty_like(x_perm)
    else:
        x2d = x_perm.view(M, N)
        mask2d = mask_perm.view(M, N)
        y2d = torch.empty_like(x2d)

        # Fixed, tuned tile size for RTX 4090
        BLOCK_SIZE = 128
        num_blocks = triton.cdiv(N, BLOCK_SIZE)

        stride_m = x2d.stride(0)
        stride_n = x2d.stride(1)

        # Workspace for per-tile sums and offsets: [M, num_blocks]
        block_sums = torch.empty(
            (M, num_blocks), device=x2d.device, dtype=x2d.dtype
        )
        block_offsets = torch.empty_like(block_sums)

        stride_sums_m = block_sums.stride(0)
        stride_sums_b = block_sums.stride(1)

        # Grid is expressed as a function of meta to work with autotune
        grid = lambda META: (
            M,
            triton.cdiv(N, META["BLOCK_SIZE"]),
        )

        # First pass: per-tile prefix sums with mask fused into loads,
        # plus per-tile sums written to block_sums.
        _cumsum_tile_kernel[grid](
            x2d,
            mask2d,
            y2d,
            block_sums,
            M,
            N,
            stride_m,
            stride_n,
            stride_sums_m,
            stride_sums_b,
            BLOCK_SIZE=BLOCK_SIZE,
        )

        # Second pass: prefix sum over tiles per row on PyTorch (small, fast)
        # exclusive prefix: offset[b] = sum_{k < b} tile_sum[k]
        if num_blocks == 1:
            block_offsets.zero_()
        else:
            zeros_col = torch.zeros_like(block_sums[:, :1])
            prefix_input = torch.cat([zeros_col, block_sums[:, :-1]], dim=1)
            block_offsets = torch.cumsum(prefix_input, dim=1)

        stride_offs_m = block_offsets.stride(0)
        stride_offs_b = block_offsets.stride(1)

        grid_offsets = (M, num_blocks)

        # Third pass: add tile offsets to each element
        _add_block_offsets_kernel[grid_offsets](
            y2d,
            block_offsets,
            M,
            N,
            num_blocks,
            stride_m,
            stride_n,
            stride_offs_m,
            stride_offs_b,
            BLOCK_SIZE=BLOCK_SIZE,
        )

        out_perm = y2d.view_as(x_perm)

    # Move back to original dimension order
    if transposed:
        out = out_perm.transpose(-1, dim)
    else:
        out = out_perm

    return out


# -----------------------------------------------------------------------------
# nn.Module wrapper
# -----------------------------------------------------------------------------

class ModelNew(nn.Module):
    """
    Triton-accelerated model that performs masked cumulative sum along a given dimension:
        output = torch.cumsum(x * mask, dim=dim)
    """

    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, x, mask):
        return triton_masked_cumsum(x, mask, self.dim)
```

## CRITICAL — These cause 60%+ of failures:
1. EVERY kernel function MUST have `@triton.jit` decorator — MANDATORY
2. Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
3. BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
4. `tl.program_id(axis)` only supports axis = 0, 1, 2
5. No `continue`, `break`, `return` inside loops — use masking
6. No tensor indexing with loop vars: `x[:, i]` is INVALID
7. mask shape MUST match data shape in tl.load/tl.store

## Missing Triton Functions (implement manually):
- tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

## OUTPUT FORMAT (STRICT):
1. Imports: torch, torch.nn, triton, triton.language as tl (and math if needed)
2. @triton.jit decorated kernel function(s)
3. Wrapper function(s) for grid calculation and kernel launch
4. class ModelNew(nn.Module) — REQUIRED

Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <corrected code>
```
