```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def _cumsum_tile_kernel(
    x_ptr,               # pointer to input [M, N]
    y_ptr,               # pointer to output [M, N]
    block_sums_ptr,      # pointer to [M, num_blocks] (tile sums)
    M,                   # number of rows
    N,                   # number of columns (cumsum dim)
    num_blocks,          # number of tiles per row = ceil_div(N, BLOCK_SIZE)
    stride_m,            # stride between rows in elements
    stride_n,            # stride between columns in elements (usually 1)
    BLOCK_SIZE: tl.constexpr,
):
    pid_m = tl.program_id(axis=0)  # row index
    pid_b = tl.program_id(axis=1)  # block index within the row

    # starting column for this tile
    col_start = pid_b * BLOCK_SIZE
    offs = col_start + tl.arange(0, BLOCK_SIZE)

    # mask for elements within the row length N
    mask = offs < N

    # base pointers for this row
    x_row_ptr = x_ptr + pid_m * stride_m
    y_row_ptr = y_ptr + pid_m * stride_m

    # load this tile, zero-padding out-of-range columns
    vals = tl.load(x_row_ptr + offs * stride_n, mask=mask, other=0.0)

    # inclusive scan within the tile (BLOCK_SIZE must be 256 here)
    idx = tl.arange(0, BLOCK_SIZE)
    # Hillis-Steele scan, unrolled for BLOCK_SIZE = 256
    vals_shift = tl.where(idx >= 1, vals[idx - 1], 0.0)
    vals = vals + vals_shift
    vals_shift = tl.where(idx >= 2, vals[idx - 2], 0.0)
    vals = vals + vals_shift
    vals_shift = tl.where(idx >= 4, vals[idx - 4], 0.0)
    vals = vals + vals_shift
    vals_shift = tl.where(idx >= 8, vals[idx - 8], 0.0)
    vals = vals + vals_shift
    vals_shift = tl.where(idx >= 16, vals[idx - 16], 0.0)
    vals = vals + vals_shift
    vals_shift = tl.where(idx >= 32, vals[idx - 32], 0.0)
    vals = vals + vals_shift
    vals_shift = tl.where(idx >= 64, vals[idx - 64], 0.0)
    vals = vals + vals_shift
    vals_shift = tl.where(idx >= 128, vals[idx - 128], 0.0)
    vals = vals + vals_shift

    # store local prefix sums for this tile
    tl.store(y_row_ptr + offs * stride_n, vals, mask=mask)

    # tile sum is the last element (includes all valid elements in this tile)
    tile_sum = vals[BLOCK_SIZE - 1]

    # store tile sum into [M, num_blocks]
    bs_offset = pid_m * num_blocks + pid_b
    tl.store(block_sums_ptr + bs_offset, tile_sum)


@triton.jit
def _add_block_offsets_kernel(
    y_ptr,                # pointer to output [M, N]
    block_offsets_ptr,    # pointer to [M, num_blocks] (exclusive prefix of tile sums)
    M,
    N,
    num_blocks,
    stride_m,
    stride_n,
    BLOCK_SIZE: tl.constexpr,
):
    pid_m = tl.program_id(axis=0)  # row index
    pid_b = tl.program_id(axis=1)  # block index within row

    col_start = pid_b * BLOCK_SIZE
    offs = col_start + tl.arange(0, BLOCK_SIZE)
    mask = offs < N

    # base pointer for this row
    y_row_ptr = y_ptr + pid_m * stride_m

    # scalar block offset for this tile
    bs_offset = pid_m * num_blocks + pid_b
    block_off = tl.load(block_offsets_ptr + bs_offset)

    # load current prefix sums, add block offset, and store back
    vals = tl.load(y_row_ptr + offs * stride_n, mask=mask, other=0.0)
    vals = vals + block_off
    tl.store(y_row_ptr + offs * stride_n, vals, mask=mask)


def triton_masked_cumsum(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:
    """
    Masked cumulative sum using Triton, equivalent to torch.cumsum(x * mask, dim=dim).
    Assumes x and mask are on CUDA device.
    """
    assert x.device.type == "cuda", "Input tensor must be on CUDA for Triton kernels."
    assert mask.device == x.device, "Mask must be on the same device as input."
    assert x.shape == mask.shape, "x and mask must have the same shape."

    # Apply mask (boolean -> same dtype as x)
    x_masked = x * mask.to(x.dtype)

    if x_masked.numel() == 0:
        return x_masked

    dim = dim if dim >= 0 else x_masked.dim() + dim
    assert 0 <= dim < x_masked.dim()

    # Move cumsum dimension to the last axis and make contiguous
    if dim != x_masked.dim() - 1:
        x_perm = x_masked.transpose(dim, -1).contiguous()
        transposed = True
        inv_dim = dim  # original position of last axis after inverse transpose
    else:
        x_perm = x_masked.contiguous()
        transposed = False
        inv_dim = None  # not used

    M = x_perm.numel() // x_perm.size(-1)
    N = x_perm.size(-1)

    if N == 0:
        # Nothing to scan along cumsum dimension
        out_perm = x_perm
    else:
        x2d = x_perm.view(M, N)
        y2d = torch.empty_like(x2d)

        BLOCK_SIZE = 256
        num_blocks = triton.cdiv(N, BLOCK_SIZE)

        # First pass: per-tile prefix sums and tile sums
        block_sums = torch.empty(
            (M, num_blocks),
            device=x.device,
            dtype=x.dtype,
        )

        stride_m = x2d.stride(0)
        stride_n = x2d.stride(1)

        grid = (M, num_blocks)
        _cumsum_tile_kernel[grid](
            x2d,
            y2d,
            block_sums,
            M,
            N,
            num_blocks,
            stride_m,
            stride_n,
            BLOCK_SIZE=BLOCK_SIZE,
        )

        # Second pass: prefix sum over tiles per row on PyTorch (small, fast)
        # exclusive prefix: offset[b] = sum_{k < b} tile_sum[k]
        block_offsets = torch.cumsum(block_sums, dim=1) - block_sums

        # Third pass: add tile offsets to each element
        _add_block_offsets_kernel[grid](
            y2d,
            block_offsets,
            M,
            N,
            num_blocks,
            stride_m,
            stride_n,
            BLOCK_SIZE=BLOCK_SIZE,
        )

        out_perm = y2d.view_as(x_perm)

    # Move back to original dimension order
    if transposed:
        # If we transposed (dim, -1) before, invert that transpose
        out = out_perm.transpose(-1, dim)
    else:
        out = out_perm

    return out


class ModelNew(nn.Module):
    """
    Triton-accelerated model that performs masked cumulative sum along a given dimension:
        output = torch.cumsum(x * mask, dim=dim)
    """

    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, x, mask):
        return triton_masked_cumsum(x, mask, self.dim)
```