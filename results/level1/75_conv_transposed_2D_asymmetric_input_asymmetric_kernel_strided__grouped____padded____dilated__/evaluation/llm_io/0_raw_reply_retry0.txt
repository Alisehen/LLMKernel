```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def conv_transpose2d_kernel(
    x_ptr,
    weight_ptr,
    bias_ptr,
    out_ptr,
    N,
    C_IN,
    H_IN,
    W_IN,
    C_OUT,
    H_OUT,
    W_OUT,
    K_H,
    K_W,
    STRIDE_H,
    STRIDE_W,
    PAD_H,
    PAD_W,
    DIL_H,
    DIL_W,
    GROUPS,
    HAS_BIAS: tl.constexpr,
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    total = N * C_OUT * H_OUT * W_OUT
    mask = offsets < total

    ow = offsets % W_OUT
    tmp = offsets // W_OUT
    oh = tmp % H_OUT
    tmp = tmp // H_OUT
    oc = tmp % C_OUT
    n = tmp // C_OUT

    ow = tl.where(mask, ow, 0)
    oh = tl.where(mask, oh, 0)
    oc = tl.where(mask, oc, 0)
    n = tl.where(mask, n, 0)

    oc_per_group = C_OUT // GROUPS
    ic_per_group = C_IN // GROUPS
    oc_group = oc // oc_per_group
    oc_in_group = oc % oc_per_group
    ic_group_start = oc_group * ic_per_group

    acc = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
    if HAS_BIAS:
        bias = tl.load(bias_ptr + oc, mask=mask, other=0.0)
        acc += bias.to(tl.float32)

    for kh in range(0, K_H):
        h_tmp = oh + PAD_H - kh * DIL_H
        mask_h = mask & (h_tmp >= 0)
        h_tmp_pos = tl.where(mask_h, h_tmp, 0)
        divisible_h = tl.where(mask_h, h_tmp_pos % STRIDE_H, 0)
        mask_h = mask_h & (divisible_h == 0)
        h_in = tl.where(mask_h, h_tmp_pos // STRIDE_H, 0)
        mask_h = mask_h & (h_in < H_IN)

        for kw in range(0, K_W):
            w_tmp = ow + PAD_W - kw * DIL_W
            mask_hw = mask_h & (w_tmp >= 0)
            w_tmp_pos = tl.where(mask_hw, w_tmp, 0)
            divisible_w = tl.where(mask_hw, w_tmp_pos % STRIDE_W, 0)
            mask_hw = mask_hw & (divisible_w == 0)
            w_in = tl.where(mask_hw, w_tmp_pos // STRIDE_W, 0)
            mask_hw = mask_hw & (w_in < W_IN)

            if not mask_hw.any():
                continue

            weight_base = ((kh * K_W) + kw)
            for ic_inner in range(0, ic_per_group):
                ic = ic_group_start + ic_inner
                inp_idx = (((n * C_IN + ic) * H_IN + h_in) * W_IN + w_in)
                val_in = tl.load(x_ptr + inp_idx, mask=mask_hw, other=0.0).to(tl.float32)

                w_offset = (((ic * oc_per_group) + oc_in_group) * K_H + kh) * K_W + kw
                val_w = tl.load(weight_ptr + w_offset).to(tl.float32)

                acc += val_in * val_w

    tl.store(out_ptr + offsets, acc, mask=mask)


def conv_transpose2d_triton(x, weight, bias, stride, padding, dilation, groups):
    assert x.is_contiguous()
    assert weight.is_contiguous()
    if bias is not None:
        assert bias.is_contiguous()

    stride_h, stride_w = stride
    pad_h, pad_w = padding
    dil_h, dil_w = dilation

    N, C_in, H_in, W_in = x.shape
    C_out = weight.shape[1] * groups
    K_h, K_w = weight.shape[2], weight.shape[3]

    H_out = (H_in - 1) * stride_h - 2 * pad_h + dil_h * (K_h - 1) + 1
    W_out = (W_in - 1) * stride_w - 2 * pad_w + dil_w * (K_w - 1) + 1

    out = torch.empty((N, C_out, H_out, W_out), device=x.device, dtype=x.dtype)

    BLOCK_SIZE = 128
    grid = lambda meta: (triton.cdiv(out.numel(), meta["BLOCK_SIZE"]),)

    conv_transpose2d_kernel[grid](
        x,
        weight,
        bias if bias is not None else x,
        out,
        N,
        C_in,
        H_in,
        W_in,
        C_out,
        H_out,
        W_out,
        K_h,
        K_w,
        stride_h,
        stride_w,
        pad_h,
        pad_w,
        dil_h,
        dil_w,
        groups,
        HAS_BIAS=bias is not None,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    return out


class ModelNew(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: tuple,
        stride: tuple = (1, 1),
        padding: tuple = (0, 0),
        dilation: tuple = (1, 1),
        groups: int = 1,
        bias: bool = False,
    ):
        super().__init__()
        if isinstance(kernel_size, int):
            kernel_size = (kernel_size, kernel_size)
        if isinstance(stride, int):
            stride = (stride, stride)
        if isinstance(padding, int):
            padding = (padding, padding)
        if isinstance(dilation, int):
            dilation = (dilation, dilation)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.groups = groups

        weight_shape = (in_channels, out_channels // groups, kernel_size[0], kernel_size[1])
        self.weight = nn.Parameter(torch.empty(weight_shape))
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.bias = None
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in = self.in_channels * self.kernel_size[0] * self.kernel_size[1]
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return conv_transpose2d_triton(
            x,
            self.weight,
            self.bias,
            self.stride,
            self.padding,
            self.dilation,
            self.groups,
        )
```