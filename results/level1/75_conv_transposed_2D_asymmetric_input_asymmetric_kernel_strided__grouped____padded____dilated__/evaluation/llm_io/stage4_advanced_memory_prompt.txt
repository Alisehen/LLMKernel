You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Final micro-tuning.

Params:
- num_warps ∈ {2,4,8,16}
- num_stages ∈ {2,3,4}

Rules:
- Change num_warps only if occupancy suggests it
- Change num_stages by ±1 only
- Do NOT modify grid or BLOCK sizes

Autotune:
- 3–6 nearby configs
- Always include original config
- Revert if gain <1–2% or unstable



[CURRENT CODE]
```python
# <complete ModelNew code with optimized Triton kernels>
import math
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def conv_transpose2d_kernel(
    x_ptr,
    weight_ptr,
    bias_ptr,
    out_ptr,
    N,
    C_IN,
    H_IN,
    W_IN,
    C_OUT,
    H_OUT,
    W_OUT,
    GROUPS,
    OC_PER_GROUP: tl.constexpr,
    IC_PER_GROUP: tl.constexpr,
    K_H: tl.constexpr,
    K_W: tl.constexpr,
    STRIDE_H: tl.constexpr,
    STRIDE_W: tl.constexpr,
    PAD_H: tl.constexpr,
    PAD_W: tl.constexpr,
    DIL_H: tl.constexpr,
    DIL_W: tl.constexpr,
    HAS_BIAS: tl.constexpr,
    OUT_DTYPE: tl.constexpr,
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    total = N * C_OUT * H_OUT * W_OUT
    mask = offsets < total

    ow = offsets % W_OUT
    tmp = offsets // W_OUT
    oh = tmp % H_OUT
    tmp = tmp // H_OUT
    oc = tmp % C_OUT
    n = tmp // C_OUT

    oc = tl.where(mask, oc, 0)
    ow = tl.where(mask, ow, 0)
    oh = tl.where(mask, oh, 0)
    n = tl.where(mask, n, 0)

    oc_group = oc // OC_PER_GROUP
    oc_in_group = oc % OC_PER_GROUP
    ic_group_start = oc_group * IC_PER_GROUP

    acc = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
    if HAS_BIAS:
        bias = tl.load(bias_ptr + oc, mask=mask, other=0.0).to(tl.float32)
        acc += bias

    for kh in tl.static_range(K_H):
        h_tmp = oh + PAD_H - kh * DIL_H
        mask_h = mask & (h_tmp >= 0)
        h_tmp = tl.where(mask_h, h_tmp, 0)
        mask_h = mask_h & ((h_tmp % STRIDE_H) == 0)
        h_in = tl.where(mask_h, h_tmp // STRIDE_H, 0)
        mask_h = mask_h & (h_in < H_IN)

        for kw in tl.static_range(K_W):
            w_tmp = ow + PAD_W - kw * DIL_W
            mask_hw = mask_h & (w_tmp >= 0)
            w_tmp = tl.where(mask_hw, w_tmp, 0)
            mask_hw = mask_hw & ((w_tmp % STRIDE_W) == 0)
            w_in = tl.where(mask_hw, w_tmp // STRIDE_W, 0)
            mask_hw = mask_hw & (w_in < W_IN)

            for ic_inner in tl.static_range(IC_PER_GROUP):
                ic = ic_group_start + ic_inner
                inp_idx = (((n * C_IN + ic) * H_IN + h_in) * W_IN + w_in)
                val_in = tl.load(x_ptr + inp_idx, mask=mask_hw, other=0.0).to(tl.float32)

                w_offset = ((((ic * OC_PER_GROUP) + oc_in_group) * K_H + kh) * K_W + kw)
                val_w = tl.load(weight_ptr + w_offset, mask=mask_hw, other=0.0).to(tl.float32)

                acc += val_in * val_w

    if OUT_DTYPE == tl.float32:
        out_val = acc
    elif OUT_DTYPE == tl.float16:
        out_val = acc.to(tl.float16)
    elif OUT_DTYPE == tl.bfloat16:
        out_val = acc.to(tl.bfloat16)
    else:
        out_val = acc

    tl.store(out_ptr + offsets, out_val, mask=mask)


def conv_transpose2d_triton(x, weight, bias, stride, padding, dilation, groups):
    assert x.is_contiguous()
    assert weight.is_contiguous()
    if bias is not None:
        assert bias.is_contiguous()

    stride_h, stride_w = stride
    pad_h, pad_w = padding
    dil_h, dil_w = dilation

    N, C_in, H_in, W_in = x.shape
    oc_per_group = weight.shape[1]
    C_out = oc_per_group * groups
    K_h, K_w = weight.shape[2], weight.shape[3]
    H_out = (H_in - 1) * stride_h - 2 * pad_h + dil_h * (K_h - 1) + 1
    W_out = (W_in - 1) * stride_w - 2 * pad_w + dil_w * (K_w - 1) + 1

    out = torch.empty((N, C_out, H_out, W_out), device=x.device, dtype=x.dtype)

    dtype_map = {
        torch.float32: tl.float32,
        torch.float16: tl.float16,
        torch.bfloat16: tl.bfloat16,
    }
    out_dtype = dtype_map[out.dtype]

    BLOCK_SIZE = 128
    grid = lambda meta: (triton.cdiv(out.numel(), meta["BLOCK_SIZE"]),)

    conv_transpose2d_kernel[grid](
        x,
        weight,
        bias if bias is not None else out,
        out,
        N,
        C_in,
        H_in,
        W_in,
        C_out,
        H_out,
        W_out,
        groups,
        oc_per_group,
        C_in // groups,
        K_h,
        K_w,
        stride_h,
        stride_w,
        pad_h,
        pad_w,
        dil_h,
        dil_w,
        bias is not None,
        out_dtype,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    return out


class ModelNew(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: tuple,
        stride: tuple = (1, 1),
        padding: tuple = (0, 0),
        dilation: tuple = (1, 1),
        groups: int = 1,
        bias: bool = False,
    ):
        super().__init__()
        if isinstance(kernel_size, int):
            kernel_size = (kernel_size, kernel_size)
        if isinstance(stride, int):
            stride = (stride, stride)
        if isinstance(padding, int):
            padding = (padding, padding)
        if isinstance(dilation, int):
            dilation = (dilation, dilation)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.groups = groups

        weight_shape = (in_channels, out_channels // groups, kernel_size[0], kernel_size[1])
        self.weight = nn.Parameter(torch.empty(weight_shape))
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.bias = None
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in = self.in_channels * self.kernel_size[0] * self.kernel_size[1]
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return conv_transpose2d_triton(
            x,
            self.weight,
            self.bias,
            self.stride,
            self.padding,
            self.dilation,
            self.groups,
        )
```

[NCU PROFILING METRICS]
{
  "conv_transpose2d_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 94.13,
    "launch__grid_size": 1574896,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 78.15,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 7.54,
    "lts__t_sector_hit_rate.pct": 98.52
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
