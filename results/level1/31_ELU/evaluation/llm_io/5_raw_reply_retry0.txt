```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        # BLOCK_SIZE=256 variants
        triton.Config({"BLOCK_SIZE": 256}, num_warps=2, num_stages=2),
        triton.Config({"BLOCK_SIZE": 256}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_SIZE": 256}, num_warps=8, num_stages=2),
        triton.Config({"BLOCK_SIZE": 256}, num_warps=4, num_stages=3),
        
        # BLOCK_SIZE=512 variants
        triton.Config({"BLOCK_SIZE": 512}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_SIZE": 512}, num_warps=8, num_stages=2),
        triton.Config({"BLOCK_SIZE": 512}, num_warps=4, num_stages=3),
        triton.Config({"BLOCK_SIZE": 512}, num_warps=8, num_stages=3),
        
        # BLOCK_SIZE=1024 variants
        triton.Config({"BLOCK_SIZE": 1024}, num_warps=8, num_stages=2),
        triton.Config({"BLOCK_SIZE": 1024}, num_warps=16, num_stages=2),
        triton.Config({"BLOCK_SIZE": 1024}, num_warps=8, num_stages=3),
        triton.Config({"BLOCK_SIZE": 1024}, num_warps=16, num_stages=3),
        
        # BLOCK_SIZE=2048 variants
        triton.Config({"BLOCK_SIZE": 2048}, num_warps=8, num_stages=3),
        triton.Config({"BLOCK_SIZE": 2048}, num_warps=16, num_stages=3),
        triton.Config({"BLOCK_SIZE": 2048}, num_warps=8, num_stages=4),
        triton.Config({"BLOCK_SIZE": 2048}, num_warps=16, num_stages=4),
    ],
    key=["n_elements"],
)
@triton.jit
def elu_kernel(
    x_ptr,
    output_ptr,
    alpha_val,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements

    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)
    
    # ELU: x if x > 0 else alpha * (exp(x) - 1)
    # Using fast math mode (implied by tl.exp)
    exp_x = tl.exp(x)
    elu_x = tl.where(x > 0, x, alpha_val * (exp_x - 1.0))
    
    tl.store(output_ptr + offsets, elu_x, mask=mask)


def triton_elu(x: torch.Tensor, alpha: float = 1.0) -> torch.Tensor:
    output = torch.empty_like(x)
    n_elements = output.numel()
    
    grid = lambda meta: (triton.cdiv(n_elements, meta["BLOCK_SIZE"]),)
    
    # Convert alpha to the same dtype as x for kernel
    alpha_val = torch.tensor(alpha, dtype=x.dtype, device=x.device).item()
    
    elu_kernel[grid](x, output, alpha_val, n_elements)
    
    return output


class ModelNew(nn.Module):
    def __init__(self, alpha: float = 1.0):
        super().__init__()
        self.alpha = alpha
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_elu(x, alpha=self.alpha)
```

This optimized version includes:

1. **Expanded autotune configurations**: Added 16 configs (4 per BLOCK_SIZE) with variations in `num_warps` and `num_stages` around the original values.

2. **Increased num_warps**: Added configurations with 16 warps (512 threads) for larger block sizes to potentially improve occupancy and hide memory latency.

3. **Adjusted num_stages**: Added configurations with Â±1 stage changes from the original values to better utilize registers and improve pipeline efficiency.

4. **Preserved kernel logic**: Kept the core ELU computation unchanged since it's already efficient with `tl.exp` and `tl.where`.

The autotuner will now test a wider range of configurations to find the optimal balance between warp count, pipeline stages, and block size for the Ada Lovelace architecture's characteristics (high memory bandwidth, 128 SMs, 1536 max threads per SM).