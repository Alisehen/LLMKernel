Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 536, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251213_160746_batch_range92to92_openai_deepseek/92_cumsum_exclusive/code/kernel_20251213_162118.py", line 148, in forward
    return triton_exclusive_cumsum(x, self.dim)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251213_160746_batch_range92to92_openai_deepseek/92_cumsum_exclusive/code/kernel_20251213_162118.py", line 134, in triton_exclusive_cumsum
    return triton_inclusive_cumsum(exclusive_input, dim)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251213_160746_batch_range92to92_openai_deepseek/92_cumsum_exclusive/code/kernel_20251213_162118.py", line 104, in triton_inclusive_cumsum
    y_2d = _triton_inclusive_cumsum_lastdim_2d(x_2d)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251213_160746_batch_range92to92_openai_deepseek/92_cumsum_exclusive/code/kernel_20251213_162118.py", line 75, in _triton_inclusive_cumsum_lastdim_2d
    _cumsum_rowwise_kernel[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 192, in run
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 192, in <dictcomp>
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 170, in _bench
    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/testing.py", line 145, in do_bench
    fn()
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 156, in kernel_call
    self.fn.run(
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 278, in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 81, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 26:4:
    """
    pid_row = tl.program_id(axis=0)

    row_x_ptr = x_ptr + pid_row * stride_x_row
    row_out_ptr = out_ptr + pid_row * stride_out_row

    offsets = tl.arange(0, BLOCK_SIZE)
    start = 0
    carry = 0  # running prefix sum for this row (same dtype as x)

    # Iterate over the row in BLOCK_SIZE-sized chunks
    while start < N:
    ^
AssertionError('Loop-carried variable carry has initial type int32 but is re-assigned to fp32 in loop! Please make sure that the type stays consistent.')

History Error:
None


PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    A model that performs an exclusive cumulative sum (does not include the current element).

    Parameters:
        dim (int): The dimension along which to perform the exclusive cumulative sum.
    """

    def __init__(self, dim):
        super(Model, self).__init__()
        self.dim = dim

    def forward(self, x):
        exclusive_cumsum = torch.cat((torch.zeros_like(x.select(self.dim, 0).unsqueeze(self.dim)), x), dim=self.dim)[:-1]
        return torch.cumsum(exclusive_cumsum, dim=self.dim)

batch_size = 32768
input_shape = (32768,)
dim = 1

def get_inputs():
    return [torch.rand(batch_size, *input_shape)]

def get_init_inputs():
    return [dim]
```

Broken Code:
```python
# <optimized Triton code>

import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE': 128}, num_warps=4, num_stages=1),
        triton.Config({'BLOCK_SIZE': 256}, num_warps=4, num_stages=2),
        triton.Config({'BLOCK_SIZE': 512}, num_warps=8, num_stages=2),
    ],
    key=['N'],
)
@triton.jit
def _cumsum_rowwise_kernel(
    x_ptr,          # *fptr: [B, N]
    out_ptr,        # *fptr: [B, N]
    N,              # int32: length of scan dimension
    stride_x_row,   # int32: stride between rows in x
    stride_out_row, # int32: stride between rows in out
    BLOCK_SIZE: tl.constexpr,
):
    """
    Single-pass inclusive cumsum along the last dimension for a 2D tensor [B, N].

    Each program handles one row (batch index), and iterates over the row in
    BLOCK_SIZE chunks, carrying a running sum across chunks. This minimizes
    global memory traffic: each element is read once and written once.
    """
    pid_row = tl.program_id(axis=0)

    row_x_ptr = x_ptr + pid_row * stride_x_row
    row_out_ptr = out_ptr + pid_row * stride_out_row

    offsets = tl.arange(0, BLOCK_SIZE)
    start = 0
    carry = 0  # running prefix sum for this row (same dtype as x)

    # Iterate over the row in BLOCK_SIZE-sized chunks
    while start < N:
        idx = start + offsets
        mask = idx < N

        x = tl.load(row_x_ptr + idx, mask=mask, other=0)
        # In-block inclusive scan, then add running prefix
        cumsum = tl.cumsum(x, axis=0) + carry
        tl.store(row_out_ptr + idx, cumsum, mask=mask)

        # Update running prefix with this block's total sum
        carry += tl.sum(x, axis=0)

        start += BLOCK_SIZE


def _triton_inclusive_cumsum_lastdim_2d(x_2d: torch.Tensor) -> torch.Tensor:
    """
    Inclusive cumsum along the last dimension of a 2D tensor using a single-pass
    Triton kernel. x_2d is expected to be contiguous with shape [B, N].
    """
    assert x_2d.dim() == 2
    B, N = x_2d.shape
    if N == 0:
        return x_2d.clone()

    out = torch.empty_like(x_2d)

    stride_x_row = x_2d.stride(0)
    stride_out_row = out.stride(0)

    # One program per row; kernel loops over N internally
    grid = (B,)
    _cumsum_rowwise_kernel[grid](
        x_2d,
        out,
        N,
        stride_x_row,
        stride_out_row,
    )

    return out


def triton_inclusive_cumsum(x: torch.Tensor, dim: int) -> torch.Tensor:
    """
    Inclusive cumsum along arbitrary dimension `dim` using the optimized Triton kernel.
    """
    if dim < 0:
        dim = x.dim() + dim

    # Move target dim to last, make contiguous, and flatten leading dims
    if dim != x.dim() - 1:
        x_moved = x.movedim(dim, -1)
    else:
        x_moved = x

    x_contig = x_moved.contiguous()
    last_dim = x_contig.shape[-1]
    B = x_contig.numel() // last_dim
    x_2d = x_contig.view(B, last_dim)

    y_2d = _triton_inclusive_cumsum_lastdim_2d(x_2d)
    y_contig = y_2d.view(x_contig.shape)

    # Move dimension back to original position
    if dim != x.dim() - 1:
        y = y_contig.movedim(-1, dim)
    else:
        y = y_contig

    return y


def triton_exclusive_cumsum(x: torch.Tensor, dim: int) -> torch.Tensor:
    """
    Exclusive cumsum matching the provided PyTorch reference:

        exclusive_cumsum = torch.cat(
            (torch.zeros_like(x.select(dim, 0).unsqueeze(dim)), x),
            dim=dim,
        )[:-1]
        return torch.cumsum(exclusive_cumsum, dim=dim)

    Only the final torch.cumsum is replaced with the Triton implementation.
    """
    if dim < 0:
        dim = x.dim() + dim

    zeros = torch.zeros_like(x.select(dim, 0).unsqueeze(dim))
    # Note: `[:-1]` slices along the first dimension, exactly as in the reference.
    exclusive_input = torch.cat((zeros, x), dim=dim)[:-1]
    return triton_inclusive_cumsum(exclusive_input, dim)


class ModelNew(nn.Module):
    """
    A model that performs an exclusive cumulative sum (does not include the current element),
    matching the semantics of the given PyTorch reference model.
    """

    def __init__(self, dim):
        super(ModelNew, self).__init__()
        self.dim = dim

    def forward(self, x):
        return triton_exclusive_cumsum(x, self.dim)
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl, AND any other modules used (e.g., import math if using math.sqrt)
   2. @triton.jit decorated kernel function(s) — NO continue/break/return inside loops (use masking)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels — THIS CLASS IS REQUIRED
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes
4. Ensure ALL imports are included at the top (common mistake: forgetting `import math`)

```python
# <corrected code>
```
