Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 533, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 131, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_085222_batch_range20to30_deepseek_deepseek/33_BatchNorm/code/kernel_20251212_101010.py", line 344, in forward
    return triton_batch_norm2d(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_085222_batch_range20to30_deepseek_deepseek/33_BatchNorm/code/kernel_20251212_101010.py", line 306, in triton_batch_norm2d
    tuned_batch_norm_apply[grid_apply](
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/runtime/jit.py", line 419, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 238, in run
    benchmark()
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 227, in benchmark
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 227, in <dictcomp>
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/hyc/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 135, in _bench
    conflicts = meta.keys() & config.kwargs.keys()
                              ^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'kwargs'

History Error:
None


PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Simple model that performs Batch Normalization.
    """
    def __init__(self, num_features: int):
        """
        Initializes the BatchNorm layer.

        Args:
            num_features (int): Number of features in the input tensor.
        """
        super(Model, self).__init__()
        self.bn = nn.BatchNorm2d(num_features=num_features)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Applies Batch Normalization to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, num_features, *).

        Returns:
            torch.Tensor: Output tensor with Batch Normalization applied, same shape as input.
        """
        return self.bn(x)

batch_size = 64
features = 64
dim1 = 512
dim2 = 512

def get_inputs():
    x = torch.rand(batch_size, features, dim1, dim2)
    return [x]

def get_init_inputs():
    return [features]
```

Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def batch_norm_reduce_kernel(
    x_ptr,
    sum_ptr,
    sum_sq_ptr,
    N: tl.constexpr,
    C: tl.constexpr,
    H: tl.constexpr,
    W: tl.constexpr,
    BLOCK_SIZE: tl.constexpr,
):
    """
    Compute sum and sum of squares per channel using atomic operations.
    Optimized for channel-parallel reduction with minimal memory traffic.
    """
    # Parallelize over channels and spatial positions
    pid = tl.program_id(axis=0)
    num_channels_per_block = tl.cdiv(C, BLOCK_SIZE)
    channel_group = pid // num_channels_per_block
    channel_idx = (pid % num_channels_per_block) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    channel_mask = channel_idx < C
    
    # Initialize accumulators
    sum_acc = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
    sum_sq_acc = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
    
    # Strides for tensor indexing
    batch_stride = C * H * W
    channel_stride = H * W
    
    # Reduce over batch and spatial dimensions
    for b in range(N):
        for hw in range(H * W):
            # Compute base index
            base_idx = b * batch_stride + channel_idx * channel_stride + hw
            # Load with mask
            x_val = tl.load(x_ptr + base_idx, mask=channel_mask)
            # Accumulate
            sum_acc += x_val
            sum_sq_acc += x_val * x_val
    
    # Atomic add to global memory
    tl.atomic_add(sum_ptr + channel_idx, sum_acc, mask=channel_mask)
    tl.atomic_add(sum_sq_ptr + channel_idx, sum_sq_acc, mask=channel_mask)


@triton.jit
def batch_norm_apply_kernel(
    x_ptr,
    y_ptr,
    mean_ptr,
    var_ptr,
    weight_ptr,
    bias_ptr,
    eps: tl.constexpr,
    N: tl.constexpr,
    C: tl.constexpr,
    H: tl.constexpr,
    W: tl.constexpr,
    TRAINING: tl.constexpr,
    BLOCK_SIZE_C: tl.constexpr,
    BLOCK_SIZE_HW: tl.constexpr,
    VEC_SIZE: tl.constexpr,
):
    """
    Apply batch normalization with optimal memory access patterns.
    Uses vectorized operations and precomputes normalization factors.
    
    BLOCK_SIZE_C: block size for channel dimension (power of 2, 16-128)
    BLOCK_SIZE_HW: block size for spatial dimension (power of 2, 16-128)
    VEC_SIZE: vector load/store size (1, 2, or 4)
    """
    # 2D grid: channels × spatial positions
    pid_c = tl.program_id(axis=0)
    pid_hw = tl.program_id(axis=1)
    
    # Channel offset
    channel_offset = pid_c * BLOCK_SIZE_C
    channel_idx = channel_offset + tl.arange(0, BLOCK_SIZE_C)
    channel_mask = channel_idx < C
    
    # Spatial offset - process VEC_SIZE elements at once
    spatial_offset = pid_hw * BLOCK_SIZE_HW * VEC_SIZE
    spatial_idx_base = spatial_offset + tl.arange(0, BLOCK_SIZE_HW) * VEC_SIZE
    spatial_mask_base = spatial_idx_base < H * W
    
    # Load normalization parameters once per channel group
    if TRAINING:
        mean = tl.load(mean_ptr + channel_idx, mask=channel_mask, other=0.0)
        var = tl.load(var_ptr + channel_idx, mask=channel_mask, other=0.0)
    else:
        mean = tl.load(mean_ptr + channel_idx, mask=channel_mask, other=0.0)
        var = tl.load(var_ptr + channel_idx, mask=channel_mask, other=0.0)
    
    weight = tl.load(weight_ptr + channel_idx, mask=channel_mask, other=0.0)
    bias = tl.load(bias_ptr + channel_idx, mask=channel_mask, other=0.0)
    
    # Precompute normalization factor with stability epsilon
    inv_std = tl.math.rsqrt(var + eps)
    scale = weight * inv_std
    shift = bias - mean * scale
    
    # Process batches with vectorized loads/stores
    batch_stride = C * H * W
    channel_stride = H * W
    
    for b in range(N):
        for vec_offset in range(VEC_SIZE):
            # Current spatial index for this vector element
            spatial_idx = spatial_idx_base + vec_offset
            spatial_mask = spatial_mask_base & (spatial_idx < H * W)
            
            # Base index calculation
            base_idx = (b * batch_stride + 
                       channel_idx[:, None] * channel_stride + 
                       spatial_idx[None, :])
            
            # Vectorized load with masks
            x = tl.load(x_ptr + base_idx, 
                        mask=channel_mask[:, None] & spatial_mask[None, :], 
                        other=0.0)
            
            # Apply normalization (vectorized over channels × spatial)
            y = x * scale[:, None] + shift[:, None]
            
            # Store result
            tl.store(y_ptr + base_idx, y,
                     mask=channel_mask[:, None] & spatial_mask[None, :])


@triton.jit
def batch_norm_update_kernel(
    running_mean_ptr,
    running_var_ptr,
    mean_ptr,
    var_ptr,
    momentum: tl.constexpr,
    C: tl.constexpr,
    BLOCK_SIZE: tl.constexpr,
):
    """
    Update running statistics with momentum.
    Optimized for parallel channel updates.
    """
    pid = tl.program_id(axis=0)
    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = offsets < C
    
    # Load current running statistics
    running_mean = tl.load(running_mean_ptr + offsets, mask=mask)
    running_var = tl.load(running_var_ptr + offsets, mask=mask)
    
    # Load batch statistics
    mean = tl.load(mean_ptr + offsets, mask=mask)
    var = tl.load(var_ptr + offsets, mask=mask)
    
    # Update with momentum
    running_mean = momentum * running_mean + (1.0 - momentum) * mean
    running_var = momentum * running_var + (1.0 - momentum) * var
    
    # Store back
    tl.store(running_mean_ptr + offsets, running_mean, mask=mask)
    tl.store(running_var_ptr + offsets, running_var, mask=mask)


def triton_batch_norm2d(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    running_mean: torch.Tensor,
    running_var: torch.Tensor,
    training: bool = True,
    momentum: float = 0.1,
    eps: float = 1e-5,
) -> torch.Tensor:
    """
    Triton-optimized batch normalization forward pass.
    """
    N, C, H, W = x.shape
    
    # Output tensor
    y = torch.empty_like(x)
    
    if training:
        # Training mode: compute batch statistics
        
        # Allocate temporary buffers
        sum_tensor = torch.zeros(C, device=x.device, dtype=torch.float32)
        sum_sq_tensor = torch.zeros(C, device=x.device, dtype=torch.float32)
        
        # Compute sum and sum of squares
        BLOCK_SIZE_REDUCE = 256
        grid_reduce = lambda meta: (triton.cdiv(C, meta['BLOCK_SIZE']) * C,)
        batch_norm_reduce_kernel[grid_reduce](
            x,
            sum_tensor,
            sum_sq_tensor,
            N, C, H, W,
            BLOCK_SIZE=BLOCK_SIZE_REDUCE,
        )
        
        # Compute mean and variance
        count = N * H * W
        mean_tensor = sum_tensor / count
        var_tensor = (sum_sq_tensor / count) - (mean_tensor * mean_tensor)
        
        # Update running statistics
        BLOCK_SIZE_UPDATE = 512
        grid_update = lambda meta: (triton.cdiv(C, meta['BLOCK_SIZE']),)
        batch_norm_update_kernel[grid_update](
            running_mean,
            running_var,
            mean_tensor,
            var_tensor,
            momentum,
            C,
            BLOCK_SIZE=BLOCK_SIZE_UPDATE,
        )
        
        # Apply normalization with batch statistics
        apply_mean = mean_tensor
        apply_var = var_tensor
    else:
        # Inference mode: use running statistics
        apply_mean = running_mean
        apply_var = running_var
    
    # Apply normalization with autotuned configurations
    # Configs optimized for Ada Lovelace (4090)
    configs = [
        {'BLOCK_SIZE_C': 64, 'BLOCK_SIZE_HW': 32, 'VEC_SIZE': 4},
        {'BLOCK_SIZE_C': 128, 'BLOCK_SIZE_HW': 16, 'VEC_SIZE': 4},
        {'BLOCK_SIZE_C': 32, 'BLOCK_SIZE_HW': 64, 'VEC_SIZE': 2},
        {'BLOCK_SIZE_C': 16, 'BLOCK_SIZE_HW': 128, 'VEC_SIZE': 1},
    ]
    
    @triton.autotune(
        configs=configs,
        key=['C', 'H', 'W'],
    )
    @triton.jit
    def tuned_batch_norm_apply(
        x_ptr, y_ptr, mean_ptr, var_ptr, weight_ptr, bias_ptr,
        eps, N, C, H, W, TRAINING,
        BLOCK_SIZE_C: tl.constexpr, BLOCK_SIZE_HW: tl.constexpr, VEC_SIZE: tl.constexpr
    ):
        # Same kernel body as batch_norm_apply_kernel
        pid_c = tl.program_id(0)
        pid_hw = tl.program_id(1)
        
        channel_offset = pid_c * BLOCK_SIZE_C
        channel_idx = channel_offset + tl.arange(0, BLOCK_SIZE_C)
        channel_mask = channel_idx < C
        
        spatial_offset = pid_hw * BLOCK_SIZE_HW * VEC_SIZE
        spatial_idx_base = spatial_offset + tl.arange(0, BLOCK_SIZE_HW) * VEC_SIZE
        spatial_mask_base = spatial_idx_base < H * W
        
        if TRAINING:
            mean = tl.load(mean_ptr + channel_idx, mask=channel_mask, other=0.0)
            var = tl.load(var_ptr + channel_idx, mask=channel_mask, other=0.0)
        else:
            mean = tl.load(mean_ptr + channel_idx, mask=channel_mask, other=0.0)
            var = tl.load(var_ptr + channel_idx, mask=channel_mask, other=0.0)
        
        weight = tl.load(weight_ptr + channel_idx, mask=channel_mask, other=0.0)
        bias = tl.load(bias_ptr + channel_idx, mask=channel_mask, other=0.0)
        
        inv_std = tl.math.rsqrt(var + eps)
        scale = weight * inv_std
        shift = bias - mean * scale
        
        batch_stride = C * H * W
        channel_stride = H * W
        
        for b in range(N):
            for vec_offset in range(VEC_SIZE):
                spatial_idx = spatial_idx_base + vec_offset
                spatial_mask = spatial_mask_base & (spatial_idx < H * W)
                
                base_idx = (b * batch_stride + 
                           channel_idx[:, None] * channel_stride + 
                           spatial_idx[None, :])
                
                x = tl.load(x_ptr + base_idx, 
                            mask=channel_mask[:, None] & spatial_mask[None, :], 
                            other=0.0)
                
                y = x * scale[:, None] + shift[:, None]
                
                tl.store(y_ptr + base_idx, y,
                         mask=channel_mask[:, None] & spatial_mask[None, :])
    
    # Launch tuned kernel
    grid_apply = lambda meta: (
        triton.cdiv(C, meta['BLOCK_SIZE_C']),
        triton.cdiv(H * W, meta['BLOCK_SIZE_HW'] * meta['VEC_SIZE']),
    )
    
    tuned_batch_norm_apply[grid_apply](
        x, y, apply_mean, apply_var, weight, bias,
        eps, N, C, H, W, training,
    )
    
    return y


class ModelNew(nn.Module):
    """
    Triton-optimized BatchNorm2d replacement.
    """
    def __init__(self, num_features: int, eps: float = 1e-5, momentum: float = 0.1):
        super().__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        
        # Learnable parameters
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
        
        # Running statistics
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
        
        # For compatibility with PyTorch's BatchNorm
        self.track_running_stats = True

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Ensure tensors are contiguous and on correct device
        x = x.contiguous()
        self.weight.data = self.weight.data.contiguous()
        self.bias.data = self.bias.data.contiguous()
        self.running_mean = self.running_mean.contiguous()
        self.running_var = self.running_var.contiguous()
        
        # Apply Triton batch normalization
        return triton_batch_norm2d(
            x,
            self.weight,
            self.bias,
            self.running_mean,
            self.running_var,
            self.training,
            self.momentum,
            self.eps,
        )
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes

```python
# <corrected code>
```
