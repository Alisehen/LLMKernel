You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

**Focus**: Optimize grid layout and parallel work distribution.

**NCU Metrics to Check**:
• `sm__throughput.avg.pct_of_peak_sustained_elapsed`: Overall SM utilization (target: >60%)
• `launch__grid_size`: Total number of blocks (controlled by BLOCK_M/N)

**Guidelines**:
- 2D outputs → 2D grid `(cdiv(M, BLOCK_M), cdiv(N, BLOCK_N))`; 1D ops → 1D grid
- 3D outputs (e.g., batch×M×N) → 3D grid `(batch, cdiv(M, BLOCK_M), cdiv(N, BLOCK_N))`
- 4D+ outputs → **flatten** to 3D: `(batch*dim1, dim2, dim3)` or `(cdiv(batch*dim1, BLOCK), dim2, dim3)`
- Prefer adding parallelism via batch/head/expert dims before reducing BLOCK sizes
- Only change grid if SM utilization is clearly low; otherwise keep original
- Always verify `grid = (...)` matches kernel indexing logic

**Important Safety Rule**:
If unsure about grid mapping, do **not** modify it. Never use more than 3 dimensions!

**Autotune Tip (safe)**:
Try 2–3 small grid variants using minimal `@autotune` on **kernel function only**.



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def l1_norm_reduce_kernel(
    x_ptr,
    sum_ptr,
    stride_x0, stride_x1,
    B, D,
    BLOCK_SIZE_D: tl.constexpr,
):
    """
    First pass: compute sum of absolute values for each row.
    Each program processes one row (B parallel).
    """
    pid_b = tl.program_id(axis=0)
    
    if pid_b >= B:
        return
    
    row_ptr = x_ptr + pid_b * stride_x0
    row_sum = 0.0  # Scalar accumulator
    
    # Process row in chunks
    for d_start in range(0, D, BLOCK_SIZE_D):
        offsets = d_start + tl.arange(0, BLOCK_SIZE_D)
        mask = offsets < D
        
        x_chunk = tl.load(row_ptr + offsets * stride_x1, mask=mask, other=0.0)
        abs_chunk = tl.abs(x_chunk)
        # tl.sum returns a scalar when used on 1D arrays
        row_sum += tl.sum(abs_chunk, axis=0)
    
    # Store scalar result
    tl.store(sum_ptr + pid_b, row_sum)

@triton.jit
def l1_norm_divide_kernel(
    x_ptr,
    sum_ptr,
    output_ptr,
    stride_x0, stride_x1,
    stride_out0, stride_out1,
    B, D,
    BLOCK_SIZE_D: tl.constexpr,
):
    """
    Second pass: divide each element by (sum / D) for normalization.
    Each program processes a column chunk for all rows.
    """
    pid_d = tl.program_id(axis=0)
    
    d_start = pid_d * BLOCK_SIZE_D
    offsets = d_start + tl.arange(0, BLOCK_SIZE_D)
    col_mask = offsets < D
    
    # Process all rows for this column chunk
    for b in range(B):
        row_ptr = x_ptr + b * stride_x0
        out_row_ptr = output_ptr + b * stride_out0
        
        # Load row sum and compute mean
        row_sum = tl.load(sum_ptr + b)
        mean_val = row_sum / D
        
        # Load data, normalize, and store
        x_chunk = tl.load(row_ptr + offsets * stride_x1, mask=col_mask, other=0.0)
        normalized = x_chunk / mean_val
        tl.store(out_row_ptr + offsets * stride_out1, normalized, mask=col_mask)

@triton.jit
def l1_norm_fused_kernel(
    x_ptr,
    output_ptr,
    stride_x0, stride_x1,
    stride_out0, stride_out1,
    B, D,
    BLOCK_SIZE_D: tl.constexpr,
):
    """
    Fused kernel that computes L1 normalization in one pass.
    Each program processes one row.
    """
    pid_b = tl.program_id(axis=0)
    
    if pid_b >= B:
        return
    
    row_ptr = x_ptr + pid_b * stride_x0
    out_row_ptr = output_ptr + pid_b * stride_out0
    
    # Initialize sum in register
    row_sum = 0.0
    
    # First pass: compute sum of absolute values
    for d_start in range(0, D, BLOCK_SIZE_D):
        offsets = d_start + tl.arange(0, BLOCK_SIZE_D)
        mask = offsets < D
        
        x_chunk = tl.load(row_ptr + offsets * stride_x1, mask=mask, other=0.0)
        abs_chunk = tl.abs(x_chunk)
        row_sum += tl.sum(abs_chunk, axis=0)
    
    # Compute mean
    mean_val = row_sum / D
    
    # Second pass: normalize and store
    for d_start in range(0, D, BLOCK_SIZE_D):
        offsets = d_start + tl.arange(0, BLOCK_SIZE_D)
        mask = offsets < D
        
        x_chunk = tl.load(row_ptr + offsets * stride_x1, mask=mask, other=0.0)
        normalized = x_chunk / mean_val
        tl.store(out_row_ptr + offsets * stride_out1, normalized, mask=mask)

def triton_l1_norm(x: torch.Tensor) -> torch.Tensor:
    """
    Wrapper function for L1 normalization using Triton kernels.
    """
    B, D = x.shape
    
    # Always use float32 for computation
    if x.dtype != torch.float32:
        x = x.float()
    
    output = torch.empty_like(x)
    
    # Choose optimal kernel based on dimensions
    # For large B or D, use fused kernel for better cache locality
    BLOCK_SIZE_D = min(1024, triton.next_power_of_2(D))
    
    if B <= 8192:
        # Fused kernel for better performance
        grid = (B,)
        l1_norm_fused_kernel[grid](
            x, output,
            x.stride(0), x.stride(1),
            output.stride(0), output.stride(1),
            B, D,
            BLOCK_SIZE_D=BLOCK_SIZE_D,
            num_warps=8 if D > 8192 else 4
        )
    else:
        # Two-pass approach for very large B
        # First pass: compute row sums
        row_sums = torch.empty(B, device=x.device, dtype=torch.float32)
        grid_reduce = (B,)  # One thread per row
        l1_norm_reduce_kernel[grid_reduce](
            x, row_sums,
            x.stride(0), x.stride(1),
            B, D,
            BLOCK_SIZE_D=BLOCK_SIZE_D,
            num_warps=4
        )
        
        # Second pass: normalize
        grid_divide = (triton.cdiv(D, BLOCK_SIZE_D),)
        l1_norm_divide_kernel[grid_divide](
            x, row_sums, output,
            x.stride(0), x.stride(1),
            output.stride(0), output.stride(1),
            B, D,
            BLOCK_SIZE_D=BLOCK_SIZE_D,
            num_warps=4
        )
    
    return output

class ModelNew(nn.Module):
    """
    Optimized L1 normalization layer using Triton kernels.
    """
    def __init__(self):
        super(ModelNew, self).__init__()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Ensure contiguous memory layout
        if not x.is_contiguous():
            x = x.contiguous()
        return triton_l1_norm(x)
```

[NCU PROFILING METRICS]
{
  "l1_norm_reduce_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 12.72,
    "launch__grid_size": 32768,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 99.2,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 97.28,
    "lts__t_sector_hit_rate.pct": 0.36
  },
  "l1_norm_divide_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 3.97,
    "launch__grid_size": 64,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 8.33,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 67.98,
    "lts__t_sector_hit_rate.pct": 57.26
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
