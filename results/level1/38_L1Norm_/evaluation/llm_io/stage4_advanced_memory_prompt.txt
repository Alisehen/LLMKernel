You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

**Focus**: Final micro-optimizations (small adjustments only).

**Parameters to Tune**:
• `num_warps`: {2, 4, 8, 16}
• `num_stages`: {2, 3, 4}

**Guidelines**:
- Adjust num_warps only if occupancy suggests it; otherwise keep the original.
- Change num_stages only by ±1 from current.
- Do not modify BLOCK sizes or grid mapping here.

**Autotune Tip (safe)**:
Use 3–6 configs around the current settings; always include the original kernel config, and autotune must wrap the JIT kernel.



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def l1_norm_kernel(
    x_ptr,
    output_ptr,
    stride_x_batch, stride_x_dim,
    stride_out_batch, stride_out_dim,
    B, D,
    BLOCK_SIZE_D: tl.constexpr,
):
    """
    Correct and optimized L1 normalization kernel.
    Each block processes one batch element and normalizes along the D dimension.
    """
    pid_batch = tl.program_id(axis=0)
    
    if pid_batch >= B:
        return
    
    # Pointers for the current batch element
    row_ptr = x_ptr + pid_batch * stride_x_batch
    out_row_ptr = output_ptr + pid_batch * stride_out_batch
    
    # Initialize accumulator for absolute sum
    abs_sum = 0.0
    
    # First pass: compute sum of absolute values
    for d_offset in range(0, D, BLOCK_SIZE_D):
        d_idx = d_offset + tl.arange(0, BLOCK_SIZE_D)
        mask = d_idx < D
        
        x_chunk = tl.load(row_ptr + d_idx * stride_x_dim, mask=mask, other=0.0)
        abs_sum += tl.sum(tl.abs(x_chunk), axis=0)
    
    # Compute mean (avoid division by zero)
    mean_val = abs_sum / D
    # Add small epsilon to prevent division by zero
    mean_val = mean_val + 1e-8
    
    # Second pass: normalize and store
    for d_offset in range(0, D, BLOCK_SIZE_D):
        d_idx = d_offset + tl.arange(0, BLOCK_SIZE_D)
        mask = d_idx < D
        
        x_chunk = tl.load(row_ptr + d_idx * stride_x_dim, mask=mask, other=0.0)
        normalized = x_chunk / mean_val
        tl.store(out_row_ptr + d_idx * stride_out_dim, normalized, mask=mask)

@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE_D': 256}, num_warps=4),
        triton.Config({'BLOCK_SIZE_D': 512}, num_warps=4),
        triton.Config({'BLOCK_SIZE_D': 1024}, num_warps=8),
        triton.Config({'BLOCK_SIZE_D': 2048}, num_warps=8),
        triton.Config({'BLOCK_SIZE_D': 4096}, num_warps=8),
    ],
    key=['D'],
)
@triton.jit
def l1_norm_kernel_autotune(
    x_ptr,
    output_ptr,
    stride_x_batch, stride_x_dim,
    stride_out_batch, stride_out_dim,
    B, D,
    BLOCK_SIZE_D: tl.constexpr,
):
    """
    Autotuned version of the L1 normalization kernel.
    """
    pid_batch = tl.program_id(axis=0)
    
    if pid_batch >= B:
        return
    
    row_ptr = x_ptr + pid_batch * stride_x_batch
    out_row_ptr = output_ptr + pid_batch * stride_out_batch
    
    abs_sum = 0.0
    
    # First pass: compute sum of absolute values
    for d_offset in range(0, D, BLOCK_SIZE_D):
        d_idx = d_offset + tl.arange(0, BLOCK_SIZE_D)
        mask = d_idx < D
        
        x_chunk = tl.load(row_ptr + d_idx * stride_x_dim, mask=mask, other=0.0)
        abs_sum += tl.sum(tl.abs(x_chunk), axis=0)
    
    # Compute mean with epsilon
    mean_val = abs_sum / D + 1e-8
    
    # Second pass: normalize and store
    for d_offset in range(0, D, BLOCK_SIZE_D):
        d_idx = d_offset + tl.arange(0, BLOCK_SIZE_D)
        mask = d_idx < D
        
        x_chunk = tl.load(row_ptr + d_idx * stride_x_dim, mask=mask, other=0.0)
        normalized = x_chunk / mean_val
        tl.store(out_row_ptr + d_idx * stride_out_dim, normalized, mask=mask)

def triton_l1_norm(x: torch.Tensor) -> torch.Tensor:
    """
    Wrapper function for L1 normalization with Triton kernels.
    """
    B, D = x.shape
    
    # Ensure contiguous memory layout and proper dtype
    x_contig = x.contiguous()
    output = torch.empty_like(x_contig)
    
    # Heuristic for block size selection
    if D <= 4096:
        # Use non-autotuned kernel for smaller dimensions
        BLOCK_SIZE_D = min(triton.next_power_of_2(D), 4096)
        num_warps = 8 if BLOCK_SIZE_D >= 2048 else 4
        
        grid = (B,)
        l1_norm_kernel[grid](
            x_contig, output,
            x_contig.stride(0), x_contig.stride(1),
            output.stride(0), output.stride(1),
            B, D,
            BLOCK_SIZE_D=BLOCK_SIZE_D,
            num_warps=num_warps,
        )
    else:
        # Use autotuned kernel for larger dimensions
        grid = (B,)
        l1_norm_kernel_autotune[grid](
            x_contig, output,
            x_contig.stride(0), x_contig.stride(1),
            output.stride(0), output.stride(1),
            B, D,
        )
    
    return output

class ModelNew(nn.Module):
    """
    Optimized L1 normalization layer using high-performance Triton kernels.
    """
    def __init__(self):
        super(ModelNew, self).__init__()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_l1_norm(x)
```

[NCU PROFILING METRICS]
{
  "l1_norm_kernel_autotune": [
    {
      "sm__throughput.avg.pct_of_peak_sustained_elapsed": 9.27,
      "launch__grid_size": 32768,
      "sm__warps_active.avg.pct_of_peak_sustained_active": 97.05,
      "dram__throughput.avg.pct_of_peak_sustained_elapsed": 88.07,
      "lts__t_sector_hit_rate.pct": 40.86
    },
    {
      "sm__throughput.avg.pct_of_peak_sustained_elapsed": 9.27,
      "launch__grid_size": 32768,
      "sm__warps_active.avg.pct_of_peak_sustained_active": 97.05,
      "dram__throughput.avg.pct_of_peak_sustained_elapsed": 88.07,
      "lts__t_sector_hit_rate.pct": 40.86
    }
  ]
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
