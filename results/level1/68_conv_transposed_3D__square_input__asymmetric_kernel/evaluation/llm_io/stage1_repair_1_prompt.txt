Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 538, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_050518_batch_range68to68_openai_deepseek/68_conv_transposed_3D__square_input__asymmetric_kernel/code/kernel_20251214_050746.py", line 258, in forward
    return triton_conv_transpose3d(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251214_050518_batch_range68to68_openai_deepseek/68_conv_transposed_3D__square_input__asymmetric_kernel/code/kernel_20251214_050746.py", line 196, in triton_conv_transpose3d
    conv_transpose3d_kernel[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 192, in run
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 192, in <dictcomp>
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 170, in _bench
    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/testing.py", line 145, in do_bench
    fn()
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 156, in kernel_call
    self.fn.run(
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 230, in compile
    key = f"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}"
                            ^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 77, in hash
    key = f"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}"
             ^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 665, in cache_key
    dependencies_finder.visit(self.parse())
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 149, in visit_FunctionDef
    self.generic_visit(node)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 215, in visit_For
    self.generic_visit(node)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 215, in visit_For
    self.generic_visit(node)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 215, in visit_For
    self.generic_visit(node)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 428, in generic_visit
    self.visit(value)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 428, in generic_visit
    self.visit(value)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 428, in generic_visit
    self.visit(value)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 142, in visit_Attribute
    ret = getattr(lhs, node.attr)
          ^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'triton.language' has no attribute 'any'

History Error:
None


PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Performs a transposed 3D convolution with a square input and an asymmetric kernel.

    Args:
        in_channels (int): Number of channels in the input tensor.
        out_channels (int): Number of channels produced by the convolution.
        kernel_size (tuple): Size of the convolution kernel (kernel_depth, kernel_width, kernel_height), 
                             where kernel_width == kernel_height.
        stride (tuple, optional): Stride of the convolution. Defaults to (1, 1, 1).
        padding (tuple, optional): Padding applied to the input. Defaults to (0, 0, 0).
        output_padding (tuple, optional): Additional size added to one side of the output shape. Defaults to (0, 0, 0).
        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.
        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):
        super(Model, self).__init__()
        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Performs the transposed 3D convolution.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, width, height).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, width_out, height_out).
        """
        return self.conv_transpose3d(x)

# Test code
batch_size = 16
in_channels = 32
out_channels = 64
kernel_depth = 3
kernel_width = 5
kernel_height = 5
depth = 64
width = 64
height = 64

def get_inputs():
    x = torch.rand(batch_size, in_channels, depth, width, height)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, (kernel_depth, kernel_width, kernel_height)]  # Provide in_channels, out_channels, kernel_size for initialization
```

Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_OUT": 32}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_OUT": 64}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_OUT": 128}, num_warps=4, num_stages=2),
    ],
    key=["N", "D_OUT", "H_OUT", "W_OUT", "C_OUT_PER_GROUP"],
)
@triton.jit
def conv_transpose3d_kernel(
    x_ptr,        # (N, C_in, D_in, H_in, W_in)
    w_ptr,        # (C_in, C_out_per_group, Kd, Kh, Kw)
    b_ptr,        # (C_out,) or dummy
    out_ptr,      # (N, C_out, D_out, H_out, W_out)
    N,
    D_IN, H_IN, W_IN,
    D_OUT, H_OUT, W_OUT,
    STRIDE_D, STRIDE_H, STRIDE_W,
    PAD_D, PAD_H, PAD_W,
    GROUPS,
    x_stride_n, x_stride_c, x_stride_d, x_stride_h, x_stride_w,
    w_stride_cin, w_stride_cout, w_stride_kd, w_stride_kh, w_stride_kw,
    out_stride_n, out_stride_c, out_stride_d, out_stride_h, out_stride_w,
    C_IN_PER_GROUP: tl.constexpr,
    C_OUT_PER_GROUP: tl.constexpr,
    KD: tl.constexpr,
    KH: tl.constexpr,
    KW: tl.constexpr,
    HAS_BIAS: tl.constexpr,
    BLOCK_OUT: tl.constexpr,
):
    # Program ids: grid = (N*D_out*H_out*W_out / BLOCK_OUT, C_out_per_group, groups)
    pid_spatial = tl.program_id(axis=0)        # over flattened (N, D_out, H_out, W_out)
    pid_cout_in_group = tl.program_id(axis=1)  # over C_out_per_group
    pid_group = tl.program_id(axis=2)          # over groups

    # Flat spatial indices for this program
    block_start = pid_spatial * BLOCK_OUT
    offs = block_start + tl.arange(0, BLOCK_OUT)
    total_spatial = N * D_OUT * H_OUT * W_OUT
    mask_o = offs < total_spatial

    # Decode n, od, oh, ow from flattened index
    tmp = offs
    dhw = D_OUT * H_OUT * W_OUT
    hw = H_OUT * W_OUT

    n = tmp // dhw
    tmp = tmp % dhw
    od = tmp // hw
    tmp = tmp % hw
    oh = tmp // W_OUT
    ow = tmp % W_OUT

    # Global output channel index
    group_id = pid_group
    c_out_in_group = pid_cout_in_group
    c_out = group_id * C_OUT_PER_GROUP + c_out_in_group

    # Precompute per-lane base offsets for input/output
    in_n_offset = n * x_stride_n
    out_offset_base = (
        n * out_stride_n
        + c_out * out_stride_c
        + od * out_stride_d
        + oh * out_stride_h
        + ow * out_stride_w
    )

    # Accumulator (fp32)
    acc = tl.zeros((BLOCK_OUT,), dtype=tl.float32)

    # Optional bias
    if HAS_BIAS:
        b = tl.load(b_ptr + c_out)
        acc += b

    # Main computation: loop over kernel and input channels in this group
    for kd in range(KD):
        od_pad_kd = od + PAD_D - kd
        id = od_pad_kd // STRIDE_D
        valid_d = (od_pad_kd % STRIDE_D == 0) & (id >= 0) & (id < D_IN)

        for kh in range(KH):
            oh_pad_kh = oh + PAD_H - kh
            ih = oh_pad_kh // STRIDE_H
            valid_h = (oh_pad_kh % STRIDE_H == 0) & (ih >= 0) & (ih < H_IN)

            for kw in range(KW):
                ow_pad_kw = ow + PAD_W - kw
                iw = ow_pad_kw // STRIDE_W
                valid_w = (ow_pad_kw % STRIDE_W == 0) & (iw >= 0) & (iw < W_IN)

                valid_spatial = valid_d & valid_h & valid_w & mask_o
                if not tl.any(valid_spatial):
                    continue

                # Per-lane base offset for this (kd,kh,kw) position, without channel
                in_dhw_offset = (
                    in_n_offset
                    + id * x_stride_d
                    + ih * x_stride_h
                    + iw * x_stride_w
                )

                for c_in_g in range(C_IN_PER_GROUP):
                    c_in = group_id * C_IN_PER_GROUP + c_in_g

                    # Input index: (n, c_in, id, ih, iw)
                    in_offset = in_dhw_offset + c_in * x_stride_c
                    x_vals = tl.load(
                        x_ptr + in_offset,
                        mask=valid_spatial,
                        other=0.0,
                    )

                    # Weight index: (c_in, c_out_in_group, kd, kh, kw)
                    w_offset = (
                        c_in * w_stride_cin
                        + c_out_in_group * w_stride_cout
                        + kd * w_stride_kd
                        + kh * w_stride_kh
                        + kw * w_stride_kw
                    )
                    w_val = tl.load(w_ptr + w_offset).to(tl.float32)

                    acc += x_vals.to(tl.float32) * w_val

    # Store to output
    tl.store(out_ptr + out_offset_base, acc, mask=mask_o)


def triton_conv_transpose3d(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    stride: tuple,
    padding: tuple,
    output_padding: tuple,
    groups: int,
) -> torch.Tensor:
    # Fallback for unsupported cases
    if (not x.is_cuda) or (x.dtype != torch.float32) or (weight.dtype != torch.float32):
        return torch.nn.functional.conv_transpose3d(
            x, weight, bias,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
            groups=groups,
        )

    N, C_in, D_in, H_in, W_in = x.shape
    C_in_w, C_out_per_group, Kd, Kh, Kw = weight.shape
    assert C_in_w == C_in, "Weight and input channel mismatch"
    assert C_in % groups == 0, "in_channels must be divisible by groups"
    C_out = C_out_per_group * groups
    C_in_per_group = C_in // groups

    stride_d, stride_h, stride_w = stride
    pad_d, pad_h, pad_w = padding
    out_pad_d, out_pad_h, out_pad_w = output_padding

    # Output size formula (PyTorch docs)
    D_out = (D_in - 1) * stride_d - 2 * pad_d + Kd + out_pad_d
    H_out = (H_in - 1) * stride_h - 2 * pad_h + Kh + out_pad_h
    W_out = (W_in - 1) * stride_w - 2 * pad_w + Kw + out_pad_w

    out = torch.empty(
        (N, C_out, D_out, H_out, W_out),
        device=x.device,
        dtype=x.dtype,
    )

    x_strides = x.stride()
    w_strides = weight.stride()
    out_strides = out.stride()

    has_bias = bias is not None
    if not has_bias:
        bias = torch.empty(1, device=x.device, dtype=x.dtype)

    # Grid: follow recommended layout for transposed conv
    def grid(meta):
        return (
            triton.cdiv(N * D_out * H_out * W_out, meta["BLOCK_OUT"]),
            C_out_per_group,
            groups,
        )

    conv_transpose3d_kernel[grid](
        x,
        weight,
        bias,
        out,
        N,
        D_in, H_in, W_in,
        D_out, H_out, W_out,
        stride_d, stride_h, stride_w,
        pad_d, pad_h, pad_w,
        groups,
        x_strides[0], x_strides[1], x_strides[2], x_strides[3], x_strides[4],
        w_strides[0], w_strides[1], w_strides[2], w_strides[3], w_strides[4],
        out_strides[0], out_strides[1], out_strides[2], out_strides[3], out_strides[4],
        C_IN_PER_GROUP=C_in_per_group,
        C_OUT_PER_GROUP=C_out_per_group,
        KD=Kd,
        KH=Kh,
        KW=Kw,
        HAS_BIAS=1 if has_bias else 0,
    )

    return out


class ModelNew(nn.Module):
    """
    Triton-based replacement for nn.ConvTranspose3d with asymmetric 3D kernels.
    Uses an optimized grid layout and autotuned spatial blocking.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: tuple,
        stride: tuple = (1, 1, 1),
        padding: tuple = (0, 0, 0),
        output_padding: tuple = (0, 0, 0),
        groups: int = 1,
        bias: bool = False,
    ) -> None:
        super().__init__()
        self.conv_transpose3d = nn.ConvTranspose3d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
            groups=groups,
            bias=bias,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        w = self.conv_transpose3d.weight
        b = self.conv_transpose3d.bias
        stride = self.conv_transpose3d.stride
        padding = self.conv_transpose3d.padding
        output_padding = self.conv_transpose3d.output_padding
        groups = self.conv_transpose3d.groups

        return triton_conv_transpose3d(
            x,
            w,
            b,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
            groups=groups,
        )
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl, AND any other modules used (e.g., import math if using math.sqrt)
   2. @triton.jit decorated kernel function(s) — NO continue/break/return inside loops (use masking)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels — THIS CLASS IS REQUIRED
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes
4. Ensure ALL imports are included at the top (common mistake: forgetting `import math`)

```python
# <corrected code>
```
