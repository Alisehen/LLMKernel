```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'num_warps': 4}),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32, 'num_warps': 4}),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64, 'num_warps': 4}),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32, 'num_warps': 4}),
    ],
    key=['M', 'N', 'K'],
)
@triton.jit
def matmul_kernel(
    a_ptr,
    b_ptr,
    c_ptr,
    M,
    N,
    K,
    stride_am,
    stride_ak,
    stride_bk,
    stride_bn,
    stride_cm,
    stride_cn,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    pid = tl.program_id(0)
    num_pid_m = tl.cdiv(M, BLOCK_M)
    num_pid_n = tl.cdiv(N, BLOCK_N)
    
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    
    # Program ID offsets
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)
    
    # Pointers for A and B
    a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)
    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)
    
    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    # Main K loop
    for k in range(0, tl.cdiv(K, BLOCK_K)):
        k_remaining = K - k * BLOCK_K
        a = tl.load(a_ptrs, 
                   mask=(offs_m[:, None] < M) & (offs_k[None, :] < k_remaining), 
                   other=0.0)
        b = tl.load(b_ptrs, 
                   mask=(offs_k[:, None] < k_remaining) & (offs_n[None, :] < N), 
                   other=0.0)
        
        accumulator = tl.dot(a, b, accumulator)
        
        a_ptrs += BLOCK_K * stride_ak
        b_ptrs += BLOCK_K * stride_bk
    
    # Store result
    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    tl.store(c_ptrs, accumulator, mask=c_mask)


def triton_matmul(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    M, K = a.shape
    K, N = b.shape
    
    assert a.is_cuda and b.is_cuda, "Inputs must be on GPU"
    assert a.dtype == b.dtype == torch.float32, "Only float32 supported"
    
    a = a.contiguous()
    b = b.contiguous()
    c = torch.empty((M, N), device=a.device, dtype=a.dtype)
    
    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']),)
    
    matmul_kernel[grid](
        a, b, c,
        M, N, K,
        a.stride(0), a.stride(1),
        b.stride(0), b.stride(1),
        c.stride(0), c.stride(1),
    )
    
    return c


class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
    
    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:
        return triton_matmul(A, B)
```

**Optimization Rationale**:

1. **Reduced `num_warps` from 8 to 4**: The NCU metrics showed extremely low warp occupancy (8.31%) despite having 65536 thread blocks. This indicates the GPU was oversubscribed with too many threads per block. Reducing to 128 threads per block (4 warps) allows more concurrent blocks per SM, improving occupancy.

2. **Simplified thread mapping**: The original complex grouping logic was removed in favor of a standard 2D grid mapping, reducing control flow overhead.

3. **Balanced tile configurations**: 
   - Maintained BLOCK_M/N as multiples of 16 and BLOCK_K as multiple of 8 for Tensor Core efficiency
   - Focused on moderate tile sizes (64-128) to avoid excessive masking waste
   - Balanced M/N dimensions (128x64, 64x128) to handle different aspect ratios
   - Included square tile (64x64) for balanced workloads

4. **Memory access patterns**: Maintained coalesced memory access through proper stride usage and tile dimensions that are powers of two.

5. **Autotune strategy**: Limited to 4 configurations (all powers of two) to avoid excessive compilation time while exploring the most promising tile size combinations for the Ada Lovelace architecture.

This configuration should significantly improve warp occupancy while maintaining efficient Tensor Core utilization on the RTX 4090.