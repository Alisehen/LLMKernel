```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE': 256, 'TILE_SIZE': 4}, num_warps=8),
        triton.Config({'BLOCK_SIZE': 512, 'TILE_SIZE': 2}, num_warps=16),
        triton.Config({'BLOCK_SIZE': 1024, 'TILE_SIZE': 1}, num_warps=32),
        triton.Config({'BLOCK_SIZE': 128, 'TILE_SIZE': 8}, num_warps=4),
    ],
    key=['n_elements'],
)
@triton.jit
def tanh_kernel(
    x_ptr,
    output_ptr,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
    TILE_SIZE: tl.constexpr,
):
    """
    Optimized tanh kernel using accurate mathematical approximation.
    """
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE * TILE_SIZE
    
    # Create offsets for vectorized operations
    offs = block_start + tl.arange(0, BLOCK_SIZE)
    
    # Process TILE_SIZE elements per thread
    for tile in range(TILE_SIZE):
        offsets = offs + tile * BLOCK_SIZE
        mask = offsets < n_elements
        
        # Load input
        x = tl.load(x_ptr + offsets, mask=mask, other=0.0)
        
        # Compute tanh using accurate formula: tanh(x) = (exp(2x) - 1) / (exp(2x) + 1)
        # But implement as: tanh(x) = 1 - 2 / (exp(2x) + 1) to avoid overflow
        x2 = 2.0 * x
        
        # For large values, use approximation to prevent overflow
        abs_x2 = tl.abs(x2)
        large_mask = abs_x2 > 40.0  # exp(40) ~ 2.35e17, beyond float32 precision
        
        # Compute for all values
        exp_x2 = tl.exp(x2)
        tanh_all = 1.0 - 2.0 / (exp_x2 + 1.0)
        
        # For large positive x, tanh ≈ 1
        # For large negative x, tanh ≈ -1
        tanh_large = tl.where(x2 > 0.0, 1.0, -1.0)
        
        # Combine results
        output = tl.where(large_mask, tanh_large, tanh_all)
        
        # Store result
        tl.store(output_ptr + offsets, output, mask=mask)


def triton_tanh(x: torch.Tensor) -> torch.Tensor:
    """
    Apply tanh activation using optimized Triton kernel.
    
    Args:
        x: Input tensor of any shape
        
    Returns:
        Output tensor with tanh applied, same shape as input
    """
    output = torch.empty_like(x)
    output_flat = output.view(-1)
    x_flat = x.view(-1)
    n_elements = x_flat.numel()
    
    if n_elements == 0:
        return output
    
    # Grid calculation function
    def grid_fn(meta):
        total_blocks = tl.cdiv(n_elements, meta['BLOCK_SIZE'] * meta['TILE_SIZE'])
        return (total_blocks,)
    
    # Launch kernel with autotuning
    tanh_kernel[grid_fn](
        x_flat, output_flat, n_elements,
        BLOCK_SIZE=256,  # Default, autotuner will override
        TILE_SIZE=4,     # Default, autotuner will override
    )
    
    return output


class ModelNew(nn.Module):
    """
    Optimized model that performs Tanh activation using Triton kernels.
    """
    def __init__(self):
        super(ModelNew, self).__init__()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Applies Tanh activation to the input tensor using optimized Triton kernel.

        Args:
            x (torch.Tensor): Input tensor of any shape.

        Returns:
            torch.Tensor: Output tensor with Tanh applied, same shape as input.
        """
        return triton_tanh(x)
```