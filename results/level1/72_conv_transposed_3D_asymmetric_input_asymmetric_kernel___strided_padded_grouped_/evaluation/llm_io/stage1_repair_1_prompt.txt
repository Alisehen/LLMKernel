Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 535, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251213_091928_batch_range68to99_openai_deepseek/72_conv_transposed_3D_asymmetric_input_asymmetric_kernel___strided_padded_grouped_/code/kernel_20251213_100717.py", line 198, in forward
    return triton_conv_transpose3d(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251213_091928_batch_range68to99_openai_deepseek/72_conv_transposed_3D_asymmetric_input_asymmetric_kernel___strided_padded_grouped_/code/kernel_20251213_100717.py", line 137, in triton_conv_transpose3d
    conv_transpose3d_kernel[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 192, in run
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 192, in <dictcomp>
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 170, in _bench
    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/testing.py", line 145, in do_bench
    fn()
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 156, in kernel_call
    self.fn.run(
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 230, in compile
    key = f"{triton_key()}-{src.hash()}-{backend.hash()}-{options.hash()}-{str(sorted(env_vars.items()))}"
                            ^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 77, in hash
    key = f"{self.fn.cache_key}-{str(self.attrs)}-{sorted_sig}-{constants_key}"
             ^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 665, in cache_key
    dependencies_finder.visit(self.parse())
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 149, in visit_FunctionDef
    self.generic_visit(node)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 215, in visit_For
    self.generic_visit(node)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 215, in visit_For
    self.generic_visit(node)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 215, in visit_For
    self.generic_visit(node)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 426, in generic_visit
    self.visit(item)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 428, in generic_visit
    self.visit(value)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 428, in generic_visit
    self.visit(value)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 428, in generic_visit
    self.visit(value)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/ast.py", line 418, in visit
    return visitor(node)
           ^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 142, in visit_Attribute
    ret = getattr(lhs, node.attr)
          ^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'triton.language' has no attribute 'any'

History Error:
None


PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Performs a 3D transposed convolution operation with asymmetric input and kernel, and optional stride.

    Args:
        in_channels (int): Number of channels in the input tensor.
        out_channels (int): Number of channels produced by the convolution.
        kernel_size (tuple of ints): Size of the convolution kernel in the form (kernel_size_depth, kernel_size_height, kernel_size_width).
        stride (tuple of ints, optional): Stride of the convolution in the form (stride_depth, stride_height, stride_width). Defaults to (1, 1, 1).
        padding (tuple of ints, optional): Padding applied to the input in the form (padding_depth, padding_height, padding_width). Defaults to (0, 0, 0).
        output_padding (tuple of ints, optional): Additional size added to one side of the output shape. Defaults to (0, 0, 0).
        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.
        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), output_padding: tuple = (0, 0, 0), groups: int = 1, bias: bool = False):
        super(Model, self).__init__()
        self.conv_transpose3d = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Performs the 3D transposed convolution.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).
        """
        return self.conv_transpose3d(x)

# Test code
batch_size = 8
in_channels = 32
out_channels = 32
kernel_size = (3, 5, 7)
depth = 12
height = 24
width = 48
stride = (2, 2, 2)
padding = (1, 2, 3)
output_padding = (1, 1, 1)
groups = 4

def get_inputs():
    x = torch.rand(batch_size, in_channels, depth, height, width)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, groups]
```

Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_IN": 32, "BLOCK_OC": 2}, num_warps=4, num_stages=3),
        triton.Config({"BLOCK_IN": 64, "BLOCK_OC": 2}, num_warps=4, num_stages=4),
        triton.Config({"BLOCK_IN": 64, "BLOCK_OC": 4}, num_warps=8, num_stages=3),
        triton.Config({"BLOCK_IN": 128, "BLOCK_OC": 4}, num_warps=8, num_stages=4),
    ],
    key=["total_in_elems", "cout_per_group"],
)
@triton.jit
def conv_transpose3d_kernel(
    x_ptr,
    w_ptr,
    out_ptr,
    total_in_elems,
    N,
    Cin,
    Cout,
    D,
    H,
    W,
    KD,
    KH,
    KW,
    stride_d,
    stride_h,
    stride_w,
    pad_d,
    pad_h,
    pad_w,
    out_pad_d,
    out_pad_h,
    out_pad_w,
    out_D,
    out_H,
    out_W,
    cin_per_group,
    cout_per_group,
    BLOCK_IN: tl.constexpr,
    BLOCK_OC: tl.constexpr,
):
    pid_in = tl.program_id(axis=0)
    pid_oc = tl.program_id(axis=1)

    in_offsets = pid_in * BLOCK_IN + tl.arange(0, BLOCK_IN)
    in_mask = in_offsets < total_in_elems
    in_offsets64 = in_offsets.to(tl.int64)

    oc_sub_block = pid_oc * BLOCK_OC + tl.arange(0, BLOCK_OC)
    oc_mask = oc_sub_block < cout_per_group
    oc_sub_block64 = oc_sub_block.to(tl.int64)

    w_idx = in_offsets64 % W
    tmp = in_offsets64 // W
    h_idx = tmp % H
    tmp = tmp // H
    d_idx = tmp % D
    tmp = tmp // D
    c_idx = tmp % Cin
    n_idx = tmp // Cin

    x_vals = tl.load(x_ptr + in_offsets64, mask=in_mask, other=0.0)

    group_idx = c_idx // cin_per_group
    oc_base = group_idx * cout_per_group

    for kd in range(0, KD):
        out_d = d_idx * stride_d - pad_d + kd
        valid_d = (out_d >= 0) & (out_d < out_D)

        for kh in range(0, KH):
            out_h = h_idx * stride_h - pad_h + kh
            valid_h = (out_h >= 0) & (out_h < out_H)

            for kw in range(0, KW):
                out_w = w_idx * stride_w - pad_w + kw
                valid_w = (out_w >= 0) & (out_w < out_W)

                base_mask = in_mask & valid_d & valid_h & valid_w
                if not tl.any(base_mask):
                    continue

                for oc_lane in range(0, BLOCK_OC):
                    oc_valid = oc_mask[oc_lane]
                    lane_mask = base_mask & oc_valid
                    if not tl.any(lane_mask):
                        continue

                    oc_sub = oc_sub_block64[oc_lane]
                    oc_full = oc_base + oc_sub

                    w_offsets = (((((c_idx * cout_per_group) + oc_sub) * KD + kd) * KH + kh) * KW + kw)
                    w_vals = tl.load(w_ptr + w_offsets, mask=lane_mask, other=0.0)

                    out_offsets = (((((n_idx * Cout) + oc_full) * out_D + out_d) * out_H + out_h) * out_W + out_w)
                    contrib = x_vals * w_vals
                    tl.atomic_add(out_ptr + out_offsets, contrib, mask=lane_mask)


def triton_conv_transpose3d(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    stride: tuple,
    padding: tuple,
    output_padding: tuple,
    groups: int,
) -> torch.Tensor:
    N, Cin, D, H, W = x.shape
    KD, KH, KW = weight.shape[2:]
    stride_d, stride_h, stride_w = stride
    pad_d, pad_h, pad_w = padding
    out_pad_d, out_pad_h, out_pad_w = output_padding
    Cout = weight.shape[1] * groups

    out_D = (D - 1) * stride_d - 2 * pad_d + KD + out_pad_d
    out_H = (H - 1) * stride_h - 2 * pad_h + KH + out_pad_h
    out_W = (W - 1) * stride_w - 2 * pad_w + KW + out_pad_w

    output = torch.zeros((N, Cout, out_D, out_H, out_W), device=x.device, dtype=x.dtype)

    total_in_elems = x.numel()
    cin_per_group = Cin // groups
    cout_per_group = Cout // groups

    grid = lambda meta: (
        triton.cdiv(total_in_elems, meta["BLOCK_IN"]),
        triton.cdiv(cout_per_group, meta["BLOCK_OC"]),
    )

    conv_transpose3d_kernel[grid](
        x,
        weight,
        output,
        total_in_elems,
        N,
        Cin,
        Cout,
        D,
        H,
        W,
        KD,
        KH,
        KW,
        stride_d,
        stride_h,
        stride_w,
        pad_d,
        pad_h,
        pad_w,
        out_pad_d,
        out_pad_h,
        out_pad_w,
        out_D,
        out_H,
        out_W,
        cin_per_group,
        cout_per_group,
    )

    if bias is not None:
        output += bias.view(1, Cout, 1, 1, 1)

    return output


class ModelNew(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: tuple,
        stride: tuple = (1, 1, 1),
        padding: tuple = (0, 0, 0),
        output_padding: tuple = (0, 0, 0),
        groups: int = 1,
        bias: bool = False,
    ):
        super().__init__()
        self.conv_transpose3d = nn.ConvTranspose3d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
            groups=groups,
            bias=bias,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_conv_transpose3d(
            x,
            self.conv_transpose3d.weight,
            self.conv_transpose3d.bias,
            self.conv_transpose3d.stride,
            self.conv_transpose3d.padding,
            self.conv_transpose3d.output_padding,
            self.conv_transpose3d.groups,
        )
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes

```python
# <corrected code>
```
