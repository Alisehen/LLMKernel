You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB



[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: BLOCK_M/N/K selection.

Metrics:
- sm__warps_active.avg.pct_of_peak_sustained_active (>50%)

Rules:
- BLOCK_* must be powers of 2
- Tensor Core: BLOCK_M/N multiple of 16, BLOCK_K multiple of 8 (preference)
- FP32: M/N ∈ {32,64,128,256}, K ∈ {16,32,64}
- Avoid oversized tiles (mask waste)
- Keep baseline tile if unsure

Autotune:
- 2–4 configs max
- Autotune ONLY on @triton.jit kernel



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def conv3d_implicit_gemm_kernel(
    x_ptr, w_ptr, b_ptr, y_ptr,
    N, C_in, D_in, H_in, W_in,
    C_out, D_out, H_out, W_out,
    KD, KH, KW,
    stride_d, stride_h, stride_w,
    pad_d, pad_h, pad_w,
    dil_d, dil_h, dil_w,
    C_in_per_group, C_out_per_group,
    groups,
    x_stride_n, x_stride_c, x_stride_d, x_stride_h, x_stride_w,
    w_stride_oc, w_stride_ic, w_stride_kd, w_stride_kh, w_stride_kw,
    y_stride_n, y_stride_c, y_stride_d, y_stride_h, y_stride_w,
    BLOCK_M: tl.constexpr,  # rows of output (N * D_out * H_out * W_out)
    BLOCK_N: tl.constexpr,  # columns of output (C_out_per_group)
    BLOCK_K: tl.constexpr,  # reduction dim (C_in_per_group * KD * KH * KW)
    HAS_BIAS: tl.constexpr,
):
    # Program ids: tile over (M, N, group)
    pid_m = tl.program_id(axis=0)
    pid_n = tl.program_id(axis=1)
    pid_g = tl.program_id(axis=2)

    M = N * D_out * H_out * W_out
    K_total = C_in_per_group * KD * KH * KW

    # Offsets in M (rows) and local N within group (cols)
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n_local = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    mask_m = offs_m < M
    mask_n = offs_n_local < C_out_per_group

    # Global output channel index (with groups)
    oc = pid_g * C_out_per_group + offs_n_local

    # Decompose linear row index M -> (n, od, oh, ow)
    tmp = offs_m
    ow = tmp % W_out
    tmp = tmp // W_out
    oh = tmp % H_out
    tmp = tmp // H_out
    od = tmp % D_out
    tmp = tmp // D_out
    n_idx = tmp  # batch index

    # Initialize accumulator
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Reduction over K dimension in tiles
    for k_start in range(0, K_total, BLOCK_K):
        offs_k = k_start + tl.arange(0, BLOCK_K)
        mask_k = offs_k < K_total

        # Decompose K -> (ic_in_group, kd, kh, kw)
        ic_in_group = offs_k // (KD * KH * KW)
        rem = offs_k % (KD * KH * KW)
        kd = rem // (KH * KW)
        rem = rem % (KH * KW)
        kh = rem // KW
        kw = rem % KW

        ic_abs = ic_in_group + pid_g * C_in_per_group  # absolute input channel index

        # Compute input coordinates for each (m, k)
        od_b = od[:, None]
        oh_b = oh[:, None]
        ow_b = ow[:, None]

        kd_b = kd[None, :]
        kh_b = kh[None, :]
        kw_b = kw[None, :]

        in_d = od_b * stride_d - pad_d + kd_b * dil_d
        in_h = oh_b * stride_h - pad_h + kh_b * dil_h
        in_w = ow_b * stride_w - pad_w + kw_b * dil_w

        # Broadcast indices for pointer arithmetic
        n_b = n_idx[:, None]
        ic_b = ic_abs[None, :]

        # Bounds check for input
        in_bounds = (
            (n_b >= 0) & (n_b < N) &
            (ic_b >= 0) & (ic_b < C_in) &
            (in_d >= 0) & (in_d < D_in) &
            (in_h >= 0) & (in_h < H_in) &
            (in_w >= 0) & (in_w < W_in)
        )

        # Also respect valid M/K ranges
        in_bounds = in_bounds & (mask_m[:, None] & mask_k[None, :])

        # Compute input pointers
        x_ptrs = (
            x_ptr
            + n_b * x_stride_n
            + ic_b * x_stride_c
            + in_d * x_stride_d
            + in_h * x_stride_h
            + in_w * x_stride_w
        )

        x_vals = tl.load(x_ptrs, mask=in_bounds, other=0.0)

        # Compute weight pointers: shape [BK, BN]
        oc_b = oc[None, :]
        icg_b = ic_in_group[:, None]
        kd_wb = kd[:, None]
        kh_wb = kh[:, None]
        kw_wb = kw[:, None]

        w_ptrs = (
            w_ptr
            + oc_b * w_stride_oc
            + icg_b * w_stride_ic
            + kd_wb * w_stride_kd
            + kh_wb * w_stride_kh
            + kw_wb * w_stride_kw
        )

        w_mask = mask_k[:, None] & mask_n[None, :]
        w_vals = tl.load(w_ptrs, mask=w_mask, other=0.0)

        # Accumulate
        acc += tl.dot(x_vals, w_vals)

    # Add bias if present
    if HAS_BIAS:
        b_vals = tl.load(b_ptr + oc, mask=mask_n, other=0.0)
        acc = acc + b_vals[None, :]

    # Store results
    n_b = n_idx[:, None]
    od_b = od[:, None]
    oh_b = oh[:, None]
    ow_b = ow[:, None]

    y_ptrs = (
        y_ptr
        + n_b * y_stride_n
        + oc[None, :] * y_stride_c
        + od_b * y_stride_d
        + oh_b * y_stride_h
        + ow_b * y_stride_w
    )

    out_mask = mask_m[:, None] & mask_n[None, :]
    tl.store(y_ptrs, acc, mask=out_mask)


def _triple(v):
    if isinstance(v, int):
        return (v, v, v)
    return v


def conv3d_triton(
    x: torch.Tensor,
    w: torch.Tensor,
    b: torch.Tensor,
    stride,
    padding,
    dilation,
    groups: int,
) -> torch.Tensor:
    # Ensure contiguous tensors
    x = x.contiguous()
    w = w.contiguous()
    if b is not None:
        b = b.contiguous()

    N, C_in, D_in, H_in, W_in = x.shape
    C_out, C_in_per_group, KD, KH, KW = w.shape
    stride_d, stride_h, stride_w = _triple(stride)
    pad_d, pad_h, pad_w = _triple(padding)
    dil_d, dil_h, dil_w = _triple(dilation)

    C_out_per_group = C_out // groups
    assert C_in == C_in_per_group * groups

    # Output dimensions (PyTorch Conv3d semantics)
    D_out = (D_in + 2 * pad_d - dil_d * (KD - 1) - 1) // stride_d + 1
    H_out = (H_in + 2 * pad_h - dil_h * (KH - 1) - 1) // stride_h + 1
    W_out = (W_in + 2 * pad_w - dil_w * (KW - 1) - 1) // stride_w + 1

    y = torch.empty((N, C_out, D_out, H_out, W_out), device=x.device, dtype=x.dtype)

    # Strides
    x_stride_n, x_stride_c, x_stride_d, x_stride_h, x_stride_w = x.stride()
    w_stride_oc, w_stride_ic, w_stride_kd, w_stride_kh, w_stride_kw = w.stride()
    y_stride_n, y_stride_c, y_stride_d, y_stride_h, y_stride_w = y.stride()

    BLOCK_M = 32
    BLOCK_N = 32
    BLOCK_K = 32

    M = N * D_out * H_out * W_out

    grid = (
        triton.cdiv(M, BLOCK_M),
        triton.cdiv(C_out_per_group, BLOCK_N),
        groups,
    )

    HAS_BIAS = b is not None
    if b is None:
        # dummy pointer, will not be used when HAS_BIAS == False
        b = w

    conv3d_implicit_gemm_kernel[grid](
        x, w, b, y,
        N, C_in, D_in, H_in, W_in,
        C_out, D_out, H_out, W_out,
        KD, KH, KW,
        stride_d, stride_h, stride_w,
        pad_d, pad_h, pad_w,
        dil_d, dil_h, dil_w,
        C_in_per_group, C_out_per_group,
        groups,
        x_stride_n, x_stride_c, x_stride_d, x_stride_h, x_stride_w,
        w_stride_oc, w_stride_ic, w_stride_kd, w_stride_kh, w_stride_kw,
        y_stride_n, y_stride_c, y_stride_d, y_stride_h, y_stride_w,
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
        BLOCK_K=BLOCK_K,
        HAS_BIAS=HAS_BIAS,
    )

    return y


class ModelNew(nn.Module):
    """
    Triton-accelerated version of the provided 3D convolution model.
    Uses an implicit-GEMM Triton kernel to compute Conv3d.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: tuple,
        stride: int = 1,
        padding: int = 0,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = False,
    ) -> None:
        super().__init__()
        # Keep a standard Conv3d module as parameter container
        self.conv3d = nn.Conv3d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
            bias=bias,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        w = self.conv3d.weight
        b = self.conv3d.bias
        return conv3d_triton(
            x,
            w,
            b,
            stride=self.conv3d.stride,
            padding=self.conv3d.padding,
            dilation=self.conv3d.dilation,
            groups=self.conv3d.groups,
        )
```

[NCU PROFILING METRICS]
{
  "conv3d_implicit_gemm_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 76.09,
    "launch__grid_size": 215760,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 33.19,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 6.44,
    "lts__t_sector_hit_rate.pct": 99.23
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
