```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def conv_transpose2d_kernel(
    x_ptr,
    w_ptr,
    b_ptr,
    out_ptr,
    H_IN,
    W_IN,
    H_OUT,
    W_OUT,
    N,
    CIN,
    COUT,
    NUM_TILES_PER_GROUP,
    STRIDE_H: tl.constexpr,
    STRIDE_W: tl.constexpr,
    PAD_H: tl.constexpr,
    PAD_W: tl.constexpr,
    OUT_PAD_H: tl.constexpr,
    OUT_PAD_W: tl.constexpr,
    CIN_PER_GROUP: tl.constexpr,
    COUT_PER_GROUP: tl.constexpr,
    GROUPS: tl.constexpr,
    KH: tl.constexpr,
    KW: tl.constexpr,
    BLOCK_CO: tl.constexpr,
    BLOCK_HW: tl.constexpr,
    HAS_BIAS: tl.constexpr,
):
    tl.static_assert(STRIDE_H == 1 and STRIDE_W == 1, "Stride > 1 not supported in this kernel")
    tl.static_assert(OUT_PAD_H == 0 and OUT_PAD_W == 0, "Non-zero output_padding not supported")
    pid_n = tl.program_id(axis=0)
    pid_go = tl.program_id(axis=1)
    pid_hw = tl.program_id(axis=2)

    hw_offsets = pid_hw * BLOCK_HW + tl.arange(0, BLOCK_HW)
    total_hw = H_OUT * W_OUT
    hw_mask = hw_offsets < total_hw
    hw_offsets = tl.where(hw_mask, hw_offsets, 0)
    ho = hw_offsets // W_OUT
    wo = hw_offsets % W_OUT

    group_id = pid_go // NUM_TILES_PER_GROUP
    tile_in_group = pid_go % NUM_TILES_PER_GROUP
    oc_start = group_id * COUT_PER_GROUP + tile_in_group * BLOCK_CO
    oc_offsets = oc_start + tl.arange(0, BLOCK_CO)
    oc_mask = oc_offsets < ((group_id + 1) * COUT_PER_GROUP)
    oc_mask = oc_mask & (oc_offsets < COUT)
    oc_offsets = tl.where(oc_mask, oc_offsets, 0)
    oc_local = oc_offsets - group_id * COUT_PER_GROUP

    acc = tl.zeros((BLOCK_CO, BLOCK_HW), dtype=tl.float32)

    cin_group_base = group_id * CIN_PER_GROUP
    for cin_local in range(CIN_PER_GROUP):
        cin_idx = cin_group_base + cin_local
        plane_base = ((pid_n * CIN + cin_idx) * H_IN) * W_IN
        for kh in range(KH):
            h_in = ho + PAD_H - kh
            h_valid = (h_in >= 0) & (h_in < H_IN)
            row_base = h_in * W_IN
            for kw in range(KW):
                w_in = wo + PAD_W - kw
                w_valid = (w_in >= 0) & (w_in < W_IN)
                valid = hw_mask & h_valid & w_valid
                offsets_in = plane_base + row_base + w_in
                inp_vals = tl.load(x_ptr + offsets_in, mask=valid, other=0.0)
                inp_vals = inp_vals.to(tl.float32)
                weight_offsets = ((cin_idx * COUT_PER_GROUP + oc_local) * KH + kh) * KW + kw
                w_vals = tl.load(w_ptr + weight_offsets, mask=oc_mask, other=0.0)
                w_vals = w_vals.to(tl.float32)
                acc += w_vals[:, None] * inp_vals[None, :]

    if HAS_BIAS:
        bias_vals = tl.load(b_ptr + oc_offsets, mask=oc_mask, other=0.0)
        acc += bias_vals[:, None]

    out_base = ((pid_n * COUT + oc_offsets) * H_OUT * W_OUT)
    store_offsets = out_base[:, None] + hw_offsets[None, :]
    store_mask = oc_mask[:, None] & hw_mask[None, :]
    tl.store(out_ptr + store_offsets, acc, mask=store_mask)


def triton_conv_transpose2d(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    stride,
    padding,
    output_padding,
    groups: int,
) -> torch.Tensor:
    assert x.is_cuda and weight.is_cuda, "Inputs must be CUDA tensors"
    assert x.dtype == torch.float32 and weight.dtype == torch.float32, "Only float32 supported"
    assert stride == (1, 1), "Kernel currently supports stride=1"
    assert output_padding == (0, 0), "Kernel currently supports output_padding=0"

    x = x.contiguous()
    weight = weight.contiguous()
    bias_tensor = bias.contiguous() if bias is not None else x.new_empty(1)

    N, Cin, H_in, W_in = x.shape
    Cout_per_group = weight.shape[1]
    KH = weight.shape[2]
    KW = weight.shape[3]
    Cout = Cout_per_group * groups
    H_out = (H_in - 1) * stride[0] - 2 * padding[0] + KH + output_padding[0]
    W_out = (W_in - 1) * stride[1] - 2 * padding[1] + KW + output_padding[1]

    out = torch.empty((N, Cout, H_out, W_out), device=x.device, dtype=x.dtype)

    BLOCK_CO = 16
    BLOCK_HW = 64
    num_tiles_per_group = triton.cdiv(Cout_per_group, BLOCK_CO)

    grid = (
        N,
        groups * num_tiles_per_group,
        triton.cdiv(H_out * W_out, BLOCK_HW),
    )

    conv_transpose2d_kernel[grid](
        x,
        weight,
        bias_tensor,
        out,
        H_in,
        W_in,
        H_out,
        W_out,
        N,
        Cin,
        Cout,
        num_tiles_per_group,
        STRIDE_H=stride[0],
        STRIDE_W=stride[1],
        PAD_H=padding[0],
        PAD_W=padding[1],
        OUT_PAD_H=output_padding[0],
        OUT_PAD_W=output_padding[1],
        CIN_PER_GROUP=Cin // groups,
        COUT_PER_GROUP=Cout_per_group,
        GROUPS=groups,
        KH=KH,
        KW=KW,
        BLOCK_CO=BLOCK_CO,
        BLOCK_HW=BLOCK_HW,
        HAS_BIAS=1 if bias is not None else 0,
    )
    return out


class ModelNew(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: int = 0,
        output_padding: int = 0,
        groups: int = 1,
        bias: bool = False,
    ) -> None:
        super().__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
            groups=groups,
            bias=bias,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        weight = self.conv_transpose.weight
        bias = self.conv_transpose.bias
        stride = (self.conv_transpose.stride[0], self.conv_transpose.stride[1])
        padding = (self.conv_transpose.padding[0], self.conv_transpose.padding[1])
        output_padding = (
            self.conv_transpose.output_padding[0],
            self.conv_transpose.output_padding[1],
        )
        groups = self.conv_transpose.groups
        return triton_conv_transpose2d(x, weight, bias, stride, padding, output_padding, groups)
```