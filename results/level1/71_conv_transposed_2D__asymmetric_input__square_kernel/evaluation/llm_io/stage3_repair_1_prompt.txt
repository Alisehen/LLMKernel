Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 535, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251213_091928_batch_range68to99_openai_deepseek/71_conv_transposed_2D__asymmetric_input__square_kernel/code/kernel_20251213_094204.py", line 204, in forward
    return triton_conv_transpose2d(x, w, b, stride, padding, output_padding, groups)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251213_091928_batch_range68to99_openai_deepseek/71_conv_transposed_2D__asymmetric_input__square_kernel/code/kernel_20251213_094204.py", line 139, in triton_conv_transpose2d
    conv_transpose2d_kernel[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 192, in run
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 192, in <dictcomp>
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 145, in _bench
    raise ValueError(f"Conflicting meta-parameters: {', '.join(conflicts)}."
ValueError: Conflicting meta-parameters: BLOCK_CO, BLOCK_HW. Make sure that you don't re-define auto-tuned symbols.

History Error:
None


PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Performs a transposed 2D convolution with asymmetric input and a square kernel.

    Args:
        in_channels (int): Number of channels in the input tensor.
        out_channels (int): Number of channels produced by the convolution.
        kernel_size (int): Size of the square convolution kernel.
        stride (int, optional): Stride of the convolution. Defaults to 1.
        padding (int, optional): Padding applied to the input. Defaults to 0.
        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.
        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.
        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):
        super(Model, self).__init__()
        self.conv_transpose2d = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Performs the transposed 2D convolution.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height_in, width_in).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).
        """
        return self.conv_transpose2d(x)

# Test code
batch_size = 8
in_channels = 32
out_channels = 32
kernel_size = 3
# large asymmetric input
height_in = 512
width_in = 1024

def get_inputs():
    x = torch.rand(batch_size, in_channels, height_in, width_in)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization
```

Broken Code:
```python
# Imports
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_CO": 32, "BLOCK_HW": 128}, num_warps=8, num_stages=2),
        triton.Config({"BLOCK_CO": 32, "BLOCK_HW": 64}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_CO": 16, "BLOCK_HW": 128}, num_warps=4, num_stages=3),
    ],
    key=["H_OUT", "W_OUT", "COUT_PER_GROUP"],
)
@triton.jit
def conv_transpose2d_kernel(
    x_ptr,
    w_ptr,
    b_ptr,
    out_ptr,
    H_IN,
    W_IN,
    H_OUT,
    W_OUT,
    N,
    CIN,
    COUT,
    NUM_TILES_PER_GROUP,
    STRIDE_H: tl.constexpr,
    STRIDE_W: tl.constexpr,
    PAD_H: tl.constexpr,
    PAD_W: tl.constexpr,
    OUT_PAD_H: tl.constexpr,
    OUT_PAD_W: tl.constexpr,
    CIN_PER_GROUP: tl.constexpr,
    COUT_PER_GROUP: tl.constexpr,
    GROUPS: tl.constexpr,
    KH: tl.constexpr,
    KW: tl.constexpr,
    BLOCK_CO: tl.constexpr,
    BLOCK_HW: tl.constexpr,
    HAS_BIAS: tl.constexpr,
):
    tl.static_assert(STRIDE_H == 1 and STRIDE_W == 1, "Only stride=1 supported.")
    tl.static_assert(OUT_PAD_H == 0 and OUT_PAD_W == 0, "output_padding must be 0.")
    pid_n = tl.program_id(0)
    pid_go = tl.program_id(1)
    pid_hw = tl.program_id(2)

    total_hw = H_OUT * W_OUT
    hw_offsets = pid_hw * BLOCK_HW + tl.arange(0, BLOCK_HW)
    hw_mask = hw_offsets < total_hw
    hw_offsets = tl.where(hw_mask, hw_offsets, 0)
    ho = hw_offsets // W_OUT
    wo = hw_offsets % W_OUT

    group_id = pid_go // NUM_TILES_PER_GROUP
    tile_in_group = pid_go % NUM_TILES_PER_GROUP
    oc_start = group_id * COUT_PER_GROUP + tile_in_group * BLOCK_CO
    oc_offsets = oc_start + tl.arange(0, BLOCK_CO)
    within_group = oc_offsets < (group_id + 1) * COUT_PER_GROUP
    oc_mask = within_group & (oc_offsets < COUT)
    oc_offsets = tl.where(oc_mask, oc_offsets, 0)
    oc_local = oc_offsets - group_id * COUT_PER_GROUP

    acc = tl.zeros((BLOCK_CO, BLOCK_HW), dtype=tl.float32)

    cin_group_base = group_id * CIN_PER_GROUP
    in_batch_base = (pid_n * CIN + cin_group_base) * H_IN * W_IN

    for cin_local in tl.static_range(CIN_PER_GROUP):
        cin_idx = cin_group_base + cin_local
        plane_base = in_batch_base + cin_local * H_IN * W_IN
        w_plane_base = ((cin_idx * COUT_PER_GROUP) + oc_local) * KH * KW

        for kh in tl.static_range(KH):
            h_in = ho + PAD_H - kh
            h_valid = (h_in >= 0) & (h_in < H_IN)
            h_clamped = tl.where(h_valid, h_in, 0)
            row_offset = h_clamped * W_IN

            for kw in tl.static_range(KW):
                w_in = wo + PAD_W - kw
                w_valid = (w_in >= 0) & (w_in < W_IN)
                w_clamped = tl.where(w_valid, w_in, 0)

                valid_mask = hw_mask & h_valid & w_valid
                inp_ptrs = plane_base + row_offset + w_clamped
                inp_vals = tl.load(x_ptr + inp_ptrs, mask=valid_mask, other=0.0).to(tl.float32)

                w_offsets = w_plane_base + kh * KW + kw
                w_vals = tl.load(w_ptr + w_offsets, mask=oc_mask, other=0.0).to(tl.float32)

                acc += w_vals[:, None] * inp_vals[None, :]

    if HAS_BIAS:
        bias_vals = tl.load(b_ptr + oc_offsets, mask=oc_mask, other=0.0).to(tl.float32)
        acc += bias_vals[:, None]

    out_plane_base = ((pid_n * COUT) + oc_offsets) * H_OUT * W_OUT
    store_ptrs = out_plane_base[:, None] + hw_offsets[None, :]
    store_mask = oc_mask[:, None] & hw_mask[None, :]
    tl.store(out_ptr + store_ptrs, acc, mask=store_mask)


def triton_conv_transpose2d(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    stride,
    padding,
    output_padding,
    groups: int,
) -> torch.Tensor:
    assert stride == (1, 1)
    assert output_padding == (0, 0)
    x = x.contiguous()
    weight = weight.contiguous()
    bias_tensor = bias.contiguous() if bias is not None else x.new_empty(1)

    N, Cin, H_in, W_in = x.shape
    Cout_per_group = weight.shape[1]
    KH, KW = weight.shape[2], weight.shape[3]
    Cout = Cout_per_group * groups

    H_out = (H_in - 1) * stride[0] - 2 * padding[0] + KH + output_padding[0]
    W_out = (W_in - 1) * stride[1] - 2 * padding[1] + KW + output_padding[1]

    out = torch.empty((N, Cout, H_out, W_out), device=x.device, dtype=x.dtype)

    BLOCK_CO = 32
    BLOCK_HW = 128
    tiles_per_group = triton.cdiv(Cout_per_group, BLOCK_CO)
    hw_tiles = triton.cdiv(H_out * W_out, BLOCK_HW)

    grid = (N, groups * tiles_per_group, hw_tiles)

    conv_transpose2d_kernel[grid](
        x,
        weight,
        bias_tensor,
        out,
        H_in,
        W_in,
        H_out,
        W_out,
        N,
        Cin,
        Cout,
        tiles_per_group,
        STRIDE_H=stride[0],
        STRIDE_W=stride[1],
        PAD_H=padding[0],
        PAD_W=padding[1],
        OUT_PAD_H=output_padding[0],
        OUT_PAD_W=output_padding[1],
        CIN_PER_GROUP=Cin // groups,
        COUT_PER_GROUP=Cout_per_group,
        GROUPS=groups,
        KH=KH,
        KW=KW,
        BLOCK_CO=BLOCK_CO,
        BLOCK_HW=BLOCK_HW,
        HAS_BIAS=1 if bias is not None else 0,
    )
    return out


class ModelNew(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: int = 0,
        output_padding: int = 0,
        groups: int = 1,
        bias: bool = False,
    ) -> None:
        super().__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
            groups=groups,
            bias=bias,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        w = self.conv_transpose.weight
        b = self.conv_transpose.bias
        stride = (self.conv_transpose.stride[0], self.conv_transpose.stride[1])
        padding = (self.conv_transpose.padding[0], self.conv_transpose.padding[1])
        output_padding = (
            self.conv_transpose.output_padding[0],
            self.conv_transpose.output_padding[1],
        )
        groups = self.conv_transpose.groups
        return triton_conv_transpose2d(x, w, b, stride, padding, output_padding, groups)
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes

```python
# <corrected code>
```
