```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_CO": 32, "BLOCK_HW": 128}, num_warps=8, num_stages=2),
        triton.Config({"BLOCK_CO": 32, "BLOCK_HW": 64}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_CO": 16, "BLOCK_HW": 128}, num_warps=4, num_stages=3),
    ],
    key=["H_OUT", "W_OUT", "COUT_PER_GROUP"],
)
@triton.jit
def conv_transpose2d_kernel(
    x_ptr,
    w_ptr,
    b_ptr,
    out_ptr,
    H_IN,
    W_IN,
    H_OUT,
    W_OUT,
    N,
    CIN,
    COUT,
    GRID_TILES_PER_GROUP,
    STRIDE_H: tl.constexpr,
    STRIDE_W: tl.constexpr,
    PAD_H: tl.constexpr,
    PAD_W: tl.constexpr,
    OUT_PAD_H: tl.constexpr,
    OUT_PAD_W: tl.constexpr,
    CIN_PER_GROUP: tl.constexpr,
    COUT_PER_GROUP: tl.constexpr,
    GROUPS: tl.constexpr,
    KH: tl.constexpr,
    KW: tl.constexpr,
    BLOCK_CO: tl.constexpr,
    BLOCK_HW: tl.constexpr,
    HAS_BIAS: tl.constexpr,
):
    tl.static_assert(STRIDE_H == 1 and STRIDE_W == 1, "Only stride=1 supported.")
    tl.static_assert(OUT_PAD_H == 0 and OUT_PAD_W == 0, "output_padding must be 0.")
    pid_n = tl.program_id(0)
    pid_go = tl.program_id(1)
    pid_hw = tl.program_id(2)

    tiles_per_group = tl.cdiv(COUT_PER_GROUP, BLOCK_CO)
    hw_tiles = tl.cdiv(H_OUT * W_OUT, BLOCK_HW)

    group_id = pid_go // GRID_TILES_PER_GROUP
    tile_in_group = pid_go % GRID_TILES_PER_GROUP
    if group_id >= GROUPS or tile_in_group >= tiles_per_group:
        return
    if pid_hw >= hw_tiles:
        return

    total_hw = H_OUT * W_OUT
    hw_offsets = pid_hw * BLOCK_HW + tl.arange(0, BLOCK_HW)
    hw_mask = hw_offsets < total_hw
    hw_offsets = tl.where(hw_mask, hw_offsets, 0)
    ho = hw_offsets // W_OUT
    wo = hw_offsets % W_OUT

    oc_start = group_id * COUT_PER_GROUP + tile_in_group * BLOCK_CO
    oc_offsets = oc_start + tl.arange(0, BLOCK_CO)
    within_group = oc_offsets < (group_id + 1) * COUT_PER_GROUP
    oc_mask = within_group & (oc_offsets < COUT)
    oc_offsets = tl.where(oc_mask, oc_offsets, 0)
    oc_local = oc_offsets - group_id * COUT_PER_GROUP

    acc = tl.zeros((BLOCK_CO, BLOCK_HW), dtype=tl.float32)

    cin_group_base = group_id * CIN_PER_GROUP
    batch_base = pid_n * CIN * H_IN * W_IN
    group_base = (batch_base + cin_group_base * H_IN * W_IN)

    for cin_local in tl.static_range(CIN_PER_GROUP):
        plane_base = group_base + cin_local * H_IN * W_IN
        cin_idx = cin_group_base + cin_local
        w_plane_base = ((cin_idx * COUT_PER_GROUP) + oc_local) * KH * KW

        for kh in tl.static_range(KH):
            h_in = ho + PAD_H - kh
            h_valid = (h_in >= 0) & (h_in < H_IN)
            h_clamped = tl.where(h_valid, h_in, 0)
            row_offset = h_clamped * W_IN

            for kw in tl.static_range(KW):
                w_in = wo + PAD_W - kw
                w_valid = (w_in >= 0) & (w_in < W_IN)
                w_clamped = tl.where(w_valid, w_in, 0)

                valid_mask = hw_mask & h_valid & w_valid
                inp_ptrs = plane_base + row_offset + w_clamped
                vals = tl.load(x_ptr + inp_ptrs, mask=valid_mask, other=0.0).to(tl.float32)

                w_offsets = w_plane_base + kh * KW + kw
                w_vals = tl.load(w_ptr + w_offsets, mask=oc_mask, other=0.0).to(tl.float32)

                acc += w_vals[:, None] * vals[None, :]

    if HAS_BIAS:
        bias_vals = tl.load(b_ptr + oc_offsets, mask=oc_mask, other=0.0).to(tl.float32)
        acc += bias_vals[:, None]

    out_plane_base = ((pid_n * COUT) + oc_offsets) * H_OUT * W_OUT
    store_ptrs = out_plane_base[:, None] + hw_offsets[None, :]
    store_mask = oc_mask[:, None] & hw_mask[None, :]
    tl.store(out_ptr + store_ptrs, acc, mask=store_mask)


def triton_conv_transpose2d(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    stride,
    padding,
    output_padding,
    groups: int,
) -> torch.Tensor:
    assert stride == (1, 1)
    assert output_padding == (0, 0)
    x = x.contiguous()
    weight = weight.contiguous()
    bias_tensor = bias.contiguous() if bias is not None else x.new_empty(1)

    N, Cin, H_in, W_in = x.shape
    Cout_per_group = weight.shape[1]
    KH, KW = weight.shape[2], weight.shape[3]
    Cout = Cout_per_group * groups

    H_out = (H_in - 1) * stride[0] - 2 * padding[0] + KH + output_padding[0]
    W_out = (W_in - 1) * stride[1] - 2 * padding[1] + KW + output_padding[1]

    out = torch.empty((N, Cout, H_out, W_out), device=x.device, dtype=x.dtype)

    BLOCK_CO_MIN = 16
    BLOCK_HW_MIN = 64
    tiles_per_group_max = triton.cdiv(Cout_per_group, BLOCK_CO_MIN)
    hw_tiles_max = triton.cdiv(H_out * W_out, BLOCK_HW_MIN)

    grid = (N, groups * tiles_per_group_max, hw_tiles_max)

    conv_transpose2d_kernel[grid](
        x,
        weight,
        bias_tensor,
        out,
        H_in,
        W_in,
        H_out,
        W_out,
        N,
        Cin,
        Cout,
        tiles_per_group_max,
        STRIDE_H=stride[0],
        STRIDE_W=stride[1],
        PAD_H=padding[0],
        PAD_W=padding[1],
        OUT_PAD_H=output_padding[0],
        OUT_PAD_W=output_padding[1],
        CIN_PER_GROUP=Cin // groups,
        COUT_PER_GROUP=Cout_per_group,
        GROUPS=groups,
        KH=KH,
        KW=KW,
        HAS_BIAS=1 if bias is not None else 0,
    )
    return out


class ModelNew(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: int = 0,
        output_padding: int = 0,
        groups: int = 1,
        bias: bool = False,
    ) -> None:
        super().__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
            groups=groups,
            bias=bias,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        w = self.conv_transpose.weight
        b = self.conv_transpose.bias
        stride = (self.conv_transpose.stride[0], self.conv_transpose.stride[1])
        padding = (self.conv_transpose.padding[0], self.conv_transpose.padding[1])
        output_padding = (
            self.conv_transpose.output_padding[0],
            self.conv_transpose.output_padding[1],
        )
        groups = self.conv_transpose.groups
        return triton_conv_transpose2d(x, w, b, stride, padding, output_padding, groups)
```