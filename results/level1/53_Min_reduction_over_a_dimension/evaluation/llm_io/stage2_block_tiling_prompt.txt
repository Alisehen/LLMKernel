You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB



[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: BLOCK_M/N/K selection.

Metrics:
- sm__warps_active.avg.pct_of_peak_sustained_active (>50%)

Rules:
- BLOCK_* must be powers of 2
- Tensor Core: BLOCK_M/N multiple of 16, BLOCK_K multiple of 8 (preference)
- FP32: M/N ∈ {32,64,128,256}, K ∈ {16,32,64}
- Avoid oversized tiles (mask waste)
- Keep baseline tile if unsure

Autotune:
- 2–4 configs max
- Autotune ONLY on @triton.jit kernel



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_M": 16, "BLOCK_N": 64}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_M": 32, "BLOCK_N": 64}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 32}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_M": 32, "BLOCK_N": 128}, num_warps=4, num_stages=2),
    ],
    key=["OUT0", "OUT1", "reduce_size", "DIM"],
)
@triton.jit
def min_reduce_3d_kernel(
    x_ptr,
    out_ptr,
    B, M, N,
    strideB, strideM, strideN,
    OUT0, OUT1,          # output shape: (OUT0, OUT1), row-major
    reduce_size,         # length of reduced dimension
    DIM: tl.constexpr,   # 0, 1, or 2
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    """
    3D min-reduction over a chosen dimension (DIM) with 2D output tiling.

    DIM = 0: reduce over B,   output shape (M, N)  -> (OUT0, OUT1) = (M, N)
    DIM = 1: reduce over M,   output shape (B, N)  -> (OUT0, OUT1) = (B, N)
    DIM = 2: reduce over N,   output shape (B, M)  -> (OUT0, OUT1) = (B, M)
    """

    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    mask_m = offs_m < OUT0
    mask_n = offs_n < OUT1
    mask_out = mask_m[:, None] & mask_n[None, :]

    # Map (offs_m, offs_n) -> (b_idx, m_idx, n_idx) of non-reduced dims
    if DIM == 0:
        # reduce over B, output (M, N)
        m_idx = offs_m[:, None]
        n_idx = offs_n[None, :]
    elif DIM == 1:
        # reduce over M, output (B, N)
        b_idx = offs_m[:, None]
        n_idx = offs_n[None, :]
    else:
        # DIM == 2: reduce over N, output (B, M)
        b_idx = offs_m[:, None]
        m_idx = offs_n[None, :]

    # Running minima for this tile
    min_val = tl.full((BLOCK_M, BLOCK_N), float("inf"), dtype=tl.float32)

    # Online reduction along the reduced dimension to minimize register use
    k = 0
    while k < reduce_size:
        if DIM == 0:
            # k is B
            b = k
            ptrs = (
                x_ptr
                + b * strideB
                + m_idx * strideM
                + n_idx * strideN
            )
        elif DIM == 1:
            # k is M
            m_red = k
            ptrs = (
                x_ptr
                + b_idx * strideB
                + m_red * strideM
                + n_idx * strideN
            )
        else:
            # DIM == 2, k is N
            n_red = k
            ptrs = (
                x_ptr
                + b_idx * strideB
                + m_idx * strideM
                + n_red * strideN
            )

        vals = tl.load(ptrs, mask=mask_out, other=float("inf"))
        min_val = tl.minimum(min_val, vals)

        k += 1

    # Store results to contiguous (OUT0, OUT1) output
    out_row = offs_m[:, None]
    out_col = offs_n[None, :]
    out_indices = out_row * OUT1 + out_col
    tl.store(out_ptr + out_indices, min_val, mask=mask_out)


def triton_min_reduce_3d(x: torch.Tensor, dim: int) -> torch.Tensor:
    """
    Triton-based min reduction over a specified dimension for 3D tensors.

    Supports CUDA float32 3D tensors; falls back to torch.min otherwise.
    """
    if x.dim() != 3 or not x.is_cuda or x.dtype != torch.float32:
        return torch.min(x, dim=dim)[0]

    dim = dim % x.dim()

    B, M, N = x.shape
    strideB, strideM, strideN = x.stride()

    if dim == 0:
        # reduce over B -> output (M, N)
        OUT0, OUT1 = M, N
        reduce_size = B
        DIM = 0
    elif dim == 1:
        # reduce over M -> output (B, N)
        OUT0, OUT1 = B, N
        reduce_size = M
        DIM = 1
    else:
        # reduce over N -> output (B, M)
        OUT0, OUT1 = B, M
        reduce_size = N
        DIM = 2

    out = torch.empty((OUT0, OUT1), device=x.device, dtype=x.dtype)

    def grid(meta):
        return (
            triton.cdiv(OUT0, meta["BLOCK_M"]),
            triton.cdiv(OUT1, meta["BLOCK_N"]),
        )

    min_reduce_3d_kernel[grid](
        x,
        out,
        B, M, N,
        strideB, strideM, strideN,
        OUT0, OUT1,
        reduce_size,
        DIM=DIM,
    )

    return out


class ModelNew(nn.Module):
    """
    Model performing min reduction over a specific dimension,
    accelerated with an optimized Triton kernel for 3D CUDA float32 inputs.
    """
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_min_reduce_3d(x, self.dim)
```

[NCU PROFILING METRICS]
{
  "min_reduce_3d_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 4.05,
    "launch__grid_size": 256,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 16.61,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 77.15,
    "lts__t_sector_hit_rate.pct": 14.49
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
