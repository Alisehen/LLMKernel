You are optimizing a Triton kernel based on algorithmic analysis.

# PyTorch Reference (Target Behavior)

```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Simple model that performs a matrix-scalar multiplication (C = A * s)
    """
    def __init__(self):
        super(Model, self).__init__()
    
    def forward(self, A: torch.Tensor, s: float) -> torch.Tensor:
        """
        Performs matrix-scalar multiplication.

        Args:
            A: Input matrix of shape (M, N)
            s: Scalar value

        Returns:
            C: Resulting matrix of shape (M, N)
        """
        return A * s

M = 16384 * 4
N = 4096 * 4

def get_inputs():
    A = torch.rand(M, N)
    s = 3.14
    return [A, s]

def get_init_inputs():
    return []  # No special initialization inputs needed
```

**CRITICAL**: Study the PyTorch code carefully to understand:
- What does `forward()` return? (full output sequence vs final hidden state only)
- What is the computational pattern?
- What are the input/output shapes?

Your optimized kernel MUST match this exact behavior.

---

# Analysis Results

**Bottleneck**: All work is an elementwise A[i] * s with no data reuse, so performance is limited by global memory bandwidth and kernel launch overhead rather than arithmetic. Implementing this as its own Triton kernel cannot beat the framework’s highly tuned fused kernels and just adds an extra pass over memory.

**Optimization Strategy**: Operator fusion: eliminate the explicit scale kernel by folding the scalar s into a neighboring operation (e.g., pre-scale weights or fuse into a following matmul/conv/elementwise op) so that A*s is never materialized as a separate tensor.

**Implementation Plan**: At the model level, rearrange computations so that the scalar is absorbed into an existing operation: for example, rewrite (A * s) @ W as A @ (s * W), or fuse the multiply with a subsequent elementwise kernel so it uses s internally without a separate pass. For static or slowly changing s, pre-scale the relevant parameters once at initialization or when s changes, and remove the Triton scale kernel entirely; for dynamic s, pass it as an extra argument to the already-existing kernel and apply the multiply inside that kernel instead of launching a separate one.

**Expected Speedup**: 30-50%

---

# Current Kernel (needs optimization)

```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def scale_kernel(
    A_ptr,          # *const T
    C_ptr,          # *mut T
    s,              # scalar multiplier
    n_elements,     # total number of elements
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements

    a = tl.load(A_ptr + offsets, mask=mask, other=0.0)
    c = a * s
    tl.store(C_ptr + offsets, c, mask=mask)


def triton_scale(A: torch.Tensor, s: float) -> torch.Tensor:
    # Ensure contiguous memory for optimal performance
    A_contig = A.contiguous()
    C = torch.empty_like(A_contig)

    n_elements = A_contig.numel()
    BLOCK_SIZE = 256

    grid = lambda meta: (triton.cdiv(n_elements, meta["BLOCK_SIZE"]),)

    scale_kernel[grid](
        A_contig,
        C,
        s,
        n_elements,
        BLOCK_SIZE=BLOCK_SIZE,
    )

    return C.view_as(A)


class ModelNew(nn.Module):
    """
    Triton-optimized model that performs matrix-scalar multiplication (C = A * s)
    """
    def __init__(self):
        super(ModelNew, self).__init__()

    def forward(self, A: torch.Tensor, s: float) -> torch.Tensor:
        return triton_scale(A, s)
```

---

# Your Task

Implement the optimization strategy above. Focus on the specific bottleneck identified.

## Key Requirements

1. **Preserve correctness**: Maintain the same input/output behavior
2. **Apply the optimization**: Follow the implementation plan exactly
3. **Use valid Triton syntax**:
   - Every kernel MUST have `@triton.jit` decorator
   - Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
   - BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
   - No `continue`, `break`, `return` inside kernels (use masking)
   - Prefer `tl.dot(a, b, allow_tf32=True)` for matmul operations

4. **CRITICAL for RNN/GRU/LSTM Persistent Kernels**:
   - Time loop MUST be inside @triton.jit kernel, NOT in Python forward()
   - **HYBRID computation strategy** (CRITICAL for performance):
     * Precompute input-side gates OUTSIDE kernel: `gates_x = (T*B, In) @ W_ih` (ONE large GEMM)
     * INSIDE kernel: only recurrent-side: `for t: gates_h = h @ W_hh` (T small GEMMs)
   - CORRECT (FAST - use this):
     ```python
     # Python forward():
     gates_x_all = x.reshape(T*B, In) @ W_ih + b_ih  # ONE large GEMM
     gates_x_all = gates_x_all.view(T, B, 3*H)
     gru_persistent_kernel[grid](gates_x_all, h0, W_hh, ...)  # Launch ONCE

     @triton.jit
     def gru_persistent_kernel(gates_x_ptr, h_ptr, W_hh_ptr, ...):
         for t in range(T):  # Inside kernel
             gates_x_t = tl.load(gates_x_ptr + t*...)  # Precomputed
             gates_h = h @ W_hh  # Only recurrent GEMM
             h = (1-z)*n + z*h   # Fuse and update
     ```

5. **Output format**:
   - Imports: `import torch, torch.nn as nn, triton, triton.language as tl`
   - `@triton.jit` kernel(s)
   - Wrapper function(s)
   - `class ModelNew(nn.Module)` — REQUIRED
   - NO testing code, NO `if __name__ == "__main__"`

---

Generate the optimized kernel now. Output ONLY the complete Python code.
