```python
# <optimized Triton code>

import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        # Original configs (kept as required baseline)
        triton.Config({"BLOCK_M": 16, "BLOCK_N": 32}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_M": 32, "BLOCK_N": 32}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 16}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_M": 32, "BLOCK_N": 64}, num_warps=4, num_stages=2),
        # Nearby micro-tuned configs, guided by low occupancy:
        # - explore higher and lower num_warps
        # - increase num_stages by +1 (allowed) to test pipelining trade-offs
        triton.Config({"BLOCK_M": 32, "BLOCK_N": 32}, num_warps=8, num_stages=3),
        triton.Config({"BLOCK_M": 32, "BLOCK_N": 64}, num_warps=2, num_stages=3),
    ],
    key=["N", "C_OUT", "D_OUT", "H_OUT", "W_OUT"],
)
@triton.jit
def conv_transpose3d_kernel(
    x_ptr,        # (N, C_IN, D_IN, H_IN, W_IN)
    w_ptr,        # (C_IN, C_OUT, KD, KH, KW)
    b_ptr,        # (C_OUT,) or dummy
    y_ptr,        # (N, C_OUT, D_OUT, H_OUT, W_OUT)
    N,            # batch size
    C_OUT,        # out_channels
    D_IN, H_IN, W_IN,
    D_OUT, H_OUT, W_OUT,
    C_IN: tl.constexpr,
    KD: tl.constexpr,
    KH: tl.constexpr,
    KW: tl.constexpr,
    HAS_BIAS: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    # -------------------------------------------------------------------------
    # Grid mapping:
    #  - pid_b: batch index n          -> axis 0
    #  - pid_m: spatial tile in DHW    -> axis 1 (BLOCK_M elements)
    #  - pid_co: output-channel tile   -> axis 2 (BLOCK_N channels)
    # -------------------------------------------------------------------------
    pid_b = tl.program_id(axis=0)
    pid_m = tl.program_id(axis=1)
    pid_co = tl.program_id(axis=2)

    # Spatial offsets [0, D_OUT*H_OUT*W_OUT)
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    M_spatial = D_OUT * H_OUT * W_OUT
    mask_m = offs_m < M_spatial

    # Output channel offsets [0, C_OUT)
    offs_co = pid_co * BLOCK_N + tl.arange(0, BLOCK_N)
    mask_co = offs_co < C_OUT

    # Decode spatial index: offs_m -> (d_out, h_out, w_out)
    stride_hw = H_OUT * W_OUT
    d_out = offs_m // stride_hw
    rem = offs_m % stride_hw
    h_out = rem // W_OUT
    w_out = rem % W_OUT

    # Accumulator in fp32 for this (BLOCK_M, BLOCK_N) tile
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # -------------------------------------------------------------------------
    # Precompute per-batch pointer offsets to reduce integer work inside loops
    # -------------------------------------------------------------------------
    x_ptr_batch = x_ptr + pid_b * (C_IN * D_IN * H_IN * W_IN)
    y_ptr_batch = y_ptr + pid_b * (C_OUT * D_OUT * H_OUT * W_OUT)

    # Spatial index for outputs, independent of channels
    spatial_index = (d_out * H_OUT + h_out) * W_OUT + w_out  # [BLOCK_M]
    co_stride = D_OUT * H_OUT * W_OUT                        # distance between output channels

    # -------------------------------------------------------------------------
    # Main accumulation loop over input channels and kernel volume
    # -------------------------------------------------------------------------
    for c_in in range(C_IN):
        x_ptr_c = x_ptr_batch + c_in * (D_IN * H_IN * W_IN)
        w_ptr_c = w_ptr + c_in * (C_OUT * KD * KH * KW)

        for kd in range(KD):
            d_in = d_out - kd
            mask_d = (d_in >= 0) & (d_in < D_IN)

            for kh in range(KH):
                h_in = h_out - kh
                mask_h = (h_in >= 0) & (h_in < H_IN)

                for kw in range(KW):
                    w_in = w_out - kw
                    mask_w = (w_in >= 0) & (w_in < W_IN)

                    # Combined mask for valid input positions in this tile
                    mask_x = mask_m & mask_d & mask_h & mask_w

                    # Compute input indices (within this batch and channel)
                    x_index = (d_in * H_IN + h_in) * W_IN + w_in  # [BLOCK_M]
                    x_vals = tl.load(x_ptr_c + x_index, mask=mask_x, other=0.0)
                    x_vals = x_vals.to(tl.float32)

                    # Compute weight indices for current (c_in, kd, kh, kw)
                    # weight layout: (C_IN, C_OUT, KD, KH, KW) contiguous
                    # index (within this c_in slice): ((((c_out) * KD + kd) * KH + kh) * KW + kw)
                    w_index = (((offs_co * KD + kd) * KH + kh) * KW + kw)  # [BLOCK_N]
                    w_vals = tl.load(w_ptr_c + w_index, mask=mask_co, other=0.0)
                    w_vals = w_vals.to(tl.float32)

                    # Outer-product update of accumulator tile
                    acc += x_vals[:, None] * w_vals[None, :]

    # Add bias if present
    if HAS_BIAS:
        b_vals = tl.load(b_ptr + offs_co, mask=mask_co, other=0.0)
        b_vals = b_vals.to(tl.float32)
        acc += b_vals[None, :]

    # -------------------------------------------------------------------------
    # Store results
    # -------------------------------------------------------------------------
    y_index = spatial_index[:, None] + offs_co[None, :] * co_stride  # [BLOCK_M, BLOCK_N]
    mask = mask_m[:, None] & mask_co[None, :]

    y_vals = acc.to(tl.float32)
    tl.store(y_ptr_batch + y_index, y_vals, mask=mask)


def conv_transpose3d_triton(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    stride,
    padding,
    output_padding,
    dilation,
    groups: int,
) -> torch.Tensor:
    """
    Triton implementation of ConvTranspose3d.

    Fast path (Triton kernel) supports:
      - CUDA tensors
      - float32 dtype
      - contiguous NCDHW layout for x, (C_IN, C_OUT, KD, KH, KW) for weight
      - groups == 1
      - stride == (1,1,1)
      - padding == (0,0,0)
      - output_padding == (0,0,0)
      - dilation == (1,1,1)

    Otherwise falls back to torch.nn.functional.conv_transpose3d.
    """
    # Fallback conditions
    if (
        (not x.is_cuda)
        or (not weight.is_cuda)
        or (x.dtype != torch.float32)
        or (weight.dtype != torch.float32)
        or (not x.is_contiguous())
        or (not weight.is_contiguous())
        or groups != 1
        or stride != (1, 1, 1)
        or padding != (0, 0, 0)
        or output_padding != (0, 0, 0)
        or dilation != (1, 1, 1)
    ):
        return torch.nn.functional.conv_transpose3d(
            x,
            weight,
            bias,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
            groups=groups,
            dilation=dilation,
        )

    N, C_IN, D_IN, H_IN, W_IN = x.shape
    C_IN_w, C_OUT, KD, KH, KW = weight.shape
    assert C_IN_w == C_IN, "Input channels mismatch between x and weight"

    # Output dimensions (general formula, though here stride=1, padding=0, dilation=1, output_pad=0)
    stride_d, stride_h, stride_w = stride
    pad_d, pad_h, pad_w = padding
    out_pad_d, out_pad_h, out_pad_w = output_padding
    dil_d, dil_h, dil_w = dilation

    D_OUT = (D_IN - 1) * stride_d - 2 * pad_d + dil_d * (KD - 1) + out_pad_d + 1
    H_OUT = (H_IN - 1) * stride_h - 2 * pad_h + dil_h * (KH - 1) + out_pad_h + 1
    W_OUT = (W_IN - 1) * stride_w - 2 * pad_w + dil_w * (KW - 1) + out_pad_w + 1

    y = torch.empty(
        (N, C_OUT, D_OUT, H_OUT, W_OUT),
        device=x.device,
        dtype=x.dtype,
    )

    has_bias = bias is not None
    if bias is None:
        # Dummy tensor, never read when has_bias == False
        bias_ptr = weight.new_empty(1)
    else:
        bias_ptr = bias
        assert bias_ptr.is_cuda and bias_ptr.dtype == torch.float32

    M_spatial = D_OUT * H_OUT * W_OUT

    def grid(meta):
        return (
            N,
            triton.cdiv(M_spatial, meta["BLOCK_M"]),
            triton.cdiv(C_OUT, meta["BLOCK_N"]),
        )

    conv_transpose3d_kernel[grid](
        x,
        weight,
        bias_ptr,
        y,
        N,
        C_OUT,
        D_IN,
        H_IN,
        W_IN,
        D_OUT,
        H_OUT,
        W_OUT,
        C_IN=C_IN,
        KD=KD,
        KH=KH,
        KW=KW,
        HAS_BIAS=has_bias,
    )

    return y


class ModelNew(nn.Module):
    """
    Same interface as the original Model, but uses a Triton kernel for
    ConvTranspose3d in the common case (CUDA, float32, groups==1, stride=1, padding=0, etc.).
    Falls back to PyTorch's implementation otherwise.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: tuple,
        stride: tuple = (1, 1, 1),
        padding: tuple = (0, 0, 0),
        output_padding: tuple = (0, 0, 0),
        groups: int = 1,
        bias: bool = False,
        dilation: tuple = (1, 1, 1),
    ) -> None:
        super().__init__()
        self.conv_transpose3d = nn.ConvTranspose3d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
            groups=groups,
            bias=bias,
            dilation=dilation,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        w = self.conv_transpose3d.weight
        b = self.conv_transpose3d.bias
        stride = self.conv_transpose3d.stride
        padding = self.conv_transpose3d.padding
        output_padding = self.conv_transpose3d.output_padding
        dilation = self.conv_transpose3d.dilation
        groups = self.conv_transpose3d.groups

        # CPU or unsupported configs: use PyTorch fallback
        if not x.is_cuda:
            return self.conv_transpose3d(x)

        return conv_transpose3d_triton(
            x,
            w,
            b,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
            dilation=dilation,
            groups=groups,
        )
```