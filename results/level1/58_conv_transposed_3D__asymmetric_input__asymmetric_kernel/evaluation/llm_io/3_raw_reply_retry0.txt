```python
# <optimized Triton code>

import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        # Small tiles (good for small problems / edge cases)
        triton.Config({"BLOCK_M": 16, "BLOCK_N": 32}, num_warps=4, num_stages=1),
        triton.Config({"BLOCK_M": 16, "BLOCK_N": 64}, num_warps=4, num_stages=2),
        # Medium tiles
        triton.Config({"BLOCK_M": 32, "BLOCK_N": 32}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_M": 32, "BLOCK_N": 64}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_M": 32, "BLOCK_N": 128}, num_warps=8, num_stages=2),
        # Large tiles (high arithmetic intensity, lower occupancy)
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 32}, num_warps=8, num_stages=2),
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 64}, num_warps=8, num_stages=3),
    ],
    key=["N", "C_OUT", "D_OUT", "H_OUT", "W_OUT"],
)
@triton.jit
def conv_transpose3d_kernel(
    x_ptr,        # (N, C_IN, D_IN, H_IN, W_IN)
    w_ptr,        # (C_IN, C_OUT, KD, KH, KW)
    b_ptr,        # (C_OUT,) or dummy
    y_ptr,        # (N, C_OUT, D_OUT, H_OUT, W_OUT)
    N,            # batch size
    C_OUT,        # out_channels
    D_IN, H_IN, W_IN,
    D_OUT, H_OUT, W_OUT,
    C_IN: tl.constexpr,
    KD: tl.constexpr,
    KH: tl.constexpr,
    KW: tl.constexpr,
    HAS_BIAS: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    # -------------------------------------------------------------------------
    # Grid mapping:
    #  - pid_b: batch index n          -> axis 0
    #  - pid_m: spatial tile in DHW    -> axis 1 (BLOCK_M elements)
    #  - pid_co: output-channel tile   -> axis 2 (BLOCK_N channels)
    # -------------------------------------------------------------------------
    pid_b = tl.program_id(axis=0)
    pid_m = tl.program_id(axis=1)
    pid_co = tl.program_id(axis=2)

    # Spatial offsets [0, D_OUT*H_OUT*W_OUT)
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    M_spatial = D_OUT * H_OUT * W_OUT
    mask_m = offs_m < M_spatial

    # Output channel offsets [0, C_OUT)
    offs_co = pid_co * BLOCK_N + tl.arange(0, BLOCK_N)
    mask_co = offs_co < C_OUT

    # Decode spatial index: offs_m -> (d_out, h_out, w_out)
    stride_hw_out = H_OUT * W_OUT
    d_out = offs_m // stride_hw_out
    rem = offs_m % stride_hw_out
    h_out = rem // W_OUT
    w_out = rem % W_OUT

    # Accumulator in fp32 for this (BLOCK_M, BLOCK_N) tile
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # -------------------------------------------------------------------------
    # Precompute strides / base pointers
    # -------------------------------------------------------------------------
    # x layout: (N, C_IN, D_IN, H_IN, W_IN)
    stride_x_c = D_IN * H_IN * W_IN
    stride_x_d = H_IN * W_IN
    stride_x_h = W_IN

    # w layout: (C_IN, C_OUT, KD, KH, KW)
    stride_w_cin = C_OUT * KD * KH * KW
    stride_w_co = KD * KH * KW
    stride_w_kd = KH * KW
    stride_w_kh = KW

    # y layout: (N, C_OUT, D_OUT, H_OUT, W_OUT)
    co_stride = D_OUT * H_OUT * W_OUT  # distance between output channels

    # Per-batch base pointers
    x_ptr_batch = x_ptr + pid_b * (C_IN * stride_x_c)
    y_ptr_batch = y_ptr + pid_b * (C_OUT * co_stride)

    # Spatial index for outputs, independent of channels
    spatial_index = (d_out * H_OUT + h_out) * W_OUT + w_out  # [BLOCK_M]

    # -------------------------------------------------------------------------
    # Main accumulation loop over input channels and kernel volume
    #   We aggressively hoist index arithmetic out of inner loops to cut down
    #   integer overhead in the hot path.
    # -------------------------------------------------------------------------
    for c_in in range(C_IN):
        # Base pointers for this input channel
        x_ptr_c = x_ptr_batch + c_in * stride_x_c
        w_ptr_c = w_ptr + c_in * stride_w_cin

        # Base weight pointer for this (c_in, offs_co) tile
        w_ptr_co = w_ptr_c + offs_co * stride_w_co

        # Loop over kernel depth/height/width with full unrolling
        for kd in tl.static_range(0, KD):
            d_in = d_out - kd
            mask_d = (d_in >= 0) & (d_in < D_IN)
            mask_md = mask_m & mask_d
            base_d = d_in * stride_x_d  # [BLOCK_M]

            w_ptr_kd = w_ptr_co + kd * stride_w_kd  # [BLOCK_N]

            for kh in tl.static_range(0, KH):
                h_in = h_out - kh
                mask_h = (h_in >= 0) & (h_in < H_IN)
                mask_mdh = mask_md & mask_h
                base_dh = base_d + h_in * stride_x_h  # [BLOCK_M]

                w_ptr_kh = w_ptr_kd + kh * stride_w_kh  # [BLOCK_N]

                # Base index that includes w_out, independent of kw
                x_index_base = base_dh + w_out  # [BLOCK_M]

                for kw in tl.static_range(0, KW):
                    # Input width coordinate
                    w_in = w_out - kw
                    mask_w = (w_in >= 0) & (w_in < W_IN)
                    mask_x = mask_mdh & mask_w

                    # Final input index for this kw
                    x_index = x_index_base - kw  # [BLOCK_M]
                    x_vals = tl.load(x_ptr_c + x_index, mask=mask_x, other=0.0)
                    x_vals = x_vals.to(tl.float32)

                    # Weight pointer for this kw (no integer muls in inner loop)
                    w_vals = tl.load(w_ptr_kh + kw, mask=mask_co, other=0.0)
                    w_vals = w_vals.to(tl.float32)

                    # Outer-product update of accumulator tile
                    acc += x_vals[:, None] * w_vals[None, :]

    # Add bias if present
    if HAS_BIAS:
        b_vals = tl.load(b_ptr + offs_co, mask=mask_co, other=0.0)
        b_vals = b_vals.to(tl.float32)
        acc += b_vals[None, :]

    # -------------------------------------------------------------------------
    # Store results
    # -------------------------------------------------------------------------
    y_index = spatial_index[:, None] + offs_co[None, :] * co_stride  # [BLOCK_M, BLOCK_N]
    mask = mask_m[:, None] & mask_co[None, :]

    y_vals = acc.to(tl.float32)
    tl.store(y_ptr_batch + y_index, y_vals, mask=mask)


def conv_transpose3d_triton(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    stride,
    padding,
    output_padding,
    dilation,
    groups: int,
) -> torch.Tensor:
    """
    High-performance Triton implementation of ConvTranspose3d for the common case:

      - CUDA tensors
      - float32 dtype
      - contiguous NCDHW layout for x
      - weight layout (C_IN, C_OUT, KD, KH, KW) contiguous
      - groups == 1
      - stride == (1,1,1)
      - padding == (0,0,0)
      - output_padding == (0,0,0)
      - dilation == (1,1,1)

    Falls back to torch.nn.functional.conv_transpose3d otherwise.
    """
    # Fallback conditions
    if (
        (not x.is_cuda)
        or (not weight.is_cuda)
        or (x.dtype != torch.float32)
        or (weight.dtype != torch.float32)
        or (not x.is_contiguous())
        or (not weight.is_contiguous())
        or groups != 1
        or stride != (1, 1, 1)
        or padding != (0, 0, 0)
        or output_padding != (0, 0, 0)
        or dilation != (1, 1, 1)
    ):
        return torch.nn.functional.conv_transpose3d(
            x,
            weight,
            bias,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
            groups=groups,
            dilation=dilation,
        )

    N, C_IN, D_IN, H_IN, W_IN = x.shape
    C_IN_w, C_OUT, KD, KH, KW = weight.shape
    assert C_IN_w == C_IN, "Input channels mismatch between x and weight"

    # Output dimensions (general formula, though here stride=1, padding=0, dilation=1, output_pad=0)
    stride_d, stride_h, stride_w = stride
    pad_d, pad_h, pad_w = padding
    out_pad_d, out_pad_h, out_pad_w = output_padding
    dil_d, dil_h, dil_w = dilation

    D_OUT = (D_IN - 1) * stride_d - 2 * pad_d + dil_d * (KD - 1) + out_pad_d + 1
    H_OUT = (H_IN - 1) * stride_h - 2 * pad_h + dil_h * (KH - 1) + out_pad_h + 1
    W_OUT = (W_IN - 1) * stride_w - 2 * pad_w + dil_w * (KW - 1) + out_pad_w + 1

    y = torch.empty(
        (N, C_OUT, D_OUT, H_OUT, W_OUT),
        device=x.device,
        dtype=x.dtype,
    )

    has_bias = bias is not None
    if bias is None:
        # Dummy tensor, never read when has_bias == False
        bias_ptr = weight.new_empty(1)
    else:
        bias_ptr = bias
        assert bias_ptr.is_cuda and bias_ptr.dtype == torch.float32

    M_spatial = D_OUT * H_OUT * W_OUT

    def grid(meta):
        return (
            N,
            triton.cdiv(M_spatial, meta["BLOCK_M"]),
            triton.cdiv(C_OUT, meta["BLOCK_N"]),
        )

    conv_transpose3d_kernel[grid](
        x,
        weight,
        bias_ptr,
        y,
        N,
        C_OUT,
        D_IN,
        H_IN,
        W_IN,
        D_OUT,
        H_OUT,
        W_OUT,
        C_IN=C_IN,
        KD=KD,
        KH=KH,
        KW=KW,
        HAS_BIAS=has_bias,
    )

    return y


class ModelNew(nn.Module):
    """
    Same interface as the original Model, but uses a highly optimized Triton
    kernel for ConvTranspose3d in the common case (CUDA, float32, groups==1,
    stride=1, padding=0, etc.). Falls back to PyTorch's implementation otherwise.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: tuple,
        stride: tuple = (1, 1, 1),
        padding: tuple = (0, 0, 0),
        output_padding: tuple = (0, 0, 0),
        groups: int = 1,
        bias: bool = False,
        dilation: tuple = (1, 1, 1),
    ) -> None:
        super().__init__()
        self.conv_transpose3d = nn.ConvTranspose3d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
            groups=groups,
            bias=bias,
            dilation=dilation,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        w = self.conv_transpose3d.weight
        b = self.conv_transpose3d.bias
        stride = self.conv_transpose3d.stride
        padding = self.conv_transpose3d.padding
        output_padding = self.conv_transpose3d.output_padding
        dilation = self.conv_transpose3d.dilation
        groups = self.conv_transpose3d.groups

        # CPU or unsupported configs: use PyTorch fallback
        if not x.is_cuda:
            return self.conv_transpose3d(x)

        return conv_transpose3d_triton(
            x,
            w,
            b,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
            dilation=dilation,
            groups=groups,
        )
```