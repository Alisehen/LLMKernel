You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU: 4090

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Grid layout & parallelism.

Metrics:
- sm__throughput.avg.pct_of_peak_sustained_elapsed (>60%)
- launch__grid_size

Rules:
- 1D: (cdiv(N, BLOCK))
- 2D: (cdiv(M, BLOCK_M), cdiv(N, BLOCK_N))
- 3D: (batch, cdiv(M, BLOCK_M), cdiv(N, BLOCK_N))
- >3D: flatten ONLY independent dims
- Prefer batch / head / expert / group parallelism before shrinking BLOCK
- For grouped operations: ensure group dimension is in grid (e.g., program_id(2) for groups)
- Change grid only if SM utilization is clearly low

Safety:
- Max 3 grid dims, static rank
- grid=(G0,G1,G2) must match tl.program_id(0/1/2)
- For grouped ops: verify group indexing is correct
- If unsure about correctness, do NOT change grid

Autotune:
- Autotune either BLOCK_* OR (num_warps, num_stages)
- If autotuning BLOCK_*, use grid=lambda META: (...)
- Never redefine BLOCK_* in both kernel and launch
- Max 2-3 configs to reduce compilation time



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def conv_transpose3d_fwd_kernel(
    x_ptr,  # [B, Ci, Di, Hi, Wi]
    w_ptr,  # [Ci, Co_g, Kd, Kh, Kw]
    b_ptr,  # [Co] or empty
    y_ptr,  # [B, Co, Do, Ho, Wo]
    B, Ci, Co, G,
    Di, Hi, Wi,
    Do, Ho, Wo,
    Ci_g, Co_g,
    Kd, Kh, Kw,
    stride_d, stride_h, stride_w,
    pad_d, pad_h, pad_w,
    out_pad_d, out_pad_h, out_pad_w,  # not used in indexing but kept for completeness
    x_stride_b, x_stride_c, x_stride_d, x_stride_h, x_stride_w,
    w_stride_ci, w_stride_co, w_stride_kd, w_stride_kh, w_stride_kw,
    y_stride_b, y_stride_c, y_stride_d, y_stride_h, y_stride_w,
    n_ctiles,
    has_bias: tl.constexpr,
    BLOCK_P: tl.constexpr,
    BLOCK_CO: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    # Program IDs
    pid_b = tl.program_id(axis=0)
    pid_co_blk = tl.program_id(axis=1)  # encodes group + channel tile within group
    pid_p = tl.program_id(axis=2)       # spatial tile

    # Decode group and channel tile within group
    g = pid_co_blk // n_ctiles
    co_tile = pid_co_blk % n_ctiles

    # Bounds check on group (should not be needed if grid is correct, but safe)
    # g in [0, G)
    # Early return not allowed; rely on grid correctness.

    # Channel tile within group
    co_start_local = co_tile * BLOCK_CO
    co_offsets_local = co_start_local + tl.arange(0, BLOCK_CO)
    co_mask_local = co_offsets_local < Co_g

    # Global output channel indices
    co_offsets_global = g * Co_g + co_offsets_local

    # Spatial tile (flattened Do*Ho*Wo index)
    P = Do * Ho * Wo
    p_start = pid_p * BLOCK_P
    p_offsets = p_start + tl.arange(0, BLOCK_P)
    p_mask = p_offsets < P

    # Decode flattened output indices into (do, ho, wo)
    p_tmp = p_offsets
    wo = p_tmp % Wo
    p_tmp = p_tmp // Wo
    ho = p_tmp % Ho
    do = p_tmp // Ho

    # Prepare accumulators: [BLOCK_P, BLOCK_CO]
    acc = tl.zeros((BLOCK_P, BLOCK_CO), dtype=tl.float32)

    # Total reduction size per group: Ci_g * Kd * Kh * Kw
    K_total = Ci_g * Kd * Kh * Kw

    # Loop over reduction dimension in chunks of BLOCK_K
    k_block = tl.arange(0, BLOCK_K)
    for k_start in range(0, K_total, BLOCK_K):
        k_offsets = k_start + k_block
        k_mask = k_offsets < K_total

        # Decode reduction index -> (ci_local, kd, kh, kw)
        tmp = k_offsets
        kw = tmp % Kw
        tmp = tmp // Kw
        kh = tmp % Kh
        tmp = tmp // Kh
        kd = tmp % Kd
        ci_local = tmp // Kd  # 0..Ci_g-1

        # Broadcast to [P, K] for spatial + kernel indexing
        do_mat = do[:, None]
        ho_mat = ho[:, None]
        wo_mat = wo[:, None]

        kd_mat = kd[None, :]
        kh_mat = kh[None, :]
        kw_mat = kw[None, :]

        # Compute input coordinates corresponding to each (output, kernel) pair
        id_ = do_mat + pad_d - kd_mat
        ih_ = ho_mat + pad_h - kh_mat
        iw_ = wo_mat + pad_w - kw_mat

        # Check alignment with stride
        align_d = (id_ % stride_d) == 0
        align_h = (ih_ % stride_h) == 0
        align_w = (iw_ % stride_w) == 0

        idi = id_ // stride_d
        ihi = ih_ // stride_h
        iwi = iw_ // stride_w

        # Range check for input indices
        in_range = (
            (idi >= 0) & (idi < Di) &
            (ihi >= 0) & (ihi < Hi) &
            (iwi >= 0) & (iwi < Wi)
        )

        # Combine masks: spatial bounds, reduction bounds, stride alignment
        mask_pk = (
            p_mask[:, None] &
            k_mask[None, :] &
            align_d & align_h & align_w &
            in_range
        )

        # Global input channel indices for this group
        ci_global = ci_local + g * Ci_g  # [K]
        ci_mat = ci_global[None, :]      # [1, K]

        # Compute input pointers for [P, K] tile
        ptrs_x = (
            x_ptr
            + pid_b * x_stride_b
            + ci_mat * x_stride_c
            + idi * x_stride_d
            + ihi * x_stride_h
            + iwi * x_stride_w
        )

        # Load input tile, cast to fp32
        a = tl.load(ptrs_x, mask=mask_pk, other=0.0)
        a = a.to(tl.float32)

        # Weight tile: [K, CO_tile]
        co_mat = co_offsets_local[None, :]  # [1, CO]
        ciw = ci_global[:, None]            # [K, 1]
        kdw = kd[:, None]
        khw = kh[:, None]
        kww = kw[:, None]

        ptrs_w = (
            w_ptr
            + ciw * w_stride_ci
            + co_mat * w_stride_co
            + kdw * w_stride_kd
            + khw * w_stride_kh
            + kww * w_stride_kw
        )
        mask_w = k_mask[:, None] & co_mask_local[None, :]

        b = tl.load(ptrs_w, mask=mask_w, other=0.0)
        b = b.to(tl.float32)

        # Matrix multiply-accumulate: [P, K] x [K, CO] -> [P, CO]
        acc += tl.dot(a, b, allow_tf32=True)

    # Optional bias addition
    if has_bias:
        bias_ptrs = b_ptr + co_offsets_global
        bias = tl.load(bias_ptrs, mask=co_mask_local, other=0.0)
        bias = bias.to(tl.float32)
        acc += bias[None, :]

    # Store results
    do_mat = do[:, None]
    ho_mat = ho[:, None]
    wo_mat = wo[:, None]
    co_mat_global = co_offsets_global[None, :]

    ptrs_y = (
        y_ptr
        + pid_b * y_stride_b
        + co_mat_global * y_stride_c
        + do_mat * y_stride_d
        + ho_mat * y_stride_h
        + wo_mat * y_stride_w
    )
    out_mask = p_mask[:, None] & co_mask_local[None, :]

    tl.store(ptrs_y, acc, mask=out_mask)


def conv_transpose3d_triton(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor | None,
    stride: tuple[int, int, int],
    padding: tuple[int, int, int],
    output_padding: tuple[int, int, int],
    groups: int,
) -> torch.Tensor:
    """
    High-performance ConvTranspose3d forward using Triton.

    Args are consistent with torch.nn.functional.conv_transpose3d.
    """
    assert x.is_cuda, "Triton ConvTranspose3d requires CUDA tensor"
    assert x.ndim == 5
    assert weight.ndim == 5

    # Shapes
    B, Ci, Di, Hi, Wi = x.shape
    Ci_w, Co_g, Kd, Kh, Kw = weight.shape
    assert Ci_w == Ci, "Weight in_channels must match input channels"

    Co = Co_g * groups
    Ci_g = Ci // groups
    Co_g_calc = Co // groups
    assert Co_g_calc == Co_g, "Invalid groups/out_channels configuration"
    assert Ci % groups == 0, "in_channels must be divisible by groups"

    sd, sh, sw = stride
    pd, ph, pw = padding
    opd, oph, opw = output_padding

    # Output shape (PyTorch formula)
    Do = (Di - 1) * sd - 2 * pd + Kd + opd
    Ho = (Hi - 1) * sh - 2 * ph + Kh + oph
    Wo = (Wi - 1) * sw - 2 * pw + Kw + opw

    # Contiguous buffers
    x_contig = x.contiguous()
    w_contig = weight.contiguous()
    if bias is not None:
        b_contig = bias.contiguous()
    else:
        b_contig = torch.empty(0, device=x.device, dtype=x.dtype)

    y = torch.empty(
        (B, Co, Do, Ho, Wo),
        device=x.device,
        dtype=x.dtype,
    )

    # Strides in elements
    x_stride_b, x_stride_c, x_stride_d, x_stride_h, x_stride_w = x_contig.stride()
    w_stride_ci, w_stride_co, w_stride_kd, w_stride_kh, w_stride_kw = w_contig.stride()
    y_stride_b, y_stride_c, y_stride_d, y_stride_h, y_stride_w = y.stride()

    # Tiling parameters (powers of two)
    BLOCK_P = 32
    BLOCK_CO = 32
    BLOCK_K = 32

    P = Do * Ho * Wo
    n_ctiles = triton.cdiv(Co_g, BLOCK_CO)

    grid = (
        B,
        groups * n_ctiles,
        triton.cdiv(P, BLOCK_P),
    )

    conv_transpose3d_fwd_kernel[grid](
        x_contig,
        w_contig,
        b_contig,
        y,
        B,
        Ci,
        Co,
        groups,
        Di,
        Hi,
        Wi,
        Do,
        Ho,
        Wo,
        Ci_g,
        Co_g,
        Kd,
        Kh,
        Kw,
        sd,
        sh,
        sw,
        pd,
        ph,
        pw,
        opd,
        oph,
        opw,
        x_stride_b,
        x_stride_c,
        x_stride_d,
        x_stride_h,
        x_stride_w,
        w_stride_ci,
        w_stride_co,
        w_stride_kd,
        w_stride_kh,
        w_stride_kw,
        y_stride_b,
        y_stride_c,
        y_stride_d,
        y_stride_h,
        y_stride_w,
        n_ctiles,
        has_bias=(bias is not None),
        BLOCK_P=BLOCK_P,
        BLOCK_CO=BLOCK_CO,
        BLOCK_K=BLOCK_K,
    )

    return y


class ModelNew(nn.Module):
    """
    Triton-accelerated replacement for the given ConvTranspose3d model.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: tuple,
        stride: tuple = (1, 1, 1),
        padding: tuple = (0, 0, 0),
        output_padding: tuple = (0, 0, 0),
        groups: int = 1,
        bias: bool = False,
    ) -> None:
        super().__init__()
        # Use PyTorch module to own parameters & initialization
        self.conv_transpose3d = nn.ConvTranspose3d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
            groups=groups,
            bias=bias,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # CPU fallback for correctness
        if not x.is_cuda:
            return self.conv_transpose3d(x)

        w = self.conv_transpose3d.weight
        b = self.conv_transpose3d.bias
        stride = self.conv_transpose3d.stride
        padding = self.conv_transpose3d.padding
        output_padding = self.conv_transpose3d.output_padding
        groups = self.conv_transpose3d.groups

        return conv_transpose3d_triton(
            x,
            w,
            b,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
            groups=groups,
        )
```

[NCU PROFILING METRICS]
{
  "conv_transpose3d_fwd_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 65.4,
    "launch__grid_size": 22688,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 33.1,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 0.67,
    "lts__t_sector_hit_rate.pct": 99.09
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

## CRITICAL — Code MUST compile and run:
1. EVERY kernel function MUST have `@triton.jit` decorator
2. Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
3. BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
4. `tl.program_id(axis)` only supports axis = 0, 1, 2
5. No `continue`, `break`, `return` inside loops — use masking
6. No tensor indexing with loop vars: `x[:, i]` is INVALID
7. mask shape MUST match data shape in tl.load/tl.store

## Missing Triton Functions (implement manually):
- tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

## OUTPUT FORMAT (STRICT):
1. Imports: torch, torch.nn, triton, triton.language as tl
2. @triton.jit decorated kernel function(s)
3. Wrapper function(s) for grid calculation and kernel launch
4. class ModelNew(nn.Module) that calls your kernels

Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
