```python
# <complete ModelNew code with optimized Triton kernels>
import torch
import torch.nn as nn
import triton
import triton.language as tl


_sum_reduce_configs = [
    triton.Config(
        {'BLOCK_SLICE': 64, 'BLOCK_R': 64, 'GROUP_R': 4},
        num_warps=4,
        num_stages=2,
    ),
    triton.Config(
        {'BLOCK_SLICE': 128, 'BLOCK_R': 128, 'GROUP_R': 2},
        num_warps=4,
        num_stages=2,
    ),
    triton.Config(
        {'BLOCK_SLICE': 128, 'BLOCK_R': 256, 'GROUP_R': 2},
        num_warps=8,
        num_stages=3,
    ),
    triton.Config(
        {'BLOCK_SLICE': 256, 'BLOCK_R': 128, 'GROUP_R': 2},
        num_warps=8,
        num_stages=2,
    ),
    triton.Config(
        {'BLOCK_SLICE': 256, 'BLOCK_R': 64, 'GROUP_R': 4},
        num_warps=8,
        num_stages=2,
    ),
]


@triton.autotune(configs=_sum_reduce_configs, key=['inner_size', 'reduce_size'])
@triton.jit
def sum_reduce_keepdim_kernel(
    x_ptr,
    out_ptr,
    reduce_size,
    inner_size,
    total_slices,
    BLOCK_SLICE: tl.constexpr,
    BLOCK_R: tl.constexpr,
    GROUP_R: tl.constexpr,
):
    pid_slice = tl.program_id(axis=0)
    pid_reduce_group = tl.program_id(axis=1)

    slice_offsets = pid_slice * BLOCK_SLICE + tl.arange(0, BLOCK_SLICE)
    slice_mask = slice_offsets < total_slices
    slice_idx64 = slice_offsets.to(tl.int64)

    outer_idx = slice_idx64 // inner_size
    inner_idx = slice_idx64 - outer_idx * inner_size
    reduce_span = reduce_size * inner_size
    base = outer_idx * reduce_span + inner_idx

    acc = tl.zeros((BLOCK_SLICE,), dtype=tl.float32)
    reduce_lane = tl.arange(0, BLOCK_R)

    tile_group = pid_reduce_group * GROUP_R
    tile_group = tile_group.to(tl.int64)

    for group_offset in tl.static_range(GROUP_R):
        tile_id = tile_group + group_offset
        r_start = tile_id * BLOCK_R
        r_start = r_start.to(tl.int64)

        tile_active = r_start < reduce_size
        reduce_idx = r_start + reduce_lane.to(tl.int64)
        reduce_mask = tile_active & (reduce_idx < reduce_size)

        ptrs = base[:, None] + reduce_idx[None, :] * inner_size
        mask = slice_mask[:, None] & reduce_mask[None, :]
        vals = tl.load(
            x_ptr + ptrs,
            mask=mask,
            other=0.0,
            cache_modifier=".cg",
        ).to(tl.float32)
        acc += tl.sum(vals, axis=1)

    tl.atomic_add(out_ptr + slice_idx64, acc, mask=slice_mask)


def triton_sum_keepdim(x: torch.Tensor, dim: int) -> torch.Tensor:
    if dim < 0:
        dim += x.ndim
    x = x.contiguous()
    reduce_size = x.shape[dim]
    out_shape = list(x.shape)
    out_shape[dim] = 1

    if reduce_size == 0:
        return torch.zeros(out_shape, device=x.device, dtype=x.dtype)

    inner_size = 1
    for i in range(dim + 1, x.ndim):
        inner_size *= x.shape[i]
    outer_size = x.numel() // (reduce_size * inner_size)
    total_slices = outer_size * inner_size
    if total_slices == 0:
        return torch.zeros(out_shape, device=x.device, dtype=x.dtype)

    out_accum = torch.zeros(out_shape, device=x.device, dtype=torch.float32)

    def grid(meta):
        block_slice = meta['BLOCK_SLICE']
        block_r = meta['BLOCK_R']
        group_r = meta['GROUP_R']
        slice_tiles = triton.cdiv(total_slices, block_slice)
        reduce_tiles = triton.cdiv(reduce_size, block_r)
        reduce_groups = triton.cdiv(reduce_tiles, group_r)
        return slice_tiles, reduce_groups

    sum_reduce_keepdim_kernel[grid](
        x,
        out_accum,
        reduce_size,
        inner_size,
        total_slices,
    )

    return out_accum.to(x.dtype)


class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_sum_keepdim(x, self.dim)
```