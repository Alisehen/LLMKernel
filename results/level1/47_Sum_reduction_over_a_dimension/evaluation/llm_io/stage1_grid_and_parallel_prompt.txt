You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Grid layout & parallelism.

Metrics:
- sm__throughput.avg.pct_of_peak_sustained_elapsed (>60%)
- launch__grid_size

Rules:
- 1D: (cdiv(N, BLOCK))
- 2D: (cdiv(M, BLOCK_M), cdiv(N, BLOCK_N))
- 3D: (batch, cdiv(M, BLOCK_M), cdiv(N, BLOCK_N))
- >3D: flatten ONLY independent dims
- Prefer batch / head / expert parallelism before shrinking BLOCK
- Change grid only if SM utilization is clearly low

Safety:
- Max 3 grid dims, static rank
- grid=(G0,G1,G2) must match tl.program_id(0/1/2)
- If unsure about correctness, do NOT change grid

Autotune:
- Autotune either BLOCK_* OR (num_warps, num_stages)
- If autotuning BLOCK_*, use grid=lambda META: (...)
- Never redefine BLOCK_* in both kernel and launch



[CURRENT CODE]
```python
# <complete ModelNew code with optimized Triton kernels>
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def sum_reduce_keepdim_kernel(
    x_ptr,
    out_ptr,
    reduce_size,
    inner_size,
    total_slices,
    BLOCK_SLICE: tl.constexpr,
    BLOCK_R: tl.constexpr,
):
    pid_slice = tl.program_id(0)
    pid_reduce = tl.program_id(1)

    slice_idx = pid_slice * BLOCK_SLICE + tl.arange(0, BLOCK_SLICE)
    reduce_idx = pid_reduce * BLOCK_R + tl.arange(0, BLOCK_R)

    slice_mask = slice_idx < total_slices
    reduce_mask = reduce_idx < reduce_size

    slice_idx64 = slice_idx.to(tl.int64)
    reduce_idx64 = reduce_idx.to(tl.int64)

    outer_idx = slice_idx64 // inner_size
    inner_idx = slice_idx64 - outer_idx * inner_size
    base = outer_idx * reduce_size * inner_size + inner_idx
    ptrs = base[:, None] + reduce_idx64[None, :] * inner_size

    mask = slice_mask[:, None] & reduce_mask[None, :]
    vals = tl.load(x_ptr + ptrs, mask=mask, other=0.0).to(tl.float32)
    acc = tl.sum(vals, axis=1)

    tl.atomic_add(out_ptr + slice_idx64, acc, mask=slice_mask)


def triton_sum_keepdim(x: torch.Tensor, dim: int) -> torch.Tensor:
    if dim < 0:
        dim += x.ndim
    x = x.contiguous()
    reduce_size = x.shape[dim]
    out_shape = list(x.shape)
    out_shape[dim] = 1
    if reduce_size == 0:
        return torch.zeros(out_shape, device=x.device, dtype=x.dtype)

    inner_size = 1
    for i in range(dim + 1, x.ndim):
        inner_size *= x.shape[i]
    outer_size = x.numel() // (reduce_size * inner_size)
    total_slices = outer_size * inner_size
    if total_slices == 0:
        return torch.zeros(out_shape, device=x.device, dtype=x.dtype)

    out_accum = torch.zeros(out_shape, device=x.device, dtype=torch.float32)

    if inner_size >= 256:
        BLOCK_SLICE = 256
    elif inner_size >= 64:
        BLOCK_SLICE = 128
    else:
        BLOCK_SLICE = 64

    if reduce_size >= 512:
        BLOCK_R = 256
    elif reduce_size >= 128:
        BLOCK_R = 128
    else:
        BLOCK_R = 64

    grid = (
        triton.cdiv(total_slices, BLOCK_SLICE),
        triton.cdiv(reduce_size, BLOCK_R),
    )

    sum_reduce_keepdim_kernel[grid](
        x,
        out_accum,
        reduce_size,
        inner_size,
        total_slices,
        BLOCK_SLICE=BLOCK_SLICE,
        BLOCK_R=BLOCK_R,
        num_warps=8 if BLOCK_R >= 256 else 4,
        num_stages=2,
    )

    return out_accum.to(x.dtype)


class ModelNew(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_sum_keepdim(x, self.dim)
```

[NCU PROFILING METRICS]
{
  "sum_reduce_keepdim_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 3.46,
    "launch__grid_size": 32768,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 98.2,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 78.9,
    "lts__t_sector_hit_rate.pct": 51.49
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
