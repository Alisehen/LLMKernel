Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 34, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 1490, in full
    shape = _shape_check_impl(shape)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 1474, in _shape_check_impl
    validate_block_shape(shape)
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/_utils.py", line 14, in validate_block_shape
    raise TypeError(f"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d)}]")
TypeError: Shape element 2 must have type `constexpr[int]`, got `constexpr[<class 'triton.language.core.tensor'>]

The above exception was the direct cause of the following exception:

triton.compiler.errors.CompilationError: at 10:11:
def zeros(shape, dtype):
    """
    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.

    :param shape: Shape of the new array, e.g., (8, 16) or (8, )
    :type shape: tuple of ints
    :param dtype: Data-type of the new array, e.g., :code:`tl.float16`
    :type dtype: DType
    """
    return core.full(shape, 0, dtype)
           ^

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 535, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_170112_batch_range40to70_deepseek_deepseek/64_conv_transposed_1D/code/kernel_20251213_013355.py", line 394, in forward
    return triton_conv_transpose1d(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_170112_batch_range40to70_deepseek_deepseek/64_conv_transposed_1D/code/kernel_20251213_013355.py", line 292, in triton_conv_transpose1d
    conv_transpose1d_kernel_large_optimized[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 192, in run
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 192, in <dictcomp>
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 170, in _bench
    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/testing.py", line 145, in do_bench
    fn()
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 156, in kernel_call
    self.fn.run(
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 278, in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 81, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 45:24:
    len_out_mask = len_out_range < length_out

    # Initialize accumulator
    acc = tl.zeros((BLOCK_SIZE_OUT_CHANNELS, BLOCK_SIZE_LENGTH_OUT), dtype=tl.float32)

    # Process input channels in blocks with improved memory access patterns
    for c_in_start in range(0, in_channels, BLOCK_SIZE_IN_CHANNELS):
        c_in_range = c_in_start + tl.arange(0, BLOCK_SIZE_IN_CHANNELS)
        in_ch_mask = c_in_range < in_channels

        # Pre-load weight slice for all kernel positions
        weight_slices = tl.zeros((BLOCK_SIZE_OUT_CHANNELS, BLOCK_SIZE_IN_CHANNELS, kernel_size), dtype=tl.float32)
                        ^

History Error:
None


PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Performs a transposed 1D convolution operation.

    Args:
        in_channels (int): Number of channels in the input tensor.
        out_channels (int): Number of channels produced by the convolution.
        kernel_size (int): Size of the convolution kernel.
        stride (int, optional): Stride of the convolution. Defaults to 1.
        padding (int, optional): Padding applied to the input. Defaults to 0.
        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.
        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.
        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, output_padding: int = 0, groups: int = 1, bias: bool = False):
        super(Model, self).__init__()
        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Performs the transposed 1D convolution.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).
        """
        return self.conv1d_transpose(x)

# Test code
batch_size = 64
in_channels = 128
out_channels = 128
kernel_size = 3
# much larger signal length for heavier workload
length = 65536

def get_inputs():
    x = torch.rand(batch_size, in_channels, length)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization
```

Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl
import math


@triton.jit
def conv_transpose1d_kernel_small(
    # Pointers to tensors
    input_ptr,
    weight_ptr,
    bias_ptr,
    output_ptr,
    # Tensor dimensions
    batch_size,
    in_channels,
    out_channels,
    length_in,
    length_out,
    kernel_size,
    stride,
    padding,
    output_padding,
    # Meta-parameters
    BLOCK_SIZE_IN_CHANNELS: tl.constexpr,
    BLOCK_SIZE_LENGTH_OUT: tl.constexpr,
    BLOCK_SIZE_KERNEL: tl.constexpr,
    HAS_BIAS: tl.constexpr,
):
    # Program ID mapping - batch dimension parallelized
    pid_batch = tl.program_id(axis=0)
    pid_out_ch = tl.program_id(axis=1)
    pid_len_out = tl.program_id(axis=2)

    # Block ranges
    out_ch_range = pid_out_ch * BLOCK_SIZE_IN_CHANNELS + tl.arange(0, BLOCK_SIZE_IN_CHANNELS)
    len_out_range = pid_len_out * BLOCK_SIZE_LENGTH_OUT + tl.arange(0, BLOCK_SIZE_LENGTH_OUT)

    # Masks
    out_ch_mask = out_ch_range < out_channels
    len_out_mask = len_out_range < length_out

    # Accumulator initialization
    acc = tl.zeros((BLOCK_SIZE_IN_CHANNELS, BLOCK_SIZE_LENGTH_OUT), dtype=tl.float32)

    # Loop over kernel positions in blocks
    for k_block in range(0, kernel_size, BLOCK_SIZE_KERNEL):
        k_range = tl.arange(0, BLOCK_SIZE_KERNEL)
        k_global = k_block + k_range
        k_mask = k_global < kernel_size

        # Compute corresponding input positions
        len_out_expanded = tl.expand_dims(len_out_range, axis=0)  # [1, BLOCK_L]
        k_expanded = tl.expand_dims(k_range, axis=1)  # [BLOCK_K, 1]
        
        len_in_float = (len_out_expanded.to(tl.float32) - k_expanded.to(tl.float32) + float(padding)) / float(stride)
        len_in_int = len_in_float.to(tl.int32)
        valid_mask = (len_in_float == len_in_int.to(tl.float32)) & (len_in_int >= 0) & (len_in_int < length_in)
        valid_mask = valid_mask & tl.expand_dims(k_mask, axis=1)  # [BLOCK_K, BLOCK_L]

        # Loop over input channels in blocks
        for c_in_start in range(0, in_channels, BLOCK_SIZE_IN_CHANNELS):
            in_ch_range = c_in_start + tl.arange(0, BLOCK_SIZE_IN_CHANNELS)
            in_ch_mask = in_ch_range < in_channels

            # Load weight block: [BLOCK_IN, BLOCK_OUT, BLOCK_K]
            w_offset = ((in_ch_range[:, None, None] * out_channels + 
                        out_ch_range[None, :, None]) * kernel_size + 
                        k_global[None, None, :])
            w_mask = in_ch_mask[:, None, None] & out_ch_mask[None, :, None] & k_mask[None, None, :]
            weight_block = tl.load(weight_ptr + w_offset, mask=w_mask, other=0.0)

            # Process each kernel position in the block
            for k_idx in range(BLOCK_SIZE_KERNEL):
                k_current = k_block + k_idx
                if k_current >= kernel_size:
                    continue
                    
                # Get valid mask for this kernel position
                k_valid_mask = tl.view(valid_mask[k_idx, :], (1, BLOCK_SIZE_LENGTH_OUT))
                
                # Get input positions for this kernel position
                len_in_for_k = tl.view(len_in_int[k_idx, :], (1, BLOCK_SIZE_LENGTH_OUT))
                
                # Load input block: [BLOCK_IN, BLOCK_L]
                input_offset = (pid_batch * in_channels * length_in + 
                               in_ch_range[:, None] * length_in + 
                               len_in_for_k)
                input_mask = in_ch_mask[:, None] & k_valid_mask
                x_block = tl.load(input_ptr + input_offset, mask=input_mask, other=0.0)

                # Get weight slice for this kernel position: [BLOCK_IN, BLOCK_OUT]
                w_slice = weight_block[:, :, k_idx]
                
                # Accumulate: acc[BLOCK_OUT, BLOCK_L] += w_slice^T @ x_block
                acc += tl.dot(tl.trans(w_slice), x_block, allow_tf32=True)

    # Add bias if present
    if HAS_BIAS:
        bias = tl.load(bias_ptr + out_ch_range, mask=out_ch_mask, other=0.0)
        acc += bias[:, None]

    # Store output
    output_offset = (pid_batch * out_channels * length_out + 
                     out_ch_range[:, None] * length_out + 
                     len_out_range[None, :])
    output_mask = out_ch_mask[:, None] & len_out_mask
    tl.store(output_ptr + output_offset, acc, mask=output_mask)


@triton.autotune(
    configs=[
        # Optimized for Ada Lovelace - maximize SM utilization and memory throughput
        triton.Config(
            {'BLOCK_SIZE_LENGTH_OUT': 128, 'BLOCK_SIZE_OUT_CHANNELS': 64, 'BLOCK_SIZE_IN_CHANNELS': 32},
            num_warps=8, num_stages=4
        ),
        triton.Config(
            {'BLOCK_SIZE_LENGTH_OUT': 64, 'BLOCK_SIZE_OUT_CHANNELS': 128, 'BLOCK_SIZE_IN_CHANNELS': 32},
            num_warps=8, num_stages=4
        ),
        triton.Config(
            {'BLOCK_SIZE_LENGTH_OUT': 64, 'BLOCK_SIZE_OUT_CHANNELS': 64, 'BLOCK_SIZE_IN_CHANNELS': 64},
            num_warps=8, num_stages=4
        ),
        # Higher warp count for better occupancy and latency hiding
        triton.Config(
            {'BLOCK_SIZE_LENGTH_OUT': 64, 'BLOCK_SIZE_OUT_CHANNELS': 64, 'BLOCK_SIZE_IN_CHANNELS': 32},
            num_warps=16, num_stages=4
        ),
        triton.Config(
            {'BLOCK_SIZE_LENGTH_OUT': 128, 'BLOCK_SIZE_OUT_CHANNELS': 32, 'BLOCK_SIZE_IN_CHANNELS': 32},
            num_warps=16, num_stages=4
        ),
        # Larger blocks for better tensor core utilization
        triton.Config(
            {'BLOCK_SIZE_LENGTH_OUT': 256, 'BLOCK_SIZE_OUT_CHANNELS': 64, 'BLOCK_SIZE_IN_CHANNELS': 32},
            num_warps=16, num_stages=4
        ),
        triton.Config(
            {'BLOCK_SIZE_LENGTH_OUT': 128, 'BLOCK_SIZE_OUT_CHANNELS': 128, 'BLOCK_SIZE_IN_CHANNELS': 32},
            num_warps=16, num_stages=4
        ),
        # Configs with different num_stages for latency hiding
        triton.Config(
            {'BLOCK_SIZE_LENGTH_OUT': 64, 'BLOCK_SIZE_OUT_CHANNELS': 64, 'BLOCK_SIZE_IN_CHANNELS': 32},
            num_warps=8, num_stages=5
        ),
        triton.Config(
            {'BLOCK_SIZE_LENGTH_OUT': 64, 'BLOCK_SIZE_OUT_CHANNELS': 64, 'BLOCK_SIZE_IN_CHANNELS': 32},
            num_warps=16, num_stages=5
        ),
    ],
    key=['in_channels', 'out_channels', 'length_out', 'kernel_size', 'stride']
)
@triton.jit
def conv_transpose1d_kernel_large_optimized(
    # Pointers to tensors
    input_ptr,
    weight_ptr,
    bias_ptr,
    output_ptr,
    # Tensor dimensions
    batch_size,
    in_channels,
    out_channels,
    length_in,
    length_out,
    kernel_size,
    stride,
    padding,
    output_padding,
    # Meta-parameters
    BLOCK_SIZE_LENGTH_OUT: tl.constexpr,
    BLOCK_SIZE_OUT_CHANNELS: tl.constexpr,
    BLOCK_SIZE_IN_CHANNELS: tl.constexpr,
    HAS_BIAS: tl.constexpr,
):
    # Program IDs - optimized for SM utilization with improved latency hiding
    pid_batch = tl.program_id(axis=0)
    pid_out_ch_block = tl.program_id(axis=1)
    pid_len_block = tl.program_id(axis=2)
    
    # Block ranges
    out_ch_range = pid_out_ch_block * BLOCK_SIZE_OUT_CHANNELS + tl.arange(0, BLOCK_SIZE_OUT_CHANNELS)
    len_out_range = pid_len_block * BLOCK_SIZE_LENGTH_OUT + tl.arange(0, BLOCK_SIZE_LENGTH_OUT)
    
    # Masks
    out_ch_mask = out_ch_range < out_channels
    len_out_mask = len_out_range < length_out
    
    # Initialize accumulator
    acc = tl.zeros((BLOCK_SIZE_OUT_CHANNELS, BLOCK_SIZE_LENGTH_OUT), dtype=tl.float32)
    
    # Process input channels in blocks with improved memory access patterns
    for c_in_start in range(0, in_channels, BLOCK_SIZE_IN_CHANNELS):
        c_in_range = c_in_start + tl.arange(0, BLOCK_SIZE_IN_CHANNELS)
        in_ch_mask = c_in_range < in_channels
        
        # Pre-load weight slice for all kernel positions
        weight_slices = tl.zeros((BLOCK_SIZE_OUT_CHANNELS, BLOCK_SIZE_IN_CHANNELS, kernel_size), dtype=tl.float32)
        
        # Load all kernel positions at once for better memory coalescing
        for k in range(kernel_size):
            w_offset = ((c_in_range[None, :] * out_channels + out_ch_range[:, None]) * kernel_size + k)
            w_mask = in_ch_mask[None, :] & out_ch_mask[:, None]
            weight_slice = tl.load(weight_ptr + w_offset, mask=w_mask, other=0.0)
            weight_slices = tl.where(tl.arange(0, kernel_size)[None, None, :] == k, 
                                     weight_slice[:, :, None], weight_slices)
        
        # Process each kernel position
        for k in range(kernel_size):
            # Compute input positions for this kernel position
            numerator = len_out_range - k + padding
            len_in_int = numerator // stride
            remainder = numerator % stride
            valid_mask = (remainder == 0) & (len_in_int >= 0) & (len_in_int < length_in)
            
            # Extract weight slice for this kernel position: [BLOCK_OUT, BLOCK_IN]
            weight_slice = weight_slices[:, :, k]
            
            # Load input block with improved memory coalescing
            input_offset = (pid_batch * in_channels * length_in + 
                           c_in_range[None, :] * length_in + 
                           len_in_int[:, None])
            input_mask = valid_mask[:, None] & in_ch_mask[None, :]
            x_block = tl.load(input_ptr + input_offset, mask=input_mask, other=0.0)
            
            # Accumulate using tensor cores with transposed weight for better utilization
            acc += tl.dot(weight_slice, tl.trans(x_block), allow_tf32=True)
    
    # Add bias if present
    if HAS_BIAS:
        bias = tl.load(bias_ptr + out_ch_range, mask=out_ch_mask, other=0.0)
        acc += bias[:, None]
    
    # Store result with coalesced writes
    output_offset = (pid_batch * out_channels * length_out + 
                     out_ch_range[:, None] * length_out + 
                     len_out_range[None, :])
    output_mask = out_ch_mask[:, None] & len_out_mask
    tl.store(output_ptr + output_offset, acc, mask=output_mask)


def triton_conv_transpose1d(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor = None,
    stride: int = 1,
    padding: int = 0,
    output_padding: int = 0,
    groups: int = 1,
) -> torch.Tensor:
    # Ensure tensors are contiguous
    x = x.contiguous()
    weight = weight.contiguous()
    if bias is not None:
        bias = bias.contiguous()
    
    # Get dimensions
    batch_size, in_channels, length_in = x.shape
    out_channels = weight.shape[1] * groups
    kernel_size = weight.shape[2]
    
    # Calculate output length
    length_out = (length_in - 1) * stride - 2 * padding + kernel_size + output_padding
    
    # Allocate output tensor
    output = torch.empty(
        batch_size, 
        out_channels, 
        length_out, 
        device=x.device, 
        dtype=x.dtype
    )
    
    # For simplicity, assume groups=1
    if groups != 1:
        raise ValueError("Groups > 1 not implemented for Triton kernel")
    
    # Choose kernel based on dimensions with optimized thresholds for Ada Lovelace
    total_elements = batch_size * out_channels * length_out
    if total_elements >= 2048 and in_channels >= 32 and out_channels >= 32:
        # Use autotuned large kernel for better SM utilization
        grid = lambda META: (
            batch_size,
            triton.cdiv(out_channels, META['BLOCK_SIZE_OUT_CHANNELS']),
            triton.cdiv(length_out, META['BLOCK_SIZE_LENGTH_OUT']),
        )
        
        conv_transpose1d_kernel_large_optimized[grid](
            x, weight, bias, output,
            batch_size, in_channels, out_channels,
            length_in, length_out, kernel_size,
            stride, padding, output_padding,
            HAS_BIAS=bias is not None,
        )
    else:
        # Use small kernel with optimized block sizes for small problems
        BLOCK_SIZE_IN_CHANNELS = 32
        BLOCK_SIZE_LENGTH_OUT = 64
        BLOCK_SIZE_KERNEL = min(kernel_size, 8)
        
        grid = (
            batch_size,
            triton.cdiv(out_channels, BLOCK_SIZE_IN_CHANNELS),
            triton.cdiv(length_out, BLOCK_SIZE_LENGTH_OUT),
        )
        
        conv_transpose1d_kernel_small[grid](
            x, weight, bias, output,
            batch_size, in_channels, out_channels,
            length_in, length_out, kernel_size,
            stride, padding, output_padding,
            BLOCK_SIZE_IN_CHANNELS=BLOCK_SIZE_IN_CHANNELS,
            BLOCK_SIZE_LENGTH_OUT=BLOCK_SIZE_LENGTH_OUT,
            BLOCK_SIZE_KERNEL=BLOCK_SIZE_KERNEL,
            HAS_BIAS=bias is not None,
        )
    
    return output


class ModelNew(nn.Module):
    """
    Performs a transposed 1D convolution operation using optimized Triton kernels.

    Args:
        in_channels (int): Number of channels in the input tensor.
        out_channels (int): Number of channels produced by the convolution.
        kernel_size (int): Size of the convolution kernel.
        stride (int, optional): Stride of the convolution. Defaults to 1.
        padding (int, optional): Padding applied to the input. Defaults to 0.
        output_padding (int, optional): Additional size added to one side of the output shape. Defaults to 0.
        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.
        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.
    """
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: int = 0,
        output_padding: int = 0,
        groups: int = 1,
        bias: bool = False,
    ):
        super().__init__()
        
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.output_padding = output_padding
        self.groups = groups
        
        # Initialize weights
        self.weight = nn.Parameter(
            torch.empty(
                in_channels,
                out_channels // groups,
                kernel_size,
            )
        )
        
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter('bias', None)
            
        self.reset_parameters()
    
    def reset_parameters(self):
        # Use Kaiming initialization for weights
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
            nn.init.uniform_(self.bias, -bound, bound)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Performs the transposed 1D convolution using optimized Triton kernels.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).
        """
        return triton_conv_transpose1d(
            x,
            self.weight,
            self.bias,
            self.stride,
            self.padding,
            self.output_padding,
            self.groups,
        )
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes

```python
# <corrected code>
```
