Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 34, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/core.py", line 2000, in store
    return semantic.store(pointer, value, mask, boundary_check, cache_modifier, eviction_policy, _builder)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/semantic.py", line 1361, in store
    return _store_legacy(ptr, val, mask, boundary_check, cache, eviction, builder)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/language/semantic.py", line 1306, in _store_legacy
    raise ValueError(f"Unsupported ptr type {ptr.type.__repr__()} in `tl.store`")
ValueError: Unsupported ptr type <['256'], fp32> in `tl.store`

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 535, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_170112_batch_range40to70_deepseek_deepseek/40_LayerNorm/code/kernel_20251212_171709.py", line 368, in forward
    output, _, _ = triton_layer_norm_forward(x, self.weight, self.bias, self.eps)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_170112_batch_range40to70_deepseek_deepseek/40_LayerNorm/code/kernel_20251212_171709.py", line 270, in triton_layer_norm_forward
    layer_norm_forward_partial_kernel[grid_partial](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 278, in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 81, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 37:4:
    x_vec = tl.load(x_ptr_sample + offsets, mask=mask, other=0.0)

    # Compute thread-local sums
    thread_sum = tl.sum(x_vec, axis=0)
    thread_sum_sq = tl.sum(x_vec * x_vec, axis=0)

    # Shared memory for block-level reduction
    shared_sum = tl.full((BLOCK_SIZE_N // VECTOR_SIZE,), 0.0, dtype=tl.float32)
    shared_sum_sq = tl.full((BLOCK_SIZE_N // VECTOR_SIZE,), 0.0, dtype=tl.float32)

    # Store thread sums in shared memory
    tl.store(shared_sum + tid, thread_sum)
    ^

History Error:
Previous Repair Attempts (avoid repeating these errors):
Attempt 1:
    thread_count = tl.sum(mask, axis=0)

    # Reduce within warp first (warp-level reduction)
    warp_sum = tl.reduce(thread_sum, 0, tl.sum)
               ^



PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Simple model that performs Layer Normalization.
    """
    def __init__(self, normalized_shape: tuple):
        """
        Initializes the LayerNorm layer.

        Args:
            normalized_shape (tuple): Shape of the input tensor to be normalized.
        """
        super(Model, self).__init__()
        self.ln = nn.LayerNorm(normalized_shape=normalized_shape)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Applies Layer Normalization to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (*, normalized_shape).

        Returns:
            torch.Tensor: Output tensor with Layer Normalization applied, same shape as input.
        """
        return self.ln(x)

batch_size = 16
features = 64
dim1 = 256
dim2 = 256

def get_inputs():
    x = torch.rand(batch_size, features, dim1, dim2)
    return [x]

def get_init_inputs():
    return [(features, dim1, dim2)]
```

Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def layer_norm_forward_partial_kernel(
    x_ptr,
    partial_sum_ptr,
    partial_sum_sq_ptr,
    n_elements_per_sample,
    sample_stride,
    BLOCK_SIZE_N: tl.constexpr,
    VECTOR_SIZE: tl.constexpr,
):
    """
    Optimized partial kernel with vectorized loads and shared memory reduction.
    Each thread processes VECTOR_SIZE elements for better memory coalescing.
    """
    pid_sample = tl.program_id(axis=0)
    pid_block = tl.program_id(axis=1)
    tid = tl.program_id(axis=2)
    
    # Calculate feature block boundaries with vectorization
    block_start = pid_block * BLOCK_SIZE_N
    thread_start = block_start + tid * VECTOR_SIZE
    offsets = thread_start + tl.arange(0, VECTOR_SIZE)
    mask = offsets < n_elements_per_sample
    
    # Vectorized load
    x_ptr_sample = x_ptr + pid_sample * sample_stride
    x_vec = tl.load(x_ptr_sample + offsets, mask=mask, other=0.0)
    
    # Compute thread-local sums
    thread_sum = tl.sum(x_vec, axis=0)
    thread_sum_sq = tl.sum(x_vec * x_vec, axis=0)
    
    # Shared memory for block-level reduction
    shared_sum = tl.full((BLOCK_SIZE_N // VECTOR_SIZE,), 0.0, dtype=tl.float32)
    shared_sum_sq = tl.full((BLOCK_SIZE_N // VECTOR_SIZE,), 0.0, dtype=tl.float32)
    
    # Store thread sums in shared memory
    tl.store(shared_sum + tid, thread_sum)
    tl.store(shared_sum_sq + tid, thread_sum_sq)
    tl.debug_barrier()
    
    # Tree reduction in shared memory
    stride = BLOCK_SIZE_N // (2 * VECTOR_SIZE)
    while stride > 0:
        if tid < stride:
            s1 = tl.load(shared_sum + tid)
            s2 = tl.load(shared_sum + tid + stride)
            tl.store(shared_sum + tid, s1 + s2)
            
            ss1 = tl.load(shared_sum_sq + tid)
            ss2 = tl.load(shared_sum_sq + tid + stride)
            tl.store(shared_sum_sq + tid, ss1 + ss2)
        stride //= 2
        tl.debug_barrier()
    
    # Only first thread writes block result
    if tid == 0:
        block_sum = tl.load(shared_sum)
        block_sum_sq = tl.load(shared_sum_sq)
        partial_idx = pid_sample * tl.cdiv(n_elements_per_sample, BLOCK_SIZE_N) + pid_block
        tl.store(partial_sum_ptr + partial_idx, block_sum)
        tl.store(partial_sum_sq_ptr + partial_idx, block_sum_sq)

@triton.jit
def layer_norm_forward_final_kernel(
    x_ptr,
    weight_ptr,
    bias_ptr,
    output_ptr,
    mean_ptr,
    rstd_ptr,
    partial_sum_ptr,
    partial_sum_sq_ptr,
    n_elements_per_sample,
    sample_stride,
    weight_bias_stride,
    num_blocks_per_sample,
    eps,
    BLOCK_SIZE_N: tl.constexpr,
    VECTOR_SIZE: tl.constexpr,
):
    """
    Optimized final kernel with warp-level reduction and vectorized operations.
    """
    pid = tl.program_id(axis=0)
    warp_id = tl.program_id(axis=1)
    lane_id = warp_id * 32 + tl.arange(0, 32)
    num_warps = 32  # Fixed number of warps for reduction
    
    # Pointers for this sample
    x_sample_ptr = x_ptr + pid * sample_stride
    output_sample_ptr = output_ptr + pid * sample_stride
    partial_base = pid * num_blocks_per_sample
    
    # Warp-stride reduction of partial sums
    sum_acc = 0.0
    sum_sq_acc = 0.0
    
    # Each warp handles a subset of blocks
    for block_idx in range(warp_id, num_blocks_per_sample, num_warps):
        partial_idx = partial_base + block_idx
        sum_acc += tl.load(partial_sum_ptr + partial_idx)
        sum_sq_acc += tl.load(partial_sum_sq_ptr + partial_idx)
    
    # Warp reduction using shuffle operations
    warp_sum = tl.sum(sum_acc, axis=0)
    warp_sum_sq = tl.sum(sum_sq_acc, axis=0)
    
    # Cross-warp reduction using shared memory
    shared_sum = tl.full((num_warps,), 0.0, dtype=tl.float32)
    shared_sum_sq = tl.full((num_warps,), 0.0, dtype=tl.float32)
    
    if tl.arange(0, 32) == 0:  # First lane in each warp
        tl.store(shared_sum + warp_id, warp_sum)
        tl.store(shared_sum_sq + warp_id, warp_sum_sq)
    tl.debug_barrier()
    
    # Final reduction by first warp
    if warp_id == 0:
        total_sum = 0.0
        total_sum_sq = 0.0
        for i in range(num_warps):
            total_sum += tl.load(shared_sum + i)
            total_sum_sq += tl.load(shared_sum_sq + i)
        
        # Compute final statistics
        mean_val = total_sum / n_elements_per_sample
        variance = (total_sum_sq / n_elements_per_sample) - (mean_val * mean_val)
        rstd_val = 1.0 / tl.sqrt(variance + eps)
        
        tl.store(mean_ptr + pid, mean_val)
        tl.store(rstd_ptr + pid, rstd_val)
    
    tl.debug_barrier()
    
    # Load statistics
    mean_val = tl.load(mean_ptr + pid)
    rstd_val = tl.load(rstd_ptr + pid)
    
    # Apply normalization with vectorized loads/stores
    thread_id = lane_id[0]  # Get scalar thread ID
    for offset in range(thread_id * VECTOR_SIZE, n_elements_per_sample, 32 * VECTOR_SIZE):
        offsets = offset + tl.arange(0, VECTOR_SIZE)
        mask = offsets < n_elements_per_sample
        
        x_block = tl.load(x_sample_ptr + offsets, mask=mask, other=0.0)
        weight_block = tl.load(weight_ptr + offsets, mask=mask, other=1.0)
        bias_block = tl.load(bias_ptr + offsets, mask=mask, other=0.0)
        
        normalized = (x_block - mean_val) * rstd_val
        output_block = normalized * weight_block + bias_block
        
        tl.store(output_sample_ptr + offsets, output_block, mask=mask)

@triton.jit
def layer_norm_backward_kernel(
    x_ptr,
    dy_ptr,
    weight_ptr,
    mean_ptr,
    rstd_ptr,
    dx_ptr,
    dweight_ptr,
    dbias_ptr,
    n_elements_per_sample,
    sample_stride,
    weight_bias_stride,
    BLOCK_SIZE_N: tl.constexpr,
    VECTOR_SIZE: tl.constexpr,
    USE_ATOMICS: tl.constexpr,
):
    """
    Optimized backward kernel with vectorized loads and warp-level reductions.
    """
    pid_sample = tl.program_id(axis=0)
    pid_block = tl.program_id(axis=1)
    tid = tl.program_id(axis=2)
    
    # Load sample statistics
    mean_val = tl.load(mean_ptr + pid_sample)
    rstd_val = tl.load(rstd_ptr + pid_sample)
    
    # Pointers for this sample and block
    x_sample_ptr = x_ptr + pid_sample * sample_stride
    dy_sample_ptr = dy_ptr + pid_sample * sample_stride
    dx_sample_ptr = dx_ptr + pid_sample * sample_stride
    
    # Calculate block boundaries with vectorization
    block_start = pid_block * BLOCK_SIZE_N
    thread_start = block_start + tid * VECTOR_SIZE
    offsets = thread_start + tl.arange(0, VECTOR_SIZE)
    mask = offsets < n_elements_per_sample
    
    # Vectorized loads
    x_vec = tl.load(x_sample_ptr + offsets, mask=mask, other=0.0)
    dy_vec = tl.load(dy_sample_ptr + offsets, mask=mask, other=0.0)
    weight_vec = tl.load(weight_ptr + offsets, mask=mask, other=1.0)
    
    # Compute normalized input
    x_hat = (x_vec - mean_val) * rstd_val
    
    # Local accumulators
    dweight_local = tl.where(mask, dy_vec * x_hat, 0.0)
    dbias_local = tl.where(mask, dy_vec, 0.0)
    
    # Compute dx components
    dx_vec = dy_vec * weight_vec
    dx_sum_local = tl.sum(dx_vec, axis=0)
    dx_dot_local = tl.sum(dx_vec * x_hat, axis=0)
    
    # Warp-level reduction for dx components
    warp_sum = tl.sum(dx_sum_local, axis=0)
    warp_dot = tl.sum(dx_dot_local, axis=0)
    
    # Compute dx with proper normalization
    inv_n = 1.0 / n_elements_per_sample
    c1 = warp_sum * inv_n * rstd_val
    c2 = warp_dot * inv_n * rstd_val
    
    dx_vec_final = (dx_vec - c1 - x_hat * c2) * rstd_val
    tl.store(dx_sample_ptr + offsets, dx_vec_final, mask=mask)
    
    # Atomic accumulation for weight and bias gradients
    if USE_ATOMICS and tid == 0:
        # Reduce within warp first
        dweight_warp = tl.sum(dweight_local, axis=0)
        dbias_warp = tl.sum(dbias_local, axis=0)
        
        if dweight_ptr is not None:
            tl.atomic_add(dweight_ptr + block_start, dweight_warp)
        if dbias_ptr is not None:
            tl.atomic_add(dbias_ptr + block_start, dbias_warp)

def triton_layer_norm_forward(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float = 1e-5):
    """Optimized Triton wrapper for LayerNorm forward pass."""
    original_shape = x.shape
    n_samples = x.shape[0]
    n_elements_per_sample = x.numel() // n_samples
    
    # Reshape to 2D
    x_2d = x.view(n_samples, -1)
    output = torch.empty_like(x_2d)
    
    # Statistics for backward pass
    mean = torch.empty(n_samples, dtype=torch.float32, device=x.device)
    rstd = torch.empty(n_samples, dtype=torch.float32, device=x.device)
    
    # Flatten weight and bias
    weight_flat = weight.view(-1)
    bias_flat = bias.view(-1)
    
    # Configuration
    BLOCK_SIZE_N = 1024
    VECTOR_SIZE = 4
    BLOCK_THREADS = BLOCK_SIZE_N // VECTOR_SIZE
    
    num_blocks_per_sample = triton.cdiv(n_elements_per_sample, BLOCK_SIZE_N)
    
    # Stage 1: Partial sums with 3D grid
    partial_sum = torch.zeros(n_samples * num_blocks_per_sample, dtype=torch.float32, device=x.device)
    partial_sum_sq = torch.zeros(n_samples * num_blocks_per_sample, dtype=torch.float32, device=x.device)
    
    grid_partial = (n_samples, num_blocks_per_sample, BLOCK_THREADS)
    
    layer_norm_forward_partial_kernel[grid_partial](
        x_2d,
        partial_sum,
        partial_sum_sq,
        n_elements_per_sample,
        x_2d.stride(0),
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        VECTOR_SIZE=VECTOR_SIZE,
        num_stages=3,
    )
    
    # Stage 2: Final reduction and normalization
    num_warps = 32
    grid_final = (n_samples, num_warps, 32)
    
    layer_norm_forward_final_kernel[grid_final](
        x_2d,
        weight_flat,
        bias_flat,
        output,
        mean,
        rstd,
        partial_sum,
        partial_sum_sq,
        n_elements_per_sample,
        x_2d.stride(0),
        weight_flat.stride(0),
        num_blocks_per_sample,
        eps,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        VECTOR_SIZE=VECTOR_SIZE,
        num_stages=2,
    )
    
    return output.view(original_shape), mean, rstd

def triton_layer_norm_backward(
    dy: torch.Tensor,
    x: torch.Tensor,
    weight: torch.Tensor,
    mean: torch.Tensor,
    rstd: torch.Tensor,
):
    """Optimized Triton wrapper for LayerNorm backward pass."""
    n_samples = x.shape[0]
    n_elements_per_sample = x.numel() // n_samples
    
    x_2d = x.view(n_samples, -1)
    dy_2d = dy.view(n_samples, -1)
    weight_flat = weight.view(-1)
    
    # Gradient tensors
    dx = torch.empty_like(x_2d)
    dweight = torch.zeros_like(weight_flat)
    dbias = torch.zeros_like(weight_flat)
    
    # Configuration
    BLOCK_SIZE_N = 1024
    VECTOR_SIZE = 4
    BLOCK_THREADS = BLOCK_SIZE_N // VECTOR_SIZE
    
    num_blocks_per_sample = triton.cdiv(n_elements_per_sample, BLOCK_SIZE_N)
    
    # Grid: 3D for maximum parallelism
    grid = (n_samples, num_blocks_per_sample, BLOCK_THREADS)
    
    layer_norm_backward_kernel[grid](
        x_2d,
        dy_2d,
        weight_flat,
        mean,
        rstd,
        dx,
        dweight,
        dbias,
        n_elements_per_sample,
        x_2d.stride(0),
        weight_flat.stride(0),
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        VECTOR_SIZE=VECTOR_SIZE,
        USE_ATOMICS=True,
        num_stages=3,
    )
    
    return dx.view(x.shape), dweight.view(weight.shape), dbias.view(weight.shape)

class ModelNew(nn.Module):
    """
    Optimized LayerNorm using Triton kernels with vectorized loads and shared memory reductions.
    """
    def __init__(self, normalized_shape: tuple):
        super().__init__()
        self.normalized_shape = normalized_shape
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.eps = 1e-5
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        output, _, _ = triton_layer_norm_forward(x, self.weight, self.bias, self.eps)
        return output
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes

```python
# <corrected code>
```
