```python
# <complete ModelNew code with optimized Triton kernels>
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config(
            {
                "BLOCK_M": 64,
                "BLOCK_N": 64,
                "BLOCK_K": 32,
            },
            num_warps=4,
            num_stages=2,
        ),
        triton.Config(
            {
                "BLOCK_M": 32,
                "BLOCK_N": 128,
                "BLOCK_K": 32,
            },
            num_warps=4,
            num_stages=2,
        ),
        triton.Config(
            {
                "BLOCK_M": 128,
                "BLOCK_N": 32,
                "BLOCK_K": 32,
            },
            num_warps=4,
            num_stages=2,
        ),
        triton.Config(
            {
                "BLOCK_M": 64,
                "BLOCK_N": 64,
                "BLOCK_K": 64,
            },
            num_warps=4,
            num_stages=2,
        ),
        triton.Config(
            {
                "BLOCK_M": 64,
                "BLOCK_N": 128,
                "BLOCK_K": 32,
            },
            num_warps=8,
            num_stages=2,
        ),
        triton.Config(
            {
                "BLOCK_M": 128,
                "BLOCK_N": 64,
                "BLOCK_K": 32,
            },
            num_warps=8,
            num_stages=2,
        ),
        triton.Config(
            {
                "BLOCK_M": 64,
                "BLOCK_N": 64,
                "BLOCK_K": 32,
            },
            num_warps=4,
            num_stages=3,
        ),
        triton.Config(
            {
                "BLOCK_M": 32,
                "BLOCK_N": 128,
                "BLOCK_K": 32,
            },
            num_warps=4,
            num_stages=3,
        ),
    ],
    key=["C_in", "C_out", "H_out", "W_out", "KH", "KW", "HAS_BIAS"],
)
@triton.jit
def conv_transpose2d_full_kernel(
    x_ptr,        # *f32, [N, C_in, H_in, W_in]
    w_ptr,        # *f32, [C_in, C_out, KH, KW]
    y_ptr,        # *f32, [N, C_out, H_out, W_out]
    bias_ptr,     # *f32, [C_out] (unused if HAS_BIAS == False)
    N, C_in, H_in, W_in,
    C_out, H_out, W_out,
    KH: tl.constexpr,
    KW: tl.constexpr,
    HAS_BIAS: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """
    ConvTranspose2d for the restricted case:
        stride = 1, padding = 0, output_padding = 0, dilation = 1, groups = 1.

    Implemented as a full 2D transposed convolution:
        y[n, co, ho, wo] = sum_{ci, kh, kw} w[ci, co, kh, kw] * x[n, ci, ho - kh, wo - kw]
    with implicit zero-padding on x outside [0..H_in-1]x[0..W_in-1].
    """
    pid_n = tl.program_id(axis=0)   # batch index
    pid_m = tl.program_id(axis=1)   # tile over spatial positions (H_out * W_out)
    pid_co = tl.program_id(axis=2)  # tile over output channels

    n = pid_n

    # Offsets in spatial (flattened ho*W_out + wo) and output-channel space
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)      # [BLOCK_M]
    offs_co = pid_co * BLOCK_N + tl.arange(0, BLOCK_N)    # [BLOCK_N]

    hw_out = H_out * W_out
    mask_m = offs_m < hw_out
    mask_co = offs_co < C_out

    # Decompose flattened spatial index into (ho, wo)
    ho = offs_m // W_out  # [BLOCK_M]
    wo = offs_m % W_out   # [BLOCK_M]

    ho_b = ho[:, None]          # [BLOCK_M, 1]
    wo_b = wo[:, None]          # [BLOCK_M, 1]
    co_b = offs_co[None, :]     # [1, BLOCK_N]

    HW_in = H_in * W_in
    base_n = n * C_in * HW_in   # batch offset in x

    # Accumulator for output tile
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Loops over kernel spatial dimensions (fully unrolled)
    for kh in range(KH):
        for kw in range(KW):
            # Compute input coordinates contributing to this (ho, wo, kh, kw)
            hi = ho - kh  # [BLOCK_M]
            wi = wo - kw  # [BLOCK_M]

            valid_hi = (hi >= 0) & (hi < H_in)
            valid_wi = (wi >= 0) & (wi < W_in)
            mask_hw = mask_m & valid_hi & valid_wi  # [BLOCK_M]

            hi_b = hi[:, None]  # [BLOCK_M, 1]
            wi_b = wi[:, None]  # [BLOCK_M, 1]

            # Loop over input channels in tiles of BLOCK_K
            for c_start in range(0, C_in, BLOCK_K):
                ci = c_start + tl.arange(0, BLOCK_K)  # [BLOCK_K]
                mask_ci = ci < C_in                   # [BLOCK_K]

                ci_row = ci[None, :]   # [1, BLOCK_K]
                ci_col = ci[:, None]   # [BLOCK_K, 1]

                # ----------------------------- Load input tile X -----------------------------
                # x[n, ci, hi, wi]
                # Linear index: (((n * C_in + ci) * H_in + hi) * W_in + wi)
                base_ci = ci_row * HW_in                              # [1, BLOCK_K]
                x_ptrs = base_n + base_ci + hi_b * W_in + wi_b        # [BLOCK_M, BLOCK_K]
                mask_x = mask_hw[:, None] & mask_ci[None, :]
                x_tile = tl.load(x_ptr + x_ptrs, mask=mask_x, other=0.0)

                # ----------------------------- Load weight tile W ---------------------------
                # w[ci, co, kh, kw]
                # Linear index: (((ci * C_out + co) * KH + kh) * KW + kw)
                base_w_ci_stride = C_out * KH * KW                    # scalar
                w_ci_base = ci_col * base_w_ci_stride                 # [BLOCK_K, 1]
                w_spatial_off = kh * KW + kw                          # scalar
                w_ptrs = w_ci_base + co_b * (KH * KW) + w_spatial_off  # [BLOCK_K, BLOCK_N]
                mask_w = mask_ci[:, None] & mask_co[None, :]
                w_tile = tl.load(w_ptr + w_ptrs, mask=mask_w, other=0.0)

                # Accumulate partial dot-product
                acc += tl.dot(x_tile, w_tile)

    # Add bias if present
    if HAS_BIAS:
        bias = tl.load(bias_ptr + offs_co, mask=mask_co, other=0.0)  # [BLOCK_N]
        acc = acc + bias[None, :]                                    # [BLOCK_M, BLOCK_N]

    # Store results: y[n, co, ho, wo]
    y_ptrs = (((n * C_out + co_b) * H_out + ho_b) * W_out + wo_b)   # [BLOCK_M, BLOCK_N]
    mask_y = mask_m[:, None] & mask_co[None, :]
    tl.store(y_ptr + y_ptrs, acc, mask=mask_y)


def triton_conv_transpose2d_full(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor | None,
) -> torch.Tensor:
    """
    Triton implementation of ConvTranspose2d for the restricted case:

        stride = 1
        padding = 0
        output_padding = 0
        dilation = 1
        groups = 1

    Args:
        x:      [N, C_in, H_in, W_in], float32, CUDA
        weight: [C_in, C_out, KH, KW], float32, CUDA
        bias:   [C_out] or None, float32, CUDA

    Returns:
        y: [N, C_out, H_out, W_out]
           where H_out = H_in + KH - 1, W_out = W_in + KW - 1
    """
    assert x.is_cuda and weight.is_cuda, "Inputs must be CUDA tensors"
    assert x.dtype == torch.float32 and weight.dtype == torch.float32, "Only float32 is supported in Triton kernel"
    assert x.ndim == 4 and weight.ndim == 4, "x must be [N,C,H,W], weight must be [C_in,C_out,KH,KW]"

    N, C_in, H_in, W_in = x.shape
    C_in_w, C_out, KH, KW = weight.shape
    assert C_in_w == C_in, "Input channels mismatch between x and weight"

    H_out = H_in + KH - 1
    W_out = W_in + KW - 1

    y = torch.empty((N, C_out, H_out, W_out), device=x.device, dtype=x.dtype)

    # 3D grid: (batch, spatial tiles, channel tiles)
    def grid(meta):
        return (
            N,
            triton.cdiv(H_out * W_out, meta["BLOCK_M"]),
            triton.cdiv(C_out, meta["BLOCK_N"]),
        )

    has_bias = bias is not None
    if bias is None:
        # Dummy pointer; will not be used when HAS_BIAS=False
        bias = y.new_empty(1)

    conv_transpose2d_full_kernel[grid](
        x, weight, y, bias,
        N, C_in, H_in, W_in,
        C_out, H_out, W_out,
        KH, KW,
        HAS_BIAS=has_bias,
    )

    return y


class ModelNew(nn.Module):
    """
    Triton-accelerated ConvTranspose2d for the restricted case:

        stride == 1
        padding == 0
        output_padding == 0
        dilation == 1
        groups == 1
        dtype == float32
        device == CUDA
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: int = 0,
        output_padding: int = 0,
        groups: int = 1,
        bias: bool = False,
    ) -> None:
        super().__init__()
        self.conv_transpose2d = nn.ConvTranspose2d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            output_padding=output_padding,
            groups=groups,
            bias=bias,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        ct = self.conv_transpose2d

        use_triton = (
            x.is_cuda
            and x.dtype == torch.float32
            and ct.weight.is_cuda
            and ct.weight.dtype == torch.float32
            and ct.groups == 1
            and ct.dilation == (1, 1)
            and ct.stride == (1, 1)
            and ct.padding == (0, 0)
            and ct.output_padding == (0, 0)
        )

        if use_triton:
            return triton_conv_transpose2d_full(x, ct.weight, ct.bias)
        else:
            return ct(x)
```