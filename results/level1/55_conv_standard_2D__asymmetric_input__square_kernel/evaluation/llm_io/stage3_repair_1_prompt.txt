Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Child process crashed:

History Error:
None


PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Performs a standard 2D convolution operation with an asymmetric input and a square kernel.

    Args:
        in_channels (int): Number of channels in the input tensor.
        out_channels (int): Number of channels produced by the convolution.
        kernel_size (int): Size of the square convolution kernel.
        stride (int, optional): Stride of the convolution. Defaults to 1.
        padding (int, optional): Padding applied to the input. Defaults to 0.
        dilation (int, optional): Spacing between kernel elements. Defaults to 1.
        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.
        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):
        super(Model, self).__init__()
        self.conv2d = nn.Conv2d(in_channels, out_channels, (kernel_size, kernel_size), stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Performs the 2D convolution.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).
        """
        return self.conv2d(x)

# Test code
batch_size = 8
# smaller spatial dims
height = 512
width = 1024
in_channels = 64  # increased channels
out_channels = 128
kernel_size = 3
# asymmetric input: make width considerably larger than height

def get_inputs():
    x = torch.rand(batch_size, in_channels, height, width)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization
```

Broken Code:
```python
# <optimized Triton code>

import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        # Smaller K, lower stages – good when memory stalls are low
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 32}, num_warps=4, num_stages=1),
        triton.Config({"BLOCK_M": 128, "BLOCK_N": 64, "BLOCK_K": 32}, num_warps=4, num_stages=1),
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 128, "BLOCK_K": 32}, num_warps=4, num_stages=1),

        # Larger tiles for higher arithmetic intensity
        triton.Config({"BLOCK_M": 128, "BLOCK_N": 128, "BLOCK_K": 32}, num_warps=8, num_stages=1),

        # Slightly deeper pipelining and larger K for some latency hiding
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 64}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_M": 128, "BLOCK_N": 64, "BLOCK_K": 64}, num_warps=8, num_stages=2),
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 128, "BLOCK_K": 64}, num_warps=8, num_stages=2),
        triton.Config({"BLOCK_M": 128, "BLOCK_N": 128, "BLOCK_K": 64}, num_warps=8, num_stages=2),

        # A couple of higher-stage variants only if memory latency becomes relevant
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 32}, num_warps=8, num_stages=2),
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 32}, num_warps=8, num_stages=3),
    ],
    key=["N", "C_out", "H", "W", "KH", "KW"],
)
@triton.jit
def conv2d_implicit_gemm_kernel(
    x_ptr,          # (N, C_in, H, W)
    w_ptr,          # (C_out, K_total) where K_total = (C_in/groups) * KH * KW
    b_ptr,          # (C_out,) or dummy
    y_ptr,          # (N, C_out, OH, OW)
    N, C_in, H, W,
    C_out, K_total,
    KH, KW,
    stride_h, stride_w,
    pad_h, pad_w,
    dil_h, dil_w,
    groups,
    OH, OW,
    HAS_BIAS: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    # -----------------------------
    # Program IDs
    # -----------------------------
    pid_m = tl.program_id(axis=0)  # over P = N * OH * OW
    pid_n = tl.program_id(axis=1)  # over OC_per_group tiles
    pid_g = tl.program_id(axis=2)  # over groups

    OC_per_group = C_out // groups
    IC_per_group = C_in // groups
    P = N * OH * OW  # number of output positions per group

    # -----------------------------
    # Offsets along output position (M) and output channels (N)
    # -----------------------------
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)  # [BM]
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)  # [BN]

    mask_m = offs_m < P
    mask_n = offs_n < OC_per_group

    # Map offs_m -> (n, oh, ow)
    OHOW = OH * OW
    n_idxs = offs_m // OHOW
    rem = offs_m % OHOW
    oh_idxs = rem // OW
    ow_idxs = rem % OW

    # Group offsets
    oc_group_offset = pid_g * OC_per_group
    ic_group_offset = pid_g * IC_per_group

    # Broadcasted indices used in the K-loop
    n_b = n_idxs[:, None]   # [BM,1]
    oh_b = oh_idxs[:, None] # [BM,1]
    ow_b = ow_idxs[:, None] # [BM,1]

    # Precompute oc-related quantities (independent of K)
    oc = oc_group_offset + offs_n        # [BN]
    oc_k_base = oc * K_total             # [BN] used for weight pointers

    # Accumulator in FP32 (even if inputs are FP16/BF16)
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Useful invariants
    KK = KH * KW

    # -----------------------------
    # Iterate over K dimension: (IC_per_group * KH * KW)
    # -----------------------------
    for k_base in range(0, K_total, BLOCK_K):
        offs_k = k_base + tl.arange(0, BLOCK_K)  # [BK]
        mask_k = offs_k < K_total

        # Decompose k index into (ic_rel, kh, kw)
        ic_rel = offs_k // KK          # [BK]
        rem_k = offs_k % KK            # [BK]
        kh = rem_k // KW               # [BK]
        kw = rem_k % KW                # [BK]

        ic = ic_group_offset + ic_rel  # [BK]

        ic_b = ic[None, :]             # [1,BK]
        kh_b = kh[None, :]             # [1,BK]
        kw_b = kw[None, :]             # [1,BK]

        # Compute input spatial indices
        ih = oh_b * stride_h - pad_h + kh_b * dil_h  # [BM,BK]
        iw = ow_b * stride_w - pad_w + kw_b * dil_w  # [BM,BK]

        in_bounds = (ih >= 0) & (ih < H) & (iw >= 0) & (iw < W)
        mask_a = (
            mask_m[:, None]
            & mask_k[None, :]
            & in_bounds
        )

        # Compute flat input offsets: ((n * C_in + ic) * H + ih) * W + iw
        nc = n_b * C_in + ic_b              # [BM,BK]
        nch = nc * H + ih                   # [BM,BK]
        nchw = nch * W + iw                 # [BM,BK]

        a_ptrs = x_ptr + nchw
        a = tl.load(a_ptrs, mask=mask_a, other=0.0)

        # Weights slice: w_ptr is (C_out, K_total), row-major
        # Use precomputed oc_k_base to reduce integer math in loop
        w_ptrs = w_ptr + (oc_k_base[None, :] + offs_k[:, None])  # [BK,BN]
        mask_w = mask_k[:, None] & mask_n[None, :]
        b = tl.load(w_ptrs, mask=mask_w, other=0.0)

        # Accumulate
        acc += tl.dot(a, b)

    # -----------------------------
    # Add bias if present
    # -----------------------------
    if HAS_BIAS:
        bias_vals = tl.load(b_ptr + oc, mask=mask_n, other=0.0)  # [BN]
        acc += bias_vals[None, :]

    # -----------------------------
    # Store result to y: (N, C_out, OH, OW)
    # -----------------------------
    oc_b = oc[None, :]  # [1,BN]

    out_offsets = ((n_b * C_out + oc_b) * OH + oh_b) * OW + ow_b  # [BM,BN]
    mask_out = mask_m[:, None] & mask_n[None, :]

    tl.store(y_ptr + out_offsets, acc, mask=mask_out)


def triton_conv2d(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor = None,
    stride=1,
    padding=0,
    dilation=1,
    groups: int = 1,
) -> torch.Tensor:
    """
    Conv2d via implicit GEMM using Triton.
    Semantics match torch.nn.functional.conv2d for NCHW layout.
    Optimized for high performance on modern NVIDIA GPUs (e.g., RTX 4090).
    """
    assert x.is_cuda and weight.is_cuda, "Inputs must be CUDA tensors"
    assert x.dtype == weight.dtype, "x and weight must have same dtype"

    # Normalize hyperparameters to pairs
    if isinstance(stride, int):
        stride_h = stride_w = stride
    else:
        stride_h, stride_w = stride

    if isinstance(padding, int):
        pad_h = pad_w = padding
    else:
        pad_h, pad_w = padding

    if isinstance(dilation, int):
        dil_h = dil_w = dilation
    else:
        dil_h, dil_w = dilation

    N, C_in, H, W = x.shape
    C_out, C_in_per_group, KH, KW = weight.shape
    assert C_in == C_in_per_group * groups, "Inconsistent in_channels and groups"

    # Output spatial dimensions
    OH = (H + 2 * pad_h - dil_h * (KH - 1) - 1) // stride_h + 1
    OW = (W + 2 * pad_w - dil_w * (KW - 1) - 1) // stride_w + 1

    # Allocate output
    y = torch.empty((N, C_out, OH, OW), device=x.device, dtype=x.dtype)

    # Prepare contiguous tensors
    x_contig = x.contiguous()
    w_mat = weight.contiguous().view(C_out, -1)  # (C_out, K_total)
    if bias is not None:
        b_contig = bias.contiguous()
    else:
        # Dummy tensor, won't be used when HAS_BIAS=False
        b_contig = x_contig.view(-1)

    P = N * OH * OW
    OC_per_group = C_out // groups
    K_total = w_mat.shape[1]

    # Grid function uses autotuned tile sizes
    def grid(meta):
        return (
            triton.cdiv(P, meta["BLOCK_M"]),
            triton.cdiv(OC_per_group, meta["BLOCK_N"]),
            groups,
        )

    conv2d_implicit_gemm_kernel[grid](
        x_contig,
        w_mat,
        b_contig,
        y,
        N,
        C_in,
        H,
        W,
        C_out,
        K_total,
        KH,
        KW,
        stride_h,
        stride_w,
        pad_h,
        pad_w,
        dil_h,
        dil_w,
        groups,
        OH,
        OW,
        HAS_BIAS=(bias is not None),
    )

    return y


class ModelNew(nn.Module):
    """
    Triton-accelerated Conv2d via implicit-GEMM, optimized for RTX 4090.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: int = 0,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = False,
    ) -> None:
        super().__init__()
        self.conv2d = nn.Conv2d(
            in_channels,
            out_channels,
            (kernel_size, kernel_size),
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
            bias=bias,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        w = self.conv2d.weight
        b = self.conv2d.bias
        return triton_conv2d(
            x,
            w,
            b,
            stride=self.conv2d.stride,
            padding=self.conv2d.padding,
            dilation=self.conv2d.dilation,
            groups=self.conv2d.groups,
        )
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl, AND any other modules used (e.g., import math if using math.sqrt)
   2. @triton.jit decorated kernel function(s) — NO continue/break/return inside loops (use masking)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels — THIS CLASS IS REQUIRED
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes
4. Ensure ALL imports are included at the top (common mistake: forgetting `import math`)

```python
# <corrected code>
```
