You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB



[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Memory efficiency & latency hiding.

Metrics:
- dram__throughput.avg.pct_of_peak_sustained_elapsed
- lts__t_sector_hit_rate.pct
- smsp__warp_issue_stalled_memory_dependency_per_warp_active.pct (<20%)

Rules:
- Increase num_stages only if memory stalls are high
- Do not rewrite access patterns without metric evidence
- Larger BLOCK_K improves reuse but increases register pressure

Autotune:
- If unsure, try num_stages ∈ {1,2,3} on kernel



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def block_sums_kernel(
    x_ptr,
    block_sums_ptr,
    B,
    N,
    n_blocks,
    BLOCK_N: tl.constexpr,
):
    # 2D grid: (B, n_blocks)
    pid_b = tl.program_id(axis=0)
    pid_block = tl.program_id(axis=1)

    block_start = pid_block * BLOCK_N
    offs = block_start + tl.arange(0, BLOCK_N)
    row_start = pid_b * N

    mask = (pid_b < B) & (offs < N)

    x = tl.load(x_ptr + row_start + offs, mask=mask, other=0.0)

    # sum of this block
    block_sum = tl.sum(x, axis=0)

    valid_block = (pid_b < B) & (pid_block < n_blocks)
    tl.store(
        block_sums_ptr + pid_b * n_blocks + pid_block,
        block_sum,
        mask=valid_block,
    )


@triton.jit
def block_prefix_cumsum_kernel(
    block_sums_ptr,
    block_prefix_ptr,
    B,
    n_blocks,
    BLOCK_NBLOCKS: tl.constexpr,
):
    # 1D grid over batch
    pid_b = tl.program_id(axis=0)

    offs = tl.arange(0, BLOCK_NBLOCKS)
    mask = (pid_b < B) & (offs < n_blocks)

    row_offset = pid_b * n_blocks
    vals = tl.load(block_sums_ptr + row_offset + offs, mask=mask, other=0.0)

    vals_scan = tl.cumsum(vals, axis=0)

    tl.store(block_prefix_ptr + row_offset + offs, vals_scan, mask=mask)


@triton.autotune(
    configs=[
        triton.Config({}, num_warps=4, num_stages=2),
        triton.Config({}, num_warps=8, num_stages=2),
        triton.Config({}, num_warps=4, num_stages=3),
    ],
    key=["N"],
)
@triton.jit
def cumsum_with_prefix_kernel(
    x_ptr,
    out_ptr,
    block_prefix_ptr,
    B,
    N,
    n_blocks,
    BLOCK_N: tl.constexpr,
):
    # 2D grid: (B, n_blocks)
    pid_b = tl.program_id(axis=0)
    pid_block = tl.program_id(axis=1)

    block_start = pid_block * BLOCK_N
    offs = block_start + tl.arange(0, BLOCK_N)
    row_start = pid_b * N

    mask = (pid_b < B) & (offs < N)

    # per-block prefix: sum of all previous blocks in this row
    block_linear_idx = pid_b * n_blocks + pid_block
    prefix = tl.load(
        block_prefix_ptr + block_linear_idx - 1,
        mask=pid_block > 0,
        other=0.0,
    )

    x = tl.load(x_ptr + row_start + offs, mask=mask, other=0.0)

    # local inclusive scan within block, then add row prefix
    x_scan = tl.cumsum(x, axis=0) + prefix

    tl.store(out_ptr + row_start + offs, x_scan, mask=mask)


def triton_inclusive_cumsum_lastdim_2d(x_2d: torch.Tensor) -> torch.Tensor:
    """
    Inclusive cumsum along the last dimension for a 2D contiguous tensor.
    x_2d: (B, N), CUDA tensor.
    Returns tensor of same shape with cumsum along dim=1.
    """
    assert x_2d.is_cuda
    assert x_2d.dim() == 2
    B, N = x_2d.shape

    if N == 0:
        return x_2d.clone()

    # Tile size along the scan dimension
    BLOCK_N = 1024  # power of two; good balance for Ada 4090
    n_blocks = triton.cdiv(N, BLOCK_N)

    out = torch.empty_like(x_2d)
    block_sums = torch.empty((B, n_blocks), device=x_2d.device, dtype=x_2d.dtype)
    block_prefix = torch.empty_like(block_sums)

    # Pass 1: per-block sums only (no per-element writes)
    grid1 = (B, n_blocks)
    block_sums_kernel[grid1](
        x_2d,
        block_sums,
        B,
        N,
        n_blocks,
        BLOCK_N=BLOCK_N,
    )

    # Pass 2: prefix sum over block_sums along block dimension
    if n_blocks <= 1024:
        BLOCK_NBLOCKS = 1024  # power of two
        grid2 = (B,)
        block_prefix_cumsum_kernel[grid2](
            block_sums,
            block_prefix,
            B,
            n_blocks,
            BLOCK_NBLOCKS=BLOCK_NBLOCKS,
        )
    else:
        # Fallback to PyTorch for very long sequences of blocks
        block_prefix.copy_(torch.cumsum(block_sums, dim=1))

    # Pass 3: final per-element scan using original x + block prefixes
    grid3 = (B, n_blocks)
    cumsum_with_prefix_kernel[grid3](
        x_2d,
        out,
        block_prefix,
        B,
        N,
        n_blocks,
        BLOCK_N=BLOCK_N,
    )

    return out


def triton_inclusive_cumsum(x: torch.Tensor, dim: int) -> torch.Tensor:
    """
    Inclusive cumsum along an arbitrary dimension using Triton.
    """
    assert x.is_cuda
    ndim = x.ndim
    if ndim == 0:
        return x.clone()

    dim = dim % ndim

    # Move target dim to the last, then flatten outer dims into batch
    perm = list(range(ndim))
    perm[dim], perm[-1] = perm[-1], perm[dim]
    x_perm = x.permute(*perm).contiguous()

    outer_shape = x_perm.shape[:-1]
    N = x_perm.shape[-1]
    if outer_shape:
        B = int(torch.tensor(outer_shape).prod().item())
    else:
        B = 1

    x_2d = x_perm.reshape(B, N)
    out_2d = triton_inclusive_cumsum_lastdim_2d(x_2d)
    out_perm = out_2d.reshape(*outer_shape, N).permute(*perm)

    return out_perm


def triton_reverse_cumsum(x: torch.Tensor, dim: int) -> torch.Tensor:
    """
    Reverse cumsum along the given dimension, implemented via
    Triton inclusive cumsum plus PyTorch flip.
    """
    dim = dim % x.ndim
    x_flipped = x.flip(dim)
    inclusive = triton_inclusive_cumsum(x_flipped, dim)
    return inclusive.flip(dim)


class ModelNew(nn.Module):
    """
    A Triton-accelerated model that performs a reverse cumulative sum operation
    along a specified dimension.
    """

    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, x):
        return triton_reverse_cumsum(x, self.dim)
```

[NCU PROFILING METRICS]
{
  "block_sums_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 8.42,
    "launch__grid_size": 1048576,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 97.93,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 92.11,
    "lts__t_sector_hit_rate.pct": 0.78
  },
  "block_prefix_cumsum_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 84.26,
    "launch__grid_size": 32768,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 74.83,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 18.57,
    "lts__t_sector_hit_rate.pct": 50.96
  },
  "cumsum_with_prefix_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 6.55,
    "launch__grid_size": 1048576,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 95.98,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 88.49,
    "lts__t_sector_hit_rate.pct": 49.81
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
