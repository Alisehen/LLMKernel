You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB



[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Final micro-tuning.

Params:
- num_warps ∈ {2,4,8,16}
- num_stages ∈ {2,3,4}

Rules:
- Change num_warps only if occupancy suggests it
- Change num_stages by ±1 only
- Do NOT modify grid or BLOCK sizes

Autotune:
- 3–6 nearby configs
- Always include original config
- Revert if gain <1–2% or unstable



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        # Balanced small tiles for higher occupancy
        triton.Config(
            {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 32},
            num_warps=4,
            num_stages=1,
        ),
        triton.Config(
            {'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 32},
            num_warps=4,
            num_stages=2,
        ),

        # More work per CTA along M
        triton.Config(
            {'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32},
            num_warps=4,
            num_stages=1,
        ),
        triton.Config(
            {'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32},
            num_warps=4,
            num_stages=2,
        ),

        # More work per CTA along N
        triton.Config(
            {'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 32},
            num_warps=4,
            num_stages=1,
        ),
        triton.Config(
            {'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 32},
            num_warps=4,
            num_stages=2,
        ),

        # Larger MN tile when register pressure allows it
        triton.Config(
            {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32},
            num_warps=4,
            num_stages=1,
        ),
        triton.Config(
            {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32},
            num_warps=4,
            num_stages=2,
        ),

        # Smaller K tile to reduce register pressure, with deeper pipelining
        triton.Config(
            {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 16},
            num_warps=4,
            num_stages=3,
        ),
    ],
    key=['N', 'C_in', 'H', 'W', 'C_out', 'OH', 'OW'],
)
@triton.jit
def conv2d_fwd_kernel(
    x_ptr,         # float*  [N, C_in, H, W]   (contiguous, NCHW)
    w_ptr,         # float*  [C_out, C_in, KH, KW] (contiguous)
    b_ptr,         # float*  [C_out] or nullptr if bias is None
    out_ptr,       # float*  [N, C_out, OH, OW]

    N, C_in, H, W,
    C_out, KH, KW,
    OH, OW,
    stride_h, stride_w,
    pad_h, pad_w,

    K_TOTAL: tl.constexpr,   # = C_in * KH * KW
    HAS_BIAS: tl.constexpr,  # bool: whether b_ptr is valid
    BLOCK_M: tl.constexpr,   # tile over output positions (N * OH * OW)
    BLOCK_N: tl.constexpr,   # tile over output channels
    BLOCK_K: tl.constexpr,   # tile over K
):
    # -----------------------------------------------------------------------
    # Program ids
    # -----------------------------------------------------------------------
    pid_m = tl.program_id(0)  # tile over output positions
    pid_n = tl.program_id(1)  # tile over output channels

    # -----------------------------------------------------------------------
    # Offsets for M (output positions) and N (output channels)
    # -----------------------------------------------------------------------
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)  # [BM]
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)  # [BN]

    OH_OW = OH * OW
    M_total = N * OH_OW
    mask_m = offs_m < M_total          # [BM]
    mask_n = offs_n < C_out            # [BN]

    tl.multiple_of(offs_m, BLOCK_M)
    tl.multiple_of(offs_n, BLOCK_N)

    # -----------------------------------------------------------------------
    # Decode linear output index m -> (n, oh, ow)
    # m in [0, N*OH*OW)
    # -----------------------------------------------------------------------
    tmp = offs_m
    n_idx = tmp // OH_OW
    rem = tmp % OH_OW
    oh = rem // OW
    ow = rem % OW

    # Broadcasted versions
    n_b = n_idx[:, None]        # [BM,1]
    oh_b = oh[:, None]          # [BM,1]
    ow_b = ow[:, None]          # [BM,1]
    mask_m_b = mask_m[:, None]  # [BM,1]

    # Precompute base spatial coordinates per output position
    base_ih = oh_b * stride_h - pad_h   # [BM,1]
    base_iw = ow_b * stride_w - pad_w   # [BM,1]

    # -----------------------------------------------------------------------
    # Initialize accumulator for [BM, BN] tile
    # -----------------------------------------------------------------------
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # -----------------------------------------------------------------------
    # Constants for K mapping
    # -----------------------------------------------------------------------
    KHW = KH * KW

    # -----------------------------------------------------------------------
    # Stream over K dimension in tiles of BLOCK_K
    # Use static_range for software pipelining with num_stages
    # -----------------------------------------------------------------------
    for k_base in tl.static_range(0, K_TOTAL, BLOCK_K):
        offs_k = k_base + tl.arange(0, BLOCK_K)        # [BK]
        k_mask = offs_k < K_TOTAL                      # [BK]

        # Map linear k -> (ic, kh, kw)
        ic = offs_k // KHW                             # [BK]
        rem_k = offs_k % KHW                           # [BK]
        kh = rem_k // KW                               # [BK]
        kw = rem_k % KW                                # [BK]

        ic_b = ic[None, :]                             # [1,BK]
        kh_b = kh[None, :]                             # [1,BK]
        kw_b = kw[None, :]                             # [1,BK]

        # Input spatial coordinates for each (m, k)
        ih = base_ih + kh_b                            # [BM,BK]
        iw = base_iw + kw_b                            # [BM,BK]

        # Valid input mask for each (m, k)
        mask_in = (
            mask_m_b &
            (ih >= 0) & (ih < H) &
            (iw >= 0) & (iw < W) &
            k_mask[None, :]
        )

        # Linear offsets into input tensor: ((n * C_in + ic) * H + ih) * W + iw
        x_offsets = ((n_b * C_in + ic_b) * H + ih) * W + iw  # [BM,BK]
        x_ptrs = x_ptr + x_offsets
        a = tl.load(x_ptrs, mask=mask_in, other=0.0).to(tl.float32)

        # -------------------------------------------------------------------
        # Load weights as [BK, BN] tile:
        # weight[oc, ic, kh, kw] contiguous
        # flatten (ic, kh, kw) -> k in [0, K_TOTAL)
        # linear index = oc * K_TOTAL + k
        # -------------------------------------------------------------------
        offs_k_b = offs_k[:, None]                     # [BK,1]
        offs_n_b = offs_n[None, :]                     # [1,BN]

        mask_w = (offs_k_b < K_TOTAL) & (offs_n_b < C_out)

        w_offsets = offs_n_b * K_TOTAL + offs_k_b      # [BK,BN]
        w_ptrs = w_ptr + w_offsets
        b = tl.load(w_ptrs, mask=mask_w, other=0.0).to(tl.float32)

        # -------------------------------------------------------------------
        # Matrix multiply accumulate: [BM,BK] x [BK,BN] -> [BM,BN]
        # -------------------------------------------------------------------
        acc += tl.dot(a, b)

    # -----------------------------------------------------------------------
    # Add bias: bias[oc]
    # -----------------------------------------------------------------------
    if HAS_BIAS:
        bias = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0)  # [BN]
        acc += bias[None, :]  # broadcast over BM

    # -----------------------------------------------------------------------
    # Write back to output: out[n, oc, oh, ow]
    # index = ((n * C_out + oc) * OH + oh) * OW + ow
    # -----------------------------------------------------------------------
    n_out = n_idx[:, None]       # [BM,1]
    oh_out = oh[:, None]         # [BM,1]
    ow_out = ow[:, None]         # [BM,1]
    oc_out = offs_n[None, :]     # [1,BN]

    out_offsets = ((n_out * C_out + oc_out) * OH + oh_out) * OW + ow_out  # [BM,BN]
    mask_out = mask_m_b & (oc_out < C_out)

    tl.store(out_ptr + out_offsets, acc, mask=mask_out)


def triton_conv2d_nchw(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    stride: int,
    padding: int,
) -> torch.Tensor:
    """
    x:      [N, C_in, H, W],  contiguous, CUDA
    weight: [C_out, C_in, KH, KW], contiguous, CUDA
    bias:   [C_out] or None, CUDA
    stride, padding: int (assumed symmetric for H/W)
    """
    assert x.is_cuda and weight.is_cuda, "Inputs must be CUDA tensors"
    assert x.dtype == torch.float32 and weight.dtype == torch.float32, "Only float32 supported"
    if bias is not None:
        assert bias.is_cuda and bias.dtype == torch.float32

    x = x.contiguous()
    weight = weight.contiguous()
    if bias is not None:
        bias = bias.contiguous()

    N, C_in, H, W = x.shape
    C_out, C_in_w, KH, KW = weight.shape
    assert C_in_w == C_in, "Incompatible in_channels"

    stride_h = stride_w = stride
    pad_h = pad_w = padding

    OH = (H + 2 * pad_h - KH) // stride_h + 1
    OW = (W + 2 * pad_w - KW) // stride_w + 1

    out = torch.empty((N, C_out, OH, OW), device=x.device, dtype=x.dtype)

    # Precompute K_TOTAL as compile-time meta-parameter for the kernel.
    K_TOTAL = C_in * KH * KW

    # Grid: 2D over (output positions, output channels)
    M_total = N * OH * OW

    def grid(meta):
        return (
            triton.cdiv(M_total, meta['BLOCK_M']),
            triton.cdiv(C_out, meta['BLOCK_N']),
        )

    conv2d_fwd_kernel[grid](
        x, weight, bias if bias is not None else x,  # dummy ptr if no bias
        out,
        N, C_in, H, W,
        C_out, KH, KW,
        OH, OW,
        stride_h, stride_w,
        pad_h, pad_w,
        K_TOTAL=K_TOTAL,
        HAS_BIAS=bias is not None,
    )

    return out


class ModelNew(nn.Module):
    def __init__(self, num_classes=1000) -> None:
        super().__init__()
        self.conv1 = nn.Conv2d(
            in_channels=3,
            out_channels=96,
            kernel_size=11,
            stride=4,
            padding=2,
            bias=True,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if not x.is_cuda:
            return self.conv1(x)

        return triton_conv2d_nchw(
            x,
            self.conv1.weight,
            self.conv1.bias,
            stride=self.conv1.stride[0],
            padding=self.conv1.padding[0],
        )
```

[NCU PROFILING METRICS]
{
  "conv2d_fwd_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 64.79,
    "launch__grid_size": 48400,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 33.05,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 26.97,
    "lts__t_sector_hit_rate.pct": 93.4
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
