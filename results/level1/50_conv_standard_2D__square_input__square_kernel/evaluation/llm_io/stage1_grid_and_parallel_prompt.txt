You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB



[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Grid layout & parallelism.

**Special Consideration for Transposed Convolution**:
⚠️ For ConvTranspose operations:
- Do NOT use naive output-only parallelism (can cause write conflicts if multiple inputs map to same output)
- Recommended grid: (batch * out_spatial, out_channels // groups, groups)
  * Example 3D: grid=(N * OD * OH * OW // BLOCK, C_out // groups, groups)
- Each thread should compute a unique subset of output elements
- If using scatter pattern, ensure no overlapping writes OR use atomic operations

Metrics:
- sm__throughput.avg.pct_of_peak_sustained_elapsed (>60%)
- launch__grid_size

Rules:
- 1D: (cdiv(N, BLOCK))
- 2D: (cdiv(M, BLOCK_M), cdiv(N, BLOCK_N))
- 3D: (batch, cdiv(M, BLOCK_M), cdiv(N, BLOCK_N))
- >3D: flatten ONLY independent dims
- Prefer batch / head / expert / group parallelism before shrinking BLOCK
- For grouped operations: ensure group dimension is in grid (e.g., program_id(2) for groups)
- Change grid only if SM utilization is clearly low

Safety:
- Max 3 grid dims, static rank
- grid=(G0,G1,G2) must match tl.program_id(0/1/2)
- For grouped ops: verify group indexing is correct
- If unsure about correctness, do NOT change grid

Autotune:
- Autotune either BLOCK_* OR (num_warps, num_stages)
- If autotuning BLOCK_*, use grid=lambda META: (...)
- Never redefine BLOCK_* in both kernel and launch



[CURRENT CODE]
```python
# complete ModelNew code with optimized Triton kernels

import torch
import torch.nn as nn
import torch.nn.functional as F
import triton
import triton.language as tl


@triton.jit
def conv2d_fwd_kernel(
    x_ptr,         # float*  [N, C_in, H, W]   (contiguous, NCHW)
    w_ptr,         # float*  [C_out, C_in, KH, KW] (contiguous)
    b_ptr,         # float*  [C_out]
    out_ptr,       # float*  [N, C_out, OH, OW]

    N, C_in, H, W,
    C_out, KH, KW,
    OH, OW,
    stride_h, stride_w,
    pad_h, pad_w,

    BLOCK_M: tl.constexpr,  # tile over output positions (N*OH*OW)
    BLOCK_N: tl.constexpr,  # tile over output channels
    BLOCK_K: tl.constexpr,  # tile over K = C_in * KH * KW (we take one big tile with masking)
):
    # -------------------------------------------------------------------------
    # Program ids
    # -------------------------------------------------------------------------
    pid_m = tl.program_id(axis=0)  # over output positions
    pid_n = tl.program_id(axis=1)  # over output channels

    # -------------------------------------------------------------------------
    # Offsets for M (output positions) and N (output channels)
    # -------------------------------------------------------------------------
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)  # [BM]
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)  # [BN]

    M_total = N * OH * OW

    mask_m = offs_m < M_total       # [BM]
    mask_n = offs_n < C_out         # [BN]

    # -------------------------------------------------------------------------
    # Decode linear output index m -> (n, oh, ow)
    # m in [0, N*OH*OW)
    # -------------------------------------------------------------------------
    tmp = offs_m
    n_idx = tmp // (OH * OW)
    rem = tmp % (OH * OW)
    oh = rem // OW
    ow = rem % OW

    # -------------------------------------------------------------------------
    # K dimension: C_in * KH * KW
    # We handle all K in a single tile of size BLOCK_K (>= K_total) with masking.
    # -------------------------------------------------------------------------
    K_total = C_in * KH * KW
    offs_k = tl.arange(0, BLOCK_K)          # [BK]
    k_mask = offs_k < K_total               # [BK]

    # Map linear k -> (ic, kh, kw)
    ic = offs_k // (KH * KW)
    rem_k = offs_k % (KH * KW)
    kh = rem_k // KW
    kw = rem_k % KW

    # -------------------------------------------------------------------------
    # Compute input coordinates for each (m, k) pair
    # ih = oh * stride_h - pad_h + kh
    # iw = ow * stride_w - pad_w + kw
    # -------------------------------------------------------------------------
    oh_b = oh[:, None]            # [BM,1]
    ow_b = ow[:, None]            # [BM,1]
    kh_b = kh[None, :]            # [1,BK]
    kw_b = kw[None, :]            # [1,BK]

    ih = oh_b * stride_h - pad_h + kh_b   # [BM,BK]
    iw = ow_b * stride_w - pad_w + kw_b   # [BM,BK]

    n_b = n_idx[:, None]          # [BM,1]
    ic_b = ic[None, :]            # [1,BK]

    # Valid input mask for each (m, k)
    mask_in = (
        (n_b >= 0) & (n_b < N) &
        (ih >= 0) & (ih < H) &
        (iw >= 0) & (iw < W)
    )

    mask_m_b = mask_m[:, None]        # [BM,1]
    mask_k_b = k_mask[None, :]        # [1,BK]
    mask_a = mask_in & mask_m_b & mask_k_b  # [BM,BK]

    # -------------------------------------------------------------------------
    # Compute linear offsets into input tensor (treated as contiguous 1D)
    # index = ((n * C_in + ic) * H + ih) * W + iw
    # -------------------------------------------------------------------------
    x_offsets = ((n_b * C_in + ic_b) * H + ih) * W + iw  # [BM,BK]
    a_ptrs = x_ptr + x_offsets

    a = tl.load(a_ptrs, mask=mask_a, other=0.0)
    a = a.to(tl.float32)  # accumulate in fp32

    # -------------------------------------------------------------------------
    # Load weights as a [K, C_out] tile (logical), using strides:
    # physical layout: index = oc * K_total + k
    # so element B[k, oc] -> index = oc * K_total + k
    # -> stride_bk = 1, stride_bn = K_total
    # -------------------------------------------------------------------------
    offs_k_b = offs_k[:, None]    # [BK,1]
    offs_n_b = offs_n[None, :]    # [1,BN]

    mask_b = (offs_k_b < K_total) & (offs_n_b < C_out)  # [BK,BN]

    w_offsets = offs_n_b * K_total + offs_k_b           # [BK,BN]
    b_ptrs = w_ptr + w_offsets

    b = tl.load(b_ptrs, mask=mask_b, other=0.0)
    b = b.to(tl.float32)  # to fp32 for dot

    # -------------------------------------------------------------------------
    # Matrix multiply: [BM,BK] x [BK,BN] -> [BM,BN]
    # -------------------------------------------------------------------------
    acc = tl.dot(a, b)

    # -------------------------------------------------------------------------
    # Add bias: bias[oc]
    # -------------------------------------------------------------------------
    bias = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0)  # [BN]
    acc += bias[None, :]  # broadcast over BM

    # -------------------------------------------------------------------------
    # Write back to output: out[n, oc, oh, ow]
    # index = ((n * C_out + oc) * OH + oh) * OW + ow
    # -------------------------------------------------------------------------
    n_out = n_idx[:, None]        # [BM,1]
    oh_out = oh[:, None]          # [BM,1]
    ow_out = ow[:, None]          # [BM,1]
    oc_out = offs_n[None, :]      # [1,BN]

    out_offsets = ((n_out * C_out + oc_out) * OH + oh_out) * OW + ow_out  # [BM,BN]
    out_ptrs = out_ptr + out_offsets

    mask_out = mask_m_b & (oc_out < C_out)

    tl.store(out_ptrs, acc, mask=mask_out)


def triton_conv2d_nchw(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    stride: int,
    padding: int,
) -> torch.Tensor:
    """
    x:      [N, C_in, H, W],  contiguous, CUDA
    weight: [C_out, C_in, KH, KW], contiguous, CUDA
    bias:   [C_out], CUDA
    stride, padding: int (assumed symmetric for H/W)
    """
    assert x.is_cuda and weight.is_cuda, "Inputs must be CUDA tensors"
    assert x.dtype == torch.float32 and weight.dtype == torch.float32, "Only float32 supported"

    x = x.contiguous()
    weight = weight.contiguous()
    bias = bias.contiguous()

    N, C_in, H, W = x.shape
    C_out, C_in_w, KH, KW = weight.shape
    assert C_in_w == C_in, "Incompatible in_channels"

    stride_h = stride_w = stride
    pad_h = pad_w = padding

    OH = (H + 2 * pad_h - KH) // stride_h + 1
    OW = (W + 2 * pad_w - KW) // stride_w + 1

    out = torch.empty((N, C_out, OH, OW), device=x.device, dtype=x.dtype)

    # Tiling parameters (powers of 2, respect shared-memory-like budget)
    BLOCK_M = 32   # output positions
    BLOCK_N = 16   # output channels
    BLOCK_K = 512  # >= C_in * KH * KW (=363 for this model)

    M_total = N * OH * OW
    grid = (
        triton.cdiv(M_total, BLOCK_M),
        triton.cdiv(C_out, BLOCK_N),
    )

    conv2d_fwd_kernel[grid](
        x, weight, bias, out,
        N, C_in, H, W,
        C_out, KH, KW,
        OH, OW,
        stride_h, stride_w,
        pad_h, pad_w,
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
        BLOCK_K=BLOCK_K,
    )

    return out


class ModelNew(nn.Module):
    def __init__(self, num_classes=1000) -> None:
        super().__init__()
        # Same conv1 definition as the original model
        self.conv1 = nn.Conv2d(
            in_channels=3,
            out_channels=96,
            kernel_size=11,
            stride=4,
            padding=2,
            bias=True,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # CPU fallback for safety
        if not x.is_cuda:
            return self.conv1(x)

        return triton_conv2d_nchw(
            x,
            self.conv1.weight,
            self.conv1.bias,
            stride=self.conv1.stride[0],
            padding=self.conv1.padding[0],
        )
```

[NCU PROFILING METRICS]
{
  "conv2d_fwd_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 23.79,
    "launch__grid_size": 145200,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 8.33,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 10.78,
    "lts__t_sector_hit_rate.pct": 97.25
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
