Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 535, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_170115_batch_range70to100_deepseek_deepseek/74_conv_transposed_1D_dilated/code/kernel_20251212_183609.py", line 178, in forward
    return triton_conv_transpose1d(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_170115_batch_range70to100_deepseek_deepseek/74_conv_transposed_1D_dilated/code/kernel_20251212_183609.py", line 134, in triton_conv_transpose1d
    conv_transpose1d_kernel[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 192, in run
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 192, in <dictcomp>
    timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 170, in _bench
    return self.do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/testing.py", line 145, in do_bench
    fn()
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 156, in kernel_call
    self.fn.run(
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 284, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/backends/nvidia/compiler.py", line 448, in <lambda>
    stages["ttir"] = lambda src, metadata: self.make_ttir(src, metadata, options)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/backends/nvidia/compiler.py", line 240, in make_ttir
    pm.run(mod)
RuntimeError: PassManager::run failed

History Error:
None


PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Performs a transposed 1D convolution operation with square input and asymmetric kernel, optionally with dilation.

    Args:
        in_channels (int): Number of channels in the input tensor.
        out_channels (int): Number of channels produced by the convolution.
        kernel_size (int): Size of the convolution kernel.
        stride (int, optional): Stride of the convolution. Defaults to 1.
        padding (int, optional): Padding applied to the input. Defaults to 0.
        dilation (int, optional): Spacing between kernel elements. Defaults to 1.
        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, bias: bool = False):
        super(Model, self).__init__()
        self.conv1d_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Performs the transposed 1D convolution.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).
        """
        return self.conv1d_transpose(x)

# Test code
batch_size = 32
in_channels = 32
out_channels = 64
kernel_size = 5
length = 131072
stride = 1
padding = 0
dilation = 3

def get_inputs():
    x = torch.rand(batch_size, in_channels, length)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, dilation]
```

Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl
from typing import Optional


@triton.autotune(
    configs=[
        # Optimized for tensor cores with better warp occupancy
        triton.Config({'BLOCK_C_IN': 16, 'BLOCK_C_OUT': 128, 'BLOCK_L': 64, 'GROUP_C_IN': 16}, num_warps=4),
        triton.Config({'BLOCK_C_IN': 16, 'BLOCK_C_OUT': 128, 'BLOCK_L': 128, 'GROUP_C_IN': 16}, num_warps=8),
        triton.Config({'BLOCK_C_IN': 16, 'BLOCK_C_OUT': 256, 'BLOCK_L': 64, 'GROUP_C_IN': 16}, num_warps=8),
        triton.Config({'BLOCK_C_IN': 32, 'BLOCK_C_OUT': 128, 'BLOCK_L': 64, 'GROUP_C_IN': 32}, num_warps=8),
        triton.Config({'BLOCK_C_IN': 32, 'BLOCK_C_OUT': 128, 'BLOCK_L': 128, 'GROUP_C_IN': 32}, num_warps=8),
        triton.Config({'BLOCK_C_IN': 32, 'BLOCK_C_OUT': 256, 'BLOCK_L': 64, 'GROUP_C_IN': 32}, num_warps=8),
    ],
    key=['C_in', 'C_out', 'L_out'],
)
@triton.jit
def conv_transpose1d_kernel(
    x_ptr,
    w_ptr,
    b_ptr,
    out_ptr,
    B,
    C_in,
    C_out,
    L_in,
    L_out,
    K,
    stride,
    padding,
    dilation,
    has_bias: tl.constexpr,
    BLOCK_C_IN: tl.constexpr,
    BLOCK_C_OUT: tl.constexpr,
    BLOCK_L: tl.constexpr,
    GROUP_C_IN: tl.constexpr,
):
    pid_b = tl.program_id(0)
    pid_c = tl.program_id(1)
    pid_l = tl.program_id(2)
    
    # Use 2D block structure for better memory coalescing
    c_out_offset = pid_c * BLOCK_C_OUT + tl.arange(0, BLOCK_C_OUT)
    l_out_offset = pid_l * BLOCK_L + tl.arange(0, BLOCK_L)
    
    c_out_mask = c_out_offset < C_out
    l_out_mask = l_out_offset < L_out
    
    # Precompute constants for faster arithmetic
    stride_inv = 1.0 / stride if stride != 1 else 1.0
    C_out_K = C_out * K
    
    acc = tl.zeros((BLOCK_C_OUT, BLOCK_L), dtype=tl.float32)
    
    # Use vectorized loads where possible
    for c_in_block in range(0, C_in, GROUP_C_IN):
        c_in_base = c_in_block + tl.arange(0, GROUP_C_IN)
        c_in_mask = c_in_base < C_in
        
        for k in range(K):
            # Load weight tile with better memory coalescing
            w_ptrs = w_ptr + (
                c_in_base[:, None] * C_out_K +
                c_out_offset[None, :] * K +
                k
            )
            w_slice = tl.load(w_ptrs, 
                           mask=c_in_mask[:, None] & c_out_mask[None, :], 
                           other=0.0)
            
            # Vectorized computation of input positions
            k_dilation = dilation * k
            l_in_pos = (l_out_offset - k_dilation + padding) * stride_inv
            
            # Integer check without division
            l_in_pos_int = l_out_offset - k_dilation + padding
            valid_l_in = (l_in_pos_int >= 0) & (l_in_pos_int < L_in * stride)
            valid_stride = (l_in_pos_int & (stride - 1)) == 0 if stride > 1 else 1
            
            in_bounds = valid_l_in & valid_stride
            l_in_pos_idx = tl.where(in_bounds, (l_in_pos_int // stride), 0)
            
            # Load input tile with better cache utilization
            x_ptrs = x_ptr + (
                pid_b * (C_in * L_in) +
                c_in_base[:, None] * L_in +
                l_in_pos_idx[None, :]
            )
            
            mask = c_in_mask[:, None] & in_bounds[None, :]
            x_tile = tl.load(x_ptrs, mask=mask, other=0.0)
            
            # Use tensor cores with optimal tile sizes
            acc += tl.dot(w_slice.T, x_tile, allow_tf32=True)
    
    if has_bias:
        b_ptrs = b_ptr + c_out_offset
        bias = tl.load(b_ptrs, mask=c_out_mask, other=0.0)
        acc += bias[:, None]
    
    # Store with coalesced writes
    out_ptrs = out_ptr + (
        pid_b * (C_out * L_out) +
        c_out_offset[:, None] * L_out +
        l_out_offset[None, :]
    )
    tl.store(out_ptrs, acc, mask=c_out_mask[:, None] & l_out_mask[None, :])


def triton_conv_transpose1d(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: Optional[torch.Tensor] = None,
    stride: int = 1,
    padding: int = 0,
    dilation: int = 1,
) -> torch.Tensor:
    B, C_in, L_in = x.shape
    C_out = weight.shape[1]
    K = weight.shape[2]
    
    L_out = (L_in - 1) * stride - 2 * padding + dilation * (K - 1) + 1
    output = torch.empty((B, C_out, L_out), device=x.device, dtype=x.dtype)
    
    grid = lambda META: (
        B,
        triton.cdiv(C_out, META['BLOCK_C_OUT']),
        triton.cdiv(L_out, META['BLOCK_L'])
    )
    
    conv_transpose1d_kernel[grid](
        x, weight,
        bias if bias is not None else torch.empty(0, device=x.device),
        output,
        B, C_in, C_out, L_in, L_out, K,
        stride, padding, dilation,
        has_bias=bias is not None,
    )
    
    return output


class ModelNew(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: int = 0,
        dilation: int = 1,
        bias: bool = False,
    ):
        super(ModelNew, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        
        self.weight = nn.Parameter(
            torch.empty(in_channels, out_channels, kernel_size)
        )
        nn.init.kaiming_uniform_(self.weight, a=5**0.5)
        
        self.bias = None
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
            fan_in = in_channels * kernel_size
            bound = 1 / (fan_in ** 0.5)
            nn.init.uniform_(self.bias, -bound, bound)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_conv_transpose1d(
            x, self.weight, self.bias,
            self.stride, self.padding, self.dilation
        )
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes

```python
# <corrected code>
```
