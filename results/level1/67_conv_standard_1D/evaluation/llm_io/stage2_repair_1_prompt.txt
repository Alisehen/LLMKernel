Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 524, in compare_and_bench
    test_model = ModelNew(*init_args, **init_kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251213_075753_batch_range67to99_openai_deepseek/67_conv_standard_1D/code/kernel_20251213_080341.py", line 156, in __init__
    self.reset_parameters()
  File "/home/hyc/LLMKernel/run/20251213_075753_batch_range67to99_openai_deepseek/67_conv_standard_1D/code/kernel_20251213_080341.py", line 159, in reset_parameters
    nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
                                            ^^^^
NameError: name 'math' is not defined

History Error:
None


PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Performs a standard 1D convolution operation.

    Args:
        in_channels (int): Number of channels in the input tensor.
        out_channels (int): Number of channels produced by the convolution.
        kernel_size (int): Size of the convolution kernel.
        stride (int, optional): Stride of the convolution. Defaults to 1.
        padding (int, optional): Padding applied to the input. Defaults to 0.
        dilation (int, optional): Spacing between kernel elements. Defaults to 1.
        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.
        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = False):
        super(Model, self).__init__()
        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Performs the 1D convolution.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, length).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, length_out).
        """
        return self.conv1d(x)

# Test code
batch_size = 32
in_channels = 64
out_channels = 128
kernel_size = 3
length = 131072

def get_inputs():
    x = torch.rand(batch_size, in_channels, length)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization
```

Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_M": 128, "BLOCK_N": 64, "BLOCK_K": 32}, num_warps=4, num_stages=3),
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 128, "BLOCK_K": 32}, num_warps=4, num_stages=3),
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 64}, num_warps=4, num_stages=4),
    ],
    key=["B", "C_OUT", "L_OUT", "K_TOTAL"],
)
@triton.jit
def conv1d_kernel(
    x_ptr,
    w_ptr,
    y_ptr,
    B,
    C_IN,
    L_IN,
    C_OUT,
    L_OUT,
    KERNEL_SIZE,
    STRIDE,
    PADDING,
    DILATION,
    K_TOTAL,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    total_m = B * L_OUT
    mask_m = offs_m < total_m
    mask_n = offs_n < C_OUT

    b_idx = offs_m // L_OUT
    l_out_idx = offs_m - b_idx * L_OUT
    in_pos = l_out_idx * STRIDE - PADDING

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    for k_start in range(0, K_TOTAL, BLOCK_K):
        k_offsets = k_start + tl.arange(0, BLOCK_K)
        k_mask = k_offsets < K_TOTAL

        ci = k_offsets // KERNEL_SIZE
        kk = k_offsets - ci * KERNEL_SIZE

        pos = in_pos[:, None] + kk[None, :] * DILATION
        valid = (pos >= 0) & (pos < L_IN)
        pos_safe = tl.where(valid, pos, 0)

        x_offsets = ((b_idx[:, None] * C_IN + ci[None, :]) * L_IN) + pos_safe
        x_mask = mask_m[:, None] & k_mask[None, :] & valid
        x_vals = tl.load(x_ptr + x_offsets, mask=x_mask, other=0.0)

        w_offsets = offs_n[None, :] * K_TOTAL + k_offsets[:, None]
        w_mask = mask_n[None, :] & k_mask[:, None]
        w_vals = tl.load(w_ptr + w_offsets, mask=w_mask, other=0.0)

        acc += tl.dot(x_vals, w_vals)

    y_offsets = (b_idx[:, None] * C_OUT + offs_n[None, :]) * L_OUT + l_out_idx[:, None]
    y_mask = mask_m[:, None] & mask_n[None, :]
    tl.store(y_ptr + y_offsets, acc, mask=y_mask)


def triton_conv1d(x: torch.Tensor,
                  weight: torch.Tensor,
                  bias: torch.Tensor = None,
                  stride: int = 1,
                  padding: int = 0,
                  dilation: int = 1) -> torch.Tensor:
    assert x.is_cuda and weight.is_cuda, "Inputs must be CUDA tensors."
    assert x.dtype == torch.float32 and weight.dtype == torch.float32, "Only float32 is supported."
    stride = int(stride)
    padding = int(padding)
    dilation = int(dilation)

    x_contig = x.contiguous()
    w_contig = weight.contiguous()

    B, C_IN, L_IN = x_contig.shape
    C_OUT, C_W, KERNEL_SIZE = w_contig.shape
    assert C_W == C_IN, "in_channels mismatch."
    assert dilation > 0 and stride > 0
    L_OUT = (L_IN + 2 * padding - dilation * (KERNEL_SIZE - 1) - 1) // stride + 1
    assert L_OUT > 0, "Calculated output length must be positive."

    y = torch.empty((B, C_OUT, L_OUT), device=x.device, dtype=x.dtype)
    K_TOTAL = C_IN * KERNEL_SIZE

    grid = lambda meta: (
        triton.cdiv(B * L_OUT, meta["BLOCK_M"]),
        triton.cdiv(C_OUT, meta["BLOCK_N"]),
    )

    conv1d_kernel[grid](
        x_contig,
        w_contig,
        y,
        B,
        C_IN,
        L_IN,
        C_OUT,
        L_OUT,
        KERNEL_SIZE,
        stride,
        padding,
        dilation,
        K_TOTAL,
    )

    if bias is not None:
        y += bias.view(1, -1, 1)

    return y


class ModelNew(nn.Module):
    """
    Triton-accelerated 1D convolution module (supports stride, padding, dilation, groups=1).
    """
    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 kernel_size: int,
                 stride: int = 1,
                 padding: int = 0,
                 dilation: int = 1,
                 groups: int = 1,
                 bias: bool = False):
        super().__init__()
        if groups != 1:
            raise NotImplementedError("This implementation currently supports only groups=1.")
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dilation = dilation

        self.weight = nn.Parameter(torch.empty(out_channels, in_channels, kernel_size))
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter("bias", None)
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in = self.in_channels * self.kernel_size
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_conv1d(
            x,
            self.weight,
            self.bias,
            stride=self.stride,
            padding=self.padding,
            dilation=self.dilation,
        )
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes

```python
# <corrected code>
```
