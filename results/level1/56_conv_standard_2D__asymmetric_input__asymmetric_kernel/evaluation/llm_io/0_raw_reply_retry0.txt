```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def conv2d_kernel(
    x_ptr,
    w_ptr,
    b_ptr,
    out_ptr,
    N,
    C_IN,
    H,
    W,
    C_OUT,
    K_H,
    K_W,
    STRIDE_H,
    STRIDE_W,
    PAD_H,
    PAD_W,
    DIL_H,
    DIL_W,
    GROUPS,
    H_OUT,
    W_OUT,
    BLOCK_CO: tl.constexpr,
    BLOCK_HW: tl.constexpr,
    HAS_BIAS: tl.constexpr,
):
    pid_co = tl.program_id(axis=0)
    pid_hw = tl.program_id(axis=1)

    co_offsets = pid_co * BLOCK_CO + tl.arange(0, BLOCK_CO)
    hw_offsets = pid_hw * BLOCK_HW + tl.arange(0, BLOCK_HW)

    hw_total = N * H_OUT * W_OUT
    hw_mask = hw_offsets < hw_total
    co_mask = co_offsets < C_OUT

    n_idx = hw_offsets // (H_OUT * W_OUT)
    rem = hw_offsets % (H_OUT * W_OUT)
    oh = rem // W_OUT
    ow = rem % W_OUT

    acc = tl.zeros((BLOCK_HW, BLOCK_CO), dtype=tl.float32)

    group_out = C_OUT // GROUPS
    group_in = C_IN // GROUPS

    for kh in range(K_H):
        ih = oh * STRIDE_H - PAD_H + kh * DIL_H
        valid_h = (ih >= 0) & (ih < H)
        for kw in range(K_W):
            iw = ow * STRIDE_W - PAD_W + kw * DIL_W
            valid_w = (iw >= 0) & (iw < W)
            base_mask_hw = hw_mask & valid_h & valid_w

            for ci_inner in range(group_in):
                ci = (co_offsets // group_out) * group_in + ci_inner

                ci_broadcast = ci[None, :]
                n_broadcast = n_idx[:, None]
                ih_broadcast = ih[:, None]
                iw_broadcast = iw[:, None]

                x_index = ((n_broadcast * C_IN + ci_broadcast) * H + ih_broadcast) * W + iw_broadcast
                mask_x = base_mask_hw[:, None] & co_mask[None, :]

                x_vals = tl.load(x_ptr + x_index, mask=mask_x, other=0.0).to(tl.float32)

                w_index = (((co_offsets * group_in + ci_inner) * K_H) + kh) * K_W + kw
                w_vals = tl.load(w_ptr + w_index, mask=co_mask, other=0.0).to(tl.float32)
                w_vals = w_vals[None, :]

                acc += x_vals * w_vals

    if HAS_BIAS:
        bias = tl.load(b_ptr + co_offsets, mask=co_mask, other=0.0).to(tl.float32)
        acc += bias[None, :]

    out_index = ((n_idx[:, None] * C_OUT + co_offsets[None, :]) * H_OUT + oh[:, None]) * W_OUT + ow[:, None]
    out_mask = hw_mask[:, None] & co_mask[None, :]
    tl.store(out_ptr + out_index, acc, mask=out_mask)


def triton_conv2d(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    stride: tuple,
    padding: tuple,
    dilation: tuple,
    groups: int,
) -> torch.Tensor:
    x = x.contiguous()
    weight = weight.contiguous()
    if bias is not None:
        bias = bias.contiguous()

    N, C_in, H, W = x.shape
    C_out, _, K_h, K_w = weight.shape
    stride_h, stride_w = stride
    pad_h, pad_w = padding
    dil_h, dil_w = dilation

    H_out = (H + 2 * pad_h - dil_h * (K_h - 1) - 1) // stride_h + 1
    W_out = (W + 2 * pad_w - dil_w * (K_w - 1) - 1) // stride_w + 1

    out = torch.empty((N, C_out, H_out, W_out), device=x.device, dtype=x.dtype)

    BLOCK_CO = 1
    BLOCK_HW = 128

    has_bias = bias is not None
    bias_ptr = bias if has_bias else x.new_empty(0)

    grid = (
        triton.cdiv(C_out, BLOCK_CO),
        triton.cdiv(N * H_out * W_out, BLOCK_HW),
    )

    conv2d_kernel[grid](
        x,
        weight,
        bias_ptr,
        out,
        N,
        C_in,
        H,
        W,
        C_out,
        K_h,
        K_w,
        stride_h,
        stride_w,
        pad_h,
        pad_w,
        dil_h,
        dil_w,
        groups,
        H_out,
        W_out,
        BLOCK_CO=BLOCK_CO,
        BLOCK_HW=BLOCK_HW,
        HAS_BIAS=has_bias,
    )

    return out


class ModelNew(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: tuple,
        stride: tuple = (1, 1),
        padding: tuple = (0, 0),
        dilation: tuple = (1, 1),
        groups: int = 1,
        bias: bool = False,
    ) -> None:
        super().__init__()
        self.conv = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
            bias=bias,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_conv2d(
            x,
            self.conv.weight,
            self.conv.bias,
            self.conv.stride,
            self.conv.padding,
            self.conv.dilation,
            self.conv.groups,
        )
```