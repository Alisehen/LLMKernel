You are optimizing a Triton kernel based on algorithmic analysis.

# PyTorch Reference (Target Behavior)

```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Performs a depthwise 2D convolution with asymmetric input and asymmetric kernel.

    Args:
        in_channels (int): Number of channels in the input tensor.
        out_channels (int): Number of channels produced by the convolution.
        kernel_size_h (int): Height of the convolution kernel.
        kernel_size_w (int): Width of the convolution kernel.
        stride_h (int, optional): Stride of the convolution in height dimension. Defaults to 1.
        stride_w (int, optional): Stride of the convolution in width dimension. Defaults to 1.
        padding_h (int, optional): Padding applied to the input in height dimension. Defaults to 0.
        padding_w (int, optional): Padding applied to the input in width dimension. Defaults to 0.
        dilation_h (int, optional): Spacing between kernel elements in height dimension. Defaults to 1.
        dilation_w (int, optional): Spacing between kernel elements in width dimension. Defaults to 1.
        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.
        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size_h: int, kernel_size_w: int, stride_h: int = 1, stride_w: int = 1, padding_h: int = 0, padding_w: int = 0, dilation_h: int = 1, dilation_w: int = 1, groups: int = 1, bias: bool = False):
        super(Model, self).__init__()
        self.conv2d = nn.Conv2d(in_channels, in_channels, (kernel_size_h, kernel_size_w), stride=(stride_h, stride_w), padding=(padding_h, padding_w), dilation=(dilation_h, dilation_w), groups=in_channels, bias=bias)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Performs the depthwise 2D convolution.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, height_out, width_out).
        """
        return self.conv2d(x)

# Test code
batch_size = 32
in_channels = 128
out_channels = 128
kernel_size_h = 3
kernel_size_w = 7
width = 256
height = 128
stride_h = 1
stride_w = 1
padding_h = 0
padding_w = 0
dilation_h = 1
dilation_w = 1
groups = in_channels

def get_inputs():
    x = torch.rand(batch_size, in_channels, height, width)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size_h, kernel_size_w, stride_h, stride_w, padding_h, padding_w, dilation_h, dilation_w, groups]
```

**CRITICAL**: Study the PyTorch code carefully to understand:
- What does `forward()` return? (full output sequence vs final hidden state only)
- What is the computational pattern?
- What are the input/output shapes?

Your optimized kernel MUST match this exact behavior.

---

# Analysis Results

**Bottleneck**: The kernel performs expensive per-element index decoding, clamping, and in-bounds masking for every (kh, kw) tap, even for interior output positions where all accesses are guaranteed valid. This adds extra integer arithmetic and divergent predicates that reduce effective FLOP throughput despite high SM utilization.

**Optimization Strategy**: Split the computation into a fast interior-depthwise-conv kernel with no padding checks (pure affine indexing, no masks) and a small boundary kernel that handles only the halo region where the receptive field crosses image edges.

**Implementation Plan**: On the host, analytically compute the interior output ranges where all (kh, kw) accesses are in-bounds (e.g., h_out in [pad_h, H_out-pad_h_tail), w_out in [...]) for the given stride/dilation/padding. Launch a specialized interior Triton kernel over this region that uses direct affine indexing without clamping or in-bounds predicates inside the K_H×K_W loops; in this kernel, remove all masking logic and compute input offsets as simple n,c,h,w linear indices. For the remaining border pixels (top/bottom rows, left/right columns, and corners), either reuse the existing generic kernel or a smaller masked version, so the heavy fraction of work runs through the simpler, branch-free interior path.

**Expected Speedup**: 10-20%

---

# Current Kernel (needs optimization)

```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def depthwise_conv2d_kernel(
    x_ptr,          # *[N, C, H, W]
    w_ptr,          # *[C, 1, K_H, K_W]
    b_ptr,          # *[C] (ignored if has_bias=False)
    y_ptr,          # *[N, C, H_out, W_out]
    N, C, H, W,
    H_out, W_out,
    stride_h, stride_w,
    padding_h, padding_w,
    dilation_h, dilation_w,
    has_bias: tl.constexpr,
    K_H: tl.constexpr,  # kernel height
    K_W: tl.constexpr,  # kernel width
    BLOCK: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK
    offs = block_start + tl.arange(0, BLOCK)
    total_elems = N * C * H_out * W_out
    mask = offs < total_elems

    # Decode flattened index -> (n, c, h_out, w_out)
    w_out_idx = offs % W_out
    tmp = offs // W_out
    h_out_idx = tmp % H_out
    tmp = tmp // H_out
    c_idx = tmp % C
    n_idx = tmp // C

    # Base offsets for this (n, c) plane
    # input:  ((n * C + c) * H + h) * W + w
    # output: ((n * C + c) * H_out + h_out) * W_out + w_out
    nc = n_idx * C + c_idx
    in_plane_offset = nc * H * W
    out_plane_offset = nc * H_out * W_out

    h_out = h_out_idx
    w_out = w_out_idx

    # Top-left input coordinate for this output position
    h_in_origin = h_out * stride_h - padding_h
    w_in_origin = w_out * stride_w - padding_w

    # Accumulator in fp32 for better precision
    acc = tl.zeros([BLOCK], dtype=tl.float32)

    # Depthwise convolution: loop over kernel spatial dims
    for kh in tl.static_range(0, K_H):
        h_in = h_in_origin + kh * dilation_h
        # Clamp for safe addressing; we'll mask out of bounds later
        h_in_clamped = tl.maximum(0, tl.minimum(h_in, H - 1))

        for kw in tl.static_range(0, K_W):
            w_in = w_in_origin + kw * dilation_w
            w_in_clamped = tl.maximum(0, tl.minimum(w_in, W - 1))

            in_bounds = (
                (h_in >= 0)
                & (h_in < H)
                & (w_in >= 0)
                & (w_in < W)
                & mask
            )

            in_offsets = in_plane_offset + h_in_clamped * W + w_in_clamped
            x_val = tl.load(x_ptr + in_offsets, mask=in_bounds, other=0.0)
            x_val = x_val.to(tl.float32)

            # Weight index: [C, 1, K_H, K_W] -> c * K_H * K_W + kh * K_W + kw
            w_offsets = c_idx * (K_H * K_W) + kh * K_W + kw
            w_val = tl.load(w_ptr + w_offsets, mask=mask, other=0.0)
            w_val = w_val.to(tl.float32)

            acc += x_val * w_val

    if has_bias:
        # Bias per channel: [C]
        b_val = tl.load(b_ptr + c_idx, mask=mask, other=0.0)
        b_val = b_val.to(tl.float32)
        acc += b_val

    out_offsets = out_plane_offset + h_out * W_out + w_out
    tl.store(y_ptr + out_offsets, acc, mask=mask)


def depthwise_conv2d_triton(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor | None,
    stride_h: int,
    stride_w: int,
    padding_h: int,
    padding_w: int,
    dilation_h: int,
    dilation_w: int,
) -> torch.Tensor:
    """
    Depthwise 2D convolution (groups == in_channels) implemented with Triton.
    x: [N, C, H, W], weight: [C, 1, K_H, K_W], bias: [C] or None
    """
    assert x.is_cuda and weight.is_cuda, "Inputs must be CUDA tensors"
    assert x.is_contiguous(), "Input must be contiguous NCHW"
    assert weight.is_contiguous(), "Weight must be contiguous"

    N, C, H, W = x.shape
    C_w, one, K_H, K_W = weight.shape
    assert one == 1, "Weight shape for depthwise must be [C, 1, K_H, K_W]"
    assert C_w == C, "Weight C dimension must match input channels"

    # Output spatial dimensions (PyTorch Conv2d formula)
    H_out = (H + 2 * padding_h - dilation_h * (K_H - 1) - 1) // stride_h + 1
    W_out = (W + 2 * padding_w - dilation_w * (K_W - 1) - 1) // stride_w + 1

    y = torch.empty((N, C, H_out, W_out), device=x.device, dtype=x.dtype)

    total_elems = N * C * H_out * W_out
    BLOCK = 256  # power-of-2 as required

    grid = lambda meta: (triton.cdiv(max(total_elems, 1), meta["BLOCK"]),)

    has_bias = bias is not None
    # Dummy pointer if no bias (never read when has_bias == False)
    b_tensor = bias if bias is not None else weight

    depthwise_conv2d_kernel[grid](
        x,
        weight,
        b_tensor,
        y,
        N,
        C,
        H,
        W,
        H_out,
        W_out,
        stride_h,
        stride_w,
        padding_h,
        padding_w,
        dilation_h,
        dilation_w,
        has_bias=has_bias,
        K_H=K_H,
        K_W=K_W,
        BLOCK=BLOCK,
    )

    return y


class ModelNew(nn.Module):
    """
    Depthwise 2D convolution implemented with Triton, parameter-compatible with the original Model.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size_h: int,
        kernel_size_w: int,
        stride_h: int = 1,
        stride_w: int = 1,
        padding_h: int = 0,
        padding_w: int = 0,
        dilation_h: int = 1,
        dilation_w: int = 1,
        groups: int = 1,
        bias: bool = False,
    ):
        super().__init__()
        # Depthwise: out_channels == in_channels, groups == in_channels
        self.conv2d = nn.Conv2d(
            in_channels,
            in_channels,
            (kernel_size_h, kernel_size_w),
            stride=(stride_h, stride_w),
            padding=(padding_h, padding_w),
            dilation=(dilation_h, dilation_w),
            groups=in_channels,
            bias=bias,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Use Triton kernel with Conv2d weights/bias
        assert x.dim() == 4, "Expected input of shape [N, C, H, W]"
        x = x.contiguous()
        w = self.conv2d.weight
        b = self.conv2d.bias

        stride_h, stride_w = self.conv2d.stride
        padding_h, padding_w = self.conv2d.padding
        dilation_h, dilation_w = self.conv2d.dilation

        return depthwise_conv2d_triton(
            x,
            w,
            b,
            stride_h=stride_h,
            stride_w=stride_w,
            padding_h=padding_h,
            padding_w=padding_w,
            dilation_h=dilation_h,
            dilation_w=dilation_w,
        )
```

---

# Your Task

Implement the optimization strategy above. Focus on the specific bottleneck identified.

## Key Requirements

1. **Preserve correctness**: Maintain the same input/output behavior
2. **Apply the optimization**: Follow the implementation plan exactly
3. **Use valid Triton syntax**:
   - Every kernel MUST have `@triton.jit` decorator
   - Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
   - BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
   - No `continue`, `break`, `return` inside kernels (use masking)
   - Prefer `tl.dot(a, b, allow_tf32=True)` for matmul operations

4. **CRITICAL for RNN/GRU/LSTM Persistent Kernels**:
   - Time loop MUST be inside @triton.jit kernel, NOT in Python forward()
   - **HYBRID computation strategy** (CRITICAL for performance):
     * Precompute input-side gates OUTSIDE kernel: `gates_x = (T*B, In) @ W_ih` (ONE large GEMM)
     * INSIDE kernel: only recurrent-side: `for t: gates_h = h @ W_hh` (T small GEMMs)
   - CORRECT (FAST - use this):
     ```python
     # Python forward():
     gates_x_all = x.reshape(T*B, In) @ W_ih + b_ih  # ONE large GEMM
     gates_x_all = gates_x_all.view(T, B, 3*H)
     gru_persistent_kernel[grid](gates_x_all, h0, W_hh, ...)  # Launch ONCE

     @triton.jit
     def gru_persistent_kernel(gates_x_ptr, h_ptr, W_hh_ptr, ...):
         for t in range(T):  # Inside kernel
             gates_x_t = tl.load(gates_x_ptr + t*...)  # Precomputed
             gates_h = h @ W_hh  # Only recurrent GEMM
             h = (1-z)*n + z*h   # Fuse and update
     ```

5. **Output format**:
   - Imports: `import torch, torch.nn as nn, triton, triton.language as tl`
   - `@triton.jit` kernel(s)
   - Wrapper function(s)
   - `class ModelNew(nn.Module)` — REQUIRED
   - NO testing code, NO `if __name__ == "__main__"`

---

Generate the optimized kernel now. Output ONLY the complete Python code.
