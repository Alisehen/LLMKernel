You are optimizing a Triton kernel based on algorithmic analysis.

# PyTorch Reference (Target Behavior)

```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Performs a standard 3D convolution operation with asymmetric input and kernel sizes.

    Args:
        in_channels (int): Number of channels in the input tensor.
        out_channels (int): Number of channels produced by the convolution.
        kernel_size (tuple): Size of the convolution kernel in the form (kernel_size_d, kernel_size_h, kernel_size_w).
        stride (tuple, optional): Stride of the convolution in the form (stride_d, stride_h, stride_w). Defaults to (1, 1, 1).
        padding (tuple, optional): Padding applied to the input in the form (padding_d, padding_h, padding_w). Defaults to (0, 0, 0).
        dilation (tuple, optional): Spacing between kernel elements in the form (dilation_d, dilation_h, dilation_w). Defaults to (1, 1, 1).
        groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.
        bias (bool, optional): If `True`, adds a learnable bias to the output. Defaults to `False`.
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size: tuple, stride: tuple = (1, 1, 1), padding: tuple = (0, 0, 0), dilation: tuple = (1, 1, 1), groups: int = 1, bias: bool = False):
        super(Model, self).__init__()
        self.conv3d = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Performs the 3D convolution.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth_out, height_out, width_out).
        """
        return self.conv3d(x)

# Test code
batch_size = 8
in_channels = 3
out_channels = 64
kernel_size = (3, 5, 7)  # Asymmetric kernel size
depth = 16
height = 128
width = 128

def get_inputs():
    x = torch.rand(batch_size, in_channels, depth, height, width)
    return [x]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size]  # Provide in_channels, out_channels, kernel_size for initialization
```

**CRITICAL**: Study the PyTorch code carefully to understand:
- What does `forward()` return? (full output sequence vs final hidden state only)
- What is the computational pattern?
- What are the input/output shapes?

Your optimized kernel MUST match this exact behavior.

---

# Analysis Results

**Bottleneck**: The kernel flattens convolution into an M×K×Co GEMM-like loop and, for every BLOCK_K chunk, recomputes (ci, kd, kh, kw) from linear K plus 3D input coordinates, then gathers from global memory with minimal reuse. This im2col-style access pattern causes excess integer arithmetic and DRAM traffic compared to a direct, spatially tiled convolution that reuses input tiles across many output elements.

**Optimization Strategy**: Replace the generic implicit-GEMM formulation with a direct 3D convolution algorithm specialized for small kernels (e.g., 3×5×7), where the kernel loops over kd/kh/kw are explicitly nested and unrolled, and input tiles for a (Do_tile×Ho_tile×Wo_tile) output block are staged once in shared memory and reused for all channels and kernel taps. This reduces index math, avoids repeated K-dimension mapping, and maximizes spatial reuse of x and w.

**Implementation Plan**: Redesign the Triton kernel so that program_id maps directly to (n, oc_tile, d_out_tile, h_out_tile, w_out_tile) instead of linear M, and iterate explicitly over kd, kh, kw as small loops (or tl.constexpr-unrolled) inside the kernel. For each block, cooperatively load the necessary 3D input patch covering the output tile plus kernel halo into shared memory (or registers via tl.load into a local tensor), then perform the convolution by sliding the small kernel over this tile, accumulating into registers for a tile of output channels before writing back once. Specialize for the known asymmetric small kernel sizes (3,5,7) and possibly Ci multiples, so that kernel loops can be unrolled and address computations simplified, aiming to match or exceed cuDNN’s direct-conv behavior for this configuration.

**Expected Speedup**: 20-40%

---

# Current Kernel (needs optimization)

```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.jit
def conv3d_fwd_kernel(
    x_ptr,        # *f32, [N, Ci, Di, Hi, Wi] contiguous
    w_ptr,        # *f32, [Co, Ci, Kd, Kh, Kw] contiguous
    b_ptr,        # *f32, [Co] (ignored if has_bias == False)
    out_ptr,      # *f32, [N, Co, Do, Ho, Wo] contiguous
    N, Ci, Di, Hi, Wi,
    Co, Do, Ho, Wo,
    stride_d, stride_h, stride_w,
    pad_d, pad_h, pad_w,
    dil_d, dil_h, dil_w,
    Kd, Kh, Kw,
    M, K,         # M = N*Do*Ho*Wo, K = Ci*Kd*Kh*Kw
    has_bias: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    pid_m = tl.program_id(axis=0)
    pid_n = tl.program_id(axis=1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    mask_m = offs_m < M
    mask_n = offs_n < Co

    # Map linear M index -> (n, do, ho, wo)
    w_out = offs_m % Wo
    tmp = offs_m // Wo
    h_out = tmp % Ho
    tmp = tmp // Ho
    d_out = tmp % Do
    n_idx = tmp // Do

    n_2d = n_idx[:, None]
    d_out_2d = d_out[:, None]
    h_out_2d = h_out[:, None]
    w_out_2d = w_out[:, None]

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    for k_start in range(0, K, BLOCK_K):
        offs_k = k_start + tl.arange(0, BLOCK_K)
        k_mask = offs_k < K

        # Map linear K index -> (ci, kd, kh, kw)
        ci = offs_k // (Kd * Kh * Kw)
        rem = offs_k % (Kd * Kh * Kw)
        kd_idx = rem // (Kh * Kw)
        rem = rem % (Kh * Kw)
        kh_idx = rem // Kw
        kw_idx = rem % Kw

        # A tile (im2col input) [BM, BK]
        ci_a = ci[None, :]
        kd_a = kd_idx[None, :]
        kh_a = kh_idx[None, :]
        kw_a = kw_idx[None, :]

        z_in = d_out_2d * stride_d - pad_d + kd_a * dil_d
        y_in = h_out_2d * stride_h - pad_h + kh_a * dil_h
        x_in = w_out_2d * stride_w - pad_w + kw_a * dil_w

        in_bounds = (
            (z_in >= 0) & (z_in < Di) &
            (y_in >= 0) & (y_in < Hi) &
            (x_in >= 0) & (x_in < Wi)
        )

        a_mask = in_bounds & (mask_m[:, None]) & (k_mask[None, :])

        nci = n_2d * Ci + ci_a
        nd = nci * Di + z_in
        nh = nd * Hi + y_in
        nw = nh * Wi + x_in

        a_ptrs = x_ptr + nw
        a = tl.load(a_ptrs, mask=a_mask, other=0.0)

        # B tile (weights) [BK, BN]
        ci_b = ci[:, None]
        kd_b = kd_idx[:, None]
        kh_b = kh_idx[:, None]
        kw_b = kw_idx[:, None]

        oc_2d = offs_n[None, :]
        b_mask = (k_mask[:, None]) & (mask_n[None, :])

        w_index = (((oc_2d * Ci + ci_b) * Kd + kd_b) * Kh + kh_b) * Kw + kw_b
        w_ptrs = w_ptr + w_index
        b = tl.load(w_ptrs, mask=b_mask, other=0.0)

        acc += tl.dot(a, b, allow_tf32=True)

    if has_bias:
        bias_vals = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0)
        acc += bias_vals[None, :]

    # Map back to output tensor indices and store
    w_out = offs_m % Wo
    tmp = offs_m // Wo
    h_out = tmp % Ho
    tmp = tmp // Ho
    d_out = tmp % Do
    n_idx = tmp // Do

    n_2d = n_idx[:, None]
    d_out_2d = d_out[:, None]
    h_out_2d = h_out[:, None]
    w_out_2d = w_out[:, None]
    oc_2d = offs_n[None, :]

    out_index = (((n_2d * Co + oc_2d) * Do + d_out_2d) * Ho + h_out_2d) * Wo + w_out_2d
    out_mask = mask_m[:, None] & mask_n[None, :]
    tl.store(out_ptr + out_index, acc, mask=out_mask)


def triton_conv3d(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    stride: tuple,
    padding: tuple,
    dilation: tuple,
    groups: int,
) -> torch.Tensor:
    # Fallback to PyTorch when Triton isn't applicable
    if (not x.is_cuda) or (not weight.is_cuda):
        return torch.nn.functional.conv3d(
            x, weight, bias, stride=stride, padding=padding, dilation=dilation, groups=groups
        )
    if groups != 1:
        return torch.nn.functional.conv3d(
            x, weight, bias, stride=stride, padding=padding, dilation=dilation, groups=groups
        )
    if x.dtype != torch.float32 or weight.dtype != torch.float32:
        return torch.nn.functional.conv3d(
            x, weight, bias, stride=stride, padding=padding, dilation=dilation, groups=groups
        )

    x = x.contiguous()
    weight = weight.contiguous()

    N, Ci, Di, Hi, Wi = x.shape
    Co, Ci_w, Kd, Kh, Kw = weight.shape
    assert Ci_w == Ci, "Incompatible in_channels between input and weight for groups=1."

    stride_d, stride_h, stride_w = stride
    pad_d, pad_h, pad_w = padding
    dil_d, dil_h, dil_w = dilation

    Do = (Di + 2 * pad_d - dil_d * (Kd - 1) - 1) // stride_d + 1
    Ho = (Hi + 2 * pad_h - dil_h * (Kh - 1) - 1) // stride_h + 1
    Wo = (Wi + 2 * pad_w - dil_w * (Kw - 1) - 1) // stride_w + 1

    if Do <= 0 or Ho <= 0 or Wo <= 0:
        return torch.nn.functional.conv3d(
            x, weight, bias, stride=stride, padding=padding, dilation=dilation, groups=groups
        )

    out = torch.empty((N, Co, Do, Ho, Wo), device=x.device, dtype=x.dtype)

    M = N * Do * Ho * Wo
    K_total = Ci * Kd * Kh * Kw
    if M == 0 or Co == 0 or K_total == 0:
        return torch.nn.functional.conv3d(
            x, weight, bias, stride=stride, padding=padding, dilation=dilation, groups=groups
        )

    BLOCK_M = 64
    BLOCK_N = 64
    BLOCK_K = 32

    grid = lambda meta: (
        triton.cdiv(M, meta["BLOCK_M"]),
        triton.cdiv(Co, meta["BLOCK_N"]),
    )

    has_bias = bias is not None
    bias_ptr = bias if bias is not None else x  # dummy pointer when has_bias is False

    conv3d_fwd_kernel[grid](
        x,
        weight,
        bias_ptr,
        out,
        N,
        Ci,
        Di,
        Hi,
        Wi,
        Co,
        Do,
        Ho,
        Wo,
        stride_d,
        stride_h,
        stride_w,
        pad_d,
        pad_h,
        pad_w,
        dil_d,
        dil_h,
        dil_w,
        Kd,
        Kh,
        Kw,
        M,
        K_total,
        has_bias=has_bias,
        BLOCK_M=BLOCK_M,
        BLOCK_N=BLOCK_N,
        BLOCK_K=BLOCK_K,
        num_warps=4,
        num_stages=2,
    )

    return out


class ModelNew(nn.Module):
    """
    3D convolution implemented with a high-performance Triton kernel (groups=1 fast path).
    Falls back to PyTorch's conv3d for non-CUDA tensors, non-fp32 dtypes, or groups != 1.
    """
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: tuple,
        stride: tuple = (1, 1, 1),
        padding: tuple = (0, 0, 0),
        dilation: tuple = (1, 1, 1),
        groups: int = 1,
        bias: bool = False,
    ):
        super(ModelNew, self).__init__()
        # Use nn.Conv3d only for parameter management / initialization
        self.conv3d = nn.Conv3d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
            bias=bias,
        )
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.groups = groups

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return triton_conv3d(
            x,
            self.conv3d.weight,
            self.conv3d.bias,
            self.stride,
            self.padding,
            self.dilation,
            self.groups,
        )
```

---

# Your Task

Implement the optimization strategy above. Focus on the specific bottleneck identified.

## Key Requirements

1. **Preserve correctness**: Maintain the same input/output behavior
2. **Apply the optimization**: Follow the implementation plan exactly
3. **Use valid Triton syntax**:
   - Every kernel MUST have `@triton.jit` decorator
   - Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
   - BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
   - No `continue`, `break`, `return` inside kernels (use masking)
   - Prefer `tl.dot(a, b, allow_tf32=True)` for matmul operations

4. **CRITICAL for RNN/GRU/LSTM Persistent Kernels**:
   - Time loop MUST be inside @triton.jit kernel, NOT in Python forward()
   - **HYBRID computation strategy** (CRITICAL for performance):
     * Precompute input-side gates OUTSIDE kernel: `gates_x = (T*B, In) @ W_ih` (ONE large GEMM)
     * INSIDE kernel: only recurrent-side: `for t: gates_h = h @ W_hh` (T small GEMMs)
   - CORRECT (FAST - use this):
     ```python
     # Python forward():
     gates_x_all = x.reshape(T*B, In) @ W_ih + b_ih  # ONE large GEMM
     gates_x_all = gates_x_all.view(T, B, 3*H)
     gru_persistent_kernel[grid](gates_x_all, h0, W_hh, ...)  # Launch ONCE

     @triton.jit
     def gru_persistent_kernel(gates_x_ptr, h_ptr, W_hh_ptr, ...):
         for t in range(T):  # Inside kernel
             gates_x_t = tl.load(gates_x_ptr + t*...)  # Precomputed
             gates_h = h @ W_hh  # Only recurrent GEMM
             h = (1-z)*n + z*h   # Fuse and update
     ```

5. **Output format**:
   - Imports: `import torch, torch.nn as nn, triton, triton.language as tl`
   - `@triton.jit` kernel(s)
   - Wrapper function(s)
   - `class ModelNew(nn.Module)` — REQUIRED
   - NO testing code, NO `if __name__ == "__main__"`

---

Generate the optimized kernel now. Output ONLY the complete Python code.
