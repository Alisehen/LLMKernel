Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 535, in compare_and_bench
    test_out, _ = _run_once(test_model, inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 132, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_170043_batch_range5to40_deepseek_deepseek/24_LogSoftmax/code/kernel_20251212_225714.py", line 230, in forward
    return triton_log_softmax(x, dim=self.dim)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_170043_batch_range5to40_deepseek_deepseek/24_LogSoftmax/code/kernel_20251212_225714.py", line 195, in triton_log_softmax
    log_softmax_kernel_large[grid](
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 347, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/runtime/jit.py", line 569, in run
    kernel = self.compile(src, target=target, options=options.__dict__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 278, in compile
    module = src.make_ir(options, codegen_fns, module_map, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/miniconda3/envs/sglang/lib/python3.11/site-packages/triton/compiler/compiler.py", line 81, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 51:43:
    sum_exps = tl.zeros((BLOCK_ROWS,), dtype=tl.float32)

    while col_start < n_cols:
        col_offsets = col_start + tl.arange(0, BLOCK_SIZE)
        col_mask = col_offsets < n_cols

        for r in range(BLOCK_ROWS):
            row_idx = row_start + r
            if row_idx < n_rows:
                x_row_ptr = x_ptr + row_idx * stride_x
                x_chunk = tl.load(x_row_ptr + col_offsets, mask=col_mask)
                exp_val = tl.exp(x_chunk - max_vals[r])
                                           ^
ValueError('Did you forget to add @triton.jit ? (`_builder` argument must be provided outside of JIT functions.)')

History Error:
Previous Repair Attempts (avoid repeating these errors):
Attempt 1:
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/run/20251212_170043_batch_range5to40_deepseek_deepseek/24_LogSoftmax/code/kernel_20251212_225415.py", line 175, in triton_log_softmax
    grid_rows = (n_rows + BLOCK_ROWS - 1) // BLOCK_ROWS
                ~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~
ZeroDivisionError: integer division or modulo by zero



PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Simple model that performs a LogSoftmax activation.
    """
    def __init__(self, dim: int = 1):
        super(Model, self).__init__()
        self.dim = dim
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Applies LogSoftmax activation to the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, dim).

        Returns:
            torch.Tensor: Output tensor with LogSoftmax applied, same shape as input.
        """
        return torch.log_softmax(x, dim=self.dim)

batch_size = 4096
dim = 393216

def get_inputs():
    x = torch.rand(batch_size, dim)
    return [x]

def get_init_inputs():
    return []  # No special initialization inputs needed
```

Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl

@triton.jit
def log_softmax_kernel_large(
    x_ptr,
    output_ptr,
    n_rows,
    n_cols,
    stride_x,
    stride_output,
    BLOCK_SIZE: tl.constexpr,
    BLOCK_ROWS: tl.constexpr,
):
    """LogSoftmax kernel optimized for large columns with multi-row processing."""
    # Row block index
    row_block_idx = tl.program_id(axis=0)
    row_start = row_block_idx * BLOCK_ROWS
    
    # Step 1: Find row maxima for numerical stability (for all rows in block)
    col_start = 0
    # Initialize max values for each row in the block
    max_vals = tl.full((BLOCK_ROWS,), -float('inf'), dtype=tl.float32)
    
    # Process in blocks to handle large columns
    while col_start < n_cols:
        col_offsets = col_start + tl.arange(0, BLOCK_SIZE)
        col_mask = col_offsets < n_cols
        
        for r in range(BLOCK_ROWS):
            row_idx = row_start + r
            if row_idx < n_rows:
                x_row_ptr = x_ptr + row_idx * stride_x
                x_chunk = tl.load(x_row_ptr + col_offsets, mask=col_mask, other=-float('inf'))
                chunk_max = tl.max(x_chunk, axis=0)
                max_vals = tl.where(tl.arange(0, BLOCK_ROWS) == r, 
                                   tl.maximum(max_vals, chunk_max), 
                                   max_vals)
        
        col_start += BLOCK_SIZE
    
    # Step 2: Compute sum of exp(x - max) for each row
    col_start = 0
    sum_exps = tl.zeros((BLOCK_ROWS,), dtype=tl.float32)
    
    while col_start < n_cols:
        col_offsets = col_start + tl.arange(0, BLOCK_SIZE)
        col_mask = col_offsets < n_cols
        
        for r in range(BLOCK_ROWS):
            row_idx = row_start + r
            if row_idx < n_rows:
                x_row_ptr = x_ptr + row_idx * stride_x
                x_chunk = tl.load(x_row_ptr + col_offsets, mask=col_mask)
                exp_val = tl.exp(x_chunk - max_vals[r])
                row_sum = tl.sum(exp_val, axis=0)
                sum_exps = tl.where(tl.arange(0, BLOCK_ROWS) == r, 
                                   sum_exps + row_sum, 
                                   sum_exps)
        
        col_start += BLOCK_SIZE
    
    # Step 3: Compute logsumexp for each row
    log_sum_exps = tl.log(sum_exps) + max_vals
    
    # Step 4: Compute log_softmax = x - logsumexp and store
    col_start = 0
    while col_start < n_cols:
        col_offsets = col_start + tl.arange(0, BLOCK_SIZE)
        col_mask = col_offsets < n_cols
        
        for r in range(BLOCK_ROWS):
            row_idx = row_start + r
            if row_idx < n_rows:
                x_row_ptr = x_ptr + row_idx * stride_x
                output_row_ptr = output_ptr + row_idx * stride_output
                
                x_chunk = tl.load(x_row_ptr + col_offsets, mask=col_mask)
                output = x_chunk - log_sum_exps[r]
                tl.store(output_row_ptr + col_offsets, output, mask=col_mask)
        
        col_start += BLOCK_SIZE

@triton.jit
def log_softmax_kernel_small(
    x_ptr,
    output_ptr,
    n_rows,
    n_cols,
    stride_x0,
    stride_x1,
    stride_output0,
    stride_output1,
    BLOCK_SIZE: tl.constexpr,
    BLOCK_ROWS: tl.constexpr,
):
    """LogSoftmax kernel for small columns with multi-row processing."""
    # Row block index
    row_block_idx = tl.program_id(axis=0)
    row_start = row_block_idx * BLOCK_ROWS
    
    # Column offsets
    col_offsets = tl.arange(0, BLOCK_SIZE)
    col_mask = col_offsets < n_cols
    
    for r in range(BLOCK_ROWS):
        row_idx = row_start + r
        if row_idx < n_rows:
            # Load entire row
            x_row_ptr = x_ptr + row_idx * stride_x0
            output_row_ptr = output_ptr + row_idx * stride_output0
            
            x_row = tl.load(x_row_ptr + col_offsets * stride_x1, mask=col_mask, other=-float('inf'))
            
            # Compute log_softmax for this row
            row_max = tl.max(x_row, axis=0)
            exp_vals = tl.exp(x_row - row_max)
            row_sum_exp = tl.sum(exp_vals, axis=0)
            log_sum_exp = tl.log(row_sum_exp) + row_max
            output = x_row - log_sum_exp
            
            # Store results
            tl.store(output_row_ptr + col_offsets * stride_output1, output, mask=col_mask)

def triton_log_softmax(x: torch.Tensor, dim: int = -1) -> torch.Tensor:
    """Triton-accelerated LogSoftmax function with optimized grid layout."""
    assert x.is_contiguous(), "Input tensor must be contiguous"
    assert dim == 1 or dim == -1, "Only dim=1 or dim=-1 is supported"
    
    # Save original shape
    original_shape = x.shape
    output = torch.empty_like(x)
    
    # Get tensor dimensions
    if x.dim() == 2:
        n_rows, n_cols = x.shape
    else:
        # Handle higher dimensions by flattening
        if dim < 0:
            dim = x.dim() + dim
        
        # Flatten all dimensions before dim into n_rows
        n_rows = 1
        for i in range(dim):
            n_rows *= x.shape[i]
        
        # Flatten all dimensions from dim onward into n_cols
        n_cols = 1
        for i in range(dim, x.dim()):
            n_cols *= x.shape[i]
        
        x = x.view(n_rows, n_cols)
        output = output.view(n_rows, n_cols)
    
    # Optimization: Process multiple rows per block to improve SM utilization
    MAX_BLOCK_SIZE = 2048
    BLOCK_SIZE = min(MAX_BLOCK_SIZE, triton.next_power_of_2(n_cols))
    
    # Tune number of rows per block based on column size
    if n_cols <= 512:
        BLOCK_ROWS = 8
    elif n_cols <= 1024:
        BLOCK_ROWS = 4
    elif n_cols <= 2048:
        BLOCK_ROWS = 2
    else:
        BLOCK_ROWS = 1
    
    # Adjust BLOCK_ROWS to not exceed available threads
    if BLOCK_SIZE * BLOCK_ROWS > 1024:
        BLOCK_ROWS = max(1, 1024 // BLOCK_SIZE)
    
    # Calculate grid with multiple rows per block
    grid_rows = (n_rows + BLOCK_ROWS - 1) // BLOCK_ROWS
    
    # Choose kernel based on column size
    if n_cols <= MAX_BLOCK_SIZE:
        # Use kernel that processes entire rows in blocks
        grid = (grid_rows,)
        log_softmax_kernel_small[grid](
            x, output,
            n_rows, n_cols,
            x.stride(0), x.stride(1),
            output.stride(0), output.stride(1),
            BLOCK_SIZE=BLOCK_SIZE,
            BLOCK_ROWS=BLOCK_ROWS,
            num_warps=8 if BLOCK_SIZE >= 512 else 4,
            num_stages=3
        )
    else:
        # Use kernel that processes rows in blocks (with column tiling)
        grid = (grid_rows,)
        log_softmax_kernel_large[grid](
            x, output,
            n_rows, n_cols,
            x.stride(0),
            output.stride(0),
            BLOCK_SIZE=BLOCK_SIZE,
            BLOCK_ROWS=BLOCK_ROWS,
            num_warps=8,
            num_stages=3
        )
    
    # Restore original shape if needed
    if output.shape != original_shape:
        output = output.view(original_shape)
    
    return output

class ModelNew(nn.Module):
    """
    Optimized model that performs LogSoftmax activation using Triton kernels.
    """
    def __init__(self, dim: int = 1):
        super(ModelNew, self).__init__()
        self.dim = dim
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Applies LogSoftmax activation using optimized Triton kernels.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Output tensor with LogSoftmax applied.
        """
        return triton_log_softmax(x, dim=self.dim)
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes

```python
# <corrected code>
```
