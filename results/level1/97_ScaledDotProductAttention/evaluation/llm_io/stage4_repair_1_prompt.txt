Fix the Triton kernel errors. Generate correct, high-performance code.

Current Error Log:
Traceback (most recent call last):
  File "/home/hyc/LLMKernel/utils/compile_and_run.py", line 514, in compare_and_bench
    inp = get_inputs()
          ^^^^^^^^^^^^
  File "/home/hyc/LLMKernel/KernelBench/level1_low_score/97_ScaledDotProductAttention.py", line 18, in get_inputs
    Q = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

History Error:
None


PyTorch Reference:
```python
import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:
        out = torch.nn.functional.scaled_dot_product_attention(Q, K, V)
        return out

batch_size = 32
num_heads = 32
sequence_length = 512
embedding_dimension = 1024

def get_inputs():
    Q = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)
    K = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)
    V = torch.rand(batch_size, num_heads, sequence_length, embedding_dimension, device='cuda', dtype=torch.float16)
    return [Q, K, V]

def get_init_inputs():
    return []
```

Broken Code:
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


# ---------------------------------------------
# Optimized batched GEMM for attention (FP16)
# ---------------------------------------------

@triton.autotune(
    configs=[
        # Original baseline config (kept exactly)
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 32},
            num_warps=4,
            num_stages=3,
        ),
        # Original wide-N tile
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 128, "BLOCK_K": 32},
            num_warps=8,
            num_stages=3,
        ),
        # Original tall-M tile
        triton.Config(
            {"BLOCK_M": 128, "BLOCK_N": 64, "BLOCK_K": 32},
            num_warps=8,
            num_stages=3,
        ),
        # Nearby variants for micro-tuning around the above tiles
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 32},
            num_warps=8,
            num_stages=3,
        ),
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 128, "BLOCK_K": 32},
            num_warps=8,
            num_stages=4,
        ),
        triton.Config(
            {"BLOCK_M": 128, "BLOCK_N": 64, "BLOCK_K": 32},
            num_warps=8,
            num_stages=4,
        ),
    ],
    key=["M", "N", "K"],
)
@triton.jit
def batched_matmul_kernel(
    A_ptr, B_ptr, C_ptr,
    BATCH, M, N, K,
    stride_ab, stride_am, stride_ak,
    stride_bb, stride_bk, stride_bn,
    stride_cb, stride_cm, stride_cn,
    scale,
    ADD_SCALE: tl.constexpr,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    """
    Compute batched matrix multiplication:
        for b in [0, BATCH):
            C[b, :, :] = (A[b, :, :] @ B[b, :, :]) * scale (if ADD_SCALE)

    Shapes:
        A: [BATCH, M, K]
        B: [BATCH, K, N]
        C: [BATCH, M, N]

    Strides are in elements.
    """
    pid_m = tl.program_id(axis=0)
    pid_n = tl.program_id(axis=1)
    pid_b = tl.program_id(axis=2)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    # Base pointers for this batch
    A_batch_ptr = A_ptr + pid_b * stride_ab
    B_batch_ptr = B_ptr + pid_b * stride_bb
    C_batch_ptr = C_ptr + pid_b * stride_cb

    # Initialize pointers for the first K-tile
    a_ptrs = A_batch_ptr + (
        offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    )
    b_ptrs = B_batch_ptr + (
        offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn
    )

    # FP32 accumulator for better precision
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    k = 0
    while k < K:
        k_offsets = k + offs_k
        k_mask = k_offsets < K

        a = tl.load(
            a_ptrs,
            mask=(offs_m[:, None] < M) & (k_mask[None, :]),
            other=0.0,
        )
        b = tl.load(
            b_ptrs,
            mask=(k_mask[:, None]) & (offs_n[None, :] < N),
            other=0.0,
        )

        # tl.dot on fp16 uses Tensor Cores when BLOCK_* satisfy TC constraints.
        acc += tl.dot(a, b)

        # Advance to next K-tile
        k += BLOCK_K
        a_ptrs += BLOCK_K * stride_ak
        b_ptrs += BLOCK_K * stride_bk

    if ADD_SCALE:
        acc *= scale

    # Write back to C
    c = acc.to(tl.float16)
    c_ptrs = C_batch_ptr + (
        offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    )
    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(c_ptrs, c, mask=c_mask)


# ---------------------------------------------
# Softmax kernel (row-wise over N)
# ---------------------------------------------

@triton.jit
def softmax_kernel(
    input_ptr, output_ptr,
    stride_ib, stride_im, stride_in,
    stride_ob, stride_om, stride_on,
    BATCH, M, N,
    BLOCK_N: tl.constexpr,
):
    """
    Row-wise softmax over last dimension N for a [BATCH, M, N] tensor.
    Each program instance handles one row.
    """
    pid_m = tl.program_id(axis=0)  # row index within sequence length (M)
    pid_b = tl.program_id(axis=1)  # batch-head index (BATCH)

    row_in_ptr = input_ptr + pid_b * stride_ib + pid_m * stride_im
    row_out_ptr = output_ptr + pid_b * stride_ob + pid_m * stride_om

    offs_n = tl.arange(0, BLOCK_N)
    mask = offs_n < N

    row = tl.load(
        row_in_ptr + offs_n * stride_in,
        mask=mask,
        other=-float("inf"),
    ).to(tl.float32)

    row_max = tl.max(row, axis=0)
    row = row - row_max
    exp_row = tl.exp(row)
    denom = tl.sum(exp_row, axis=0)
    softmax = exp_row / denom

    tl.store(
        row_out_ptr + offs_n * stride_on,
        softmax.to(tl.float16),
        mask=mask,
    )


# ---------------------------------------------
# Wrapper: Scaled Dot-Product Attention
# ---------------------------------------------

def triton_scaled_dot_product_attention(Q: torch.Tensor,
                                        K: torch.Tensor,
                                        V: torch.Tensor) -> torch.Tensor:
    """
    Triton implementation of scaled dot-product attention:

        attn = softmax(Q @ K^T / sqrt(d_k))
        out  = attn @ V

    Inputs:
        Q, K, V: [B, H, S, D], dtype=float16, CUDA tensors
    Output:
        out: [B, H, S, D], dtype=float16
    """
    assert Q.is_cuda and K.is_cuda and V.is_cuda, "Inputs must be CUDA tensors"
    assert Q.dtype == torch.float16 and K.dtype == torch.float16 and V.dtype == torch.float16
    assert Q.shape == K.shape == V.shape

    B, H, S, D = Q.shape
    BH = B * H

    # Flatten [B, H] into batch dimension
    Q_flat = Q.contiguous().view(BH, S, D)
    K_flat = K.contiguous().view(BH, S, D)
    V_flat = V.contiguous().view(BH, S, D)

    device = Q.device

    # 1) Compute attention scores = Q @ K^T / sqrt(D)
    #    Result shape: [BH, S, S]
    scores = torch.empty((BH, S, S), device=device, dtype=torch.float16)

    def grid_scores(meta):
        return (
            triton.cdiv(S, meta["BLOCK_M"]),
            triton.cdiv(S, meta["BLOCK_N"]),
            BH,
        )

    scale = 1.0 / float(D**0.5)

    batched_matmul_kernel[grid_scores](
        Q_flat, K_flat, scores,
        BH, S, S, D,
        # A strides: Q_flat as [BH, M=S, K=D]
        Q_flat.stride(0), Q_flat.stride(1), Q_flat.stride(2),
        # B strides: treat K_flat as [BH, K=D, N=S] via transpose of last two dims
        K_flat.stride(0), K_flat.stride(2), K_flat.stride(1),
        # C strides: scores as [BH, M=S, N=S]
        scores.stride(0), scores.stride(1), scores.stride(2),
        scale,
        ADD_SCALE=True,
    )

    # 2) Softmax over last dimension of scores: [BH, S, S]
    probs = torch.empty_like(scores)

    BLOCK_SOFTMAX = 512  # tuned for typical S=512; power-of-two, avoids oversized tiles

    grid_softmax = (S, BH)
    softmax_kernel[grid_softmax](
        scores, probs,
        scores.stride(0), scores.stride(1), scores.stride(2),
        probs.stride(0), probs.stride(1), probs.stride(2),
        BH, S, S,
        BLOCK_N=BLOCK_SOFTMAX,
        num_warps=4,
        num_stages=2,  # micro-tuned: sufficient pipeline depth without over-buffering
    )

    # 3) Compute output = probs @ V
    #    probs [BH, S, S], V [BH, S, D] -> out_flat [BH, S, D]
    out_flat = torch.empty((BH, S, D), device=device, dtype=torch.float16)

    def grid_out(meta):
        return (
            triton.cdiv(S, meta["BLOCK_M"]),
            triton.cdiv(D, meta["BLOCK_N"]),
            BH,
        )

    batched_matmul_kernel[grid_out](
        probs, V_flat, out_flat,
        BH, S, D, S,
        # A strides: probs as [BH, M=S, K=S]
        probs.stride(0), probs.stride(1), probs.stride(2),
        # B strides: V_flat as [BH, K=S, N=D]
        V_flat.stride(0), V_flat.stride(1), V_flat.stride(2),
        # C strides: out_flat as [BH, M=S, N=D]
        out_flat.stride(0), out_flat.stride(1), out_flat.stride(2),
        1.0,
        ADD_SCALE=False,
    )

    # Reshape back to [B, H, S, D]
    out = out_flat.view(B, H, S, D)
    return out


# ---------------------------------------------
# nn.Module wrapper
# ---------------------------------------------

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:
        return triton_scaled_dot_product_attention(Q, K, V)
```

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl, AND any other modules used (e.g., import math if using math.sqrt)
   2. @triton.jit decorated kernel function(s) — NO continue/break/return inside loops (use masking)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels — THIS CLASS IS REQUIRED
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs
3. Learn from previous repair attempts to avoid repeating the same mistakes
4. Ensure ALL imports are included at the top (common mistake: forgetting `import math`)

```python
# <corrected code>
```
