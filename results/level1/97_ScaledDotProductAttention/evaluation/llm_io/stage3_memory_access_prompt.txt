You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU
GPU Name: 4090
Architecture: Ada Lovelace
• Compute Capability: 8.9
• Number of SMs: 128
• Memory Bandwidth: 1008 GB/s
• TF32 Tensor Core TFLOPS: 82.6 with dense
• BFLOAT16 Tensor Core TFLOPS: 165.2 with dense
• FP16 Tensor Core TFLOPS: 165.2 with dense
• Maximum number of registers per thread: 255
• Maximum threads per block: 1024
• Maximum threads per SM: 1536
• Warp size: 32
• Maximum concurrent warps per SM: 48
• Shared memory capacity per SM: 100 KB
• Maximum shared memory per thread block: 99 KB
• L2 cache (global, all SM shared): 72 MB



[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Memory efficiency & latency hiding.

Metrics:
- dram__throughput.avg.pct_of_peak_sustained_elapsed
- lts__t_sector_hit_rate.pct
- smsp__warp_issue_stalled_memory_dependency_per_warp_active.pct (<20%)

Rules:
- Increase num_stages only if memory stalls are high
- Do not rewrite access patterns without metric evidence
- Larger BLOCK_K improves reuse but increases register pressure

Autotune:
- If unsure, try num_stages ∈ {1,2,3} on kernel



[CURRENT CODE]
```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


# ---------------------------------------------
# Optimized batched GEMM for attention (FP16)
# ---------------------------------------------

@triton.autotune(
    configs=[
        # Baseline-ish, good for many shapes, balanced occupancy
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 32},
            num_warps=4,
            num_stages=3,
        ),
        # Wider N tile – good when N is large (e.g., S or D = 128/256)
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 128, "BLOCK_K": 32},
            num_warps=8,
            num_stages=3,
        ),
        # Taller M tile – good when M is large (e.g., S = 512)
        triton.Config(
            {"BLOCK_M": 128, "BLOCK_N": 64, "BLOCK_K": 32},
            num_warps=8,
            num_stages=3,
        ),
    ],
    key=["M", "N", "K"],
)
@triton.jit
def batched_matmul_kernel(
    A_ptr, B_ptr, C_ptr,
    BATCH, M, N, K,
    stride_ab, stride_am, stride_ak,
    stride_bb, stride_bk, stride_bn,
    stride_cb, stride_cm, stride_cn,
    scale,
    ADD_SCALE: tl.constexpr,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    """
    Compute batched matrix multiplication:
        for b in [0, BATCH):
            C[b, :, :] = (A[b, :, :] @ B[b, :, :]) * scale (if ADD_SCALE)

    Shapes:
        A: [BATCH, M, K]
        B: [BATCH, K, N]
        C: [BATCH, M, N]

    Strides are in elements.
    """
    pid_m = tl.program_id(axis=0)
    pid_n = tl.program_id(axis=1)
    pid_b = tl.program_id(axis=2)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    # Base pointers for this batch
    A_batch_ptr = A_ptr + pid_b * stride_ab
    B_batch_ptr = B_ptr + pid_b * stride_bb
    C_batch_ptr = C_ptr + pid_b * stride_cb

    # FP32 accumulator for better precision
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    k = 0
    while k < K:
        k_offsets = k + offs_k
        k_mask = k_offsets < K

        # Pointers for A [M, K]
        a_ptrs = A_batch_ptr + (
            offs_m[:, None] * stride_am + k_offsets[None, :] * stride_ak
        )
        # Pointers for B [K, N]
        b_ptrs = B_batch_ptr + (
            k_offsets[:, None] * stride_bk + offs_n[None, :] * stride_bn
        )

        a = tl.load(
            a_ptrs,
            mask=(offs_m[:, None] < M) & (k_mask[None, :]),
            other=0.0,
        )
        b = tl.load(
            b_ptrs,
            mask=(k_mask[:, None]) & (offs_n[None, :] < N),
            other=0.0,
        )

        # tl.dot on fp16 uses Tensor Cores when BLOCK_* satisfy TC constraints.
        acc += tl.dot(a, b)

        k += BLOCK_K

    if ADD_SCALE:
        acc *= scale

    # Write back to C
    c = acc.to(tl.float16)
    c_ptrs = C_batch_ptr + (
        offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    )
    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(c_ptrs, c, mask=c_mask)


# ---------------------------------------------
# Softmax kernel (row-wise over N)
# ---------------------------------------------

@triton.jit
def softmax_kernel(
    input_ptr, output_ptr,
    stride_ib, stride_im, stride_in,
    stride_ob, stride_om, stride_on,
    BATCH, M, N,
    BLOCK_N: tl.constexpr,
):
    """
    Row-wise softmax over last dimension N for a [BATCH, M, N] tensor.
    Each program instance handles one row.
    """
    pid_m = tl.program_id(axis=0)  # row index within sequence length (M)
    pid_b = tl.program_id(axis=1)  # batch-head index (BATCH)

    row_in_ptr = input_ptr + pid_b * stride_ib + pid_m * stride_im
    row_out_ptr = output_ptr + pid_b * stride_ob + pid_m * stride_om

    offs_n = tl.arange(0, BLOCK_N)
    mask = offs_n < N

    row = tl.load(
        row_in_ptr + offs_n * stride_in,
        mask=mask,
        other=-float("inf"),
    ).to(tl.float32)

    row_max = tl.max(row, axis=0)
    row = row - row_max
    exp_row = tl.exp(row)
    denom = tl.sum(exp_row, axis=0)
    softmax = exp_row / denom

    tl.store(
        row_out_ptr + offs_n * stride_on,
        softmax.to(tl.float16),
        mask=mask,
    )


# ---------------------------------------------
# Wrapper: Scaled Dot-Product Attention
# ---------------------------------------------

def triton_scaled_dot_product_attention(Q: torch.Tensor,
                                        K: torch.Tensor,
                                        V: torch.Tensor) -> torch.Tensor:
    """
    Triton implementation of scaled dot-product attention:

        attn = softmax(Q @ K^T / sqrt(d_k))
        out  = attn @ V

    Inputs:
        Q, K, V: [B, H, S, D], dtype=float16, CUDA tensors
    Output:
        out: [B, H, S, D], dtype=float16
    """
    assert Q.is_cuda and K.is_cuda and V.is_cuda, "Inputs must be CUDA tensors"
    assert Q.dtype == torch.float16 and K.dtype == torch.float16 and V.dtype == torch.float16
    assert Q.shape == K.shape == V.shape

    B, H, S, D = Q.shape
    BH = B * H

    # Flatten [B, H] into batch dimension
    Q_flat = Q.contiguous().view(BH, S, D)
    K_flat = K.contiguous().view(BH, S, D)
    V_flat = V.contiguous().view(BH, S, D)

    device = Q.device

    # 1) Compute attention scores = Q @ K^T / sqrt(D)
    #    Result shape: [BH, S, S]
    scores = torch.empty((BH, S, S), device=device, dtype=torch.float16)

    def grid_scores(meta):
        return (
            triton.cdiv(S, meta["BLOCK_M"]),
            triton.cdiv(S, meta["BLOCK_N"]),
            BH,
        )

    scale = 1.0 / float(D**0.5)

    batched_matmul_kernel[grid_scores](
        Q_flat, K_flat, scores,
        BH, S, S, D,
        # A strides: Q_flat as [BH, M=S, K=D]
        Q_flat.stride(0), Q_flat.stride(1), Q_flat.stride(2),
        # B strides: treat K_flat as [BH, K=D, N=S] via transpose of last two dims
        K_flat.stride(0), K_flat.stride(2), K_flat.stride(1),
        # C strides: scores as [BH, M=S, N=S]
        scores.stride(0), scores.stride(1), scores.stride(2),
        scale,
        ADD_SCALE=True,
    )

    # 2) Softmax over last dimension of scores: [BH, S, S]
    probs = torch.empty_like(scores)

    BLOCK_SOFTMAX = 512  # tuned for typical S=512; power-of-two, avoids oversized tiles

    grid_softmax = (S, BH)
    softmax_kernel[grid_softmax](
        scores, probs,
        scores.stride(0), scores.stride(1), scores.stride(2),
        probs.stride(0), probs.stride(1), probs.stride(2),
        BH, S, S,
        BLOCK_N=BLOCK_SOFTMAX,
        num_warps=4,
        num_stages=1,
    )

    # 3) Compute output = probs @ V
    #    probs [BH, S, S], V [BH, S, D] -> out_flat [BH, S, D]
    out_flat = torch.empty((BH, S, D), device=device, dtype=torch.float16)

    def grid_out(meta):
        return (
            triton.cdiv(S, meta["BLOCK_M"]),
            triton.cdiv(D, meta["BLOCK_N"]),
            BH,
        )

    batched_matmul_kernel[grid_out](
        probs, V_flat, out_flat,
        BH, S, D, S,
        # A strides: probs as [BH, M=S, K=S]
        probs.stride(0), probs.stride(1), probs.stride(2),
        # B strides: V_flat as [BH, K=S, N=D]
        V_flat.stride(0), V_flat.stride(1), V_flat.stride(2),
        # C strides: out_flat as [BH, M=S, N=D]
        out_flat.stride(0), out_flat.stride(1), out_flat.stride(2),
        1.0,
        ADD_SCALE=False,
    )

    # Reshape back to [B, H, S, D]
    out = out_flat.view(B, H, S, D)
    return out


# ---------------------------------------------
# nn.Module wrapper
# ---------------------------------------------

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:
        return triton_scaled_dot_product_attention(Q, K, V)
```

[NCU PROFILING METRICS]
{
  "batched_matmul_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 44.22,
    "launch__grid_size": 65536,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 49.58,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 65.01,
    "lts__t_sector_hit_rate.pct": 88.47
  },
  "softmax_kernel": {
    "sm__throughput.avg.pct_of_peak_sustained_elapsed": 31.29,
    "launch__grid_size": 524288,
    "sm__warps_active.avg.pct_of_peak_sustained_active": 95.58,
    "dram__throughput.avg.pct_of_peak_sustained_elapsed": 93.55,
    "lts__t_sector_hit_rate.pct": 51.88
  }
}

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

OUTPUT RULES (STRICT):
1. Follow this exact order:
   1. Imports: torch, torch.nn, triton, triton.language as tl
   2. @triton.jit decorated kernel function(s)
   3. Wrapper function(s) for grid calculation and kernel launch
   4. class ModelNew(nn.Module) that calls your kernels
2. Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
