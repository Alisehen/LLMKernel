```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


# -------------------------
# Low-level Triton kernels
# -------------------------

@triton.jit
def linear_kernel(
    x_ptr, w_ptr, b_ptr, y_ptr,
    M, N, K,
    stride_xm, stride_xk,
    stride_wk, stride_wn,
    stride_ym, stride_yn,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    """
    y[M, N] = x[M, K] @ w[K, N] + b[N]
    Accumulate in fp32, allow TF32.
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk
    w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    for k in range(0, K, BLOCK_K):
        k_remaining = K - k

        x_mask = (offs_m[:, None] < M) & (offs_k[None, :] < k_remaining)
        w_mask = (offs_k[:, None] < k_remaining) & (offs_n[None, :] < N)

        x = tl.load(x_ptrs, mask=x_mask, other=0.0)
        w = tl.load(w_ptrs, mask=w_mask, other=0.0)

        acc += tl.dot(x, w, allow_tf32=True)

        x_ptrs += BLOCK_K * stride_xk
        w_ptrs += BLOCK_K * stride_wk

    # Add bias
    bias = tl.load(b_ptr + offs_n, mask=offs_n < N, other=0.0)
    acc += bias[None, :]

    y_ptrs = y_ptr + offs_m[:, None] * stride_ym + offs_n[None, :] * stride_yn
    y_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(y_ptrs, acc, mask=y_mask)


@triton.jit
def lstm_pointwise_kernel(
    gates_ptr, c_prev_ptr, h_new_ptr, c_new_ptr,
    M, H,
    stride_gm, stride_gn,
    stride_cm, stride_cn,
    stride_hm, stride_hn,
    stride_cnm, stride_cnn,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    """
    Pointwise LSTM update:
      gates: [M, 4H] = [i, f, g, o] pre-activations
      c_prev: [M, H]
      Outputs:
        c_new = f * c_prev + i * g
        h_new = o * tanh(c_new)

    sigmoid(x) = 1 / (1 + exp(-x))
    tanh(x)    = (exp(2x) - 1) / (exp(2x) + 1)
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)

    mask_m = offs_m[:, None] < M
    mask_n = offs_n[None, :] < H
    mask = mask_m & mask_n

    # Base pointers for i gate
    gates_i_ptrs = gates_ptr + offs_m[:, None] * stride_gm + offs_n[None, :] * stride_gn

    gi = tl.load(gates_i_ptrs, mask=mask, other=0.0)
    gf = tl.load(gates_i_ptrs + H * stride_gn, mask=mask, other=0.0)
    gg = tl.load(gates_i_ptrs + 2 * H * stride_gn, mask=mask, other=0.0)
    go = tl.load(gates_i_ptrs + 3 * H * stride_gn, mask=mask, other=0.0)

    c_prev = tl.load(
        c_prev_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn,
        mask=mask,
        other=0.0,
    )

    one = 1.0

    # sigmoid
    i = one / (one + tl.exp(-gi))
    f = one / (one + tl.exp(-gf))
    o = one / (one + tl.exp(-go))

    # tanh for g
    x2 = 2.0 * gg
    ex2 = tl.exp(x2)
    g = (ex2 - 1.0) / (ex2 + 1.0)

    # cell update
    c_new = f * c_prev + i * g

    # tanh(c_new)
    c2 = 2.0 * c_new
    ec2 = tl.exp(c2)
    tanh_c = (ec2 - 1.0) / (ec2 + 1.0)

    h_new = o * tanh_c

    tl.store(
        h_new_ptr + offs_m[:, None] * stride_hm + offs_n[None, :] * stride_hn,
        h_new,
        mask=mask,
    )
    tl.store(
        c_new_ptr + offs_m[:, None] * stride_cnm + offs_n[None, :] * stride_cnn,
        c_new,
        mask=mask,
    )


# -------------------------
# Wrapper functions
# -------------------------

def triton_linear(x, w, b, out=None):
    """
    High-performance linear: y = x @ w + b
    x: [M, K], w: [K, N], b: [N]
    """
    assert x.is_cuda and w.is_cuda and b.is_cuda
    M, K = x.shape
    K_w, N = w.shape
    assert K == K_w, f"Incompatible shapes: x={x.shape}, w={w.shape}"

    if out is None or out.shape != (M, N) or out.dtype != x.dtype or out.device != x.device:
        out = torch.empty((M, N), device=x.device, dtype=x.dtype)

    grid = lambda META: (
        triton.cdiv(M, META["BLOCK_M"]),
        triton.cdiv(N, META["BLOCK_N"]),
    )

    linear_kernel[grid](
        x, w, b, out,
        M, N, K,
        x.stride(0), x.stride(1),
        w.stride(0), w.stride(1),
        out.stride(0), out.stride(1),
        BLOCK_M=64, BLOCK_N=64, BLOCK_K=32,
        num_warps=4, num_stages=2,
    )
    return out


def lstm_pointwise(gates, c_prev, h_out=None, c_out=None):
    """
    gates: [M, 4H], c_prev: [M, H]
    Returns (h_new, c_new) each [M, H]
    """
    assert gates.is_cuda and c_prev.is_cuda
    M, fourH = gates.shape
    H = fourH // 4
    assert c_prev.shape == (M, H)

    if h_out is None or h_out.shape != (M, H) or h_out.dtype != gates.dtype or h_out.device != gates.device:
        h_out = torch.empty((M, H), device=gates.device, dtype=gates.dtype)
    if c_out is None or c_out.shape != (M, H) or c_out.dtype != gates.dtype or c_out.device != gates.device:
        c_out = torch.empty((M, H), device=gates.device, dtype=gates.dtype)

    grid = lambda META: (
        triton.cdiv(M, META["BLOCK_M"]),
        triton.cdiv(H, META["BLOCK_N"]),
    )

    lstm_pointwise_kernel[grid](
        gates, c_prev, h_out, c_out,
        M, H,
        gates.stride(0), gates.stride(1),
        c_prev.stride(0), c_prev.stride(1),
        h_out.stride(0), h_out.stride(1),
        c_out.stride(0), c_out.stride(1),
        BLOCK_M=32, BLOCK_N=64,
        num_warps=4, num_stages=2,
    )
    return h_out, c_out


# -------------------------
# High-level ModelNew
# -------------------------

class ModelNew(nn.Module):
    """
    Triton-optimized reimplementation of the original LSTM model.

    Uses nn.LSTM and nn.Linear only as parameter containers so that
    state_dict() is compatible with the original Model. The actual
    computations are done via custom Triton kernels.
    """

    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):
        super(ModelNew, self).__init__()
        # Parameter containers with identical structure to original Model
        self.lstm = nn.LSTM(
            input_size,
            hidden_size,
            num_layers,
            batch_first=True,
            dropout=dropout,
            bidirectional=False,
        )
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x, h0, c0):
        """
        x:  [batch, seq_len, input_size]
        h0: [num_layers, batch, hidden_size]
        c0: [num_layers, batch, hidden_size]

        Returns final cell state c_n (same as original: state[1]).
        """
        assert x.is_cuda, "Inputs must be on CUDA for Triton kernels."
        batch_size, seq_len, _ = x.shape
        num_layers = self.lstm.num_layers
        hidden_size = self.lstm.hidden_size
        dropout_p = self.lstm.dropout

        # current inputs to each layer (time-major kept in PyTorch layout)
        layer_input = x

        h_n_list = []
        c_n_list = []

        for layer in range(num_layers):
            # Extract PyTorch LSTM parameters for this layer
            weight_ih = getattr(self.lstm, f"weight_ih_l{layer}")  # [4H, input_size or H]
            weight_hh = getattr(self.lstm, f"weight_hh_l{layer}")  # [4H, H]
            bias_ih = getattr(self.lstm, f"bias_ih_l{layer}")      # [4H]
            bias_hh = getattr(self.lstm, f"bias_hh_l{layer}")      # [4H]

            # Combine weights and biases as in standard LSTM: gates = x*W_ih^T + h*W_hh^T + b_ih + b_hh
            D_in = layer_input.size(2)
            H = hidden_size

            # W_cat: [4H, D_in + H] -> W_cat_t: [D_in + H, 4H]
            W_cat = torch.cat([weight_ih[:, :D_in], weight_hh], dim=1)
            W_cat_t = W_cat.t().contiguous()
            bias = bias_ih + bias_hh

            # Initial hidden and cell states for this layer: [batch, H]
            h_t = h0[layer]  # view into h0; used as c_prev/h_prev
            c_t = c0[layer]

            # Buffers reused across timesteps to avoid reallocations
            layer_output = torch.empty(
                (batch_size, seq_len, H), device=x.device, dtype=x.dtype
            )
            gates_buf = torch.empty(
                (batch_size, 4 * H), device=x.device, dtype=x.dtype
            )
            h_buf = torch.empty(
                (batch_size, H), device=x.device, dtype=x.dtype
            )
            c_buf = torch.empty(
                (batch_size, H), device=x.device, dtype=x.dtype
            )

            for t in range(seq_len):
                x_t = layer_input[:, t, :]  # [batch, D_in]
                # Concatenate input and previous hidden: [batch, D_in + H]
                cat_input = torch.cat([x_t, h_t], dim=1)

                # Linear for 4 gates: [batch, 4H]
                gates_buf = triton_linear(cat_input, W_cat_t, bias, out=gates_buf)

                # Pointwise LSTM nonlinearity
                h_buf, c_buf = lstm_pointwise(gates_buf, c_t, h_out=h_buf, c_out=c_buf)

                # Save output and update states
                layer_output[:, t, :] = h_buf
                h_t = h_buf
                c_t = c_buf

            # Save final states for this layer
            h_n_list.append(h_t)
            c_n_list.append(c_t)

            # Dropout on outputs of this layer except the last
            if (layer < num_layers - 1) and (dropout_p > 0.0) and self.training:
                layer_input = torch.nn.functional.dropout(
                    layer_output, p=dropout_p, training=True
                )
            else:
                layer_input = layer_output

        # Final output sequence from top layer
        out = layer_input  # [batch, seq_len, hidden_size]

        # Decode last time step using Triton linear
        last_timestep = out[:, -1, :]  # [batch, hidden_size]
        fc_weight_t = self.fc.weight.t().contiguous()  # [hidden_size, output_size]
        _ = triton_linear(last_timestep, fc_weight_t, self.fc.bias)

        # Stack final states to match nn.LSTM API: [num_layers, batch, hidden_size]
        c_n = torch.stack(c_n_list, dim=0)
        return c_n
```