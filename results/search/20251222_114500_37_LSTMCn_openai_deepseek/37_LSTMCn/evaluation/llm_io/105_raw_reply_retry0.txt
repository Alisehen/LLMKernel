```python
# <corrected code>

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import triton
import triton.language as tl


# --------- Helper activation functions (implemented with tl primitives) ---------


@triton.jit
def _sigmoid(x):
    # Numerically stable sigmoid implemented with Triton primitives
    zero = 0.0
    one = 1.0
    z = tl.where(x >= zero, tl.exp(-x), tl.exp(x))
    return tl.where(x >= zero, one / (one + z), z / (one + z))


@triton.jit
def _tanh(x):
    # tanh(x) = 2*sigmoid(2x) - 1
    return 2.0 * _sigmoid(2.0 * x) - 1.0


# --------- Triton kernels ---------


@triton.jit
def linear_kernel(
    a_ptr, b_ptr, bias_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    # Program IDs for tiles along M (rows) and N (cols)
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    # Offsets for rows and columns in the output tile
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    # Pointers to tiles of A and B at k = 0
    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn

    # Accumulator in FP32 for numeric stability
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Loop over K dimension
    k = 0
    while k < K:
        k_offsets = k + offs_k

        # Load tiles of A and B with proper masks
        a = tl.load(
            a_ptrs,
            mask=(offs_m[:, None] < M) & (k_offsets[None, :] < K),
            other=0.0,
        )
        b = tl.load(
            b_ptrs,
            mask=(k_offsets[:, None] < K) & (offs_n[None, :] < N),
            other=0.0,
        )

        # Accumulate in FP32
        acc += tl.dot(a.to(tl.float32), b.to(tl.float32))

        # Advance pointers along K
        a_ptrs += BLOCK_K * stride_ak
        b_ptrs += BLOCK_K * stride_bk
        k += BLOCK_K

    # Add bias (broadcast over rows)
    bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0).to(tl.float32)
    acc += bias[None, :]

    # Store result
    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    tl.store(
        c_ptrs,
        acc,
        mask=(offs_m[:, None] < M) & (offs_n[None, :] < N),
    )


@triton.jit
def lstm_pointwise_kernel(
    gates_x_ptr, gates_h_ptr,
    c_ptr, h_ptr,
    B, H,
    stride_gxb, stride_gxg,
    stride_ghb, stride_ghg,
    stride_cb, stride_ch,
    stride_hb, stride_hh,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,
):
    # Tile identifiers
    pid_m = tl.program_id(0)  # batch dimension
    pid_n = tl.program_id(1)  # hidden dimension

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)  # batch indices
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)  # hidden indices

    idx_b = offs_m[:, None]  # (BLOCK_M, 1)
    idx_h = offs_n[None, :]  # (1, BLOCK_N)

    mask = (idx_b < B) & (idx_h < H)

    # Load gate pre-activations from x and h projections
    # Layout gates: [i | f | g | o] along last dimension (size 4H)
    # gates_x, gates_h shape: (B, 4H)
    i_x = tl.load(
        gates_x_ptr + idx_b * stride_gxb + idx_h * stride_gxg,
        mask=mask,
        other=0.0,
    )
    f_x = tl.load(
        gates_x_ptr + idx_b * stride_gxb + (idx_h + H) * stride_gxg,
        mask=mask,
        other=0.0,
    )
    g_x = tl.load(
        gates_x_ptr + idx_b * stride_gxb + (idx_h + 2 * H) * stride_gxg,
        mask=mask,
        other=0.0,
    )
    o_x = tl.load(
        gates_x_ptr + idx_b * stride_gxb + (idx_h + 3 * H) * stride_gxg,
        mask=mask,
        other=0.0,
    )

    i_h = tl.load(
        gates_h_ptr + idx_b * stride_ghb + idx_h * stride_ghg,
        mask=mask,
        other=0.0,
    )
    f_h = tl.load(
        gates_h_ptr + idx_b * stride_ghb + (idx_h + H) * stride_ghg,
        mask=mask,
        other=0.0,
    )
    g_h = tl.load(
        gates_h_ptr + idx_b * stride_ghb + (idx_h + 2 * H) * stride_ghg,
        mask=mask,
        other=0.0,
    )
    o_h = tl.load(
        gates_h_ptr + idx_b * stride_ghb + (idx_h + 3 * H) * stride_ghg,
        mask=mask,
        other=0.0,
    )

    # Combine contributions in FP32
    i = (i_x + i_h).to(tl.float32)
    f = (f_x + f_h).to(tl.float32)
    g = (g_x + g_h).to(tl.float32)
    o = (o_x + o_h).to(tl.float32)

    # Non-linearities
    i = _sigmoid(i)
    f = _sigmoid(f)
    o = _sigmoid(o)
    g = _tanh(g)

    # Load previous cell state
    c_prev = tl.load(
        c_ptr + idx_b * stride_cb + idx_h * stride_ch,
        mask=mask,
        other=0.0,
    ).to(tl.float32)

    # c_t = f * c_{t-1} + i * g
    c_new = f * c_prev + i * g

    # h_t = o * tanh(c_t)
    h_new = o * _tanh(c_new)

    # Store back (in-place update of c and h)
    tl.store(
        c_ptr + idx_b * stride_cb + idx_h * stride_ch,
        c_new,
        mask=mask,
    )
    tl.store(
        h_ptr + idx_b * stride_hb + idx_h * stride_hh,
        h_new,
        mask=mask,
    )


# --------- Wrapper functions ---------


def triton_linear(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    out: torch.Tensor | None = None,
) -> torch.Tensor:
    """
    Linear layer with interface matching torch.nn.functional.linear:
    y = x @ weight.T + bias

    Uses the Triton kernel `linear_kernel` for high-performance GEMM.
    """
    assert x.is_cuda and weight.is_cuda and bias.is_cuda
    assert x.dim() == 2 and weight.dim() == 2 and bias.dim() == 1

    M, K = x.shape
    N, K_w = weight.shape
    assert K_w == K
    assert bias.shape[0] == N

    if out is None:
        out = torch.empty((M, N), device=x.device, dtype=x.dtype)
    else:
        assert out.shape == (M, N)
        assert out.device == x.device and out.dtype == x.dtype

    # Strides
    stride_am, stride_ak = x.stride(0), x.stride(1)
    # Treat weight as B of shape (K, N) via strides (k,n) -> weight[n, k]
    stride_bk, stride_bn = weight.stride(1), weight.stride(0)
    stride_cm, stride_cn = out.stride(0), out.stride(1)

    # Tile sizes (power-of-2 as required)
    BLOCK_M = 64
    BLOCK_N = 64
    BLOCK_K = 32

    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))

    linear_kernel[grid](
        x, weight, bias, out,
        M, N, K,
        stride_am, stride_ak,
        stride_bk, stride_bn,
        stride_cm, stride_cn,
        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,
        num_warps=4,
        num_stages=2,
    )

    return out


def triton_lstm_pointwise(
    gates_x: torch.Tensor,
    gates_h: torch.Tensor,
    c: torch.Tensor,
    h: torch.Tensor,
):
    """
    Pointwise LSTM cell update using Triton kernel.

    gates_x, gates_h: (B, 4H)
    c, h: (B, H) updated in-place to c_t, h_t
    """
    assert gates_x.is_cuda and gates_h.is_cuda and c.is_cuda and h.is_cuda

    assert gates_x.shape == gates_h.shape
    B, fourH = gates_x.shape
    assert fourH % 4 == 0
    H = fourH // 4
    assert c.shape == (B, H)
    assert h.shape == (B, H)

    stride_gxb, stride_gxg = gates_x.stride(0), gates_x.stride(1)
    stride_ghb, stride_ghg = gates_h.stride(0), gates_h.stride(1)
    stride_cb, stride_ch = c.stride(0), c.stride(1)
    stride_hb, stride_hh = h.stride(0), h.stride(1)

    BLOCK_M = 32
    BLOCK_N = 64

    grid = (triton.cdiv(B, BLOCK_M), triton.cdiv(H, BLOCK_N))

    lstm_pointwise_kernel[grid](
        gates_x, gates_h,
        c, h,
        B, H,
        stride_gxb, stride_gxg,
        stride_ghb, stride_ghg,
        stride_cb, stride_ch,
        stride_hb, stride_hh,
        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N,
        num_warps=4,
        num_stages=2,
    )


# --------- Model implementation ---------


class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):
        """
        Custom LSTM + Linear using high-performance Triton kernels.

        - Multi-layer unidirectional LSTM (PyTorch-compatible semantics)
        - Final Linear on the last time step of the last layer (computed but not returned)
        - forward() returns final cell states (c_n), shape (num_layers, batch, hidden_size)
        """
        super(ModelNew, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = float(dropout)

        # LSTM parameters
        self.weight_ih = nn.ParameterList()
        self.weight_hh = nn.ParameterList()
        self.bias_ih = nn.ParameterList()
        self.bias_hh = nn.ParameterList()

        for layer in range(num_layers):
            layer_input_size = input_size if layer == 0 else hidden_size

            w_ih = nn.Parameter(torch.empty(4 * hidden_size, layer_input_size))
            w_hh = nn.Parameter(torch.empty(4 * hidden_size, hidden_size))
            b_ih = nn.Parameter(torch.empty(4 * hidden_size))
            b_hh = nn.Parameter(torch.empty(4 * hidden_size))

            self.weight_ih.append(w_ih)
            self.weight_hh.append(w_hh)
            self.bias_ih.append(b_ih)
            self.bias_hh.append(b_hh)

        # Final fully-connected layer
        self.fc_weight = nn.Parameter(torch.empty(output_size, hidden_size))
        self.fc_bias = nn.Parameter(torch.empty(output_size))

        self.reset_parameters()

    def reset_parameters(self):
        # Initialize LSTM weights/biases; keep this compatible with the reference model.
        for layer in range(self.num_layers):
            w_ih = self.weight_ih[layer]
            w_hh = self.weight_hh[layer]
            b_ih = self.bias_ih[layer]
            b_hh = self.bias_hh[layer]

            # Kaiming uniform for weights
            nn.init.kaiming_uniform_(w_ih, a=math.sqrt(5))
            nn.init.kaiming_uniform_(w_hh, a=math.sqrt(5))

            # Biases to zero
            nn.init.zeros_(b_ih)
            nn.init.zeros_(b_hh)

        # Linear layer init like nn.Linear
        fan_in = self.fc_weight.size(1)
        bound = 1.0 / math.sqrt(fan_in)
        nn.init.uniform_(self.fc_weight, -bound, bound)
        nn.init.uniform_(self.fc_bias, -bound, bound)

    def forward(self, x, h0, c0):
        """
        x:  (batch, seq_len, input_size)
        h0: (num_layers, batch, hidden_size)
        c0: (num_layers, batch, hidden_size)

        Returns:
            c_n: final cell states, shape (num_layers, batch, hidden_size)
        """
        assert x.is_cuda, "Triton kernels require CUDA tensors"

        device = x.device
        batch_size, seq_len, _ = x.shape
        H = self.hidden_size

        # Current states per layer: each (B, H)
        h = [h0[layer].to(device=device).contiguous() for layer in range(self.num_layers)]
        c = [c0[layer].to(device=device).contiguous() for layer in range(self.num_layers)]

        # Temporary buffers for gate pre-activations per layer: (B, 4H)
        gates_x = [
            torch.empty((batch_size, 4 * H), device=device, dtype=x.dtype)
            for _ in range(self.num_layers)
        ]
        gates_h = [
            torch.empty((batch_size, 4 * H), device=device, dtype=x.dtype)
            for _ in range(self.num_layers)
        ]

        # Recurrent computation over time (PyTorch-compatible LSTM equations)
        for t in range(seq_len):
            # Input to first layer at this time step
            layer_input = x[:, t, :]  # (B, input_size)

            for layer in range(self.num_layers):
                # Linear projections for gates: (B, 4H)
                wx = self.weight_ih[layer]
                wh = self.weight_hh[layer]
                bix = self.bias_ih[layer]
                bhh = self.bias_hh[layer]

                # gates_x = layer_input @ W_ih^T + b_ih
                # gates_h = h[layer]   @ W_hh^T + b_hh
                triton_linear(layer_input, wx, bix, out=gates_x[layer])
                triton_linear(h[layer],      wh, bhh, out=gates_h[layer])

                # Pointwise LSTM cell update (in-place on h[layer], c[layer])
                triton_lstm_pointwise(gates_x[layer], gates_h[layer], c[layer], h[layer])

                # Output of this layer (h_t^l) becomes input to next layer
                layer_input = h[layer]

                # Apply dropout on outputs of all layers except the last (PyTorch nn.LSTM semantics)
                if self.dropout > 0.0 and layer < self.num_layers - 1:
                    layer_input = F.dropout(layer_input, p=self.dropout, training=self.training)

        # Final hidden state of last layer (B, H)
        last_h = h[-1]

        # Final linear layer on last hidden state (computed for parity / benchmarking)
        _ = triton_linear(last_h, self.fc_weight, self.fc_bias)

        # Stack final states to (num_layers, batch, hidden_size)
        h_n = torch.stack(h, dim=0)
        c_n = torch.stack(c, dim=0)

        # Match original Model: return cell states c_n
        return c_n
```