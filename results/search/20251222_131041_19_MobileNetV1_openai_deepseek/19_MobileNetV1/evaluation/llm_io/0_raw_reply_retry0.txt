```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


# ------------------------------------------------------------
# 2D Convolution Kernels (NCHW)
# ------------------------------------------------------------

@triton.jit
def conv2d_nchw_kernel(
    x_ptr, w_ptr, y_ptr,
    N, C_in, H_in, W_in,
    C_out, H_out, W_out,
    stride_h, stride_w, pad_h, pad_w,
    x_stride_n, x_stride_c, x_stride_h, x_stride_w,
    w_stride_co, w_stride_ci, w_stride_kh, w_stride_kw,
    y_stride_n, y_stride_c, y_stride_h, y_stride_w,
    BLOCK_CO: tl.constexpr,
    KERNEL_H: tl.constexpr,
    KERNEL_W: tl.constexpr,
):
    pid_pos = tl.program_id(0)  # flattened (n, oh, ow)
    pid_co_block = tl.program_id(1)

    HW_out = H_out * W_out

    # Decode (n, oh, ow) from pid_pos (all scalars)
    n_idx = pid_pos // HW_out
    tmp = pid_pos - n_idx * HW_out
    oh = tmp // W_out
    ow = tmp - oh * W_out

    # If pid_pos >= N*H_out*W_out, this program is idle (shouldn't happen if grid is sized correctly)

    # Compute top-left input coordinates (with padding)
    in_y0 = oh * stride_h - pad_h
    in_x0 = ow * stride_w - pad_w

    offs_co = pid_co_block * BLOCK_CO + tl.arange(0, BLOCK_CO)
    co_mask = offs_co < C_out

    acc = tl.zeros((BLOCK_CO,), dtype=tl.float32)

    # Loop over input channels and kernel window
    for ci in range(0, C_in):
        for ky in range(0, KERNEL_H):
            in_y = in_y0 + ky
            in_y_in = (in_y >= 0) & (in_y < H_in)
            for kx in range(0, KERNEL_W):
                in_x = in_x0 + kx
                in_x_in = (in_x >= 0) & (in_x < W_in)
                in_bounds = in_y_in & in_x_in

                x_offset = (
                    n_idx * x_stride_n
                    + ci * x_stride_c
                    + in_y * x_stride_h
                    + in_x * x_stride_w
                )

                x_val = tl.load(
                    x_ptr + x_offset,
                    mask=in_bounds,
                    other=0.0,
                )

                w_ptrs = (
                    w_ptr
                    + offs_co * w_stride_co
                    + ci * w_stride_ci
                    + ky * w_stride_kh
                    + kx * w_stride_kw
                )
                w_vals = tl.load(
                    w_ptrs,
                    mask=co_mask,
                    other=0.0,
                )

                acc += x_val * w_vals

    y_offset_base = (
        n_idx * y_stride_n
        + oh * y_stride_h
        + ow * y_stride_w
    )
    y_ptrs = y_ptr + y_offset_base + offs_co * y_stride_c
    tl.store(y_ptrs, acc, mask=co_mask)


@triton.jit
def depthwise_conv2d_3x3_nchw_kernel(
    x_ptr, w_ptr, y_ptr,
    N, C, H_in, W_in,
    H_out, W_out,
    stride_h, stride_w, pad_h, pad_w,
    x_stride_n, x_stride_c, x_stride_h, x_stride_w,
    w_stride_c, w_stride_kh, w_stride_kw,
    y_stride_n, y_stride_c, y_stride_h, y_stride_w,
    BLOCK_C: tl.constexpr,
    KERNEL_H: tl.constexpr,
    KERNEL_W: tl.constexpr,
):
    pid_pos = tl.program_id(0)  # flattened (n, oh, ow)
    pid_c_block = tl.program_id(1)

    HW_out = H_out * W_out

    # Decode (n, oh, ow)
    n_idx = pid_pos // HW_out
    tmp = pid_pos - n_idx * HW_out
    oh = tmp // W_out
    ow = tmp - oh * W_out

    in_y0 = oh * stride_h - pad_h
    in_x0 = ow * stride_w - pad_w

    offs_c = pid_c_block * BLOCK_C + tl.arange(0, BLOCK_C)
    c_mask = offs_c < C

    acc = tl.zeros((BLOCK_C,), dtype=tl.float32)

    for ky in range(0, KERNEL_H):
        in_y = in_y0 + ky
        in_y_in = (in_y >= 0) & (in_y < H_in)
        for kx in range(0, KERNEL_W):
            in_x = in_x0 + kx
            in_x_in = (in_x >= 0) & (in_x < W_in)
            in_bounds_scalar = in_y_in & in_x_in

            x_ptrs = (
                x_ptr
                + n_idx * x_stride_n
                + offs_c * x_stride_c
                + in_y * x_stride_h
                + in_x * x_stride_w
            )

            x_vals = tl.load(
                x_ptrs,
                mask=c_mask & in_bounds_scalar,
                other=0.0,
            )

            w_ptrs = (
                w_ptr
                + offs_c * w_stride_c
                + ky * w_stride_kh
                + kx * w_stride_kw
            )

            w_vals = tl.load(
                w_ptrs,
                mask=c_mask,
                other=0.0,
            )

            acc += x_vals * w_vals

    y_ptrs = (
        y_ptr
        + n_idx * y_stride_n
        + offs_c * y_stride_c
        + oh * y_stride_h
        + ow * y_stride_w
    )
    tl.store(y_ptrs, acc, mask=c_mask)


# ------------------------------------------------------------
# Global Average Pool (over H,W) Kernel
# ------------------------------------------------------------

@triton.jit
def global_avg_pool2d_nchw_kernel(
    x_ptr, y_ptr,
    N, C, H, W,
    x_stride_n, x_stride_c, x_stride_h, x_stride_w,
    y_stride_n, y_stride_c, y_stride_h, y_stride_w,
    BLOCK_C: tl.constexpr,
):
    pid_n = tl.program_id(0)
    pid_c_block = tl.program_id(1)

    offs_c = pid_c_block * BLOCK_C + tl.arange(0, BLOCK_C)
    c_mask = offs_c < C

    acc = tl.zeros((BLOCK_C,), dtype=tl.float32)

    for h in range(0, H):
        for w in range(0, W):
            x_ptrs = (
                x_ptr
                + pid_n * x_stride_n
                + offs_c * x_stride_c
                + h * x_stride_h
                + w * x_stride_w
            )
            vals = tl.load(x_ptrs, mask=c_mask, other=0.0)
            acc += vals

    denom = H * W
    acc = acc / denom

    y_ptrs = (
        y_ptr
        + pid_n * y_stride_n
        + offs_c * y_stride_c
    )
    tl.store(y_ptrs, acc, mask=c_mask)


# ------------------------------------------------------------
# Linear (GEMM) Kernel
# ------------------------------------------------------------

@triton.jit
def linear_gemm_kernel(
    a_ptr, b_ptr, bias_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    a_ptrs = (
        a_ptr
        + offs_m[:, None] * stride_am
        + offs_k[None, :] * stride_ak
    )
    b_ptrs = (
        b_ptr
        + offs_k[:, None] * stride_bk
        + offs_n[None, :] * stride_bn
    )

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    k_remaining = K
    while k_remaining > 0:
        k_mask = offs_k[None, :] < k_remaining

        a = tl.load(
            a_ptrs,
            mask=(offs_m[:, None] < M) & k_mask,
            other=0.0,
        )
        b = tl.load(
            b_ptrs,
            mask=k_mask.T & (offs_n[None, :] < N),
            other=0.0,
        )
        acc += tl.dot(a, b, allow_tf32=True)

        a_ptrs += BLOCK_K * stride_ak
        b_ptrs += BLOCK_K * stride_bk
        k_remaining -= BLOCK_K

    # Add bias
    bias = tl.load(
        bias_ptr + offs_n,
        mask=offs_n < N,
        other=0.0,
    )
    acc += bias[None, :]

    c_ptrs = (
        c_ptr
        + offs_m[:, None] * stride_cm
        + offs_n[None, :] * stride_cn
    )
    tl.store(
        c_ptrs,
        acc,
        mask=(offs_m[:, None] < M) & (offs_n[None, :] < N),
    )


# ------------------------------------------------------------
# Python Wrappers Around Kernels
# ------------------------------------------------------------

def triton_conv2d_3x3(x: torch.Tensor, weight: torch.Tensor, stride: int = 1) -> torch.Tensor:
    """
    Standard 3x3 convolution (groups=1), NCHW layout, padding=1.
    """
    assert x.is_cuda and weight.is_cuda
    assert x.dim() == 4
    assert weight.dim() == 4 and weight.shape[2] == 3 and weight.shape[3] == 3

    N, C_in, H_in, W_in = x.shape
    C_out, C_in_w, KH, KW = weight.shape
    assert C_in_w == C_in

    pad = 1
    H_out = (H_in + 2 * pad - KH) // stride + 1
    W_out = (W_in + 2 * pad - KW) // stride + 1

    y = torch.empty((N, C_out, H_out, W_out), device=x.device, dtype=torch.float32)

    grid = lambda META: (
        N * H_out * W_out,
        triton.cdiv(C_out, META["BLOCK_CO"]),
    )

    conv2d_nchw_kernel[grid](
        x, weight, y,
        N, C_in, H_in, W_in,
        C_out, H_out, W_out,
        stride, stride, pad, pad,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        weight.stride(0), weight.stride(1), weight.stride(2), weight.stride(3),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3),
        BLOCK_CO=64,
        KERNEL_H=3,
        KERNEL_W=3,
    )
    return y


def triton_conv2d_1x1(x: torch.Tensor, weight: torch.Tensor) -> torch.Tensor:
    """
    1x1 convolution (groups=1), NCHW layout, stride=1, padding=0.
    """
    assert x.is_cuda and weight.is_cuda
    assert x.dim() == 4
    assert weight.dim() == 4 and weight.shape[2] == 1 and weight.shape[3] == 1

    N, C_in, H_in, W_in = x.shape
    C_out, C_in_w, KH, KW = weight.shape
    assert C_in_w == C_in

    stride = 1
    pad = 0
    H_out = (H_in + 2 * pad - KH) // stride + 1
    W_out = (W_in + 2 * pad - KW) // stride + 1

    y = torch.empty((N, C_out, H_out, W_out), device=x.device, dtype=torch.float32)

    grid = lambda META: (
        N * H_out * W_out,
        triton.cdiv(C_out, META["BLOCK_CO"]),
    )

    conv2d_nchw_kernel[grid](
        x, weight, y,
        N, C_in, H_in, W_in,
        C_out, H_out, W_out,
        stride, stride, pad, pad,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        weight.stride(0), weight.stride(1), weight.stride(2), weight.stride(3),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3),
        BLOCK_CO=64,
        KERNEL_H=1,
        KERNEL_W=1,
    )
    return y


def triton_depthwise_conv2d_3x3(x: torch.Tensor, weight: torch.Tensor, stride: int = 1) -> torch.Tensor:
    """
    Depthwise 3x3 convolution with groups=C_in, NCHW layout, padding=1.
    weight shape: (C_in, 1, 3, 3) as in PyTorch depthwise conv (groups=C_in).
    """
    assert x.is_cuda and weight.is_cuda
    assert x.dim() == 4
    assert weight.dim() == 4 and weight.shape[2] == 3 and weight.shape[3] == 3

    N, C_in, H_in, W_in = x.shape
    C_w, Ci_group, KH, KW = weight.shape
    assert C_w == C_in and Ci_group == 1

    pad = 1
    H_out = (H_in + 2 * pad - KH) // stride + 1
    W_out = (W_in + 2 * pad - KW) // stride + 1

    y = torch.empty((N, C_in, H_out, W_out), device=x.device, dtype=torch.float32)

    grid = lambda META: (
        N * H_out * W_out,
        triton.cdiv(C_in, META["BLOCK_C"]),
    )

    depthwise_conv2d_3x3_nchw_kernel[grid](
        x, weight, y,
        N, C_in, H_in, W_in,
        H_out, W_out,
        stride, stride, pad, pad,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        weight.stride(0), weight.stride(2), weight.stride(3),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3),
        BLOCK_C=64,
        KERNEL_H=3,
        KERNEL_W=3,
    )
    return y


def triton_global_avg_pool2d(x: torch.Tensor) -> torch.Tensor:
    """
    Global average pool over spatial (H, W) dimensions, preserving N,C and producing N x C x 1 x 1.
    """
    assert x.is_cuda
    assert x.dim() == 4
    N, C, H, W = x.shape

    y = torch.empty((N, C, 1, 1), device=x.device, dtype=torch.float32)

    grid = lambda META: (
        N,
        triton.cdiv(C, META["BLOCK_C"]),
    )

    global_avg_pool2d_nchw_kernel[grid](
        x, y,
        N, C, H, W,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3),
        BLOCK_C=64,
    )
    return y


def triton_linear(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:
    """
    x: [M, K]
    weight: [N, K] (as in nn.Linear, out_features x in_features)
    bias: [N]
    Output: [M, N] = x @ weight^T + bias
    """
    assert x.is_cuda and weight.is_cuda and bias.is_cuda
    assert x.dim() == 2 and weight.dim() == 2 and bias.dim() == 1

    M, K = x.shape
    N = weight.shape[0]
    assert weight.shape[1] == K
    assert bias.shape[0] == N

    # B is K x N
    b = weight.t().contiguous()

    y = torch.empty((M, N), device=x.device, dtype=torch.float32)

    grid = lambda META: (
        triton.cdiv(M, META["BLOCK_M"]),
        triton.cdiv(N, META["BLOCK_N"]),
    )

    linear_gemm_kernel[grid](
        x, b, bias, y,
        M, N, K,
        x.stride(0), x.stride(1),
        b.stride(0), b.stride(1),
        y.stride(0), y.stride(1),
        BLOCK_M=64,
        BLOCK_N=64,
        BLOCK_K=32,
    )
    return y


# ------------------------------------------------------------
# High-level Modules using Triton kernels
# ------------------------------------------------------------

class ConvBNReLU(nn.Module):
    def __init__(self, in_channels, out_channels, stride):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, 3, 3) * 0.02)
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.stride = stride

    def forward(self, x):
        x = triton_conv2d_3x3(x, self.weight, self.stride)
        x = self.bn(x)
        x = self.relu(x)
        return x


class DepthwiseSeparableConv(nn.Module):
    def __init__(self, inp, oup, stride):
        super().__init__()
        # Depthwise
        self.dw_weight = nn.Parameter(torch.randn(inp, 1, 3, 3) * 0.02)
        self.dw_bn = nn.BatchNorm2d(inp)
        self.dw_relu = nn.ReLU(inplace=True)
        self.stride = stride

        # Pointwise
        self.pw_weight = nn.Parameter(torch.randn(oup, inp, 1, 1) * 0.02)
        self.pw_bn = nn.BatchNorm2d(oup)
        self.pw_relu = nn.ReLU(inplace=True)

    def forward(self, x):
        # Depthwise 3x3
        x = triton_depthwise_conv2d_3x3(x, self.dw_weight, self.stride)
        x = self.dw_bn(x)
        x = self.dw_relu(x)

        # Pointwise 1x1
        x = triton_conv2d_1x1(x, self.pw_weight)
        x = self.pw_bn(x)
        x = self.pw_relu(x)
        return x


# ------------------------------------------------------------
# Optimized MobileNetV1 using Triton kernels
# ------------------------------------------------------------

class ModelNew(nn.Module):
    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):
        super(ModelNew, self).__init__()

        def c(ch):
            return int(ch * alpha)

        self.model = nn.Sequential(
            ConvBNReLU(input_channels, c(32), 2),
            DepthwiseSeparableConv(c(32), c(64), 1),
            DepthwiseSeparableConv(c(64), c(128), 2),
            DepthwiseSeparableConv(c(128), c(128), 1),
            DepthwiseSeparableConv(c(128), c(256), 2),
            DepthwiseSeparableConv(c(256), c(256), 1),
            DepthwiseSeparableConv(c(256), c(512), 2),
            DepthwiseSeparableConv(c(512), c(512), 1),
            DepthwiseSeparableConv(c(512), c(512), 1),
            DepthwiseSeparableConv(c(512), c(512), 1),
            DepthwiseSeparableConv(c(512), c(512), 1),
            DepthwiseSeparableConv(c(512), c(512), 1),
            DepthwiseSeparableConv(c(512), c(1024), 2),
            DepthwiseSeparableConv(c(1024), c(1024), 1),
        )

        self.out_channels = c(1024)
        self.fc_weight = nn.Parameter(torch.randn(num_classes, self.out_channels) * 0.01)
        self.fc_bias = nn.Parameter(torch.zeros(num_classes))

    def forward(self, x):
        # Expect x to be on CUDA for Triton kernels
        x = self.model(x)
        x = triton_global_avg_pool2d(x)  # (N, C, 1, 1)
        x = x.view(x.size(0), -1)
        x = triton_linear(x, self.fc_weight, self.fc_bias)
        return x
```