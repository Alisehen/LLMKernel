```python
import torch
import torch.nn as nn
import triton
import triton.language as tl


# ------------------------------
# Triton Kernels
# ------------------------------


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 32}, num_warps=4, num_stages=3),
        triton.Config({"BLOCK_M": 64, "BLOCK_N": 32, "BLOCK_K": 32}, num_warps=4, num_stages=3),
        triton.Config({"BLOCK_M": 32, "BLOCK_N": 64, "BLOCK_K": 32}, num_warps=2, num_stages=3),
    ],
    key=["M", "N", "K"],
)
@triton.jit
def matmul_bias_kernel(
    a_ptr, b_ptr, bias_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    """
    Compute C = A @ B + bias

    A: [M, K]
    B: [K, N]
    bias: [N]
    C: [M, N]
    """
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    m_mask = offs_m < M
    n_mask = offs_n < N

    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    for k in range(0, K, BLOCK_K):
        k_offsets = k + offs_k
        k_mask = k_offsets < K

        a = tl.load(
            a_ptrs,
            mask=m_mask[:, None] & k_mask[None, :],
            other=0.0,
        )
        b = tl.load(
            b_ptrs,
            mask=k_mask[:, None] & n_mask[None, :],
            other=0.0,
        )

        # Accumulate in fp32, allow TF32 on Ampere+/Ada for speed
        acc += tl.dot(a, b, allow_tf32=True)

        a_ptrs += BLOCK_K * stride_ak
        b_ptrs += BLOCK_K * stride_bk

    # Add bias
    bias = tl.load(bias_ptr + offs_n, mask=n_mask, other=0.0)
    acc += bias[None, :]

    # Store result
    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    tl.store(
        c_ptrs,
        acc,
        mask=m_mask[:, None] & n_mask[None, :],
    )


@triton.autotune(
    configs=[
        triton.Config({"BLOCK_B": 64, "BLOCK_H": 32}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_B": 32, "BLOCK_H": 64}, num_warps=4, num_stages=2),
        triton.Config({"BLOCK_B": 32, "BLOCK_H": 32}, num_warps=2, num_stages=2),
    ],
    key=["B", "H"],
)
@triton.jit
def gru_pointwise_kernel(
    x_lin_ptr, h_lin_ptr, h_prev_ptr, h_next_ptr,
    B, H,
    stride_xb, stride_xh,
    stride_hb, stride_hh,
    stride_hpb, stride_hph,
    stride_hnb, stride_hnh,
    BLOCK_B: tl.constexpr, BLOCK_H: tl.constexpr,
):
    """
    Pointwise GRU update:

    x_lin: [B, 3H] = x @ W_ih^T + b_ih
    h_lin: [B, 3H] = h_prev @ W_hh^T + b_hh
    h_prev: [B, H]
    h_next: [B, H]

    Using:
        r = sigmoid(i_r + h_r)
        z = sigmoid(i_z + h_z)
        n = tanh(i_n + r * h_n)
        h = (1 - z) * n + z * h_prev
    """
    pid_b = tl.program_id(0)
    pid_h = tl.program_id(1)

    offs_b = pid_b * BLOCK_B + tl.arange(0, BLOCK_B)
    offs_h = pid_h * BLOCK_H + tl.arange(0, BLOCK_H)

    mask_b = offs_b[:, None] < B
    mask_h = offs_h[None, :] < H
    mask = mask_b & mask_h

    # Base pointers for gates in x_lin and h_lin
    x_base = x_lin_ptr + offs_b[:, None] * stride_xb + offs_h[None, :] * stride_xh
    h_base = h_lin_ptr + offs_b[:, None] * stride_hb + offs_h[None, :] * stride_hh

    # i_r, h_r
    x_r = tl.load(x_base, mask=mask, other=0.0)
    h_r = tl.load(h_base, mask=mask, other=0.0)

    # i_z, h_z
    x_z = tl.load(x_base + H * stride_xh, mask=mask, other=0.0)
    h_z = tl.load(h_base + H * stride_hh, mask=mask, other=0.0)

    # i_n, h_n
    x_n = tl.load(x_base + 2 * H * stride_xh, mask=mask, other=0.0)
    h_n = tl.load(h_base + 2 * H * stride_hh, mask=mask, other=0.0)

    # h_prev
    h_prev = tl.load(
        h_prev_ptr + offs_b[:, None] * stride_hpb + offs_h[None, :] * stride_hph,
        mask=mask,
        other=0.0,
    )

    # Compute in fp32 for stability
    x_r = tl.cast(x_r, tl.float32)
    h_r = tl.cast(h_r, tl.float32)
    x_z = tl.cast(x_z, tl.float32)
    h_z = tl.cast(h_z, tl.float32)
    x_n = tl.cast(x_n, tl.float32)
    h_n = tl.cast(h_n, tl.float32)
    h_prev = tl.cast(h_prev, tl.float32)

    # r gate: sigmoid(i_r + h_r)
    r_in = x_r + h_r
    r = 1.0 / (1.0 + tl.exp(-r_in))

    # z gate: sigmoid(i_z + h_z)
    z_in = x_z + h_z
    z = 1.0 / (1.0 + tl.exp(-z_in))

    # n gate: tanh(i_n + r * h_n)
    n_in = x_n + r * h_n
    # tanh(x) = (e^{2x} - 1) / (e^{2x} + 1)
    e2 = tl.exp(2.0 * n_in)
    n = (e2 - 1.0) / (e2 + 1.0)

    # h = (1 - z) * n + z * h_prev
    h_next = (1.0 - z) * n + z * h_prev

    tl.store(
        h_next_ptr + offs_b[:, None] * stride_hnb + offs_h[None, :] * stride_hnh,
        h_next,
        mask=mask,
    )


# ------------------------------
# Python wrappers around kernels
# ------------------------------


def matmul_bias(a: torch.Tensor, b: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:
    """
    a: [M, K]
    b: [K, N]
    bias: [N]
    return: [M, N] = a @ b + bias
    """
    assert a.is_cuda and b.is_cuda and bias.is_cuda
    M, K = a.shape
    K2, N = b.shape
    assert K2 == K
    assert bias.numel() == N

    c = torch.empty((M, N), device=a.device, dtype=a.dtype)

    def grid(meta):
        return (
            triton.cdiv(M, meta["BLOCK_M"]),
            triton.cdiv(N, meta["BLOCK_N"]),
        )

    matmul_bias_kernel[grid](
        a, b, bias, c,
        M, N, K,
        a.stride(0), a.stride(1),
        b.stride(0), b.stride(1),
        c.stride(0), c.stride(1),
    )
    return c


def gru_pointwise(x_lin: torch.Tensor, h_lin: torch.Tensor, h_prev: torch.Tensor) -> torch.Tensor:
    """
    x_lin: [B, 3H]
    h_lin: [B, 3H]
    h_prev: [B, H]
    return: h_next [B, H]
    """
    assert x_lin.is_cuda and h_lin.is_cuda and h_prev.is_cuda
    B, threeH = x_lin.shape
    assert h_lin.shape == x_lin.shape
    assert threeH % 3 == 0
    H = threeH // 3
    assert h_prev.shape == (B, H)

    h_next = torch.empty_like(h_prev)

    def grid(meta):
        return (
            triton.cdiv(B, meta["BLOCK_B"]),
            triton.cdiv(H, meta["BLOCK_H"]),
        )

    gru_pointwise_kernel[grid](
        x_lin, h_lin, h_prev, h_next,
        B, H,
        x_lin.stride(0), x_lin.stride(1),
        h_lin.stride(0), h_lin.stride(1),
        h_prev.stride(0), h_prev.stride(1),
        h_next.stride(0), h_next.stride(1),
    )
    return h_next


# ------------------------------
# High-performance GRU Module
# ------------------------------


class ModelNew(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=3, bias=True, batch_first=False):
        super(ModelNew, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bias_flag = bias
        self.batch_first = batch_first
        self.num_directions = 2  # bidirectional=True

        self.weight_ih = nn.ParameterList()
        self.weight_hh = nn.ParameterList()
        self.bias_ih = nn.ParameterList()
        self.bias_hh = nn.ParameterList()

        for layer in range(num_layers):
            layer_input_size = input_size if layer == 0 else hidden_size * self.num_directions
            for direction in range(self.num_directions):
                w_ih = nn.Parameter(torch.empty(3 * hidden_size, layer_input_size))
                w_hh = nn.Parameter(torch.empty(3 * hidden_size, hidden_size))
                if bias:
                    b_ih = nn.Parameter(torch.zeros(3 * hidden_size))
                    b_hh = nn.Parameter(torch.zeros(3 * hidden_size))
                else:
                    b_ih = nn.Parameter(torch.zeros(3 * hidden_size), requires_grad=False)
                    b_hh = nn.Parameter(torch.zeros(3 * hidden_size), requires_grad=False)

                # Simple initialization roughly similar to PyTorch GRU (uniform)
                stdv = 1.0 / (hidden_size ** 0.5)
                w_ih.data.uniform_(-stdv, stdv)
                w_hh.data.uniform_(-stdv, stdv)
                if bias:
                    b_ih.data.uniform_(-stdv, stdv)
                    b_hh.data.uniform_(-stdv, stdv)

                self.weight_ih.append(w_ih)
                self.weight_hh.append(w_hh)
                self.bias_ih.append(b_ih)
                self.bias_hh.append(b_hh)

    def _layer_direction_index(self, layer: int, direction: int) -> int:
        return layer * self.num_directions + direction

    def forward(self, x: torch.Tensor, h0: torch.Tensor):
        """
        x: (seq_len, batch, input_size) if batch_first=False
           (batch, seq_len, input_size) if batch_first=True
        h0: (num_layers * num_directions, batch, hidden_size)
        """
        assert x.is_cuda and h0.is_cuda, "Inputs must be CUDA tensors"

        if self.batch_first:
            # Convert to (T, B, F)
            x = x.transpose(0, 1)

        seq_len, batch_size, _ = x.shape
        device = x.device
        dtype = x.dtype

        # Pretranspose weights for efficient matmul: [in, 3H] / [H, 3H]
        weight_ih_t = []
        weight_hh_t = []
        for l in range(self.num_layers * self.num_directions):
            weight_ih_t.append(self.weight_ih[l].transpose(0, 1).contiguous())
            weight_hh_t.append(self.weight_hh[l].transpose(0, 1).contiguous())

        # Output container for last layer
        output = torch.empty(
            (seq_len, batch_size, self.hidden_size * self.num_directions),
            device=device,
            dtype=dtype,
        )

        # Final hidden state for all layers and directions
        h_n = torch.empty(
            (self.num_layers * self.num_directions, batch_size, self.hidden_size),
            device=device,
            dtype=dtype,
        )

        # Current input to this layer (T, B, F)
        layer_input = x

        for layer in range(self.num_layers):
            layer_output_f = torch.empty(
                (seq_len, batch_size, self.hidden_size),
                device=device,
                dtype=dtype,
            )
            layer_output_b = torch.empty(
                (seq_len, batch_size, self.hidden_size),
                device=device,
                dtype=dtype,
            )

            # Flatten layer input once per layer: [T, B, F] -> [T*B, F]
            layer_input_2d = layer_input.reshape(seq_len * batch_size, -1)

            # Forward direction
            idx_f = self._layer_direction_index(layer, 0)
            W_ih_f_t = weight_ih_t[idx_f]
            W_hh_f_t = weight_hh_t[idx_f]
            b_ih_f = self.bias_ih[idx_f]
            b_hh_f = self.bias_hh[idx_f]
            h_prev_f = h0[idx_f]

            # Precompute x @ W_ih^T + b_ih for all timesteps (forward dir)
            x_lin_f_2d = matmul_bias(layer_input_2d, W_ih_f_t, b_ih_f)  # [T*B, 3H]
            x_lin_f = x_lin_f_2d.view(seq_len, batch_size, 3 * self.hidden_size)

            for t in range(seq_len):
                x_t_lin_f = x_lin_f[t]  # [B, 3H]
                h_lin_f = matmul_bias(h_prev_f, W_hh_f_t, b_hh_f)  # [B, 3H]
                h_next_f = gru_pointwise(x_t_lin_f, h_lin_f, h_prev_f)
                layer_output_f[t] = h_next_f
                h_prev_f = h_next_f

            h_n[idx_f] = h_prev_f

            # Backward direction
            idx_b = self._layer_direction_index(layer, 1)
            W_ih_b_t = weight_ih_t[idx_b]
            W_hh_b_t = weight_hh_t[idx_b]
            b_ih_b = self.bias_ih[idx_b]
            b_hh_b = self.bias_hh[idx_b]
            h_prev_b = h0[idx_b]

            # Precompute x @ W_ih^T + b_ih for all timesteps (backward dir)
            x_lin_b_2d = matmul_bias(layer_input_2d, W_ih_b_t, b_ih_b)  # [T*B, 3H]
            x_lin_b = x_lin_b_2d.view(seq_len, batch_size, 3 * self.hidden_size)

            for t_rev in range(seq_len - 1, -1, -1):
                x_t_lin_b = x_lin_b[t_rev]  # [B, 3H]
                h_lin_b = matmul_bias(h_prev_b, W_hh_b_t, b_hh_b)  # [B, 3H]
                h_next_b = gru_pointwise(x_t_lin_b, h_lin_b, h_prev_b)
                layer_output_b[t_rev] = h_next_b
                h_prev_b = h_next_b

            h_n[idx_b] = h_prev_b

            # Concatenate directions to form input for next layer
            layer_output = torch.cat([layer_output_f, layer_output_b], dim=2)
            layer_input = layer_output

            if layer == self.num_layers - 1:
                output = layer_output

        if self.batch_first:
            output = output.transpose(0, 1)

        return output, h_n
```