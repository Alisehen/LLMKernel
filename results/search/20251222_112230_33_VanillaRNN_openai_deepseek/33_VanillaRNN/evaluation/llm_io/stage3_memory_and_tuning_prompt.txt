You are a Triton kernel optimization specialist. Generate the FASTEST possible kernel.

# Target GPU: 4090

[OPTIMIZATION STAGE]

## Current Optimization Stage

Focus: Memory pattern and parameter tuning for fused operations.

Key Principle:
- Fusion benefit = eliminated INTERMEDIATE stores
- Multiple input loads are OK; intermediate stores are NOT

Memory Rules:
- ✅ Multiple tl.load() for different inputs (x, weight, bias) - OK
- ❌ tl.store() for intermediate results - NEVER (this is what fusion eliminates)
- ✅ Single tl.store() for final output - required

Verification:
- Count tl.store() calls: should equal number of OUTPUT tensors (usually 1)
- Intermediate values: must stay in registers between ops
- If you see store-then-load pattern for same data → BUG, refactor

Parameters to tune:
- num_warps ∈ {4, 8}
- num_stages ∈ {2, 3}

Conditional Tuning Rules:

IF register pressure LOW (regs < 96, no spill):
  - Try num_warps=8 for compute-bound fusion
  - num_stages=3 may help hide latency

IF register pressure HIGH (regs > 128 or occupancy_limit_registers):
  - Use num_warps=4 (fewer warps = more registers per warp)
  - Keep num_stages=2 (higher stages need more registers)

IF multi-input fusion (3+ distinct loads):
  - num_stages=2 preferred (each stage buffers all inputs)
  - num_warps=4 often better than 8

Autotune:
- Max 3-4 configs combining num_stages and num_warps
- Always include conservative baseline (num_warps=4, num_stages=2)
- Revert if gain < 2%



[CURRENT CODE]
```python
# <optimized Triton code>
import torch
import torch.nn as nn
import triton
import triton.language as tl


@triton.autotune(
    configs=[
        # Larger, higher-parallelism config (best if registers allow)
        triton.Config(
            {'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32},
            num_warps=8,
            num_stages=3,
        ),
        # More conservative: smaller N, fewer warps (less register pressure)
        triton.Config(
            {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32},
            num_warps=4,
            num_stages=3,
        ),
        # Smallest fallback: lowest register pressure
        triton.Config(
            {'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 32},
            num_warps=4,
            num_stages=3,
        ),
    ],
    key=['B', 'O'],
)
@triton.jit
def fused_i2h_tanh_kernel(
    x_ptr, h_ptr, w_ptr, b_ptr, out_ptr,
    B, I, H, O,
    stride_xm, stride_xk,
    stride_hm, stride_hk,
    stride_wo, stride_wk,
    stride_om, stride_on,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    # 2D grid over [B, O]
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)  # [BLOCK_M]
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)  # [BLOCK_N]
    offs_k = tl.arange(0, BLOCK_K)                    # [BLOCK_K]

    offs_m_2d = offs_m[:, None]  # [BLOCK_M, 1]
    offs_n_2d = offs_n[None, :]  # [1, BLOCK_N]

    # Base masks reused across the kernel – keeps mask logic simple
    mask_m = offs_m_2d < B
    mask_n = offs_n_2d < O
    out_mask = mask_m & mask_n

    # Output pointers
    out_ptrs = out_ptr + offs_m_2d * stride_om + offs_n_2d * stride_on

    # FP32 accumulator tile for numerical stability and tensor cores
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # ------------------------
    # Part 1: x @ W[:, :I]^T
    # ------------------------
    a_ptrs_x = x_ptr + offs_m_2d * stride_xm + offs_k[None, :] * stride_xk
    b_ptrs_x = w_ptr + offs_n_2d * stride_wo + offs_k[:, None] * stride_wk

    for k in range(0, I, BLOCK_K):
        k_vec = k + offs_k  # [BLOCK_K]

        kmask_k = k_vec[None, :] < I           # [1, BLOCK_K]
        kmask_k_T = k_vec[:, None] < I         # [BLOCK_K, 1]

        a_mask = mask_m & kmask_k              # [BLOCK_M, BLOCK_K]
        b_mask = kmask_k_T & mask_n            # [BLOCK_K, BLOCK_N]

        a = tl.load(a_ptrs_x, mask=a_mask, other=0.0)
        b = tl.load(b_ptrs_x, mask=b_mask, other=0.0)

        # Accumulate in FP32, enable TF32 on tensor cores for FP32 inputs
        acc += tl.dot(a.to(tl.float32), b.to(tl.float32), allow_tf32=True)

        a_ptrs_x += BLOCK_K * stride_xk
        b_ptrs_x += BLOCK_K * stride_wk

    # ------------------------
    # Part 2: h @ W[:, I:]^T
    # ------------------------
    w_ptr_h = w_ptr + I * stride_wk
    a_ptrs_h = h_ptr + offs_m_2d * stride_hm + offs_k[None, :] * stride_hk
    b_ptrs_h = w_ptr_h + offs_n_2d * stride_wo + offs_k[:, None] * stride_wk

    for k in range(0, H, BLOCK_K):
        k_vec = k + offs_k  # [BLOCK_K]

        kmask_k = k_vec[None, :] < H
        kmask_k_T = k_vec[:, None] < H

        a_mask = mask_m & kmask_k
        b_mask = kmask_k_T & mask_n

        a = tl.load(a_ptrs_h, mask=a_mask, other=0.0)
        b = tl.load(b_ptrs_h, mask=b_mask, other=0.0)

        acc += tl.dot(a.to(tl.float32), b.to(tl.float32), allow_tf32=True)

        a_ptrs_h += BLOCK_K * stride_hk
        b_ptrs_h += BLOCK_K * stride_wk

    # ------------------------
    # Fused post-ops: bias + tanh
    # ------------------------

    # Bias broadcast over M
    bias_ptrs = b_ptr + offs_n_2d
    bias = tl.load(bias_ptrs, mask=mask_n, other=0.0)  # [1, BLOCK_N]
    acc = acc + bias  # broadcast along M

    # Manual tanh: tanh(x) = (e^{2x} - 1) / (e^{2x} + 1)
    two = 2.0
    e2x = tl.exp(acc * two)
    acc = (e2x - 1.0) / (e2x + 1.0)

    # Store (Triton will cast FP32 acc to out_ptr dtype if needed)
    tl.store(out_ptrs, acc, mask=out_mask)


@triton.autotune(
    configs=[
        triton.Config(
            {'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32},
            num_warps=8,
            num_stages=3,
        ),
        triton.Config(
            {'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32},
            num_warps=4,
            num_stages=3,
        ),
        triton.Config(
            {'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 32},
            num_warps=4,
            num_stages=3,
        ),
    ],
    key=['M', 'N'],
)
@triton.jit
def linear_kernel(
    a_ptr, w_ptr, b_ptr, out_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_wo, stride_wk,
    stride_om, stride_on,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    # 2D grid over [M, N]
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    offs_m_2d = offs_m[:, None]
    offs_n_2d = offs_n[None, :]

    mask_m = offs_m_2d < M
    mask_n = offs_n_2d < N
    out_mask = mask_m & mask_n

    out_ptrs = out_ptr + offs_m_2d * stride_om + offs_n_2d * stride_on

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    a_ptrs = a_ptr + offs_m_2d * stride_am + offs_k[None, :] * stride_ak
    b_ptrs = w_ptr + offs_n_2d * stride_wo + offs_k[:, None] * stride_wk

    for k in range(0, K, BLOCK_K):
        k_vec = k + offs_k

        kmask_k = k_vec[None, :] < K
        kmask_k_T = k_vec[:, None] < K

        a_mask = mask_m & kmask_k
        b_mask = kmask_k_T & mask_n

        a = tl.load(a_ptrs, mask=a_mask, other=0.0)
        b = tl.load(b_ptrs, mask=b_mask, other=0.0)

        acc += tl.dot(a.to(tl.float32), b.to(tl.float32), allow_tf32=True)

        a_ptrs += BLOCK_K * stride_ak
        b_ptrs += BLOCK_K * stride_wk

    # Bias broadcast over M
    bias_ptrs = b_ptr + offs_n_2d
    bias = tl.load(bias_ptrs, mask=mask_n, other=0.0)
    acc = acc + bias

    tl.store(out_ptrs, acc, mask=out_mask)


def fused_i2h_tanh(x: torch.Tensor,
                   h: torch.Tensor,
                   weight: torch.Tensor,
                   bias: torch.Tensor) -> torch.Tensor:
    """
    Compute tanh((x, h) @ weight.T + bias) with Triton.
    x: (B, I)
    h: (B, H)
    weight: (O, I + H)
    bias: (O,)
    returns: (B, O)
    """
    assert x.is_cuda and h.is_cuda and weight.is_cuda and bias.is_cuda
    B, I = x.shape
    B_h, H = h.shape
    assert B == B_h

    O, K_tot = weight.shape
    assert K_tot == I + H
    assert bias.shape[0] == O

    x_c = x.contiguous()
    h_c = h.contiguous()
    w_c = weight.contiguous()

    # Output in same dtype as inputs; accumulation is FP32 in-kernel
    out = torch.empty((B, O), device=x.device, dtype=x.dtype)

    def grid(meta):
        return (
            triton.cdiv(B, meta['BLOCK_M']),
            triton.cdiv(O, meta['BLOCK_N']),
        )

    fused_i2h_tanh_kernel[grid](
        x_c, h_c, w_c, bias, out,
        B, I, H, O,
        x_c.stride(0), x_c.stride(1),
        h_c.stride(0), h_c.stride(1),
        w_c.stride(0), w_c.stride(1),
        out.stride(0), out.stride(1),
    )

    return out


def linear_triton(a: torch.Tensor,
                  weight: torch.Tensor,
                  bias: torch.Tensor) -> torch.Tensor:
    """
    Compute a @ weight.T + bias with Triton.
    a: (M, K)
    weight: (N, K)
    bias: (N,)
    returns: (M, N)
    """
    assert a.is_cuda and weight.is_cuda and bias.is_cuda
    M, K = a.shape
    N, K_w = weight.shape
    assert K == K_w
    assert bias.shape[0] == N

    a_c = a.contiguous()
    w_c = weight.contiguous()

    out = torch.empty((M, N), device=a.device, dtype=a.dtype)

    def grid(meta):
        return (
            triton.cdiv(M, meta['BLOCK_M']),
            triton.cdiv(N, meta['BLOCK_N']),
        )

    linear_kernel[grid](
        a_c, w_c, bias, out,
        M, N, K,
        a_c.stride(0), a_c.stride(1),
        w_c.stride(0), w_c.stride(1),
        out.stride(0), out.stride(1),
    )

    return out


class ModelNew(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        """
        Vanilla RNN with Triton-optimized kernels for:
        - fused input+hidden -> hidden + tanh
        - linear hidden -> output
        """
        super(ModelNew, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
        self.h2o = nn.Linear(hidden_size, output_size)

        self.hidden = None

    def forward(self, x: torch.Tensor, initial_hidden: torch.Tensor = None) -> torch.Tensor:
        """
        x: (batch_size, input_size)
        initial_hidden: (batch_size, hidden_size) or None
        returns: (batch_size, output_size)
        """
        assert x.is_cuda

        B = x.shape[0]

        if initial_hidden is not None:
            self.hidden = initial_hidden.to(device=x.device, dtype=x.dtype)
        elif self.hidden is None or self.hidden.shape[0] != B:
            self.hidden = torch.zeros(B, self.hidden_size, device=x.device, dtype=x.dtype)
        else:
            self.hidden = self.hidden.to(device=x.device, dtype=x.dtype)

        self.hidden = fused_i2h_tanh(
            x, self.hidden,
            self.i2h.weight, self.i2h.bias
        )

        output = linear_triton(
            self.hidden,
            self.h2o.weight, self.h2o.bias
        )

        return output
```

[NCU PROFILING METRICS]
No NCU metrics available

**Task**: Analyze the NCU metrics and current code, then generate optimized code that maximizes performance.

## CRITICAL — Code MUST compile and run:
1. EVERY kernel function MUST have `@triton.jit` decorator
2. Grid size MUST be > 0: use `triton.cdiv(N, BLOCK)` or `max(1, N // BLOCK)`
3. BLOCK sizes MUST be power-of-2: 16, 32, 64, 128, 256
4. `tl.program_id(axis)` only supports axis = 0, 1, 2
5. No `continue`, `break`, `return` inside loops — use masking
6. No tensor indexing with loop vars: `x[:, i]` is INVALID
7. mask shape MUST match data shape in tl.load/tl.store

## Missing Triton Functions (implement manually):
- tl.tanh, tl.sigmoid, tl.gelu, tl.silu, tl.softmax, tl.mish

## OUTPUT FORMAT (STRICT):
1. Imports: torch, torch.nn, triton, triton.language as tl
2. @triton.jit decorated kernel function(s)
3. Wrapper function(s) for grid calculation and kernel launch
4. class ModelNew(nn.Module) that calls your kernels

Do NOT include: testing code, if __name__, get_inputs, get_init_inputs

```python
# <optimized Triton code>
```
