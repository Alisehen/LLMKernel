{
  "worth_optimizing": "yes",
  "reason": "The GRU is launched once per timestep from Python, causing massive kernel launch overhead and preventing the GPU from exploiting temporal reuse.",
  "bottleneck": "The time loop over seq_len (512) is in Python, so gru_step_triton is launched 512× per layer×direction, each doing a small GRU step. This per-timestep launch pattern dominates runtime and underutilizes the GPU compared to a persistent RNN kernel that iterates over time inside the device.",
  "optimisation method": "Replace the per-timestep GRU step kernel with a persistent GRU kernel that keeps the time loop inside Triton: launch once per (layer, direction), and have the kernel iterate over all timesteps, using the precomputed input-side gates and recurrent weights to update h_t in registers/shared memory. This removes hundreds of launches and better amortizes memory traffic for recurrent weights and hidden state.",
  "modification plan": "Design a new triton kernel, e.g. gru_layer_kernel, whose inputs are ig_r/ig_z/ig_n of shape (T, B, H), initial h0 (B, H), and w_hh/b_hh; inside the kernel, assign each program instance a tile over batch×hidden and run a for t in range(T) loop that computes the recurrent matmul (h @ w_hh), applies gate functions, and writes h_t to the appropriate output slice. Call this kernel once per (layer, direction), replacing the Python-level for t in range(seq_len) loop and the per-timestep gru_step_triton launches, while reusing the existing matmul_bias precomputation for input gates. Ensure that recurrent weights are loaded in a cache-friendly layout and reused across timesteps without re-reading them from global memory unnecessarily.",
  "expected_speedup": "7-10x vs the current Triton implementation (bringing it roughly to or better than the PyTorch baseline)."
}